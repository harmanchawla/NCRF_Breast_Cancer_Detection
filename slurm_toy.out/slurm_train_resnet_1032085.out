INFO:root:2019-05-04 05:01:39, Epoch : 1, Step : 1, Training Loss : 0.81466, Training Acc : 0.378, Run Time : 22.86
INFO:root:2019-05-04 05:01:39, Epoch : 1, Step : 2, Training Loss : 0.78934, Training Acc : 0.383, Run Time : 0.47
INFO:root:2019-05-04 05:01:40, Epoch : 1, Step : 3, Training Loss : 0.74167, Training Acc : 0.389, Run Time : 0.54
INFO:root:2019-05-04 05:01:42, Epoch : 1, Step : 4, Training Loss : 0.68861, Training Acc : 0.500, Run Time : 2.21
INFO:root:2019-05-04 05:01:43, Epoch : 1, Step : 5, Training Loss : 0.61314, Training Acc : 0.706, Run Time : 0.69
INFO:root:2019-05-04 05:01:49, Epoch : 1, Step : 6, Training Loss : 0.61877, Training Acc : 0.661, Run Time : 5.96
INFO:root:2019-05-04 05:01:50, Epoch : 1, Step : 7, Training Loss : 0.56750, Training Acc : 0.689, Run Time : 1.12
INFO:root:2019-05-04 05:01:50, Epoch : 1, Step : 8, Training Loss : 0.65740, Training Acc : 0.672, Run Time : 0.46
INFO:root:2019-05-04 05:01:51, Epoch : 1, Step : 9, Training Loss : 0.60195, Training Acc : 0.689, Run Time : 0.40
INFO:root:2019-05-04 05:01:54, Epoch : 1, Step : 10, Training Loss : 0.46900, Training Acc : 0.583, Run Time : 2.81
wsi/bin/train.py:115: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  data_tumor = Variable(data_tumor.cuda(async=True), volatile=True)
wsi/bin/train.py:119: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  data_normal = Variable(data_normal.cuda(async=True), volatile=True)
INFO:root:2019-05-04 05:06:06, Epoch : 1, Step : 10, Validation Loss : 0.75155, Validation Acc : 0.439, Run Time : 250.83
INFO:root:2019-05-04 05:06:21, Epoch : 2, Step : 11, Training Loss : 0.74829, Training Acc : 0.622, Run Time : 13.78
INFO:root:2019-05-04 05:06:21, Epoch : 2, Step : 12, Training Loss : 0.80299, Training Acc : 0.617, Run Time : 0.55
INFO:root:2019-05-04 05:06:22, Epoch : 2, Step : 13, Training Loss : 0.64081, Training Acc : 0.650, Run Time : 0.60
INFO:root:2019-05-04 05:06:23, Epoch : 2, Step : 14, Training Loss : 0.69760, Training Acc : 0.672, Run Time : 0.81
INFO:root:2019-05-04 05:06:26, Epoch : 2, Step : 15, Training Loss : 0.69885, Training Acc : 0.678, Run Time : 3.22
INFO:root:2019-05-04 05:06:27, Epoch : 2, Step : 16, Training Loss : 0.60270, Training Acc : 0.683, Run Time : 0.86
INFO:root:2019-05-04 05:06:35, Epoch : 2, Step : 17, Training Loss : 0.58201, Training Acc : 0.689, Run Time : 8.47
INFO:root:2019-05-04 05:06:36, Epoch : 2, Step : 18, Training Loss : 0.51588, Training Acc : 0.672, Run Time : 0.48
INFO:root:2019-05-04 05:06:36, Epoch : 2, Step : 19, Training Loss : 0.55145, Training Acc : 0.689, Run Time : 0.45
INFO:root:2019-05-04 05:06:36, Epoch : 2, Step : 20, Training Loss : 0.44491, Training Acc : 0.583, Run Time : 0.26
INFO:root:2019-05-04 05:10:53, Epoch : 2, Step : 20, Validation Loss : 0.69176, Validation Acc : 0.446, Run Time : 253.91
INFO:root:2019-05-04 05:11:08, Epoch : 3, Step : 21, Training Loss : 0.53788, Training Acc : 0.622, Run Time : 13.19
INFO:root:2019-05-04 05:11:08, Epoch : 3, Step : 22, Training Loss : 0.58637, Training Acc : 0.617, Run Time : 0.56
INFO:root:2019-05-04 05:11:09, Epoch : 3, Step : 23, Training Loss : 0.51961, Training Acc : 0.711, Run Time : 0.38
INFO:root:2019-05-04 05:11:09, Epoch : 3, Step : 24, Training Loss : 0.58042, Training Acc : 0.706, Run Time : 0.76
INFO:root:2019-05-04 05:11:15, Epoch : 3, Step : 25, Training Loss : 0.52448, Training Acc : 0.772, Run Time : 6.00
INFO:root:2019-05-04 05:11:16, Epoch : 3, Step : 26, Training Loss : 0.55256, Training Acc : 0.750, Run Time : 0.73
INFO:root:2019-05-04 05:11:17, Epoch : 3, Step : 27, Training Loss : 0.56939, Training Acc : 0.772, Run Time : 0.73
INFO:root:2019-05-04 05:11:20, Epoch : 3, Step : 28, Training Loss : 0.50111, Training Acc : 0.800, Run Time : 3.73
INFO:root:2019-05-04 05:11:21, Epoch : 3, Step : 29, Training Loss : 0.58068, Training Acc : 0.750, Run Time : 0.45
INFO:root:2019-05-04 05:11:21, Epoch : 3, Step : 30, Training Loss : 0.50044, Training Acc : 0.528, Run Time : 0.37
INFO:root:2019-05-04 05:15:40, Epoch : 3, Step : 30, Validation Loss : 0.62455, Validation Acc : 0.751, Run Time : 257.98
INFO:root:2019-05-04 05:15:54, Epoch : 4, Step : 31, Training Loss : 0.56465, Training Acc : 0.661, Run Time : 12.57
INFO:root:2019-05-04 05:15:54, Epoch : 4, Step : 32, Training Loss : 0.52239, Training Acc : 0.689, Run Time : 0.57
INFO:root:2019-05-04 05:15:55, Epoch : 4, Step : 33, Training Loss : 0.53288, Training Acc : 0.728, Run Time : 0.67
INFO:root:2019-05-04 05:15:55, Epoch : 4, Step : 34, Training Loss : 0.55837, Training Acc : 0.694, Run Time : 0.54
INFO:root:2019-05-04 05:16:02, Epoch : 4, Step : 35, Training Loss : 0.46241, Training Acc : 0.778, Run Time : 6.75
INFO:root:2019-05-04 05:16:03, Epoch : 4, Step : 36, Training Loss : 0.51265, Training Acc : 0.694, Run Time : 0.60
INFO:root:2019-05-04 05:16:06, Epoch : 4, Step : 37, Training Loss : 0.53667, Training Acc : 0.711, Run Time : 3.64
INFO:root:2019-05-04 05:16:07, Epoch : 4, Step : 38, Training Loss : 0.49866, Training Acc : 0.761, Run Time : 0.81
INFO:root:2019-05-04 05:16:08, Epoch : 4, Step : 39, Training Loss : 0.48102, Training Acc : 0.689, Run Time : 0.48
INFO:root:2019-05-04 05:16:09, Epoch : 4, Step : 40, Training Loss : 0.42823, Training Acc : 0.583, Run Time : 0.85
INFO:root:2019-05-04 05:20:27, Epoch : 4, Step : 40, Validation Loss : 0.62101, Validation Acc : 0.671, Run Time : 257.31
INFO:root:2019-05-04 05:20:40, Epoch : 5, Step : 41, Training Loss : 0.51146, Training Acc : 0.661, Run Time : 11.78
INFO:root:2019-05-04 05:20:41, Epoch : 5, Step : 42, Training Loss : 0.52775, Training Acc : 0.644, Run Time : 0.58
INFO:root:2019-05-04 05:20:42, Epoch : 5, Step : 43, Training Loss : 0.47681, Training Acc : 0.756, Run Time : 1.12
INFO:root:2019-05-04 05:20:43, Epoch : 5, Step : 44, Training Loss : 0.50079, Training Acc : 0.694, Run Time : 1.69
INFO:root:2019-05-04 05:20:44, Epoch : 5, Step : 45, Training Loss : 0.44315, Training Acc : 0.783, Run Time : 0.80
INFO:root:2019-05-04 05:20:47, Epoch : 5, Step : 46, Training Loss : 0.43142, Training Acc : 0.789, Run Time : 2.24
INFO:root:2019-05-04 05:20:50, Epoch : 5, Step : 47, Training Loss : 0.47701, Training Acc : 0.756, Run Time : 3.92
INFO:root:2019-05-04 05:20:51, Epoch : 5, Step : 48, Training Loss : 0.38458, Training Acc : 0.850, Run Time : 0.56
INFO:root:2019-05-04 05:20:57, Epoch : 5, Step : 49, Training Loss : 0.46295, Training Acc : 0.822, Run Time : 5.61
INFO:root:2019-05-04 05:20:57, Epoch : 5, Step : 50, Training Loss : 0.44092, Training Acc : 0.556, Run Time : 0.32
INFO:root:2019-05-04 05:25:09, Epoch : 5, Step : 50, Validation Loss : 0.61746, Validation Acc : 0.705, Run Time : 250.31
INFO:root:2019-05-04 05:25:22, Epoch : 6, Step : 51, Training Loss : 0.49765, Training Acc : 0.800, Run Time : 12.27
INFO:root:2019-05-04 05:25:23, Epoch : 6, Step : 52, Training Loss : 0.48681, Training Acc : 0.739, Run Time : 0.66
INFO:root:2019-05-04 05:25:23, Epoch : 6, Step : 53, Training Loss : 0.41923, Training Acc : 0.839, Run Time : 0.59
INFO:root:2019-05-04 05:25:24, Epoch : 6, Step : 54, Training Loss : 0.48571, Training Acc : 0.761, Run Time : 1.17
INFO:root:2019-05-04 05:25:26, Epoch : 6, Step : 55, Training Loss : 0.40275, Training Acc : 0.800, Run Time : 1.36
INFO:root:2019-05-04 05:25:27, Epoch : 6, Step : 56, Training Loss : 0.40088, Training Acc : 0.811, Run Time : 1.59
INFO:root:2019-05-04 05:25:33, Epoch : 6, Step : 57, Training Loss : 0.42845, Training Acc : 0.789, Run Time : 5.80
INFO:root:2019-05-04 05:25:34, Epoch : 6, Step : 58, Training Loss : 0.38299, Training Acc : 0.844, Run Time : 0.51
INFO:root:2019-05-04 05:25:34, Epoch : 6, Step : 59, Training Loss : 0.47229, Training Acc : 0.767, Run Time : 0.49
INFO:root:2019-05-04 05:25:35, Epoch : 6, Step : 60, Training Loss : 0.35267, Training Acc : 0.594, Run Time : 0.98
INFO:root:2019-05-04 05:29:57, Epoch : 6, Step : 60, Validation Loss : 0.71013, Validation Acc : 0.694, Run Time : 259.90
INFO:root:2019-05-04 05:30:09, Epoch : 7, Step : 61, Training Loss : 0.38555, Training Acc : 0.833, Run Time : 11.92
INFO:root:2019-05-04 05:30:10, Epoch : 7, Step : 62, Training Loss : 0.44565, Training Acc : 0.750, Run Time : 0.57
INFO:root:2019-05-04 05:30:11, Epoch : 7, Step : 63, Training Loss : 0.41566, Training Acc : 0.794, Run Time : 0.60
INFO:root:2019-05-04 05:30:13, Epoch : 7, Step : 64, Training Loss : 0.43217, Training Acc : 0.844, Run Time : 2.86
INFO:root:2019-05-04 05:30:21, Epoch : 7, Step : 65, Training Loss : 0.44218, Training Acc : 0.794, Run Time : 7.91
INFO:root:2019-05-04 05:30:22, Epoch : 7, Step : 66, Training Loss : 0.36788, Training Acc : 0.833, Run Time : 1.04
INFO:root:2019-05-04 05:30:23, Epoch : 7, Step : 67, Training Loss : 0.42094, Training Acc : 0.778, Run Time : 0.99
INFO:root:2019-05-04 05:30:26, Epoch : 7, Step : 68, Training Loss : 0.39788, Training Acc : 0.828, Run Time : 2.57
INFO:root:2019-05-04 05:30:27, Epoch : 7, Step : 69, Training Loss : 0.39989, Training Acc : 0.778, Run Time : 0.73
INFO:root:2019-05-04 05:30:29, Epoch : 7, Step : 70, Training Loss : 0.33471, Training Acc : 0.606, Run Time : 2.47
INFO:root:2019-05-04 05:34:42, Epoch : 7, Step : 70, Validation Loss : 1.04868, Validation Acc : 0.611, Run Time : 252.18
INFO:root:2019-05-04 05:34:54, Epoch : 8, Step : 71, Training Loss : 0.48078, Training Acc : 0.750, Run Time : 11.20
INFO:root:2019-05-04 05:34:54, Epoch : 8, Step : 72, Training Loss : 0.42637, Training Acc : 0.783, Run Time : 0.57
INFO:root:2019-05-04 05:34:56, Epoch : 8, Step : 73, Training Loss : 0.41151, Training Acc : 0.783, Run Time : 1.40
INFO:root:2019-05-04 05:34:56, Epoch : 8, Step : 74, Training Loss : 0.38360, Training Acc : 0.800, Run Time : 0.58
INFO:root:2019-05-04 05:35:09, Epoch : 8, Step : 75, Training Loss : 0.42639, Training Acc : 0.772, Run Time : 12.14
INFO:root:2019-05-04 05:35:09, Epoch : 8, Step : 76, Training Loss : 0.36525, Training Acc : 0.844, Run Time : 0.44
INFO:root:2019-05-04 05:35:10, Epoch : 8, Step : 77, Training Loss : 0.40538, Training Acc : 0.811, Run Time : 0.60
INFO:root:2019-05-04 05:35:11, Epoch : 8, Step : 78, Training Loss : 0.42497, Training Acc : 0.806, Run Time : 0.95
INFO:root:2019-05-04 05:35:11, Epoch : 8, Step : 79, Training Loss : 0.40690, Training Acc : 0.794, Run Time : 0.63
INFO:root:2019-05-04 05:35:15, Epoch : 8, Step : 80, Training Loss : 0.47003, Training Acc : 0.506, Run Time : 3.75
INFO:root:2019-05-04 05:41:16, Epoch : 8, Step : 80, Validation Loss : 1.55245, Validation Acc : 0.567, Run Time : 359.59
INFO:root:2019-05-04 05:41:28, Epoch : 9, Step : 81, Training Loss : 0.42840, Training Acc : 0.783, Run Time : 11.78
INFO:root:2019-05-04 05:41:29, Epoch : 9, Step : 82, Training Loss : 0.41415, Training Acc : 0.761, Run Time : 0.61
INFO:root:2019-05-04 05:41:29, Epoch : 9, Step : 83, Training Loss : 0.40305, Training Acc : 0.811, Run Time : 0.74
INFO:root:2019-05-04 05:41:32, Epoch : 9, Step : 84, Training Loss : 0.48802, Training Acc : 0.739, Run Time : 2.41
INFO:root:2019-05-04 05:41:33, Epoch : 9, Step : 85, Training Loss : 0.38832, Training Acc : 0.783, Run Time : 0.84
INFO:root:2019-05-04 05:41:41, Epoch : 9, Step : 86, Training Loss : 0.33963, Training Acc : 0.806, Run Time : 8.91
INFO:root:2019-05-04 05:41:42, Epoch : 9, Step : 87, Training Loss : 0.40025, Training Acc : 0.794, Run Time : 0.48
INFO:root:2019-05-04 05:41:42, Epoch : 9, Step : 88, Training Loss : 0.33552, Training Acc : 0.828, Run Time : 0.49
INFO:root:2019-05-04 05:41:45, Epoch : 9, Step : 89, Training Loss : 0.44705, Training Acc : 0.772, Run Time : 2.35
INFO:root:2019-05-04 05:41:45, Epoch : 9, Step : 90, Training Loss : 0.45179, Training Acc : 0.517, Run Time : 0.34
INFO:root:2019-05-04 05:45:57, Epoch : 9, Step : 90, Validation Loss : 1.39530, Validation Acc : 0.586, Run Time : 250.28
INFO:root:2019-05-04 05:46:12, Epoch : 10, Step : 91, Training Loss : 0.41586, Training Acc : 0.761, Run Time : 14.19
INFO:root:2019-05-04 05:46:13, Epoch : 10, Step : 92, Training Loss : 0.35533, Training Acc : 0.850, Run Time : 0.56
INFO:root:2019-05-04 05:46:13, Epoch : 10, Step : 93, Training Loss : 0.37906, Training Acc : 0.833, Run Time : 0.59
INFO:root:2019-05-04 05:46:14, Epoch : 10, Step : 94, Training Loss : 0.47416, Training Acc : 0.739, Run Time : 0.89
INFO:root:2019-05-04 05:46:18, Epoch : 10, Step : 95, Training Loss : 0.38055, Training Acc : 0.828, Run Time : 3.40
INFO:root:2019-05-04 05:46:18, Epoch : 10, Step : 96, Training Loss : 0.33600, Training Acc : 0.839, Run Time : 0.64
INFO:root:2019-05-04 05:46:21, Epoch : 10, Step : 97, Training Loss : 0.46396, Training Acc : 0.772, Run Time : 3.16
INFO:root:2019-05-04 05:46:22, Epoch : 10, Step : 98, Training Loss : 0.39945, Training Acc : 0.839, Run Time : 0.66
INFO:root:2019-05-04 05:46:23, Epoch : 10, Step : 99, Training Loss : 0.42504, Training Acc : 0.761, Run Time : 0.52
INFO:root:2019-05-04 05:46:29, Epoch : 10, Step : 100, Training Loss : 0.36871, Training Acc : 0.589, Run Time : 6.48
INFO:root:2019-05-04 05:50:41, Epoch : 10, Step : 100, Validation Loss : 1.51875, Validation Acc : 0.586, Run Time : 251.38
