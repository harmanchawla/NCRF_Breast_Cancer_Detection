INFO:root:2019-05-04 05:01:39, Epoch : 1, Step : 1, Training Loss : 0.81466, Training Acc : 0.378, Run Time : 22.86
INFO:root:2019-05-04 05:01:39, Epoch : 1, Step : 2, Training Loss : 0.78934, Training Acc : 0.383, Run Time : 0.47
INFO:root:2019-05-04 05:01:40, Epoch : 1, Step : 3, Training Loss : 0.74167, Training Acc : 0.389, Run Time : 0.54
INFO:root:2019-05-04 05:01:42, Epoch : 1, Step : 4, Training Loss : 0.68861, Training Acc : 0.500, Run Time : 2.21
INFO:root:2019-05-04 05:01:43, Epoch : 1, Step : 5, Training Loss : 0.61314, Training Acc : 0.706, Run Time : 0.69
INFO:root:2019-05-04 05:01:49, Epoch : 1, Step : 6, Training Loss : 0.61877, Training Acc : 0.661, Run Time : 5.96
INFO:root:2019-05-04 05:01:50, Epoch : 1, Step : 7, Training Loss : 0.56750, Training Acc : 0.689, Run Time : 1.12
INFO:root:2019-05-04 05:01:50, Epoch : 1, Step : 8, Training Loss : 0.65740, Training Acc : 0.672, Run Time : 0.46
INFO:root:2019-05-04 05:01:51, Epoch : 1, Step : 9, Training Loss : 0.60195, Training Acc : 0.689, Run Time : 0.40
INFO:root:2019-05-04 05:01:54, Epoch : 1, Step : 10, Training Loss : 0.46900, Training Acc : 0.583, Run Time : 2.81
wsi/bin/train.py:115: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  data_tumor = Variable(data_tumor.cuda(async=True), volatile=True)
wsi/bin/train.py:119: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  data_normal = Variable(data_normal.cuda(async=True), volatile=True)
INFO:root:2019-05-04 05:06:06, Epoch : 1, Step : 10, Validation Loss : 0.75155, Validation Acc : 0.439, Run Time : 250.83
INFO:root:2019-05-04 05:06:21, Epoch : 2, Step : 11, Training Loss : 0.74829, Training Acc : 0.622, Run Time : 13.78
INFO:root:2019-05-04 05:06:21, Epoch : 2, Step : 12, Training Loss : 0.80299, Training Acc : 0.617, Run Time : 0.55
INFO:root:2019-05-04 05:06:22, Epoch : 2, Step : 13, Training Loss : 0.64081, Training Acc : 0.650, Run Time : 0.60
INFO:root:2019-05-04 05:06:23, Epoch : 2, Step : 14, Training Loss : 0.69760, Training Acc : 0.672, Run Time : 0.81
INFO:root:2019-05-04 05:06:26, Epoch : 2, Step : 15, Training Loss : 0.69885, Training Acc : 0.678, Run Time : 3.22
INFO:root:2019-05-04 05:06:27, Epoch : 2, Step : 16, Training Loss : 0.60270, Training Acc : 0.683, Run Time : 0.86
INFO:root:2019-05-04 05:06:35, Epoch : 2, Step : 17, Training Loss : 0.58201, Training Acc : 0.689, Run Time : 8.47
INFO:root:2019-05-04 05:06:36, Epoch : 2, Step : 18, Training Loss : 0.51588, Training Acc : 0.672, Run Time : 0.48
INFO:root:2019-05-04 05:06:36, Epoch : 2, Step : 19, Training Loss : 0.55145, Training Acc : 0.689, Run Time : 0.45
INFO:root:2019-05-04 05:06:36, Epoch : 2, Step : 20, Training Loss : 0.44491, Training Acc : 0.583, Run Time : 0.26
