INFO:root:2019-05-11 12:22:08, Epoch : 1, Step : 1, Training Loss : 0.93038, Training Acc : 0.192, Run Time : 191.32
INFO:root:2019-05-11 12:22:29, Epoch : 1, Step : 2, Training Loss : 0.82418, Training Acc : 0.258, Run Time : 21.04
INFO:root:2019-05-11 12:22:45, Epoch : 1, Step : 3, Training Loss : 0.73251, Training Acc : 0.367, Run Time : 16.31
INFO:root:2019-05-11 12:22:55, Epoch : 1, Step : 4, Training Loss : 0.66854, Training Acc : 0.550, Run Time : 10.06
INFO:root:2019-05-11 12:23:02, Epoch : 1, Step : 5, Training Loss : 0.62676, Training Acc : 0.694, Run Time : 6.66
INFO:root:2019-05-11 12:23:16, Epoch : 1, Step : 6, Training Loss : 0.61973, Training Acc : 0.694, Run Time : 14.56
INFO:root:2019-05-11 12:23:34, Epoch : 1, Step : 7, Training Loss : 0.66474, Training Acc : 0.633, Run Time : 17.36
INFO:root:2019-05-11 12:23:54, Epoch : 1, Step : 8, Training Loss : 0.78298, Training Acc : 0.603, Run Time : 20.88
INFO:root:2019-05-11 12:24:01, Epoch : 1, Step : 9, Training Loss : 0.78543, Training Acc : 0.600, Run Time : 6.64
INFO:root:2019-05-11 12:24:19, Epoch : 1, Step : 10, Training Loss : 0.87025, Training Acc : 0.558, Run Time : 17.52
INFO:root:2019-05-11 12:24:23, Epoch : 1, Step : 11, Training Loss : 0.76934, Training Acc : 0.606, Run Time : 4.22
INFO:root:2019-05-11 12:24:45, Epoch : 1, Step : 12, Training Loss : 0.85182, Training Acc : 0.556, Run Time : 22.06
INFO:root:2019-05-11 12:24:49, Epoch : 1, Step : 13, Training Loss : 0.80932, Training Acc : 0.561, Run Time : 4.25
INFO:root:2019-05-11 12:25:11, Epoch : 1, Step : 14, Training Loss : 0.79006, Training Acc : 0.547, Run Time : 21.73
INFO:root:2019-05-11 12:25:19, Epoch : 1, Step : 15, Training Loss : 0.71787, Training Acc : 0.553, Run Time : 8.66
INFO:root:2019-05-11 12:25:25, Epoch : 1, Step : 16, Training Loss : 0.66479, Training Acc : 0.611, Run Time : 5.73
INFO:root:2019-05-11 12:25:38, Epoch : 1, Step : 17, Training Loss : 0.67929, Training Acc : 0.542, Run Time : 12.54
INFO:root:2019-05-11 12:25:59, Epoch : 1, Step : 18, Training Loss : 0.68372, Training Acc : 0.578, Run Time : 21.64
INFO:root:2019-05-11 12:26:02, Epoch : 1, Step : 19, Training Loss : 0.70662, Training Acc : 0.433, Run Time : 2.45
INFO:root:2019-05-11 12:26:47, Epoch : 1, Step : 20, Training Loss : 0.71949, Training Acc : 0.406, Run Time : 45.50
INFO:root:2019-05-11 12:26:55, Epoch : 1, Step : 21, Training Loss : 0.71626, Training Acc : 0.442, Run Time : 7.66
INFO:root:2019-05-11 12:27:32, Epoch : 1, Step : 22, Training Loss : 0.72503, Training Acc : 0.419, Run Time : 37.07
INFO:root:2019-05-11 12:27:41, Epoch : 1, Step : 23, Training Loss : 0.71691, Training Acc : 0.400, Run Time : 9.07
INFO:root:2019-05-11 12:28:05, Epoch : 1, Step : 24, Training Loss : 0.70351, Training Acc : 0.475, Run Time : 24.27
INFO:root:2019-05-11 12:28:09, Epoch : 1, Step : 25, Training Loss : 0.68634, Training Acc : 0.558, Run Time : 4.02
INFO:root:2019-05-11 12:28:38, Epoch : 1, Step : 26, Training Loss : 0.67833, Training Acc : 0.569, Run Time : 28.73
INFO:root:2019-05-11 12:29:00, Epoch : 1, Step : 27, Training Loss : 0.67564, Training Acc : 0.544, Run Time : 21.96
INFO:root:2019-05-11 12:29:30, Epoch : 1, Step : 28, Training Loss : 0.67903, Training Acc : 0.581, Run Time : 29.71
INFO:root:2019-05-11 12:30:09, Epoch : 1, Step : 29, Training Loss : 0.67195, Training Acc : 0.583, Run Time : 38.72
INFO:root:2019-05-11 12:30:40, Epoch : 1, Step : 30, Training Loss : 0.62454, Training Acc : 0.658, Run Time : 31.85
INFO:root:2019-05-11 12:30:56, Epoch : 1, Step : 31, Training Loss : 0.67467, Training Acc : 0.597, Run Time : 15.78
INFO:root:2019-05-11 12:31:28, Epoch : 1, Step : 32, Training Loss : 0.67640, Training Acc : 0.578, Run Time : 31.77
INFO:root:2019-05-11 12:31:42, Epoch : 1, Step : 33, Training Loss : 0.74893, Training Acc : 0.533, Run Time : 13.85
INFO:root:2019-05-11 12:32:07, Epoch : 1, Step : 34, Training Loss : 0.74925, Training Acc : 0.528, Run Time : 24.79
INFO:root:2019-05-11 12:32:15, Epoch : 1, Step : 35, Training Loss : 0.72503, Training Acc : 0.531, Run Time : 8.48
INFO:root:2019-05-11 12:32:22, Epoch : 1, Step : 36, Training Loss : 0.68320, Training Acc : 0.567, Run Time : 7.37
INFO:root:2019-05-11 12:32:57, Epoch : 1, Step : 37, Training Loss : 0.68471, Training Acc : 0.542, Run Time : 34.85
INFO:root:2019-05-11 12:33:13, Epoch : 1, Step : 38, Training Loss : 0.67514, Training Acc : 0.536, Run Time : 15.84
INFO:root:2019-05-11 12:33:16, Epoch : 1, Step : 39, Training Loss : 0.68162, Training Acc : 0.508, Run Time : 2.80
INFO:root:2019-05-11 12:33:41, Epoch : 1, Step : 40, Training Loss : 0.67530, Training Acc : 0.567, Run Time : 25.21
INFO:root:2019-05-11 12:33:45, Epoch : 1, Step : 41, Training Loss : 0.67703, Training Acc : 0.581, Run Time : 3.61
INFO:root:2019-05-11 12:34:11, Epoch : 1, Step : 42, Training Loss : 0.68268, Training Acc : 0.553, Run Time : 26.01
INFO:root:2019-05-11 12:34:37, Epoch : 1, Step : 43, Training Loss : 0.67915, Training Acc : 0.561, Run Time : 25.82
INFO:root:2019-05-11 12:34:50, Epoch : 1, Step : 44, Training Loss : 0.68644, Training Acc : 0.503, Run Time : 13.36
INFO:root:2019-05-11 12:35:12, Epoch : 1, Step : 45, Training Loss : 0.67127, Training Acc : 0.578, Run Time : 21.58
INFO:root:2019-05-11 12:35:22, Epoch : 1, Step : 46, Training Loss : 0.66133, Training Acc : 0.628, Run Time : 10.21
INFO:root:2019-05-11 12:35:42, Epoch : 1, Step : 47, Training Loss : 0.63953, Training Acc : 0.706, Run Time : 20.73
INFO:root:2019-05-11 12:36:00, Epoch : 1, Step : 48, Training Loss : 0.64971, Training Acc : 0.603, Run Time : 17.05
INFO:root:2019-05-11 12:36:13, Epoch : 1, Step : 49, Training Loss : 0.66313, Training Acc : 0.600, Run Time : 13.81
INFO:root:2019-05-11 12:36:34, Epoch : 1, Step : 50, Training Loss : 0.64754, Training Acc : 0.611, Run Time : 20.67
INFO:root:2019-05-11 12:36:38, Epoch : 1, Step : 51, Training Loss : 0.68316, Training Acc : 0.575, Run Time : 3.78
INFO:root:2019-05-11 12:36:51, Epoch : 1, Step : 52, Training Loss : 0.70899, Training Acc : 0.561, Run Time : 13.44
INFO:root:2019-05-11 12:37:13, Epoch : 1, Step : 53, Training Loss : 0.70732, Training Acc : 0.553, Run Time : 22.17
INFO:root:2019-05-11 12:37:28, Epoch : 1, Step : 54, Training Loss : 0.69672, Training Acc : 0.569, Run Time : 14.15
INFO:root:2019-05-11 12:37:45, Epoch : 1, Step : 55, Training Loss : 0.69818, Training Acc : 0.547, Run Time : 17.25
INFO:root:2019-05-11 12:38:06, Epoch : 1, Step : 56, Training Loss : 0.68098, Training Acc : 0.581, Run Time : 21.36
INFO:root:2019-05-11 12:38:15, Epoch : 1, Step : 57, Training Loss : 0.64933, Training Acc : 0.617, Run Time : 9.24
INFO:root:2019-05-11 12:38:41, Epoch : 1, Step : 58, Training Loss : 0.67330, Training Acc : 0.606, Run Time : 25.91
INFO:root:2019-05-11 12:39:17, Epoch : 1, Step : 59, Training Loss : 0.64841, Training Acc : 0.628, Run Time : 36.21
INFO:root:2019-05-11 12:39:23, Epoch : 1, Step : 60, Training Loss : 0.67692, Training Acc : 0.603, Run Time : 5.87
INFO:root:2019-05-11 12:40:11, Epoch : 1, Step : 61, Training Loss : 0.64806, Training Acc : 0.631, Run Time : 47.35
INFO:root:2019-05-11 12:40:18, Epoch : 1, Step : 62, Training Loss : 0.66745, Training Acc : 0.544, Run Time : 7.54
INFO:root:2019-05-11 12:40:22, Epoch : 1, Step : 63, Training Loss : 0.64448, Training Acc : 0.608, Run Time : 3.97
INFO:root:2019-05-11 12:40:47, Epoch : 1, Step : 64, Training Loss : 0.66872, Training Acc : 0.528, Run Time : 25.14
INFO:root:2019-05-11 12:41:05, Epoch : 1, Step : 65, Training Loss : 0.64074, Training Acc : 0.589, Run Time : 17.58
INFO:root:2019-05-11 12:41:12, Epoch : 1, Step : 66, Training Loss : 0.66219, Training Acc : 0.594, Run Time : 6.72
INFO:root:2019-05-11 12:41:49, Epoch : 1, Step : 67, Training Loss : 0.64613, Training Acc : 0.572, Run Time : 37.45
INFO:root:2019-05-11 12:42:08, Epoch : 1, Step : 68, Training Loss : 0.61797, Training Acc : 0.642, Run Time : 19.32
INFO:root:2019-05-11 12:42:18, Epoch : 1, Step : 69, Training Loss : 0.63827, Training Acc : 0.636, Run Time : 9.29
INFO:root:2019-05-11 12:42:19, Epoch : 1, Step : 70, Training Loss : 0.68405, Training Acc : 0.578, Run Time : 1.36
INFO:root:2019-05-11 12:42:22, Epoch : 1, Step : 71, Training Loss : 0.63760, Training Acc : 0.625, Run Time : 2.62
INFO:root:2019-05-11 12:42:49, Epoch : 1, Step : 72, Training Loss : 0.65397, Training Acc : 0.603, Run Time : 27.03
INFO:root:2019-05-11 12:42:58, Epoch : 1, Step : 73, Training Loss : 0.63137, Training Acc : 0.631, Run Time : 9.04
INFO:root:2019-05-11 12:43:01, Epoch : 1, Step : 74, Training Loss : 0.67200, Training Acc : 0.575, Run Time : 3.49
INFO:root:2019-05-11 12:43:33, Epoch : 1, Step : 75, Training Loss : 0.70686, Training Acc : 0.556, Run Time : 31.97
INFO:root:2019-05-11 12:43:57, Epoch : 1, Step : 76, Training Loss : 0.71269, Training Acc : 0.550, Run Time : 23.47
INFO:root:2019-05-11 12:44:04, Epoch : 1, Step : 77, Training Loss : 0.66448, Training Acc : 0.575, Run Time : 7.75
INFO:root:2019-05-11 12:44:36, Epoch : 1, Step : 78, Training Loss : 0.70886, Training Acc : 0.511, Run Time : 31.87
INFO:root:2019-05-11 12:44:40, Epoch : 1, Step : 79, Training Loss : 0.69537, Training Acc : 0.528, Run Time : 3.72
INFO:root:2019-05-11 12:44:53, Epoch : 1, Step : 80, Training Loss : 0.70677, Training Acc : 0.514, Run Time : 12.86
INFO:root:2019-05-11 12:44:56, Epoch : 1, Step : 81, Training Loss : 0.69510, Training Acc : 0.528, Run Time : 2.67
INFO:root:2019-05-11 12:45:20, Epoch : 1, Step : 82, Training Loss : 0.69386, Training Acc : 0.519, Run Time : 24.26
INFO:root:2019-05-11 12:45:36, Epoch : 1, Step : 83, Training Loss : 0.69211, Training Acc : 0.508, Run Time : 15.82
INFO:root:2019-05-11 12:46:11, Epoch : 1, Step : 84, Training Loss : 0.69718, Training Acc : 0.478, Run Time : 34.92
INFO:root:2019-05-11 12:46:18, Epoch : 1, Step : 85, Training Loss : 0.70415, Training Acc : 0.464, Run Time : 7.02
INFO:root:2019-05-11 12:46:51, Epoch : 1, Step : 86, Training Loss : 0.70189, Training Acc : 0.458, Run Time : 33.47
INFO:root:2019-05-11 12:47:07, Epoch : 1, Step : 87, Training Loss : 0.69121, Training Acc : 0.558, Run Time : 15.84
INFO:root:2019-05-11 12:47:22, Epoch : 1, Step : 88, Training Loss : 0.69561, Training Acc : 0.517, Run Time : 14.66
INFO:root:2019-05-11 12:47:37, Epoch : 1, Step : 89, Training Loss : 0.68654, Training Acc : 0.517, Run Time : 15.34
INFO:root:2019-05-11 12:47:53, Epoch : 1, Step : 90, Training Loss : 0.68569, Training Acc : 0.561, Run Time : 15.98
INFO:root:2019-05-11 12:48:04, Epoch : 1, Step : 91, Training Loss : 0.67547, Training Acc : 0.528, Run Time : 10.80
INFO:root:2019-05-11 12:48:43, Epoch : 1, Step : 92, Training Loss : 0.68098, Training Acc : 0.572, Run Time : 39.27
INFO:root:2019-05-11 12:48:52, Epoch : 1, Step : 93, Training Loss : 0.66707, Training Acc : 0.569, Run Time : 9.13
INFO:root:2019-05-11 12:49:07, Epoch : 1, Step : 94, Training Loss : 0.68474, Training Acc : 0.542, Run Time : 14.82
INFO:root:2019-05-11 12:49:54, Epoch : 1, Step : 95, Training Loss : 0.67479, Training Acc : 0.550, Run Time : 46.79
INFO:root:2019-05-11 12:50:00, Epoch : 1, Step : 96, Training Loss : 0.65756, Training Acc : 0.619, Run Time : 6.25
INFO:root:2019-05-11 12:50:02, Epoch : 1, Step : 97, Training Loss : 0.64185, Training Acc : 0.642, Run Time : 2.53
INFO:root:2019-05-11 12:50:36, Epoch : 1, Step : 98, Training Loss : 0.62773, Training Acc : 0.658, Run Time : 33.54
INFO:root:2019-05-11 12:50:43, Epoch : 1, Step : 99, Training Loss : 0.61791, Training Acc : 0.678, Run Time : 6.97
INFO:root:2019-05-11 12:51:23, Epoch : 1, Step : 100, Training Loss : 0.61953, Training Acc : 0.686, Run Time : 40.40
INFO:root:2019-05-11 12:51:30, Epoch : 1, Step : 101, Training Loss : 0.52001, Training Acc : 0.831, Run Time : 6.71
INFO:root:2019-05-11 12:51:51, Epoch : 1, Step : 102, Training Loss : 0.51481, Training Acc : 0.811, Run Time : 21.34
INFO:root:2019-05-11 12:52:06, Epoch : 1, Step : 103, Training Loss : 0.50787, Training Acc : 0.797, Run Time : 15.03
INFO:root:2019-05-11 12:52:23, Epoch : 1, Step : 104, Training Loss : 0.50034, Training Acc : 0.792, Run Time : 16.20
INFO:root:2019-05-11 12:52:45, Epoch : 1, Step : 105, Training Loss : 0.49291, Training Acc : 0.800, Run Time : 22.80
INFO:root:2019-05-11 12:52:50, Epoch : 1, Step : 106, Training Loss : 0.48585, Training Acc : 0.808, Run Time : 4.86
INFO:root:2019-05-11 12:53:25, Epoch : 1, Step : 107, Training Loss : 0.46777, Training Acc : 0.817, Run Time : 35.03
INFO:root:2019-05-11 12:53:53, Epoch : 1, Step : 108, Training Loss : 0.47674, Training Acc : 0.808, Run Time : 27.87
INFO:root:2019-05-11 12:53:57, Epoch : 1, Step : 109, Training Loss : 0.45927, Training Acc : 0.825, Run Time : 3.88
INFO:root:2019-05-11 12:54:15, Epoch : 1, Step : 110, Training Loss : 0.49096, Training Acc : 0.814, Run Time : 18.37
INFO:root:2019-05-11 12:55:35, Epoch : 1, Step : 111, Training Loss : 0.54275, Training Acc : 0.786, Run Time : 79.19
INFO:root:2019-05-11 12:56:48, Epoch : 1, Step : 112, Training Loss : 0.58238, Training Acc : 0.767, Run Time : 73.71
INFO:root:2019-05-11 12:56:55, Epoch : 1, Step : 113, Training Loss : 0.62475, Training Acc : 0.742, Run Time : 6.43
INFO:root:2019-05-11 12:57:41, Epoch : 1, Step : 114, Training Loss : 0.59162, Training Acc : 0.756, Run Time : 46.02
INFO:root:2019-05-11 12:58:05, Epoch : 1, Step : 115, Training Loss : 0.61901, Training Acc : 0.747, Run Time : 24.40
INFO:root:2019-05-11 12:58:47, Epoch : 1, Step : 116, Training Loss : 0.58393, Training Acc : 0.753, Run Time : 41.33
INFO:root:2019-05-11 12:59:04, Epoch : 1, Step : 117, Training Loss : 0.60295, Training Acc : 0.728, Run Time : 17.18
INFO:root:2019-05-11 12:59:33, Epoch : 1, Step : 118, Training Loss : 0.58818, Training Acc : 0.722, Run Time : 29.13
INFO:root:2019-05-11 12:59:56, Epoch : 1, Step : 119, Training Loss : 0.58919, Training Acc : 0.717, Run Time : 23.13
INFO:root:2019-05-11 13:00:47, Epoch : 1, Step : 120, Training Loss : 0.60227, Training Acc : 0.700, Run Time : 51.40
INFO:root:2019-05-11 13:01:33, Epoch : 1, Step : 121, Training Loss : 0.62088, Training Acc : 0.678, Run Time : 45.20
INFO:root:2019-05-11 13:01:57, Epoch : 1, Step : 122, Training Loss : 0.62680, Training Acc : 0.681, Run Time : 24.38
INFO:root:2019-05-11 13:02:31, Epoch : 1, Step : 123, Training Loss : 0.62130, Training Acc : 0.686, Run Time : 33.77
INFO:root:2019-05-11 13:02:39, Epoch : 1, Step : 124, Training Loss : 0.61612, Training Acc : 0.692, Run Time : 8.43
INFO:root:2019-05-11 13:03:30, Epoch : 1, Step : 125, Training Loss : 0.64315, Training Acc : 0.661, Run Time : 50.45
INFO:root:2019-05-11 13:04:33, Epoch : 1, Step : 126, Training Loss : 0.63986, Training Acc : 0.656, Run Time : 63.17
INFO:root:2019-05-11 13:04:41, Epoch : 1, Step : 127, Training Loss : 0.64018, Training Acc : 0.667, Run Time : 8.44
INFO:root:2019-05-11 13:05:46, Epoch : 1, Step : 128, Training Loss : 0.63032, Training Acc : 0.681, Run Time : 64.89
INFO:root:2019-05-11 13:06:12, Epoch : 1, Step : 129, Training Loss : 0.63361, Training Acc : 0.681, Run Time : 25.65
INFO:root:2019-05-11 13:06:20, Epoch : 1, Step : 130, Training Loss : 0.62139, Training Acc : 0.683, Run Time : 7.90
INFO:root:2019-05-11 13:07:10, Epoch : 1, Step : 131, Training Loss : 0.60655, Training Acc : 0.683, Run Time : 50.37
INFO:root:2019-05-11 13:07:17, Epoch : 1, Step : 132, Training Loss : 0.61011, Training Acc : 0.689, Run Time : 7.06
INFO:root:2019-05-11 13:07:46, Epoch : 1, Step : 133, Training Loss : 0.61206, Training Acc : 0.689, Run Time : 28.67
INFO:root:2019-05-11 13:07:51, Epoch : 1, Step : 134, Training Loss : 0.62365, Training Acc : 0.678, Run Time : 4.91
INFO:root:2019-05-11 13:08:13, Epoch : 1, Step : 135, Training Loss : 0.62258, Training Acc : 0.672, Run Time : 22.53
INFO:root:2019-05-11 13:08:16, Epoch : 1, Step : 136, Training Loss : 0.61340, Training Acc : 0.681, Run Time : 2.73
INFO:root:2019-05-11 13:08:33, Epoch : 1, Step : 137, Training Loss : 0.61418, Training Acc : 0.692, Run Time : 17.44
INFO:root:2019-05-11 13:08:38, Epoch : 1, Step : 138, Training Loss : 0.62069, Training Acc : 0.686, Run Time : 4.44
INFO:root:2019-05-11 13:09:11, Epoch : 1, Step : 139, Training Loss : 0.59833, Training Acc : 0.703, Run Time : 33.01
INFO:root:2019-05-11 13:09:52, Epoch : 1, Step : 140, Training Loss : 0.58833, Training Acc : 0.706, Run Time : 41.41
INFO:root:2019-05-11 13:10:01, Epoch : 1, Step : 141, Training Loss : 0.57119, Training Acc : 0.719, Run Time : 9.17
INFO:root:2019-05-11 13:10:22, Epoch : 1, Step : 142, Training Loss : 0.60856, Training Acc : 0.711, Run Time : 20.54
INFO:root:2019-05-11 13:10:25, Epoch : 1, Step : 143, Training Loss : 0.61025, Training Acc : 0.703, Run Time : 2.80
INFO:root:2019-05-11 13:10:41, Epoch : 1, Step : 144, Training Loss : 0.59892, Training Acc : 0.706, Run Time : 16.65
INFO:root:2019-05-11 13:10:51, Epoch : 1, Step : 145, Training Loss : 0.57931, Training Acc : 0.711, Run Time : 9.78
INFO:root:2019-05-11 13:11:27, Epoch : 1, Step : 146, Training Loss : 0.57940, Training Acc : 0.703, Run Time : 36.31
INFO:root:2019-05-11 13:12:03, Epoch : 1, Step : 147, Training Loss : 0.60512, Training Acc : 0.706, Run Time : 35.12
INFO:root:2019-05-11 13:12:08, Epoch : 1, Step : 148, Training Loss : 0.58666, Training Acc : 0.686, Run Time : 5.29
INFO:root:2019-05-11 13:12:11, Epoch : 1, Step : 149, Training Loss : 0.60452, Training Acc : 0.672, Run Time : 3.24
INFO:root:2019-05-11 13:12:27, Epoch : 1, Step : 150, Training Loss : 0.60392, Training Acc : 0.689, Run Time : 16.33
INFO:root:2019-05-11 13:12:33, Epoch : 1, Step : 151, Training Loss : 0.60539, Training Acc : 0.694, Run Time : 5.42
INFO:root:2019-05-11 13:12:56, Epoch : 1, Step : 152, Training Loss : 0.63639, Training Acc : 0.658, Run Time : 22.78
INFO:root:2019-05-11 13:12:58, Epoch : 1, Step : 153, Training Loss : 0.60385, Training Acc : 0.678, Run Time : 2.43
INFO:root:2019-05-11 13:13:05, Epoch : 1, Step : 154, Training Loss : 0.59242, Training Acc : 0.694, Run Time : 6.75
INFO:root:2019-05-11 13:13:19, Epoch : 1, Step : 155, Training Loss : 0.58845, Training Acc : 0.703, Run Time : 13.81
INFO:root:2019-05-11 13:13:35, Epoch : 1, Step : 156, Training Loss : 0.56893, Training Acc : 0.728, Run Time : 16.30
INFO:root:2019-05-11 13:13:55, Epoch : 1, Step : 157, Training Loss : 0.59493, Training Acc : 0.700, Run Time : 19.66
INFO:root:2019-05-11 13:14:01, Epoch : 1, Step : 158, Training Loss : 0.58760, Training Acc : 0.686, Run Time : 6.51
INFO:root:2019-05-11 13:14:15, Epoch : 1, Step : 159, Training Loss : 0.57355, Training Acc : 0.714, Run Time : 14.26
INFO:root:2019-05-11 13:14:34, Epoch : 1, Step : 160, Training Loss : 0.54788, Training Acc : 0.722, Run Time : 19.01
INFO:root:2019-05-11 13:14:50, Epoch : 1, Step : 161, Training Loss : 0.58278, Training Acc : 0.692, Run Time : 16.09
INFO:root:2019-05-11 13:14:57, Epoch : 1, Step : 162, Training Loss : 0.58886, Training Acc : 0.694, Run Time : 6.87
INFO:root:2019-05-11 13:15:29, Epoch : 1, Step : 163, Training Loss : 0.60839, Training Acc : 0.672, Run Time : 32.15
INFO:root:2019-05-11 13:15:34, Epoch : 1, Step : 164, Training Loss : 0.55161, Training Acc : 0.725, Run Time : 4.67
INFO:root:2019-05-11 13:15:37, Epoch : 1, Step : 165, Training Loss : 0.52356, Training Acc : 0.753, Run Time : 2.51
INFO:root:2019-05-11 13:16:31, Epoch : 1, Step : 166, Training Loss : 0.56038, Training Acc : 0.728, Run Time : 54.57
INFO:root:2019-05-11 13:16:51, Epoch : 1, Step : 167, Training Loss : 0.54411, Training Acc : 0.731, Run Time : 19.80
INFO:root:2019-05-11 13:16:53, Epoch : 1, Step : 168, Training Loss : 0.58716, Training Acc : 0.694, Run Time : 2.09
INFO:root:2019-05-11 13:17:15, Epoch : 1, Step : 169, Training Loss : 0.54024, Training Acc : 0.731, Run Time : 22.28
INFO:root:2019-05-11 13:18:00, Epoch : 1, Step : 170, Training Loss : 0.55780, Training Acc : 0.708, Run Time : 44.75
INFO:root:2019-05-11 13:18:22, Epoch : 1, Step : 171, Training Loss : 0.53377, Training Acc : 0.731, Run Time : 21.77
INFO:root:2019-05-11 13:18:30, Epoch : 1, Step : 172, Training Loss : 0.56617, Training Acc : 0.706, Run Time : 8.33
INFO:root:2019-05-11 13:19:11, Epoch : 1, Step : 173, Training Loss : 0.52002, Training Acc : 0.753, Run Time : 40.41
INFO:root:2019-05-11 13:19:18, Epoch : 1, Step : 174, Training Loss : 0.53915, Training Acc : 0.739, Run Time : 6.91
INFO:root:2019-05-11 13:19:43, Epoch : 1, Step : 175, Training Loss : 0.55779, Training Acc : 0.714, Run Time : 24.93
INFO:root:2019-05-11 13:19:45, Epoch : 1, Step : 176, Training Loss : 0.51334, Training Acc : 0.750, Run Time : 2.97
INFO:root:2019-05-11 13:20:08, Epoch : 1, Step : 177, Training Loss : 0.49824, Training Acc : 0.753, Run Time : 22.46
INFO:root:2019-05-11 13:20:27, Epoch : 1, Step : 178, Training Loss : 0.52422, Training Acc : 0.747, Run Time : 19.37
INFO:root:2019-05-11 13:20:41, Epoch : 1, Step : 179, Training Loss : 0.53413, Training Acc : 0.733, Run Time : 13.43
INFO:root:2019-05-11 13:20:46, Epoch : 1, Step : 180, Training Loss : 0.54561, Training Acc : 0.739, Run Time : 5.28
INFO:root:2019-05-11 13:20:48, Epoch : 1, Step : 181, Training Loss : 0.53876, Training Acc : 0.725, Run Time : 2.46
INFO:root:2019-05-11 13:21:15, Epoch : 1, Step : 182, Training Loss : 0.52298, Training Acc : 0.744, Run Time : 26.55
INFO:root:2019-05-11 13:21:31, Epoch : 1, Step : 183, Training Loss : 0.49795, Training Acc : 0.753, Run Time : 15.95
INFO:root:2019-05-11 13:21:52, Epoch : 1, Step : 184, Training Loss : 0.49948, Training Acc : 0.758, Run Time : 20.64
INFO:root:2019-05-11 13:22:06, Epoch : 1, Step : 185, Training Loss : 0.54442, Training Acc : 0.725, Run Time : 14.77
INFO:root:2019-05-11 13:22:24, Epoch : 1, Step : 186, Training Loss : 0.53122, Training Acc : 0.736, Run Time : 17.81
INFO:root:2019-05-11 13:22:38, Epoch : 1, Step : 187, Training Loss : 0.52163, Training Acc : 0.733, Run Time : 13.34
INFO:root:2019-05-11 13:23:14, Epoch : 1, Step : 188, Training Loss : 0.52027, Training Acc : 0.744, Run Time : 36.18
INFO:root:2019-05-11 13:23:39, Epoch : 1, Step : 189, Training Loss : 0.52457, Training Acc : 0.731, Run Time : 25.73
INFO:root:2019-05-11 13:23:49, Epoch : 1, Step : 190, Training Loss : 0.52929, Training Acc : 0.728, Run Time : 9.90
INFO:root:2019-05-11 13:24:15, Epoch : 1, Step : 191, Training Loss : 0.51251, Training Acc : 0.744, Run Time : 25.99
INFO:root:2019-05-11 13:24:32, Epoch : 1, Step : 192, Training Loss : 0.53303, Training Acc : 0.722, Run Time : 16.80
INFO:root:2019-05-11 13:24:42, Epoch : 1, Step : 193, Training Loss : 0.52463, Training Acc : 0.717, Run Time : 10.06
INFO:root:2019-05-11 13:25:08, Epoch : 1, Step : 194, Training Loss : 0.54662, Training Acc : 0.700, Run Time : 25.58
INFO:root:2019-05-11 13:25:10, Epoch : 1, Step : 195, Training Loss : 0.54622, Training Acc : 0.703, Run Time : 2.71
INFO:root:2019-05-11 13:25:13, Epoch : 1, Step : 196, Training Loss : 0.54168, Training Acc : 0.700, Run Time : 2.78
INFO:root:2019-05-11 13:26:01, Epoch : 1, Step : 197, Training Loss : 0.51012, Training Acc : 0.719, Run Time : 48.14
INFO:root:2019-05-11 13:26:23, Epoch : 1, Step : 198, Training Loss : 0.51939, Training Acc : 0.769, Run Time : 21.76
INFO:root:2019-05-11 13:26:37, Epoch : 1, Step : 199, Training Loss : 0.50829, Training Acc : 0.753, Run Time : 13.86
INFO:root:2019-05-11 13:26:57, Epoch : 1, Step : 200, Training Loss : 0.48271, Training Acc : 0.803, Run Time : 20.14
INFO:root:2019-05-11 13:27:25, Epoch : 1, Step : 201, Training Loss : 0.51702, Training Acc : 0.836, Run Time : 27.61
INFO:root:2019-05-11 13:28:02, Epoch : 1, Step : 202, Training Loss : 0.47049, Training Acc : 0.839, Run Time : 37.32
INFO:root:2019-05-11 13:28:42, Epoch : 1, Step : 203, Training Loss : 0.54206, Training Acc : 0.786, Run Time : 40.22
INFO:root:2019-05-11 13:28:56, Epoch : 1, Step : 204, Training Loss : 0.60423, Training Acc : 0.756, Run Time : 13.83
INFO:root:2019-05-11 13:28:59, Epoch : 1, Step : 205, Training Loss : 0.64522, Training Acc : 0.719, Run Time : 2.96
INFO:root:2019-05-11 13:29:35, Epoch : 1, Step : 206, Training Loss : 0.62758, Training Acc : 0.728, Run Time : 35.71
INFO:root:2019-05-11 13:29:44, Epoch : 1, Step : 207, Training Loss : 0.70645, Training Acc : 0.664, Run Time : 9.48
INFO:root:2019-05-11 13:30:23, Epoch : 1, Step : 208, Training Loss : 0.59103, Training Acc : 0.722, Run Time : 38.46
INFO:root:2019-05-11 13:30:29, Epoch : 1, Step : 209, Training Loss : 0.61014, Training Acc : 0.733, Run Time : 6.44
INFO:root:2019-05-11 13:31:13, Epoch : 1, Step : 210, Training Loss : 0.72503, Training Acc : 0.647, Run Time : 43.84
INFO:root:2019-05-11 13:31:32, Epoch : 1, Step : 211, Training Loss : 0.70297, Training Acc : 0.656, Run Time : 19.47
INFO:root:2019-05-11 13:32:12, Epoch : 1, Step : 212, Training Loss : 0.71246, Training Acc : 0.625, Run Time : 39.14
INFO:root:2019-05-11 13:32:30, Epoch : 1, Step : 213, Training Loss : 0.73712, Training Acc : 0.589, Run Time : 17.89
INFO:root:2019-05-11 13:32:38, Epoch : 1, Step : 214, Training Loss : 0.70599, Training Acc : 0.581, Run Time : 8.06
INFO:root:2019-05-11 13:32:40, Epoch : 1, Step : 215, Training Loss : 0.62815, Training Acc : 0.656, Run Time : 2.63
INFO:root:2019-05-11 13:33:05, Epoch : 1, Step : 216, Training Loss : 0.58758, Training Acc : 0.717, Run Time : 24.76
INFO:root:2019-05-11 13:33:25, Epoch : 1, Step : 217, Training Loss : 0.62216, Training Acc : 0.653, Run Time : 19.77
INFO:root:2019-05-11 13:33:47, Epoch : 1, Step : 218, Training Loss : 0.63279, Training Acc : 0.625, Run Time : 22.04
INFO:root:2019-05-11 13:34:07, Epoch : 1, Step : 219, Training Loss : 0.64506, Training Acc : 0.669, Run Time : 19.97
INFO:root:2019-05-11 13:34:14, Epoch : 1, Step : 220, Training Loss : 0.67973, Training Acc : 0.583, Run Time : 7.40
INFO:root:2019-05-11 13:34:33, Epoch : 1, Step : 221, Training Loss : 0.66313, Training Acc : 0.631, Run Time : 18.41
INFO:root:2019-05-11 13:34:55, Epoch : 1, Step : 222, Training Loss : 0.63462, Training Acc : 0.631, Run Time : 21.98
INFO:root:2019-05-11 13:34:57, Epoch : 1, Step : 223, Training Loss : 0.64344, Training Acc : 0.631, Run Time : 2.68
INFO:root:2019-05-11 13:35:59, Epoch : 1, Step : 224, Training Loss : 0.63209, Training Acc : 0.703, Run Time : 61.93
INFO:root:2019-05-11 13:36:42, Epoch : 1, Step : 225, Training Loss : 0.59496, Training Acc : 0.792, Run Time : 42.60
INFO:root:2019-05-11 13:37:20, Epoch : 1, Step : 226, Training Loss : 0.59869, Training Acc : 0.700, Run Time : 38.26
INFO:root:2019-05-11 13:38:03, Epoch : 1, Step : 227, Training Loss : 0.57914, Training Acc : 0.731, Run Time : 43.00
INFO:root:2019-05-11 13:38:38, Epoch : 1, Step : 228, Training Loss : 0.54241, Training Acc : 0.792, Run Time : 35.22
INFO:root:2019-05-11 13:38:41, Epoch : 1, Step : 229, Training Loss : 0.45982, Training Acc : 0.833, Run Time : 2.87
INFO:root:2019-05-11 13:39:40, Epoch : 1, Step : 230, Training Loss : 0.49852, Training Acc : 0.764, Run Time : 58.49
INFO:root:2019-05-11 13:39:47, Epoch : 1, Step : 231, Training Loss : 0.59274, Training Acc : 0.683, Run Time : 7.70
INFO:root:2019-05-11 13:40:56, Epoch : 1, Step : 232, Training Loss : 0.61662, Training Acc : 0.672, Run Time : 68.95
INFO:root:2019-05-11 13:41:36, Epoch : 1, Step : 233, Training Loss : 0.60784, Training Acc : 0.669, Run Time : 40.18
INFO:root:2019-05-11 13:42:26, Epoch : 1, Step : 234, Training Loss : 0.64975, Training Acc : 0.642, Run Time : 49.91
INFO:root:2019-05-11 13:42:33, Epoch : 1, Step : 235, Training Loss : 0.67457, Training Acc : 0.642, Run Time : 6.27
INFO:root:2019-05-11 13:43:03, Epoch : 1, Step : 236, Training Loss : 0.66049, Training Acc : 0.644, Run Time : 30.89
INFO:root:2019-05-11 13:43:10, Epoch : 1, Step : 237, Training Loss : 0.83681, Training Acc : 0.561, Run Time : 6.50
INFO:root:2019-05-11 13:43:40, Epoch : 1, Step : 238, Training Loss : 0.81566, Training Acc : 0.536, Run Time : 29.97
INFO:root:2019-05-11 13:44:12, Epoch : 1, Step : 239, Training Loss : 0.74124, Training Acc : 0.572, Run Time : 32.37
INFO:root:2019-05-11 13:44:20, Epoch : 1, Step : 240, Training Loss : 0.75443, Training Acc : 0.536, Run Time : 7.44
INFO:root:2019-05-11 13:44:41, Epoch : 1, Step : 241, Training Loss : 0.69614, Training Acc : 0.533, Run Time : 21.71
INFO:root:2019-05-11 13:44:57, Epoch : 1, Step : 242, Training Loss : 0.66571, Training Acc : 0.561, Run Time : 15.09
INFO:root:2019-05-11 13:45:15, Epoch : 1, Step : 243, Training Loss : 0.66458, Training Acc : 0.589, Run Time : 18.65
INFO:root:2019-05-11 13:45:58, Epoch : 1, Step : 244, Training Loss : 0.66793, Training Acc : 0.547, Run Time : 43.13
INFO:root:2019-05-11 13:48:01, Epoch : 1, Step : 245, Training Loss : 0.67443, Training Acc : 0.553, Run Time : 122.94
INFO:root:2019-05-11 13:48:29, Epoch : 1, Step : 246, Training Loss : 0.70824, Training Acc : 0.497, Run Time : 27.71
INFO:root:2019-05-11 13:48:36, Epoch : 1, Step : 247, Training Loss : 0.72047, Training Acc : 0.486, Run Time : 6.88
INFO:root:2019-05-11 13:49:27, Epoch : 1, Step : 248, Training Loss : 0.71153, Training Acc : 0.497, Run Time : 50.74
INFO:root:2019-05-11 13:49:35, Epoch : 1, Step : 249, Training Loss : 0.70460, Training Acc : 0.494, Run Time : 8.32
INFO:root:2019-05-11 13:50:03, Epoch : 1, Step : 250, Training Loss : 0.69439, Training Acc : 0.553, Run Time : 27.79
INFO:root:2019-05-11 13:50:19, Epoch : 1, Step : 251, Training Loss : 0.69433, Training Acc : 0.514, Run Time : 16.04
INFO:root:2019-05-11 13:50:36, Epoch : 1, Step : 252, Training Loss : 0.68334, Training Acc : 0.583, Run Time : 17.64
INFO:root:2019-05-11 13:50:55, Epoch : 1, Step : 253, Training Loss : 0.66742, Training Acc : 0.567, Run Time : 19.08
INFO:root:2019-05-11 13:51:18, Epoch : 1, Step : 254, Training Loss : 0.68610, Training Acc : 0.581, Run Time : 22.54
INFO:root:2019-05-11 13:51:26, Epoch : 1, Step : 255, Training Loss : 0.64827, Training Acc : 0.628, Run Time : 7.86
INFO:root:2019-05-11 13:51:47, Epoch : 1, Step : 256, Training Loss : 0.67217, Training Acc : 0.650, Run Time : 20.92
INFO:root:2019-05-11 13:52:16, Epoch : 1, Step : 257, Training Loss : 0.65333, Training Acc : 0.600, Run Time : 29.32
INFO:root:2019-05-11 13:52:19, Epoch : 1, Step : 258, Training Loss : 0.66556, Training Acc : 0.594, Run Time : 2.66
INFO:root:2019-05-11 13:52:55, Epoch : 1, Step : 259, Training Loss : 0.65800, Training Acc : 0.583, Run Time : 36.68
INFO:root:2019-05-11 13:52:58, Epoch : 1, Step : 260, Training Loss : 0.66996, Training Acc : 0.569, Run Time : 2.69
INFO:root:2019-05-11 13:53:24, Epoch : 1, Step : 261, Training Loss : 0.65209, Training Acc : 0.592, Run Time : 26.16
INFO:root:2019-05-11 13:53:43, Epoch : 1, Step : 262, Training Loss : 0.69114, Training Acc : 0.558, Run Time : 18.94
INFO:root:2019-05-11 13:53:53, Epoch : 1, Step : 263, Training Loss : 0.67276, Training Acc : 0.597, Run Time : 9.40
INFO:root:2019-05-11 13:54:10, Epoch : 1, Step : 264, Training Loss : 0.67224, Training Acc : 0.553, Run Time : 17.68
INFO:root:2019-05-11 13:54:20, Epoch : 1, Step : 265, Training Loss : 0.65175, Training Acc : 0.578, Run Time : 9.66
INFO:root:2019-05-11 13:54:39, Epoch : 1, Step : 266, Training Loss : 0.66947, Training Acc : 0.647, Run Time : 19.06
INFO:root:2019-05-11 13:54:42, Epoch : 1, Step : 267, Training Loss : 0.65131, Training Acc : 0.678, Run Time : 2.98
INFO:root:2019-05-11 13:55:13, Epoch : 1, Step : 268, Training Loss : 0.65907, Training Acc : 0.636, Run Time : 31.11
INFO:root:2019-05-11 13:55:16, Epoch : 1, Step : 269, Training Loss : 0.66448, Training Acc : 0.536, Run Time : 2.54
INFO:root:2019-05-11 13:55:35, Epoch : 1, Step : 270, Training Loss : 0.65911, Training Acc : 0.578, Run Time : 18.83
INFO:root:2019-05-11 13:56:05, Epoch : 1, Step : 271, Training Loss : 0.65167, Training Acc : 0.644, Run Time : 30.60
INFO:root:2019-05-11 13:56:28, Epoch : 1, Step : 272, Training Loss : 0.66482, Training Acc : 0.600, Run Time : 23.28
INFO:root:2019-05-11 13:56:52, Epoch : 1, Step : 273, Training Loss : 0.64257, Training Acc : 0.661, Run Time : 23.49
INFO:root:2019-05-11 13:56:58, Epoch : 1, Step : 274, Training Loss : 0.65333, Training Acc : 0.625, Run Time : 6.36
INFO:root:2019-05-11 13:57:15, Epoch : 1, Step : 275, Training Loss : 0.66977, Training Acc : 0.664, Run Time : 16.86
INFO:root:2019-05-11 13:57:17, Epoch : 1, Step : 276, Training Loss : 0.63329, Training Acc : 0.644, Run Time : 2.40
INFO:root:2019-05-11 13:57:34, Epoch : 1, Step : 277, Training Loss : 0.64636, Training Acc : 0.603, Run Time : 16.07
INFO:root:2019-05-11 13:57:52, Epoch : 1, Step : 278, Training Loss : 0.68759, Training Acc : 0.594, Run Time : 18.78
INFO:root:2019-05-11 13:57:56, Epoch : 1, Step : 279, Training Loss : 0.64784, Training Acc : 0.617, Run Time : 3.29
INFO:root:2019-05-11 13:58:24, Epoch : 1, Step : 280, Training Loss : 0.63965, Training Acc : 0.617, Run Time : 28.68
INFO:root:2019-05-11 13:58:50, Epoch : 1, Step : 281, Training Loss : 0.63345, Training Acc : 0.589, Run Time : 25.57
INFO:root:2019-05-11 13:58:59, Epoch : 1, Step : 282, Training Loss : 0.61447, Training Acc : 0.639, Run Time : 9.09
INFO:root:2019-05-11 13:59:13, Epoch : 1, Step : 283, Training Loss : 0.65540, Training Acc : 0.606, Run Time : 13.62
INFO:root:2019-05-11 13:59:15, Epoch : 1, Step : 284, Training Loss : 0.64669, Training Acc : 0.650, Run Time : 2.45
INFO:root:2019-05-11 13:59:30, Epoch : 1, Step : 285, Training Loss : 0.60439, Training Acc : 0.678, Run Time : 15.44
INFO:root:2019-05-11 13:59:47, Epoch : 1, Step : 286, Training Loss : 0.61969, Training Acc : 0.686, Run Time : 16.83
INFO:root:2019-05-11 14:00:02, Epoch : 1, Step : 287, Training Loss : 0.59554, Training Acc : 0.719, Run Time : 14.66
INFO:root:2019-05-11 14:00:17, Epoch : 1, Step : 288, Training Loss : 0.61737, Training Acc : 0.664, Run Time : 14.58
INFO:root:2019-05-11 14:00:32, Epoch : 1, Step : 289, Training Loss : 0.64213, Training Acc : 0.628, Run Time : 15.54
INFO:root:2019-05-11 14:00:37, Epoch : 1, Step : 290, Training Loss : 0.61083, Training Acc : 0.656, Run Time : 4.81
INFO:root:2019-05-11 14:00:39, Epoch : 1, Step : 291, Training Loss : 0.64616, Training Acc : 0.631, Run Time : 1.75
INFO:root:2019-05-11 14:00:59, Epoch : 1, Step : 292, Training Loss : 0.58527, Training Acc : 0.689, Run Time : 20.47
INFO:root:2019-05-11 14:01:02, Epoch : 1, Step : 293, Training Loss : 0.58276, Training Acc : 0.703, Run Time : 2.73
INFO:root:2019-05-11 14:01:24, Epoch : 1, Step : 294, Training Loss : 0.59457, Training Acc : 0.681, Run Time : 21.85
INFO:root:2019-05-11 14:01:33, Epoch : 1, Step : 295, Training Loss : 0.54408, Training Acc : 0.733, Run Time : 9.05
INFO:root:2019-05-11 14:01:57, Epoch : 1, Step : 296, Training Loss : 0.54055, Training Acc : 0.728, Run Time : 24.34
INFO:root:2019-05-11 14:02:21, Epoch : 1, Step : 297, Training Loss : 0.51100, Training Acc : 0.761, Run Time : 24.14
INFO:root:2019-05-11 14:02:27, Epoch : 1, Step : 298, Training Loss : 0.53115, Training Acc : 0.756, Run Time : 6.05
INFO:root:2019-05-11 14:02:51, Epoch : 1, Step : 299, Training Loss : 0.53518, Training Acc : 0.742, Run Time : 23.77
INFO:root:2019-05-11 14:02:56, Epoch : 1, Step : 300, Training Loss : 0.41153, Training Acc : 0.844, Run Time : 5.41
INFO:root:2019-05-11 14:03:23, Epoch : 1, Step : 301, Training Loss : 0.63489, Training Acc : 0.683, Run Time : 26.14
INFO:root:2019-05-11 14:03:32, Epoch : 1, Step : 302, Training Loss : 0.73825, Training Acc : 0.597, Run Time : 9.62
INFO:root:2019-05-11 14:03:45, Epoch : 1, Step : 303, Training Loss : 0.74379, Training Acc : 0.594, Run Time : 13.06
INFO:root:2019-05-11 14:03:48, Epoch : 1, Step : 304, Training Loss : 0.76739, Training Acc : 0.572, Run Time : 2.64
INFO:root:2019-05-11 14:04:04, Epoch : 1, Step : 305, Training Loss : 0.83679, Training Acc : 0.511, Run Time : 15.70
INFO:root:2019-05-11 14:04:06, Epoch : 1, Step : 306, Training Loss : 0.77090, Training Acc : 0.547, Run Time : 2.78
INFO:root:2019-05-11 14:04:43, Epoch : 1, Step : 307, Training Loss : 0.77361, Training Acc : 0.514, Run Time : 36.41
INFO:root:2019-05-11 14:05:15, Epoch : 1, Step : 308, Training Loss : 0.69564, Training Acc : 0.567, Run Time : 31.70
INFO:root:2019-05-11 14:05:18, Epoch : 1, Step : 309, Training Loss : 0.66769, Training Acc : 0.567, Run Time : 3.33
INFO:root:2019-05-11 14:05:34, Epoch : 1, Step : 310, Training Loss : 0.64529, Training Acc : 0.619, Run Time : 15.91
INFO:root:2019-05-11 14:05:40, Epoch : 1, Step : 311, Training Loss : 0.67823, Training Acc : 0.586, Run Time : 5.78
INFO:root:2019-05-11 14:05:52, Epoch : 1, Step : 312, Training Loss : 0.71392, Training Acc : 0.514, Run Time : 12.58
INFO:root:2019-05-11 14:05:55, Epoch : 1, Step : 313, Training Loss : 0.70005, Training Acc : 0.569, Run Time : 2.92
INFO:root:2019-05-11 14:06:33, Epoch : 1, Step : 314, Training Loss : 0.71182, Training Acc : 0.494, Run Time : 38.42
INFO:root:2019-05-11 14:06:59, Epoch : 1, Step : 315, Training Loss : 0.70216, Training Acc : 0.531, Run Time : 25.77
INFO:root:2019-05-11 14:07:02, Epoch : 1, Step : 316, Training Loss : 0.70023, Training Acc : 0.561, Run Time : 2.90
INFO:root:2019-05-11 14:07:04, Epoch : 1, Step : 317, Training Loss : 0.68797, Training Acc : 0.558, Run Time : 2.12
INFO:root:2019-05-11 14:07:28, Epoch : 1, Step : 318, Training Loss : 0.66312, Training Acc : 0.589, Run Time : 24.03
INFO:root:2019-05-11 14:07:58, Epoch : 1, Step : 319, Training Loss : 0.64956, Training Acc : 0.672, Run Time : 29.84
INFO:root:2019-05-11 14:08:07, Epoch : 1, Step : 320, Training Loss : 0.67226, Training Acc : 0.567, Run Time : 8.98
INFO:root:2019-05-11 14:08:24, Epoch : 1, Step : 321, Training Loss : 0.61111, Training Acc : 0.739, Run Time : 16.79
INFO:root:2019-05-11 14:08:40, Epoch : 1, Step : 322, Training Loss : 0.63091, Training Acc : 0.606, Run Time : 16.17
INFO:root:2019-05-11 14:08:50, Epoch : 1, Step : 323, Training Loss : 0.69408, Training Acc : 0.556, Run Time : 10.22
INFO:root:2019-05-11 14:09:50, Epoch : 1, Step : 324, Training Loss : 0.67173, Training Acc : 0.594, Run Time : 59.78
INFO:root:2019-05-11 14:09:56, Epoch : 1, Step : 325, Training Loss : 0.62595, Training Acc : 0.619, Run Time : 6.00
INFO:root:2019-05-11 14:10:14, Epoch : 1, Step : 326, Training Loss : 0.67932, Training Acc : 0.553, Run Time : 17.81
INFO:root:2019-05-11 14:10:32, Epoch : 1, Step : 327, Training Loss : 0.68351, Training Acc : 0.553, Run Time : 18.24
INFO:root:2019-05-11 14:10:46, Epoch : 1, Step : 328, Training Loss : 0.68992, Training Acc : 0.569, Run Time : 14.08
INFO:root:2019-05-11 14:10:50, Epoch : 1, Step : 329, Training Loss : 0.66727, Training Acc : 0.558, Run Time : 3.78
INFO:root:2019-05-11 14:10:53, Epoch : 1, Step : 330, Training Loss : 0.64823, Training Acc : 0.606, Run Time : 3.02
INFO:root:2019-05-11 14:11:11, Epoch : 1, Step : 331, Training Loss : 0.64220, Training Acc : 0.628, Run Time : 17.51
INFO:root:2019-05-11 14:11:24, Epoch : 1, Step : 332, Training Loss : 0.67604, Training Acc : 0.581, Run Time : 13.28
INFO:root:2019-05-11 14:11:40, Epoch : 1, Step : 333, Training Loss : 0.67082, Training Acc : 0.586, Run Time : 16.18
INFO:root:2019-05-11 14:12:13, Epoch : 1, Step : 334, Training Loss : 0.65371, Training Acc : 0.681, Run Time : 32.67
INFO:root:2019-05-11 14:12:15, Epoch : 1, Step : 335, Training Loss : 0.66935, Training Acc : 0.619, Run Time : 2.13
INFO:root:2019-05-11 14:12:19, Epoch : 1, Step : 336, Training Loss : 0.66326, Training Acc : 0.636, Run Time : 4.04
INFO:root:2019-05-11 14:12:42, Epoch : 1, Step : 337, Training Loss : 0.66824, Training Acc : 0.617, Run Time : 23.31
INFO:root:2019-05-11 14:13:00, Epoch : 1, Step : 338, Training Loss : 0.67867, Training Acc : 0.564, Run Time : 18.00
INFO:root:2019-05-11 14:13:16, Epoch : 1, Step : 339, Training Loss : 0.65925, Training Acc : 0.639, Run Time : 15.66
INFO:root:2019-05-11 14:13:26, Epoch : 1, Step : 340, Training Loss : 0.65378, Training Acc : 0.642, Run Time : 9.75
INFO:root:2019-05-11 14:13:41, Epoch : 1, Step : 341, Training Loss : 0.67315, Training Acc : 0.583, Run Time : 15.61
INFO:root:2019-05-11 14:14:31, Epoch : 1, Step : 342, Training Loss : 0.67034, Training Acc : 0.639, Run Time : 50.08
INFO:root:2019-05-11 14:14:40, Epoch : 1, Step : 343, Training Loss : 0.65029, Training Acc : 0.628, Run Time : 8.53
INFO:root:2019-05-11 14:14:59, Epoch : 1, Step : 344, Training Loss : 0.64480, Training Acc : 0.606, Run Time : 18.98
INFO:root:2019-05-11 14:15:14, Epoch : 1, Step : 345, Training Loss : 0.62931, Training Acc : 0.617, Run Time : 15.10
INFO:root:2019-05-11 14:15:37, Epoch : 1, Step : 346, Training Loss : 0.67048, Training Acc : 0.539, Run Time : 23.37
INFO:root:2019-05-11 14:15:42, Epoch : 1, Step : 347, Training Loss : 0.65653, Training Acc : 0.556, Run Time : 5.13
INFO:root:2019-05-11 14:15:45, Epoch : 1, Step : 348, Training Loss : 0.67777, Training Acc : 0.533, Run Time : 3.10
INFO:root:2019-05-11 14:16:10, Epoch : 1, Step : 349, Training Loss : 0.64320, Training Acc : 0.567, Run Time : 24.38
INFO:root:2019-05-11 14:16:26, Epoch : 1, Step : 350, Training Loss : 0.69054, Training Acc : 0.511, Run Time : 16.04
INFO:root:2019-05-11 14:16:39, Epoch : 1, Step : 351, Training Loss : 0.67597, Training Acc : 0.531, Run Time : 13.37
INFO:root:2019-05-11 14:16:46, Epoch : 1, Step : 352, Training Loss : 0.65738, Training Acc : 0.600, Run Time : 6.79
INFO:root:2019-05-11 14:17:20, Epoch : 1, Step : 353, Training Loss : 0.65558, Training Acc : 0.647, Run Time : 34.37
INFO:root:2019-05-11 14:17:29, Epoch : 1, Step : 354, Training Loss : 0.65584, Training Acc : 0.675, Run Time : 8.79
INFO:root:2019-05-11 14:17:48, Epoch : 1, Step : 355, Training Loss : 0.65455, Training Acc : 0.644, Run Time : 19.14
INFO:root:2019-05-11 14:17:52, Epoch : 1, Step : 356, Training Loss : 0.66119, Training Acc : 0.636, Run Time : 3.31
INFO:root:2019-05-11 14:18:18, Epoch : 1, Step : 357, Training Loss : 0.65096, Training Acc : 0.669, Run Time : 26.82
INFO:root:2019-05-11 14:18:23, Epoch : 1, Step : 358, Training Loss : 0.64703, Training Acc : 0.642, Run Time : 4.36
INFO:root:2019-05-11 14:18:48, Epoch : 1, Step : 359, Training Loss : 0.68003, Training Acc : 0.564, Run Time : 24.84
INFO:root:2019-05-11 14:18:58, Epoch : 1, Step : 360, Training Loss : 0.66132, Training Acc : 0.644, Run Time : 10.71
INFO:root:2019-05-11 14:19:18, Epoch : 1, Step : 361, Training Loss : 0.66835, Training Acc : 0.614, Run Time : 19.43
INFO:root:2019-05-11 14:19:39, Epoch : 1, Step : 362, Training Loss : 0.65861, Training Acc : 0.622, Run Time : 20.97
INFO:root:2019-05-11 14:19:49, Epoch : 1, Step : 363, Training Loss : 0.65326, Training Acc : 0.706, Run Time : 10.48
INFO:root:2019-05-11 14:19:52, Epoch : 1, Step : 364, Training Loss : 0.64823, Training Acc : 0.764, Run Time : 2.87
INFO:root:2019-05-11 14:20:27, Epoch : 1, Step : 365, Training Loss : 0.65483, Training Acc : 0.686, Run Time : 35.03
INFO:root:2019-05-11 14:20:41, Epoch : 1, Step : 366, Training Loss : 0.63247, Training Acc : 0.767, Run Time : 14.08
INFO:root:2019-05-11 14:20:55, Epoch : 1, Step : 367, Training Loss : 0.64112, Training Acc : 0.739, Run Time : 13.99
INFO:root:2019-05-11 14:21:01, Epoch : 1, Step : 368, Training Loss : 0.63882, Training Acc : 0.708, Run Time : 5.97
INFO:root:2019-05-11 14:21:30, Epoch : 1, Step : 369, Training Loss : 0.63461, Training Acc : 0.714, Run Time : 28.76
INFO:root:2019-05-11 14:21:49, Epoch : 1, Step : 370, Training Loss : 0.65059, Training Acc : 0.611, Run Time : 18.78
INFO:root:2019-05-11 14:21:59, Epoch : 1, Step : 371, Training Loss : 0.67161, Training Acc : 0.536, Run Time : 10.15
INFO:root:2019-05-11 14:22:13, Epoch : 1, Step : 372, Training Loss : 0.64906, Training Acc : 0.636, Run Time : 14.23
INFO:root:2019-05-11 14:22:31, Epoch : 1, Step : 373, Training Loss : 0.62104, Training Acc : 0.678, Run Time : 18.41
INFO:root:2019-05-11 14:22:45, Epoch : 1, Step : 374, Training Loss : 0.62377, Training Acc : 0.683, Run Time : 13.11
INFO:root:2019-05-11 14:23:13, Epoch : 1, Step : 375, Training Loss : 0.64598, Training Acc : 0.633, Run Time : 28.37
INFO:root:2019-05-11 14:23:20, Epoch : 1, Step : 376, Training Loss : 0.66558, Training Acc : 0.533, Run Time : 7.13
INFO:root:2019-05-11 14:23:39, Epoch : 1, Step : 377, Training Loss : 0.65725, Training Acc : 0.581, Run Time : 18.79
INFO:root:2019-05-11 14:24:08, Epoch : 1, Step : 378, Training Loss : 0.63230, Training Acc : 0.686, Run Time : 29.31
INFO:root:2019-05-11 14:24:13, Epoch : 1, Step : 379, Training Loss : 0.62969, Training Acc : 0.747, Run Time : 5.09
INFO:root:2019-05-11 14:24:39, Epoch : 1, Step : 380, Training Loss : 0.63890, Training Acc : 0.689, Run Time : 25.69
INFO:root:2019-05-11 14:24:59, Epoch : 1, Step : 381, Training Loss : 0.63858, Training Acc : 0.683, Run Time : 20.45
INFO:root:2019-05-11 14:25:16, Epoch : 1, Step : 382, Training Loss : 0.63269, Training Acc : 0.719, Run Time : 16.28
INFO:root:2019-05-11 14:25:23, Epoch : 1, Step : 383, Training Loss : 0.63473, Training Acc : 0.708, Run Time : 7.22
INFO:root:2019-05-11 14:26:06, Epoch : 1, Step : 384, Training Loss : 0.62012, Training Acc : 0.781, Run Time : 42.62
INFO:root:2019-05-11 14:26:12, Epoch : 1, Step : 385, Training Loss : 0.63495, Training Acc : 0.711, Run Time : 6.06
INFO:root:2019-05-11 14:26:49, Epoch : 1, Step : 386, Training Loss : 0.64687, Training Acc : 0.703, Run Time : 37.83
INFO:root:2019-05-11 14:26:53, Epoch : 1, Step : 387, Training Loss : 0.62638, Training Acc : 0.731, Run Time : 3.50
INFO:root:2019-05-11 14:27:23, Epoch : 1, Step : 388, Training Loss : 0.63172, Training Acc : 0.747, Run Time : 29.96
INFO:root:2019-05-11 14:27:38, Epoch : 1, Step : 389, Training Loss : 0.63212, Training Acc : 0.725, Run Time : 14.76
INFO:root:2019-05-11 14:27:45, Epoch : 1, Step : 390, Training Loss : 0.61155, Training Acc : 0.786, Run Time : 6.91
INFO:root:2019-05-11 14:28:11, Epoch : 1, Step : 391, Training Loss : 0.61269, Training Acc : 0.775, Run Time : 26.33
INFO:root:2019-05-11 14:28:15, Epoch : 1, Step : 392, Training Loss : 0.61653, Training Acc : 0.728, Run Time : 3.89
INFO:root:2019-05-11 14:28:43, Epoch : 1, Step : 393, Training Loss : 0.61884, Training Acc : 0.697, Run Time : 28.63
INFO:root:2019-05-11 14:28:51, Epoch : 1, Step : 394, Training Loss : 0.63482, Training Acc : 0.658, Run Time : 7.61
INFO:root:2019-05-11 14:29:35, Epoch : 1, Step : 395, Training Loss : 0.62302, Training Acc : 0.661, Run Time : 44.22
INFO:root:2019-05-11 14:29:42, Epoch : 1, Step : 396, Training Loss : 0.60846, Training Acc : 0.717, Run Time : 6.56
INFO:root:2019-05-11 14:30:04, Epoch : 1, Step : 397, Training Loss : 0.60544, Training Acc : 0.739, Run Time : 22.05
INFO:root:2019-05-11 14:30:48, Epoch : 1, Step : 398, Training Loss : 0.62935, Training Acc : 0.672, Run Time : 43.86
INFO:root:2019-05-11 14:31:02, Epoch : 1, Step : 399, Training Loss : 0.58308, Training Acc : 0.786, Run Time : 14.56
INFO:root:2019-05-11 14:31:16, Epoch : 1, Step : 400, Training Loss : 0.57282, Training Acc : 0.786, Run Time : 14.18
INFO:root:2019-05-11 14:31:24, Epoch : 1, Step : 401, Training Loss : 0.57005, Training Acc : 0.783, Run Time : 7.12
INFO:root:2019-05-11 14:31:55, Epoch : 1, Step : 402, Training Loss : 0.55283, Training Acc : 0.769, Run Time : 31.82
INFO:root:2019-05-11 14:32:11, Epoch : 1, Step : 403, Training Loss : 0.53754, Training Acc : 0.758, Run Time : 15.94
INFO:root:2019-05-11 14:32:31, Epoch : 1, Step : 404, Training Loss : 0.56242, Training Acc : 0.703, Run Time : 20.05
INFO:root:2019-05-11 14:32:40, Epoch : 1, Step : 405, Training Loss : 0.55324, Training Acc : 0.714, Run Time : 8.97
INFO:root:2019-05-11 14:33:37, Epoch : 1, Step : 406, Training Loss : 0.54943, Training Acc : 0.706, Run Time : 56.20
INFO:root:2019-05-11 14:33:45, Epoch : 1, Step : 407, Training Loss : 0.54159, Training Acc : 0.711, Run Time : 8.61
INFO:root:2019-05-11 14:34:10, Epoch : 1, Step : 408, Training Loss : 0.59957, Training Acc : 0.664, Run Time : 25.24
INFO:root:2019-05-11 14:34:17, Epoch : 1, Step : 409, Training Loss : 0.61231, Training Acc : 0.667, Run Time : 6.49
INFO:root:2019-05-11 14:34:33, Epoch : 1, Step : 410, Training Loss : 0.64161, Training Acc : 0.642, Run Time : 16.54
INFO:root:2019-05-11 14:34:37, Epoch : 1, Step : 411, Training Loss : 0.70145, Training Acc : 0.597, Run Time : 4.00
INFO:root:2019-05-11 14:34:58, Epoch : 1, Step : 412, Training Loss : 0.65106, Training Acc : 0.628, Run Time : 20.99
INFO:root:2019-05-11 14:35:11, Epoch : 1, Step : 413, Training Loss : 0.69297, Training Acc : 0.614, Run Time : 12.62
INFO:root:2019-05-11 14:35:26, Epoch : 1, Step : 414, Training Loss : 0.72168, Training Acc : 0.456, Run Time : 15.20
INFO:root:2019-05-11 14:35:43, Epoch : 1, Step : 415, Training Loss : 0.68237, Training Acc : 0.492, Run Time : 16.84
INFO:root:2019-05-11 14:35:47, Epoch : 1, Step : 416, Training Loss : 0.64541, Training Acc : 0.517, Run Time : 3.81
INFO:root:2019-05-11 14:36:05, Epoch : 1, Step : 417, Training Loss : 0.63872, Training Acc : 0.519, Run Time : 17.97
INFO:root:2019-05-11 14:36:13, Epoch : 1, Step : 418, Training Loss : 0.66926, Training Acc : 0.531, Run Time : 7.99
INFO:root:2019-05-11 14:36:19, Epoch : 1, Step : 419, Training Loss : 0.66474, Training Acc : 0.572, Run Time : 5.99
INFO:root:2019-05-11 14:36:30, Epoch : 1, Step : 420, Training Loss : 0.62638, Training Acc : 0.608, Run Time : 11.30
INFO:root:2019-05-11 14:36:34, Epoch : 1, Step : 421, Training Loss : 0.63992, Training Acc : 0.633, Run Time : 4.25
INFO:root:2019-05-11 14:36:40, Epoch : 1, Step : 422, Training Loss : 0.65660, Training Acc : 0.611, Run Time : 5.11
INFO:root:2019-05-11 14:36:54, Epoch : 1, Step : 423, Training Loss : 0.62654, Training Acc : 0.633, Run Time : 14.84
INFO:root:2019-05-11 14:37:04, Epoch : 1, Step : 424, Training Loss : 0.61971, Training Acc : 0.689, Run Time : 9.82
INFO:root:2019-05-11 14:37:30, Epoch : 1, Step : 425, Training Loss : 0.59525, Training Acc : 0.656, Run Time : 25.51
INFO:root:2019-05-11 14:37:33, Epoch : 1, Step : 426, Training Loss : 0.60111, Training Acc : 0.642, Run Time : 3.18
INFO:root:2019-05-11 14:38:05, Epoch : 1, Step : 427, Training Loss : 0.58951, Training Acc : 0.669, Run Time : 31.82
INFO:root:2019-05-11 14:38:21, Epoch : 1, Step : 428, Training Loss : 0.57368, Training Acc : 0.686, Run Time : 16.29
INFO:root:2019-05-11 14:38:54, Epoch : 1, Step : 429, Training Loss : 0.60113, Training Acc : 0.578, Run Time : 32.74
INFO:root:2019-05-11 14:38:59, Epoch : 1, Step : 430, Training Loss : 0.58196, Training Acc : 0.603, Run Time : 5.51
INFO:root:2019-05-11 14:39:03, Epoch : 1, Step : 431, Training Loss : 0.57949, Training Acc : 0.575, Run Time : 3.77
INFO:root:2019-05-11 14:39:31, Epoch : 1, Step : 432, Training Loss : 0.53757, Training Acc : 0.642, Run Time : 28.12
INFO:root:2019-05-11 14:39:44, Epoch : 1, Step : 433, Training Loss : 0.65716, Training Acc : 0.492, Run Time : 12.64
INFO:root:2019-05-11 14:40:00, Epoch : 1, Step : 434, Training Loss : 0.54764, Training Acc : 0.672, Run Time : 15.98
INFO:root:2019-05-11 14:40:12, Epoch : 1, Step : 435, Training Loss : 0.51896, Training Acc : 0.681, Run Time : 12.13
INFO:root:2019-05-11 14:40:37, Epoch : 1, Step : 436, Training Loss : 0.49963, Training Acc : 0.661, Run Time : 24.77
INFO:root:2019-05-11 14:40:43, Epoch : 1, Step : 437, Training Loss : 0.49190, Training Acc : 0.683, Run Time : 6.07
INFO:root:2019-05-11 14:41:10, Epoch : 1, Step : 438, Training Loss : 0.51328, Training Acc : 0.686, Run Time : 27.15
INFO:root:2019-05-11 14:41:26, Epoch : 1, Step : 439, Training Loss : 0.51591, Training Acc : 0.672, Run Time : 16.22
INFO:root:2019-05-11 14:41:33, Epoch : 1, Step : 440, Training Loss : 0.47572, Training Acc : 0.722, Run Time : 7.12
INFO:root:2019-05-11 14:41:54, Epoch : 1, Step : 441, Training Loss : 0.56513, Training Acc : 0.650, Run Time : 20.71
INFO:root:2019-05-11 14:41:57, Epoch : 1, Step : 442, Training Loss : 0.48541, Training Acc : 0.706, Run Time : 2.80
INFO:root:2019-05-11 14:42:19, Epoch : 1, Step : 443, Training Loss : 0.50957, Training Acc : 0.669, Run Time : 22.36
INFO:root:2019-05-11 14:42:31, Epoch : 1, Step : 444, Training Loss : 0.45298, Training Acc : 0.761, Run Time : 12.05
INFO:root:2019-05-11 14:42:49, Epoch : 1, Step : 445, Training Loss : 0.48283, Training Acc : 0.711, Run Time : 17.57
INFO:root:2019-05-11 14:43:05, Epoch : 1, Step : 446, Training Loss : 0.51171, Training Acc : 0.686, Run Time : 16.75
INFO:root:2019-05-11 14:43:12, Epoch : 1, Step : 447, Training Loss : 0.46489, Training Acc : 0.797, Run Time : 6.35
INFO:root:2019-05-11 14:43:33, Epoch : 1, Step : 448, Training Loss : 0.50844, Training Acc : 0.733, Run Time : 20.71
INFO:root:2019-05-11 14:43:59, Epoch : 1, Step : 449, Training Loss : 0.47602, Training Acc : 0.783, Run Time : 26.07
INFO:root:2019-05-11 14:44:13, Epoch : 1, Step : 450, Training Loss : 0.53118, Training Acc : 0.758, Run Time : 14.83
INFO:root:2019-05-11 14:44:43, Epoch : 1, Step : 451, Training Loss : 0.50201, Training Acc : 0.750, Run Time : 29.25
INFO:root:2019-05-11 14:44:51, Epoch : 1, Step : 452, Training Loss : 0.54243, Training Acc : 0.719, Run Time : 7.93
INFO:root:2019-05-11 14:45:07, Epoch : 1, Step : 453, Training Loss : 0.47322, Training Acc : 0.803, Run Time : 16.67
INFO:root:2019-05-11 14:45:27, Epoch : 1, Step : 454, Training Loss : 0.49877, Training Acc : 0.769, Run Time : 19.33
INFO:root:2019-05-11 14:45:48, Epoch : 1, Step : 455, Training Loss : 0.53838, Training Acc : 0.719, Run Time : 21.52
INFO:root:2019-05-11 14:45:59, Epoch : 1, Step : 456, Training Loss : 0.53736, Training Acc : 0.711, Run Time : 10.45
INFO:root:2019-05-11 14:46:41, Epoch : 1, Step : 457, Training Loss : 0.49923, Training Acc : 0.764, Run Time : 41.96
INFO:root:2019-05-11 14:46:55, Epoch : 1, Step : 458, Training Loss : 0.47496, Training Acc : 0.792, Run Time : 14.71
INFO:root:2019-05-11 14:47:19, Epoch : 1, Step : 459, Training Loss : 0.55168, Training Acc : 0.714, Run Time : 23.71
INFO:root:2019-05-11 14:47:29, Epoch : 1, Step : 460, Training Loss : 0.50710, Training Acc : 0.717, Run Time : 9.66
INFO:root:2019-05-11 14:48:00, Epoch : 1, Step : 461, Training Loss : 0.49701, Training Acc : 0.736, Run Time : 31.83
INFO:root:2019-05-11 14:48:18, Epoch : 1, Step : 462, Training Loss : 0.48105, Training Acc : 0.761, Run Time : 17.59
INFO:root:2019-05-11 14:48:38, Epoch : 1, Step : 463, Training Loss : 0.48513, Training Acc : 0.833, Run Time : 20.31
INFO:root:2019-05-11 14:48:48, Epoch : 1, Step : 464, Training Loss : 0.49524, Training Acc : 0.750, Run Time : 9.62
INFO:root:2019-05-11 14:49:15, Epoch : 1, Step : 465, Training Loss : 0.45903, Training Acc : 0.803, Run Time : 26.91
INFO:root:2019-05-11 14:49:25, Epoch : 1, Step : 466, Training Loss : 0.52982, Training Acc : 0.703, Run Time : 10.42
INFO:root:2019-05-11 14:50:00, Epoch : 1, Step : 467, Training Loss : 0.49933, Training Acc : 0.775, Run Time : 34.41
INFO:root:2019-05-11 14:50:02, Epoch : 1, Step : 468, Training Loss : 0.50240, Training Acc : 0.731, Run Time : 2.64
INFO:root:2019-05-11 14:50:14, Epoch : 1, Step : 469, Training Loss : 0.46054, Training Acc : 0.794, Run Time : 11.66
INFO:root:2019-05-11 14:50:17, Epoch : 1, Step : 470, Training Loss : 0.43060, Training Acc : 0.817, Run Time : 2.78
INFO:root:2019-05-11 14:50:39, Epoch : 1, Step : 471, Training Loss : 0.44707, Training Acc : 0.761, Run Time : 21.78
INFO:root:2019-05-11 14:50:58, Epoch : 1, Step : 472, Training Loss : 0.57443, Training Acc : 0.714, Run Time : 18.94
INFO:root:2019-05-11 14:51:06, Epoch : 1, Step : 473, Training Loss : 0.46514, Training Acc : 0.803, Run Time : 8.66
INFO:root:2019-05-11 14:51:36, Epoch : 1, Step : 474, Training Loss : 0.48243, Training Acc : 0.742, Run Time : 30.27
INFO:root:2019-05-11 14:51:44, Epoch : 1, Step : 475, Training Loss : 0.44952, Training Acc : 0.775, Run Time : 7.81
INFO:root:2019-05-11 14:52:24, Epoch : 1, Step : 476, Training Loss : 0.45606, Training Acc : 0.775, Run Time : 39.81
INFO:root:2019-05-11 14:52:28, Epoch : 1, Step : 477, Training Loss : 0.47894, Training Acc : 0.797, Run Time : 4.07
INFO:root:2019-05-11 14:53:04, Epoch : 1, Step : 478, Training Loss : 0.46080, Training Acc : 0.786, Run Time : 35.43
INFO:root:2019-05-11 14:53:18, Epoch : 1, Step : 479, Training Loss : 0.46082, Training Acc : 0.794, Run Time : 14.20
INFO:root:2019-05-11 14:53:35, Epoch : 1, Step : 480, Training Loss : 0.42684, Training Acc : 0.775, Run Time : 16.88
INFO:root:2019-05-11 14:54:06, Epoch : 1, Step : 481, Training Loss : 0.42337, Training Acc : 0.811, Run Time : 31.51
INFO:root:2019-05-11 14:54:13, Epoch : 1, Step : 482, Training Loss : 0.45952, Training Acc : 0.794, Run Time : 6.62
INFO:root:2019-05-11 14:54:43, Epoch : 1, Step : 483, Training Loss : 0.44519, Training Acc : 0.800, Run Time : 30.31
INFO:root:2019-05-11 14:54:50, Epoch : 1, Step : 484, Training Loss : 0.49491, Training Acc : 0.731, Run Time : 6.44
INFO:root:2019-05-11 14:54:52, Epoch : 1, Step : 485, Training Loss : 0.47344, Training Acc : 0.797, Run Time : 2.68
INFO:root:2019-05-11 14:55:12, Epoch : 1, Step : 486, Training Loss : 0.44014, Training Acc : 0.783, Run Time : 19.63
INFO:root:2019-05-11 14:55:28, Epoch : 1, Step : 487, Training Loss : 0.47232, Training Acc : 0.750, Run Time : 16.64
INFO:root:2019-05-11 14:55:37, Epoch : 1, Step : 488, Training Loss : 0.47564, Training Acc : 0.744, Run Time : 8.48
INFO:root:2019-05-11 14:55:52, Epoch : 1, Step : 489, Training Loss : 0.42263, Training Acc : 0.803, Run Time : 14.89
INFO:root:2019-05-11 14:55:54, Epoch : 1, Step : 490, Training Loss : 0.39343, Training Acc : 0.856, Run Time : 2.56
INFO:root:2019-05-11 14:56:17, Epoch : 1, Step : 491, Training Loss : 0.34895, Training Acc : 0.889, Run Time : 22.45
INFO:root:2019-05-11 14:56:19, Epoch : 1, Step : 492, Training Loss : 0.34188, Training Acc : 0.883, Run Time : 2.57
INFO:root:2019-05-11 14:56:38, Epoch : 1, Step : 493, Training Loss : 0.34929, Training Acc : 0.883, Run Time : 18.53
INFO:root:2019-05-11 14:56:54, Epoch : 1, Step : 494, Training Loss : 0.36663, Training Acc : 0.853, Run Time : 16.20
INFO:root:2019-05-11 14:57:03, Epoch : 1, Step : 495, Training Loss : 0.33025, Training Acc : 0.908, Run Time : 9.22
INFO:root:2019-05-11 14:57:27, Epoch : 1, Step : 496, Training Loss : 0.41217, Training Acc : 0.842, Run Time : 23.90
INFO:root:2019-05-11 14:57:31, Epoch : 1, Step : 497, Training Loss : 0.33993, Training Acc : 0.875, Run Time : 3.97
INFO:root:2019-05-11 14:57:42, Epoch : 1, Step : 498, Training Loss : 0.25082, Training Acc : 0.931, Run Time : 11.15
INFO:root:2019-05-11 14:57:45, Epoch : 1, Step : 499, Training Loss : 0.27345, Training Acc : 0.906, Run Time : 2.44
INFO:root:2019-05-11 14:58:02, Epoch : 1, Step : 500, Training Loss : 0.29629, Training Acc : 0.936, Run Time : 17.12
INFO:root:2019-05-11 14:58:18, Epoch : 1, Step : 501, Training Loss : 0.47999, Training Acc : 0.778, Run Time : 16.40
INFO:root:2019-05-11 14:58:27, Epoch : 1, Step : 502, Training Loss : 0.50891, Training Acc : 0.753, Run Time : 8.95
INFO:root:2019-05-11 14:58:34, Epoch : 1, Step : 503, Training Loss : 0.50835, Training Acc : 0.750, Run Time : 6.26
INFO:root:2019-05-11 14:58:55, Epoch : 1, Step : 504, Training Loss : 0.54161, Training Acc : 0.739, Run Time : 21.80
INFO:root:2019-05-11 14:59:05, Epoch : 1, Step : 505, Training Loss : 0.50991, Training Acc : 0.733, Run Time : 9.49
INFO:root:2019-05-11 14:59:36, Epoch : 1, Step : 506, Training Loss : 0.52053, Training Acc : 0.742, Run Time : 31.30
INFO:root:2019-05-11 14:59:50, Epoch : 1, Step : 507, Training Loss : 0.70140, Training Acc : 0.700, Run Time : 14.03
INFO:root:2019-05-11 15:00:07, Epoch : 1, Step : 508, Training Loss : 0.61855, Training Acc : 0.703, Run Time : 17.08
INFO:root:2019-05-11 15:00:25, Epoch : 1, Step : 509, Training Loss : 0.67912, Training Acc : 0.656, Run Time : 17.88
INFO:root:2019-05-11 15:00:48, Epoch : 1, Step : 510, Training Loss : 0.71931, Training Acc : 0.628, Run Time : 22.52
INFO:root:2019-05-11 15:00:54, Epoch : 1, Step : 511, Training Loss : 0.65404, Training Acc : 0.631, Run Time : 5.99
INFO:root:2019-05-11 15:00:56, Epoch : 1, Step : 512, Training Loss : 0.63030, Training Acc : 0.650, Run Time : 2.63
INFO:root:2019-05-11 15:01:22, Epoch : 1, Step : 513, Training Loss : 0.57115, Training Acc : 0.689, Run Time : 25.57
INFO:root:2019-05-11 15:01:52, Epoch : 1, Step : 514, Training Loss : 0.61893, Training Acc : 0.642, Run Time : 29.87
INFO:root:2019-05-11 15:01:56, Epoch : 1, Step : 515, Training Loss : 0.62511, Training Acc : 0.633, Run Time : 4.57
INFO:root:2019-05-11 15:02:09, Epoch : 1, Step : 516, Training Loss : 0.64523, Training Acc : 0.656, Run Time : 13.02
INFO:root:2019-05-11 15:02:12, Epoch : 1, Step : 517, Training Loss : 0.64178, Training Acc : 0.661, Run Time : 2.88
INFO:root:2019-05-11 15:02:29, Epoch : 1, Step : 518, Training Loss : 0.63191, Training Acc : 0.664, Run Time : 16.52
INFO:root:2019-05-11 15:02:47, Epoch : 1, Step : 519, Training Loss : 0.65930, Training Acc : 0.644, Run Time : 18.22
INFO:root:2019-05-11 15:03:02, Epoch : 1, Step : 520, Training Loss : 0.68594, Training Acc : 0.633, Run Time : 15.35
INFO:root:2019-05-11 15:03:15, Epoch : 1, Step : 521, Training Loss : 0.58761, Training Acc : 0.700, Run Time : 12.41
INFO:root:2019-05-11 15:03:23, Epoch : 1, Step : 522, Training Loss : 0.59969, Training Acc : 0.672, Run Time : 8.67
INFO:root:2019-05-11 15:03:31, Epoch : 1, Step : 523, Training Loss : 0.61349, Training Acc : 0.689, Run Time : 7.43
INFO:root:2019-05-11 15:03:56, Epoch : 1, Step : 524, Training Loss : 0.72508, Training Acc : 0.561, Run Time : 24.94
INFO:root:2019-05-11 15:04:00, Epoch : 1, Step : 525, Training Loss : 0.58007, Training Acc : 0.664, Run Time : 3.88
INFO:root:2019-05-11 15:04:12, Epoch : 1, Step : 526, Training Loss : 0.60426, Training Acc : 0.700, Run Time : 12.08
INFO:root:2019-05-11 15:04:13, Epoch : 1, Step : 527, Training Loss : 0.55427, Training Acc : 0.731, Run Time : 1.39
INFO:root:2019-05-11 15:04:33, Epoch : 1, Step : 528, Training Loss : 0.54415, Training Acc : 0.711, Run Time : 19.51
INFO:root:2019-05-11 15:04:47, Epoch : 1, Step : 529, Training Loss : 0.58837, Training Acc : 0.694, Run Time : 14.03
INFO:root:2019-05-11 15:05:02, Epoch : 1, Step : 530, Training Loss : 0.54576, Training Acc : 0.669, Run Time : 15.70
INFO:root:2019-05-11 15:05:15, Epoch : 1, Step : 531, Training Loss : 0.53399, Training Acc : 0.728, Run Time : 12.97
INFO:root:2019-05-11 15:05:23, Epoch : 1, Step : 532, Training Loss : 0.52840, Training Acc : 0.700, Run Time : 7.60
INFO:root:2019-05-11 15:05:39, Epoch : 1, Step : 533, Training Loss : 0.55007, Training Acc : 0.681, Run Time : 16.61
INFO:root:2019-05-11 15:05:42, Epoch : 1, Step : 534, Training Loss : 0.50999, Training Acc : 0.722, Run Time : 2.44
INFO:root:2019-05-11 15:06:07, Epoch : 1, Step : 535, Training Loss : 0.47597, Training Acc : 0.750, Run Time : 24.63
INFO:root:2019-05-11 15:06:09, Epoch : 1, Step : 536, Training Loss : 0.44016, Training Acc : 0.747, Run Time : 2.80
INFO:root:2019-05-11 15:06:23, Epoch : 1, Step : 537, Training Loss : 0.46096, Training Acc : 0.767, Run Time : 13.88
INFO:root:2019-05-11 15:06:25, Epoch : 1, Step : 538, Training Loss : 0.43699, Training Acc : 0.772, Run Time : 1.91
INFO:root:2019-05-11 15:07:03, Epoch : 1, Step : 539, Training Loss : 0.45254, Training Acc : 0.764, Run Time : 37.56
INFO:root:2019-05-11 15:07:09, Epoch : 1, Step : 540, Training Loss : 0.40847, Training Acc : 0.828, Run Time : 6.57
INFO:root:2019-05-11 15:07:28, Epoch : 1, Step : 541, Training Loss : 0.35951, Training Acc : 0.886, Run Time : 18.89
INFO:root:2019-05-11 15:07:47, Epoch : 1, Step : 542, Training Loss : 0.38713, Training Acc : 0.864, Run Time : 18.34
INFO:root:2019-05-11 15:07:53, Epoch : 1, Step : 543, Training Loss : 0.36961, Training Acc : 0.844, Run Time : 6.87
INFO:root:2019-05-11 15:08:47, Epoch : 1, Step : 544, Training Loss : 0.38564, Training Acc : 0.833, Run Time : 53.32
INFO:root:2019-05-11 15:09:23, Epoch : 1, Step : 545, Training Loss : 0.38249, Training Acc : 0.819, Run Time : 35.94
INFO:root:2019-05-11 15:09:30, Epoch : 1, Step : 546, Training Loss : 0.41327, Training Acc : 0.797, Run Time : 7.54
INFO:root:2019-05-11 15:10:11, Epoch : 1, Step : 547, Training Loss : 0.37003, Training Acc : 0.828, Run Time : 41.21
INFO:root:2019-05-11 15:10:21, Epoch : 1, Step : 548, Training Loss : 0.34488, Training Acc : 0.836, Run Time : 10.08
INFO:root:2019-05-11 15:11:03, Epoch : 1, Step : 549, Training Loss : 0.34873, Training Acc : 0.831, Run Time : 42.02
INFO:root:2019-05-11 15:11:33, Epoch : 1, Step : 550, Training Loss : 0.43465, Training Acc : 0.783, Run Time : 29.35
INFO:root:2019-05-11 15:12:11, Epoch : 1, Step : 551, Training Loss : 0.41611, Training Acc : 0.792, Run Time : 38.65
INFO:root:2019-05-11 15:12:46, Epoch : 1, Step : 552, Training Loss : 0.40059, Training Acc : 0.800, Run Time : 34.74
INFO:root:2019-05-11 15:13:16, Epoch : 1, Step : 553, Training Loss : 0.45387, Training Acc : 0.781, Run Time : 29.48
INFO:root:2019-05-11 15:13:22, Epoch : 1, Step : 554, Training Loss : 0.49171, Training Acc : 0.764, Run Time : 6.59
INFO:root:2019-05-11 15:13:55, Epoch : 1, Step : 555, Training Loss : 0.45845, Training Acc : 0.769, Run Time : 32.75
INFO:root:2019-05-11 15:14:22, Epoch : 1, Step : 556, Training Loss : 0.45652, Training Acc : 0.772, Run Time : 27.03
INFO:root:2019-05-11 15:14:37, Epoch : 1, Step : 557, Training Loss : 0.48411, Training Acc : 0.778, Run Time : 15.44
INFO:root:2019-05-11 15:14:44, Epoch : 1, Step : 558, Training Loss : 0.44340, Training Acc : 0.803, Run Time : 6.59
INFO:root:2019-05-11 15:15:10, Epoch : 1, Step : 559, Training Loss : 0.42037, Training Acc : 0.778, Run Time : 26.10
INFO:root:2019-05-11 15:15:25, Epoch : 1, Step : 560, Training Loss : 0.42874, Training Acc : 0.803, Run Time : 15.26
INFO:root:2019-05-11 15:15:32, Epoch : 1, Step : 561, Training Loss : 0.43863, Training Acc : 0.792, Run Time : 6.57
INFO:root:2019-05-11 15:16:05, Epoch : 1, Step : 562, Training Loss : 0.42848, Training Acc : 0.800, Run Time : 32.69
INFO:root:2019-05-11 15:16:13, Epoch : 1, Step : 563, Training Loss : 0.44380, Training Acc : 0.750, Run Time : 8.07
INFO:root:2019-05-11 15:16:41, Epoch : 1, Step : 564, Training Loss : 0.40872, Training Acc : 0.817, Run Time : 28.25
INFO:root:2019-05-11 15:16:43, Epoch : 1, Step : 565, Training Loss : 0.37734, Training Acc : 0.828, Run Time : 2.28
INFO:root:2019-05-11 15:17:01, Epoch : 1, Step : 566, Training Loss : 0.36532, Training Acc : 0.844, Run Time : 17.30
INFO:root:2019-05-11 15:17:18, Epoch : 1, Step : 567, Training Loss : 0.36753, Training Acc : 0.822, Run Time : 17.36
INFO:root:2019-05-11 15:18:17, Epoch : 1, Step : 568, Training Loss : 0.35375, Training Acc : 0.858, Run Time : 59.46
INFO:root:2019-05-11 15:18:48, Epoch : 1, Step : 569, Training Loss : 0.37764, Training Acc : 0.867, Run Time : 30.65
INFO:root:2019-05-11 15:19:15, Epoch : 1, Step : 570, Training Loss : 0.36600, Training Acc : 0.853, Run Time : 26.86
INFO:root:2019-05-11 15:19:24, Epoch : 1, Step : 571, Training Loss : 0.37667, Training Acc : 0.786, Run Time : 9.20
INFO:root:2019-05-11 15:19:47, Epoch : 1, Step : 572, Training Loss : 0.35917, Training Acc : 0.847, Run Time : 22.63
INFO:root:2019-05-11 15:19:50, Epoch : 1, Step : 573, Training Loss : 0.32092, Training Acc : 0.869, Run Time : 3.11
INFO:root:2019-05-11 15:20:15, Epoch : 1, Step : 574, Training Loss : 0.37399, Training Acc : 0.828, Run Time : 24.77
INFO:root:2019-05-11 15:20:18, Epoch : 1, Step : 575, Training Loss : 0.40441, Training Acc : 0.783, Run Time : 2.99
INFO:root:2019-05-11 15:20:20, Epoch : 1, Step : 576, Training Loss : 0.44334, Training Acc : 0.800, Run Time : 2.44
INFO:root:2019-05-11 15:21:00, Epoch : 1, Step : 577, Training Loss : 0.60168, Training Acc : 0.706, Run Time : 40.29
INFO:root:2019-05-11 15:21:29, Epoch : 1, Step : 578, Training Loss : 0.44166, Training Acc : 0.856, Run Time : 28.84
INFO:root:2019-05-11 15:21:38, Epoch : 1, Step : 579, Training Loss : 0.48866, Training Acc : 0.828, Run Time : 8.36
INFO:root:2019-05-11 15:21:51, Epoch : 1, Step : 580, Training Loss : 0.54386, Training Acc : 0.764, Run Time : 13.83
INFO:root:2019-05-11 15:22:05, Epoch : 1, Step : 581, Training Loss : 0.53908, Training Acc : 0.744, Run Time : 13.32
INFO:root:2019-05-11 15:22:10, Epoch : 1, Step : 582, Training Loss : 0.43546, Training Acc : 0.800, Run Time : 5.78
INFO:root:2019-05-11 15:22:30, Epoch : 1, Step : 583, Training Loss : 0.42411, Training Acc : 0.808, Run Time : 19.07
INFO:root:2019-05-11 15:22:45, Epoch : 1, Step : 584, Training Loss : 0.42679, Training Acc : 0.825, Run Time : 15.27
INFO:root:2019-05-11 15:23:01, Epoch : 1, Step : 585, Training Loss : 0.42773, Training Acc : 0.817, Run Time : 16.25
INFO:root:2019-05-11 15:23:45, Epoch : 1, Step : 586, Training Loss : 0.45618, Training Acc : 0.781, Run Time : 43.62
INFO:root:2019-05-11 15:24:10, Epoch : 1, Step : 587, Training Loss : 0.43609, Training Acc : 0.781, Run Time : 25.62
INFO:root:2019-05-11 15:24:14, Epoch : 1, Step : 588, Training Loss : 0.40457, Training Acc : 0.797, Run Time : 3.92
INFO:root:2019-05-11 15:24:33, Epoch : 1, Step : 589, Training Loss : 0.34727, Training Acc : 0.878, Run Time : 18.42
INFO:root:2019-05-11 15:24:53, Epoch : 1, Step : 590, Training Loss : 0.38085, Training Acc : 0.808, Run Time : 19.86
INFO:root:2019-05-11 15:25:07, Epoch : 1, Step : 591, Training Loss : 0.35334, Training Acc : 0.844, Run Time : 14.66
INFO:root:2019-05-11 15:25:18, Epoch : 1, Step : 592, Training Loss : 0.37005, Training Acc : 0.825, Run Time : 10.42
INFO:root:2019-05-11 15:25:47, Epoch : 1, Step : 593, Training Loss : 0.38933, Training Acc : 0.850, Run Time : 29.13
INFO:root:2019-05-11 15:26:02, Epoch : 1, Step : 594, Training Loss : 0.36297, Training Acc : 0.844, Run Time : 15.38
INFO:root:2019-05-11 15:26:17, Epoch : 1, Step : 595, Training Loss : 0.33437, Training Acc : 0.836, Run Time : 14.55
INFO:root:2019-05-11 15:26:33, Epoch : 1, Step : 596, Training Loss : 0.30514, Training Acc : 0.869, Run Time : 16.00
INFO:root:2019-05-11 15:26:50, Epoch : 1, Step : 597, Training Loss : 0.34624, Training Acc : 0.875, Run Time : 17.20
INFO:root:2019-05-11 15:27:08, Epoch : 1, Step : 598, Training Loss : 0.33657, Training Acc : 0.853, Run Time : 18.43
INFO:root:2019-05-11 15:27:51, Epoch : 1, Step : 599, Training Loss : 0.34369, Training Acc : 0.844, Run Time : 43.03
INFO:root:2019-05-11 15:27:59, Epoch : 1, Step : 600, Training Loss : 0.29709, Training Acc : 0.867, Run Time : 7.97
INFO:root:2019-05-11 15:28:24, Epoch : 1, Step : 601, Training Loss : 0.36126, Training Acc : 0.864, Run Time : 24.91
INFO:root:2019-05-11 15:28:37, Epoch : 1, Step : 602, Training Loss : 0.32627, Training Acc : 0.875, Run Time : 12.59
INFO:root:2019-05-11 15:28:57, Epoch : 1, Step : 603, Training Loss : 0.47010, Training Acc : 0.772, Run Time : 20.50
INFO:root:2019-05-11 15:29:04, Epoch : 1, Step : 604, Training Loss : 0.51718, Training Acc : 0.742, Run Time : 7.06
INFO:root:2019-05-11 15:29:41, Epoch : 1, Step : 605, Training Loss : 0.53197, Training Acc : 0.722, Run Time : 36.97
INFO:root:2019-05-11 15:29:54, Epoch : 1, Step : 606, Training Loss : 0.52309, Training Acc : 0.717, Run Time : 12.47
INFO:root:2019-05-11 15:30:28, Epoch : 1, Step : 607, Training Loss : 0.50944, Training Acc : 0.714, Run Time : 34.06
INFO:root:2019-05-11 15:30:30, Epoch : 1, Step : 608, Training Loss : 0.54212, Training Acc : 0.728, Run Time : 2.62
INFO:root:2019-05-11 15:31:02, Epoch : 1, Step : 609, Training Loss : 0.50351, Training Acc : 0.711, Run Time : 31.45
INFO:root:2019-05-11 15:31:12, Epoch : 1, Step : 610, Training Loss : 0.49978, Training Acc : 0.794, Run Time : 9.84
INFO:root:2019-05-11 15:31:30, Epoch : 1, Step : 611, Training Loss : 0.49650, Training Acc : 0.769, Run Time : 18.71
INFO:root:2019-05-11 15:31:49, Epoch : 1, Step : 612, Training Loss : 0.40962, Training Acc : 0.836, Run Time : 18.06
INFO:root:2019-05-11 15:32:06, Epoch : 1, Step : 613, Training Loss : 0.47672, Training Acc : 0.711, Run Time : 17.40
INFO:root:2019-05-11 15:32:33, Epoch : 1, Step : 614, Training Loss : 0.52444, Training Acc : 0.675, Run Time : 26.97
INFO:root:2019-05-11 15:32:59, Epoch : 1, Step : 615, Training Loss : 0.63812, Training Acc : 0.672, Run Time : 26.60
INFO:root:2019-05-11 15:33:14, Epoch : 1, Step : 616, Training Loss : 0.55050, Training Acc : 0.747, Run Time : 14.89
INFO:root:2019-05-11 15:33:20, Epoch : 1, Step : 617, Training Loss : 0.58318, Training Acc : 0.664, Run Time : 6.03
INFO:root:2019-05-11 15:34:00, Epoch : 1, Step : 618, Training Loss : 0.55423, Training Acc : 0.697, Run Time : 39.10
INFO:root:2019-05-11 15:34:06, Epoch : 1, Step : 619, Training Loss : 0.53504, Training Acc : 0.731, Run Time : 6.72
INFO:root:2019-05-11 15:35:19, Epoch : 1, Step : 620, Training Loss : 0.56480, Training Acc : 0.656, Run Time : 73.25
INFO:root:2019-05-11 15:35:40, Epoch : 1, Step : 621, Training Loss : 0.64082, Training Acc : 0.617, Run Time : 20.96
INFO:root:2019-05-11 15:35:59, Epoch : 1, Step : 622, Training Loss : 0.60412, Training Acc : 0.625, Run Time : 18.74
INFO:root:2019-05-11 15:36:26, Epoch : 1, Step : 623, Training Loss : 0.63790, Training Acc : 0.622, Run Time : 26.33
INFO:root:2019-05-11 15:36:33, Epoch : 1, Step : 624, Training Loss : 0.60139, Training Acc : 0.636, Run Time : 7.05
INFO:root:2019-05-11 15:37:12, Epoch : 1, Step : 625, Training Loss : 0.64332, Training Acc : 0.614, Run Time : 38.97
INFO:root:2019-05-11 15:37:49, Epoch : 1, Step : 626, Training Loss : 0.63598, Training Acc : 0.614, Run Time : 37.33
INFO:root:2019-05-11 15:38:28, Epoch : 1, Step : 627, Training Loss : 0.65036, Training Acc : 0.661, Run Time : 38.85
INFO:root:2019-05-11 15:38:35, Epoch : 1, Step : 628, Training Loss : 0.61412, Training Acc : 0.664, Run Time : 7.47
INFO:root:2019-05-11 15:38:46, Epoch : 1, Step : 629, Training Loss : 0.63846, Training Acc : 0.683, Run Time : 10.71
INFO:root:2019-05-11 15:39:11, Epoch : 1, Step : 630, Training Loss : 0.59632, Training Acc : 0.719, Run Time : 24.85
INFO:root:2019-05-11 15:39:15, Epoch : 1, Step : 631, Training Loss : 0.69104, Training Acc : 0.581, Run Time : 4.13
INFO:root:2019-05-11 15:39:38, Epoch : 1, Step : 632, Training Loss : 0.72816, Training Acc : 0.564, Run Time : 23.61
INFO:root:2019-05-11 15:40:10, Epoch : 1, Step : 633, Training Loss : 0.63479, Training Acc : 0.667, Run Time : 31.62
INFO:root:2019-05-11 15:40:32, Epoch : 1, Step : 634, Training Loss : 0.57784, Training Acc : 0.769, Run Time : 21.53
INFO:root:2019-05-11 15:40:48, Epoch : 1, Step : 635, Training Loss : 0.56326, Training Acc : 0.744, Run Time : 16.42
INFO:root:2019-05-11 15:40:56, Epoch : 1, Step : 636, Training Loss : 0.58051, Training Acc : 0.747, Run Time : 7.91
INFO:root:2019-05-11 15:41:27, Epoch : 1, Step : 637, Training Loss : 0.57032, Training Acc : 0.700, Run Time : 31.01
INFO:root:2019-05-11 15:41:34, Epoch : 1, Step : 638, Training Loss : 0.60804, Training Acc : 0.658, Run Time : 7.29
INFO:root:2019-05-11 15:42:04, Epoch : 1, Step : 639, Training Loss : 0.62200, Training Acc : 0.683, Run Time : 29.78
INFO:root:2019-05-11 15:42:20, Epoch : 1, Step : 640, Training Loss : 0.61444, Training Acc : 0.681, Run Time : 15.92
INFO:root:2019-05-11 15:42:58, Epoch : 1, Step : 641, Training Loss : 0.59873, Training Acc : 0.672, Run Time : 37.80
INFO:root:2019-05-11 15:43:05, Epoch : 1, Step : 642, Training Loss : 0.61268, Training Acc : 0.664, Run Time : 6.94
INFO:root:2019-05-11 15:43:34, Epoch : 1, Step : 643, Training Loss : 0.56398, Training Acc : 0.644, Run Time : 29.31
INFO:root:2019-05-11 15:43:43, Epoch : 1, Step : 644, Training Loss : 0.60723, Training Acc : 0.622, Run Time : 9.16
INFO:root:2019-05-11 15:44:00, Epoch : 1, Step : 645, Training Loss : 0.59662, Training Acc : 0.672, Run Time : 16.93
INFO:root:2019-05-11 15:44:04, Epoch : 1, Step : 646, Training Loss : 0.55251, Training Acc : 0.731, Run Time : 3.68
INFO:root:2019-05-11 15:44:18, Epoch : 1, Step : 647, Training Loss : 0.58018, Training Acc : 0.719, Run Time : 14.53
INFO:root:2019-05-11 15:44:21, Epoch : 1, Step : 648, Training Loss : 0.61461, Training Acc : 0.644, Run Time : 2.82
INFO:root:2019-05-11 15:44:48, Epoch : 1, Step : 649, Training Loss : 0.64659, Training Acc : 0.642, Run Time : 26.46
INFO:root:2019-05-11 15:44:51, Epoch : 1, Step : 650, Training Loss : 0.61561, Training Acc : 0.664, Run Time : 3.64
INFO:root:2019-05-11 15:45:13, Epoch : 1, Step : 651, Training Loss : 0.63879, Training Acc : 0.683, Run Time : 21.84
INFO:root:2019-05-11 15:45:19, Epoch : 1, Step : 652, Training Loss : 0.60103, Training Acc : 0.706, Run Time : 5.88
INFO:root:2019-05-11 15:45:40, Epoch : 1, Step : 653, Training Loss : 0.60017, Training Acc : 0.733, Run Time : 21.43
INFO:root:2019-05-11 15:46:16, Epoch : 1, Step : 654, Training Loss : 0.58578, Training Acc : 0.747, Run Time : 35.95
INFO:root:2019-05-11 15:46:24, Epoch : 1, Step : 655, Training Loss : 0.59038, Training Acc : 0.756, Run Time : 7.99
INFO:root:2019-05-11 15:47:00, Epoch : 1, Step : 656, Training Loss : 0.60066, Training Acc : 0.692, Run Time : 35.38
INFO:root:2019-05-11 15:47:29, Epoch : 1, Step : 657, Training Loss : 0.57166, Training Acc : 0.714, Run Time : 28.86
INFO:root:2019-05-11 15:48:08, Epoch : 1, Step : 658, Training Loss : 0.59736, Training Acc : 0.661, Run Time : 39.92
INFO:root:2019-05-11 15:48:36, Epoch : 1, Step : 659, Training Loss : 0.53978, Training Acc : 0.775, Run Time : 27.91
INFO:root:2019-05-11 15:48:43, Epoch : 1, Step : 660, Training Loss : 0.55909, Training Acc : 0.772, Run Time : 6.99
INFO:root:2019-05-11 15:49:04, Epoch : 1, Step : 661, Training Loss : 0.54192, Training Acc : 0.764, Run Time : 20.44
INFO:root:2019-05-11 15:49:11, Epoch : 1, Step : 662, Training Loss : 0.51652, Training Acc : 0.786, Run Time : 7.07
INFO:root:2019-05-11 15:49:41, Epoch : 1, Step : 663, Training Loss : 0.52861, Training Acc : 0.761, Run Time : 29.76
INFO:root:2019-05-11 15:49:57, Epoch : 1, Step : 664, Training Loss : 0.55900, Training Acc : 0.733, Run Time : 16.50
INFO:root:2019-05-11 15:50:19, Epoch : 1, Step : 665, Training Loss : 0.56011, Training Acc : 0.739, Run Time : 21.47
INFO:root:2019-05-11 15:50:39, Epoch : 1, Step : 666, Training Loss : 0.54829, Training Acc : 0.739, Run Time : 19.90
INFO:root:2019-05-11 15:50:46, Epoch : 1, Step : 667, Training Loss : 0.46779, Training Acc : 0.831, Run Time : 7.07
INFO:root:2019-05-11 15:51:23, Epoch : 1, Step : 668, Training Loss : 0.44636, Training Acc : 0.831, Run Time : 37.06
INFO:root:2019-05-11 15:51:37, Epoch : 1, Step : 669, Training Loss : 0.44959, Training Acc : 0.833, Run Time : 14.62
INFO:root:2019-05-11 15:52:15, Epoch : 1, Step : 670, Training Loss : 0.43671, Training Acc : 0.847, Run Time : 37.41
INFO:root:2019-05-11 15:52:24, Epoch : 1, Step : 671, Training Loss : 0.45433, Training Acc : 0.814, Run Time : 9.22
INFO:root:2019-05-11 15:53:38, Epoch : 1, Step : 672, Training Loss : 0.41977, Training Acc : 0.814, Run Time : 73.77
INFO:root:2019-05-11 15:53:47, Epoch : 1, Step : 673, Training Loss : 0.32879, Training Acc : 0.878, Run Time : 9.54
INFO:root:2019-05-11 15:55:00, Epoch : 1, Step : 674, Training Loss : 0.36382, Training Acc : 0.881, Run Time : 73.00
INFO:root:2019-05-11 15:55:51, Epoch : 1, Step : 675, Training Loss : 0.34764, Training Acc : 0.897, Run Time : 51.00
INFO:root:2019-05-11 15:56:10, Epoch : 1, Step : 676, Training Loss : 0.39421, Training Acc : 0.878, Run Time : 18.56
INFO:root:2019-05-11 15:56:51, Epoch : 1, Step : 677, Training Loss : 0.42627, Training Acc : 0.825, Run Time : 40.73
INFO:root:2019-05-11 15:56:58, Epoch : 1, Step : 678, Training Loss : 0.40616, Training Acc : 0.831, Run Time : 7.93
INFO:root:2019-05-11 15:57:37, Epoch : 1, Step : 679, Training Loss : 0.46495, Training Acc : 0.808, Run Time : 38.77
INFO:root:2019-05-11 15:57:54, Epoch : 1, Step : 680, Training Loss : 0.42166, Training Acc : 0.828, Run Time : 17.24
INFO:root:2019-05-11 15:58:16, Epoch : 1, Step : 681, Training Loss : 0.52360, Training Acc : 0.789, Run Time : 21.45
INFO:root:2019-05-11 15:58:24, Epoch : 1, Step : 682, Training Loss : 0.57312, Training Acc : 0.769, Run Time : 7.86
INFO:root:2019-05-11 15:59:19, Epoch : 1, Step : 683, Training Loss : 0.57165, Training Acc : 0.764, Run Time : 55.64
INFO:root:2019-05-11 15:59:36, Epoch : 1, Step : 684, Training Loss : 0.60871, Training Acc : 0.736, Run Time : 16.42
INFO:root:2019-05-11 15:59:58, Epoch : 1, Step : 685, Training Loss : 0.65424, Training Acc : 0.708, Run Time : 22.41
INFO:root:2019-05-11 16:00:06, Epoch : 1, Step : 686, Training Loss : 0.57495, Training Acc : 0.733, Run Time : 7.40
INFO:root:2019-05-11 16:00:31, Epoch : 1, Step : 687, Training Loss : 0.48377, Training Acc : 0.756, Run Time : 25.58
INFO:root:2019-05-11 16:00:51, Epoch : 1, Step : 688, Training Loss : 0.44211, Training Acc : 0.786, Run Time : 19.47
INFO:root:2019-05-11 16:00:57, Epoch : 1, Step : 689, Training Loss : 0.47493, Training Acc : 0.764, Run Time : 5.83
INFO:root:2019-05-11 16:01:28, Epoch : 1, Step : 690, Training Loss : 0.47739, Training Acc : 0.756, Run Time : 31.68
INFO:root:2019-05-11 16:01:46, Epoch : 1, Step : 691, Training Loss : 0.47620, Training Acc : 0.769, Run Time : 17.68
INFO:root:2019-05-11 16:02:07, Epoch : 1, Step : 692, Training Loss : 0.44382, Training Acc : 0.792, Run Time : 20.91
INFO:root:2019-05-11 16:02:21, Epoch : 1, Step : 693, Training Loss : 0.46073, Training Acc : 0.767, Run Time : 14.12
INFO:root:2019-05-11 16:02:30, Epoch : 1, Step : 694, Training Loss : 0.43140, Training Acc : 0.828, Run Time : 9.05
INFO:root:2019-05-11 16:03:05, Epoch : 1, Step : 695, Training Loss : 0.45378, Training Acc : 0.808, Run Time : 34.58
INFO:root:2019-05-11 16:03:14, Epoch : 1, Step : 696, Training Loss : 0.43050, Training Acc : 0.822, Run Time : 9.68
INFO:root:2019-05-11 16:03:35, Epoch : 1, Step : 697, Training Loss : 0.39467, Training Acc : 0.867, Run Time : 20.54
INFO:root:2019-05-11 16:03:37, Epoch : 1, Step : 698, Training Loss : 0.37707, Training Acc : 0.897, Run Time : 2.52
INFO:root:2019-05-11 16:03:57, Epoch : 1, Step : 699, Training Loss : 0.31991, Training Acc : 0.919, Run Time : 19.45
INFO:root:2019-05-11 16:04:22, Epoch : 1, Step : 700, Training Loss : 0.34073, Training Acc : 0.928, Run Time : 24.92
INFO:root:2019-05-11 16:04:47, Epoch : 1, Step : 701, Training Loss : 0.35958, Training Acc : 0.844, Run Time : 25.38
INFO:root:2019-05-11 16:04:49, Epoch : 1, Step : 702, Training Loss : 0.37091, Training Acc : 0.833, Run Time : 2.15
INFO:root:2019-05-11 16:04:52, Epoch : 1, Step : 703, Training Loss : 0.37807, Training Acc : 0.842, Run Time : 3.22
INFO:root:2019-05-11 16:05:13, Epoch : 1, Step : 704, Training Loss : 0.35831, Training Acc : 0.833, Run Time : 20.37
INFO:root:2019-05-11 16:05:22, Epoch : 1, Step : 705, Training Loss : 0.34361, Training Acc : 0.842, Run Time : 9.30
INFO:root:2019-05-11 16:05:46, Epoch : 1, Step : 706, Training Loss : 0.35769, Training Acc : 0.836, Run Time : 24.25
INFO:root:2019-05-11 16:05:51, Epoch : 1, Step : 707, Training Loss : 0.32815, Training Acc : 0.844, Run Time : 4.62
INFO:root:2019-05-11 16:06:12, Epoch : 1, Step : 708, Training Loss : 0.35815, Training Acc : 0.822, Run Time : 20.59
INFO:root:2019-05-11 16:06:16, Epoch : 1, Step : 709, Training Loss : 0.33527, Training Acc : 0.842, Run Time : 4.29
INFO:root:2019-05-11 16:06:43, Epoch : 1, Step : 710, Training Loss : 0.37358, Training Acc : 0.828, Run Time : 26.91
INFO:root:2019-05-11 16:06:58, Epoch : 1, Step : 711, Training Loss : 0.40107, Training Acc : 0.808, Run Time : 14.86
INFO:root:2019-05-11 16:07:14, Epoch : 1, Step : 712, Training Loss : 0.38191, Training Acc : 0.828, Run Time : 16.50
INFO:root:2019-05-11 16:07:22, Epoch : 1, Step : 713, Training Loss : 0.37701, Training Acc : 0.839, Run Time : 7.50
INFO:root:2019-05-11 16:07:48, Epoch : 1, Step : 714, Training Loss : 0.37926, Training Acc : 0.828, Run Time : 26.01
INFO:root:2019-05-11 16:08:03, Epoch : 1, Step : 715, Training Loss : 0.47347, Training Acc : 0.792, Run Time : 14.94
INFO:root:2019-05-11 16:08:23, Epoch : 1, Step : 716, Training Loss : 0.47025, Training Acc : 0.778, Run Time : 20.43
INFO:root:2019-05-11 16:08:38, Epoch : 1, Step : 717, Training Loss : 0.44731, Training Acc : 0.778, Run Time : 14.76
INFO:root:2019-05-11 16:08:52, Epoch : 1, Step : 718, Training Loss : 0.42799, Training Acc : 0.792, Run Time : 14.69
INFO:root:2019-05-11 16:09:02, Epoch : 1, Step : 719, Training Loss : 0.45508, Training Acc : 0.761, Run Time : 9.73
INFO:root:2019-05-11 16:09:27, Epoch : 1, Step : 720, Training Loss : 0.50079, Training Acc : 0.758, Run Time : 24.71
INFO:root:2019-05-11 16:09:47, Epoch : 1, Step : 721, Training Loss : 0.52665, Training Acc : 0.767, Run Time : 20.23
INFO:root:2019-05-11 16:10:06, Epoch : 1, Step : 722, Training Loss : 0.43678, Training Acc : 0.806, Run Time : 19.41
INFO:root:2019-05-11 16:10:20, Epoch : 1, Step : 723, Training Loss : 0.45070, Training Acc : 0.786, Run Time : 14.02
INFO:root:2019-05-11 16:10:31, Epoch : 1, Step : 724, Training Loss : 0.39985, Training Acc : 0.822, Run Time : 10.42
INFO:root:2019-05-11 16:10:47, Epoch : 1, Step : 725, Training Loss : 0.42684, Training Acc : 0.811, Run Time : 16.54
INFO:root:2019-05-11 16:10:50, Epoch : 1, Step : 726, Training Loss : 0.43476, Training Acc : 0.819, Run Time : 2.66
INFO:root:2019-05-11 16:11:19, Epoch : 1, Step : 727, Training Loss : 0.41959, Training Acc : 0.847, Run Time : 28.39
INFO:root:2019-05-11 16:11:24, Epoch : 1, Step : 728, Training Loss : 0.39722, Training Acc : 0.828, Run Time : 5.07
INFO:root:2019-05-11 16:11:49, Epoch : 1, Step : 729, Training Loss : 0.39824, Training Acc : 0.850, Run Time : 25.74
INFO:root:2019-05-11 16:11:59, Epoch : 1, Step : 730, Training Loss : 0.41822, Training Acc : 0.811, Run Time : 9.37
INFO:root:2019-05-11 16:12:28, Epoch : 1, Step : 731, Training Loss : 0.43755, Training Acc : 0.800, Run Time : 28.85
INFO:root:2019-05-11 16:12:31, Epoch : 1, Step : 732, Training Loss : 0.40740, Training Acc : 0.847, Run Time : 3.73
INFO:root:2019-05-11 16:12:33, Epoch : 1, Step : 733, Training Loss : 0.41755, Training Acc : 0.839, Run Time : 2.09
INFO:root:2019-05-11 16:12:52, Epoch : 1, Step : 734, Training Loss : 0.43308, Training Acc : 0.831, Run Time : 18.67
INFO:root:2019-05-11 16:13:28, Epoch : 1, Step : 735, Training Loss : 0.42049, Training Acc : 0.842, Run Time : 35.97
INFO:root:2019-05-11 16:13:32, Epoch : 1, Step : 736, Training Loss : 0.39059, Training Acc : 0.850, Run Time : 3.58
INFO:root:2019-05-11 16:14:01, Epoch : 1, Step : 737, Training Loss : 0.37461, Training Acc : 0.853, Run Time : 29.07
INFO:root:2019-05-11 16:14:14, Epoch : 1, Step : 738, Training Loss : 0.30858, Training Acc : 0.892, Run Time : 13.30
INFO:root:2019-05-11 16:14:30, Epoch : 1, Step : 739, Training Loss : 0.32478, Training Acc : 0.872, Run Time : 15.58
INFO:root:2019-05-11 16:14:36, Epoch : 1, Step : 740, Training Loss : 0.33597, Training Acc : 0.864, Run Time : 6.74
INFO:root:2019-05-11 16:15:01, Epoch : 1, Step : 741, Training Loss : 0.28240, Training Acc : 0.908, Run Time : 24.46
INFO:root:2019-05-11 16:15:22, Epoch : 1, Step : 742, Training Loss : 0.33318, Training Acc : 0.839, Run Time : 21.20
INFO:root:2019-05-11 16:15:30, Epoch : 1, Step : 743, Training Loss : 0.30499, Training Acc : 0.867, Run Time : 8.46
INFO:root:2019-05-11 16:15:54, Epoch : 1, Step : 744, Training Loss : 0.29065, Training Acc : 0.875, Run Time : 23.48
INFO:root:2019-05-11 16:15:59, Epoch : 1, Step : 745, Training Loss : 0.31108, Training Acc : 0.856, Run Time : 5.37
INFO:root:2019-05-11 16:16:45, Epoch : 1, Step : 746, Training Loss : 0.32746, Training Acc : 0.853, Run Time : 45.28
INFO:root:2019-05-11 16:16:51, Epoch : 1, Step : 747, Training Loss : 0.32312, Training Acc : 0.847, Run Time : 6.52
INFO:root:2019-05-11 16:17:20, Epoch : 1, Step : 748, Training Loss : 0.32045, Training Acc : 0.881, Run Time : 29.30
INFO:root:2019-05-11 16:17:23, Epoch : 1, Step : 749, Training Loss : 0.31058, Training Acc : 0.878, Run Time : 2.46
INFO:root:2019-05-11 16:17:29, Epoch : 1, Step : 750, Training Loss : 0.30502, Training Acc : 0.861, Run Time : 5.79
INFO:root:2019-05-11 16:17:47, Epoch : 1, Step : 751, Training Loss : 0.35659, Training Acc : 0.847, Run Time : 18.37
INFO:root:2019-05-11 16:18:12, Epoch : 1, Step : 752, Training Loss : 0.34492, Training Acc : 0.858, Run Time : 24.78
INFO:root:2019-05-11 16:18:14, Epoch : 1, Step : 753, Training Loss : 0.34832, Training Acc : 0.856, Run Time : 2.62
INFO:root:2019-05-11 16:18:42, Epoch : 1, Step : 754, Training Loss : 0.31889, Training Acc : 0.864, Run Time : 27.30
INFO:root:2019-05-11 16:18:56, Epoch : 1, Step : 755, Training Loss : 0.26978, Training Acc : 0.883, Run Time : 14.20
INFO:root:2019-05-11 16:19:02, Epoch : 1, Step : 756, Training Loss : 0.31892, Training Acc : 0.861, Run Time : 6.60
INFO:root:2019-05-11 16:19:46, Epoch : 1, Step : 757, Training Loss : 0.28046, Training Acc : 0.878, Run Time : 43.11
INFO:root:2019-05-11 16:20:15, Epoch : 1, Step : 758, Training Loss : 0.33016, Training Acc : 0.844, Run Time : 28.94
INFO:root:2019-05-11 16:20:43, Epoch : 1, Step : 759, Training Loss : 0.31446, Training Acc : 0.861, Run Time : 28.95
INFO:root:2019-05-11 16:21:04, Epoch : 1, Step : 760, Training Loss : 0.26681, Training Acc : 0.881, Run Time : 20.61
INFO:root:2019-05-11 16:21:20, Epoch : 1, Step : 761, Training Loss : 0.33738, Training Acc : 0.853, Run Time : 16.03
INFO:root:2019-05-11 16:21:37, Epoch : 1, Step : 762, Training Loss : 0.32695, Training Acc : 0.872, Run Time : 17.13
INFO:root:2019-05-11 16:22:04, Epoch : 1, Step : 763, Training Loss : 0.29252, Training Acc : 0.878, Run Time : 26.39
INFO:root:2019-05-11 16:22:12, Epoch : 1, Step : 764, Training Loss : 0.29272, Training Acc : 0.892, Run Time : 8.33
INFO:root:2019-05-11 16:22:36, Epoch : 1, Step : 765, Training Loss : 0.24567, Training Acc : 0.919, Run Time : 24.14
INFO:root:2019-05-11 16:22:53, Epoch : 1, Step : 766, Training Loss : 0.23400, Training Acc : 0.931, Run Time : 16.53
INFO:root:2019-05-11 16:23:09, Epoch : 1, Step : 767, Training Loss : 0.23041, Training Acc : 0.931, Run Time : 16.55
INFO:root:2019-05-11 16:23:40, Epoch : 1, Step : 768, Training Loss : 0.25232, Training Acc : 0.908, Run Time : 30.96
INFO:root:2019-05-11 16:23:47, Epoch : 1, Step : 769, Training Loss : 0.28596, Training Acc : 0.869, Run Time : 7.16
INFO:root:2019-05-11 16:24:40, Epoch : 1, Step : 770, Training Loss : 0.31681, Training Acc : 0.856, Run Time : 52.45
INFO:root:2019-05-11 16:24:46, Epoch : 1, Step : 771, Training Loss : 0.32255, Training Acc : 0.853, Run Time : 6.60
INFO:root:2019-05-11 16:25:17, Epoch : 1, Step : 772, Training Loss : 0.35343, Training Acc : 0.833, Run Time : 30.90
INFO:root:2019-05-11 16:25:23, Epoch : 1, Step : 773, Training Loss : 0.35560, Training Acc : 0.825, Run Time : 6.22
INFO:root:2019-05-11 16:25:45, Epoch : 1, Step : 774, Training Loss : 0.32380, Training Acc : 0.839, Run Time : 21.13
INFO:root:2019-05-11 16:26:06, Epoch : 1, Step : 775, Training Loss : 0.36146, Training Acc : 0.831, Run Time : 21.61
INFO:root:2019-05-11 16:26:29, Epoch : 1, Step : 776, Training Loss : 0.32093, Training Acc : 0.836, Run Time : 22.43
INFO:root:2019-05-11 16:26:38, Epoch : 1, Step : 777, Training Loss : 0.31961, Training Acc : 0.847, Run Time : 8.97
INFO:root:2019-05-11 16:27:16, Epoch : 1, Step : 778, Training Loss : 0.35607, Training Acc : 0.842, Run Time : 38.49
INFO:root:2019-05-11 16:27:26, Epoch : 1, Step : 779, Training Loss : 0.33850, Training Acc : 0.831, Run Time : 10.01
INFO:root:2019-05-11 16:28:10, Epoch : 1, Step : 780, Training Loss : 0.34034, Training Acc : 0.828, Run Time : 44.13
INFO:root:2019-05-11 16:28:20, Epoch : 1, Step : 781, Training Loss : 0.29007, Training Acc : 0.856, Run Time : 10.02
INFO:root:2019-05-11 16:29:14, Epoch : 1, Step : 782, Training Loss : 0.33381, Training Acc : 0.833, Run Time : 53.83
INFO:root:2019-05-11 16:29:31, Epoch : 1, Step : 783, Training Loss : 0.27152, Training Acc : 0.892, Run Time : 16.78
INFO:root:2019-05-11 16:30:05, Epoch : 1, Step : 784, Training Loss : 0.26515, Training Acc : 0.875, Run Time : 34.05
INFO:root:2019-05-11 16:30:09, Epoch : 1, Step : 785, Training Loss : 0.30093, Training Acc : 0.889, Run Time : 4.41
INFO:root:2019-05-11 16:30:57, Epoch : 1, Step : 786, Training Loss : 0.33733, Training Acc : 0.808, Run Time : 47.42
INFO:root:2019-05-11 16:31:13, Epoch : 1, Step : 787, Training Loss : 0.28512, Training Acc : 0.875, Run Time : 15.94
INFO:root:2019-05-11 16:31:34, Epoch : 1, Step : 788, Training Loss : 0.28463, Training Acc : 0.878, Run Time : 21.62
INFO:root:2019-05-11 16:32:06, Epoch : 1, Step : 789, Training Loss : 0.29017, Training Acc : 0.892, Run Time : 31.63
INFO:root:2019-05-11 16:33:19, Epoch : 1, Step : 790, Training Loss : 0.28862, Training Acc : 0.897, Run Time : 72.68
INFO:root:2019-05-11 16:33:34, Epoch : 1, Step : 791, Training Loss : 0.24647, Training Acc : 0.925, Run Time : 15.80
INFO:root:2019-05-11 16:34:09, Epoch : 1, Step : 792, Training Loss : 0.18883, Training Acc : 0.944, Run Time : 34.70
INFO:root:2019-05-11 16:34:19, Epoch : 1, Step : 793, Training Loss : 0.19107, Training Acc : 0.933, Run Time : 9.70
INFO:root:2019-05-11 16:34:22, Epoch : 1, Step : 794, Training Loss : 0.21397, Training Acc : 0.914, Run Time : 2.69
INFO:root:2019-05-11 16:35:16, Epoch : 1, Step : 795, Training Loss : 0.23277, Training Acc : 0.914, Run Time : 54.51
INFO:root:2019-05-11 16:35:30, Epoch : 1, Step : 796, Training Loss : 0.22934, Training Acc : 0.911, Run Time : 13.59
INFO:root:2019-05-11 16:36:02, Epoch : 1, Step : 797, Training Loss : 0.21968, Training Acc : 0.928, Run Time : 32.63
INFO:root:2019-05-11 16:37:43, Epoch : 1, Step : 798, Training Loss : 0.21676, Training Acc : 0.928, Run Time : 100.84
INFO:root:2019-05-11 16:37:51, Epoch : 1, Step : 799, Training Loss : 0.26019, Training Acc : 0.914, Run Time : 7.94
INFO:root:2019-05-11 16:39:39, Epoch : 1, Step : 800, Training Loss : 0.32432, Training Acc : 0.894, Run Time : 108.22
INFO:root:2019-05-11 16:39:49, Epoch : 1, Step : 801, Training Loss : 1.15110, Training Acc : 0.589, Run Time : 9.79
INFO:root:2019-05-11 16:41:14, Epoch : 1, Step : 802, Training Loss : 1.43644, Training Acc : 0.522, Run Time : 84.53
INFO:root:2019-05-11 16:41:20, Epoch : 1, Step : 803, Training Loss : 1.28752, Training Acc : 0.514, Run Time : 6.61
INFO:root:2019-05-11 16:42:37, Epoch : 1, Step : 804, Training Loss : 1.19816, Training Acc : 0.519, Run Time : 76.73
INFO:root:2019-05-11 16:42:43, Epoch : 1, Step : 805, Training Loss : 1.18959, Training Acc : 0.525, Run Time : 6.48
INFO:root:2019-05-11 16:43:10, Epoch : 1, Step : 806, Training Loss : 1.01837, Training Acc : 0.511, Run Time : 26.58
INFO:root:2019-05-11 16:43:40, Epoch : 1, Step : 807, Training Loss : 0.90229, Training Acc : 0.578, Run Time : 30.35
INFO:root:2019-05-11 16:44:46, Epoch : 1, Step : 808, Training Loss : 0.84343, Training Acc : 0.614, Run Time : 65.56
INFO:root:2019-05-11 16:45:19, Epoch : 1, Step : 809, Training Loss : 0.77243, Training Acc : 0.592, Run Time : 32.99
INFO:root:2019-05-11 16:45:36, Epoch : 1, Step : 810, Training Loss : 0.76442, Training Acc : 0.547, Run Time : 17.36
INFO:root:2019-05-11 16:45:54, Epoch : 1, Step : 811, Training Loss : 0.74338, Training Acc : 0.608, Run Time : 17.35
INFO:root:2019-05-11 16:46:09, Epoch : 1, Step : 812, Training Loss : 0.76586, Training Acc : 0.497, Run Time : 15.78
INFO:root:2019-05-11 16:46:17, Epoch : 1, Step : 813, Training Loss : 0.77034, Training Acc : 0.519, Run Time : 7.81
INFO:root:2019-05-11 16:47:14, Epoch : 1, Step : 814, Training Loss : 0.79912, Training Acc : 0.525, Run Time : 56.82
INFO:root:2019-05-11 16:47:23, Epoch : 1, Step : 815, Training Loss : 0.81767, Training Acc : 0.522, Run Time : 9.33
INFO:root:2019-05-11 16:48:50, Epoch : 1, Step : 816, Training Loss : 0.82978, Training Acc : 0.500, Run Time : 87.19
INFO:root:2019-05-11 16:49:03, Epoch : 1, Step : 817, Training Loss : 0.81308, Training Acc : 0.519, Run Time : 12.25
INFO:root:2019-05-11 16:49:40, Epoch : 1, Step : 818, Training Loss : 0.80119, Training Acc : 0.503, Run Time : 37.24
INFO:root:2019-05-11 16:49:47, Epoch : 1, Step : 819, Training Loss : 0.76593, Training Acc : 0.475, Run Time : 7.25
INFO:root:2019-05-11 16:50:21, Epoch : 1, Step : 820, Training Loss : 0.72391, Training Acc : 0.503, Run Time : 33.31
INFO:root:2019-05-11 16:50:58, Epoch : 1, Step : 821, Training Loss : 0.70201, Training Acc : 0.536, Run Time : 37.11
INFO:root:2019-05-11 16:51:05, Epoch : 1, Step : 822, Training Loss : 0.68868, Training Acc : 0.519, Run Time : 7.59
INFO:root:2019-05-11 16:51:22, Epoch : 1, Step : 823, Training Loss : 0.68788, Training Acc : 0.594, Run Time : 17.21
INFO:root:2019-05-11 16:51:51, Epoch : 1, Step : 824, Training Loss : 0.69207, Training Acc : 0.608, Run Time : 28.46
INFO:root:2019-05-11 16:51:58, Epoch : 1, Step : 825, Training Loss : 0.66709, Training Acc : 0.603, Run Time : 7.42
INFO:root:2019-05-11 16:52:26, Epoch : 1, Step : 826, Training Loss : 0.68903, Training Acc : 0.589, Run Time : 28.07
INFO:root:2019-05-11 16:52:42, Epoch : 1, Step : 827, Training Loss : 0.65902, Training Acc : 0.606, Run Time : 15.63
INFO:root:2019-05-11 16:53:37, Epoch : 1, Step : 828, Training Loss : 0.69290, Training Acc : 0.536, Run Time : 55.18
INFO:root:2019-05-11 16:53:53, Epoch : 1, Step : 829, Training Loss : 0.64880, Training Acc : 0.572, Run Time : 15.45
INFO:root:2019-05-11 16:54:16, Epoch : 1, Step : 830, Training Loss : 0.66745, Training Acc : 0.603, Run Time : 23.21
INFO:root:2019-05-11 16:54:48, Epoch : 1, Step : 831, Training Loss : 0.61125, Training Acc : 0.622, Run Time : 31.67
INFO:root:2019-05-11 16:54:54, Epoch : 1, Step : 832, Training Loss : 0.60030, Training Acc : 0.692, Run Time : 6.68
INFO:root:2019-05-11 16:55:22, Epoch : 1, Step : 833, Training Loss : 0.64074, Training Acc : 0.619, Run Time : 28.18
INFO:root:2019-05-11 16:55:36, Epoch : 1, Step : 834, Training Loss : 0.65335, Training Acc : 0.597, Run Time : 13.64
INFO:root:2019-05-11 16:55:42, Epoch : 1, Step : 835, Training Loss : 0.61538, Training Acc : 0.617, Run Time : 6.04
INFO:root:2019-05-11 16:56:16, Epoch : 1, Step : 836, Training Loss : 0.68212, Training Acc : 0.581, Run Time : 33.52
INFO:root:2019-05-11 16:56:18, Epoch : 1, Step : 837, Training Loss : 0.70980, Training Acc : 0.547, Run Time : 2.41
INFO:root:2019-05-11 16:56:20, Epoch : 1, Step : 838, Training Loss : 0.66773, Training Acc : 0.567, Run Time : 2.36
INFO:root:2019-05-11 16:56:34, Epoch : 1, Step : 839, Training Loss : 0.64447, Training Acc : 0.617, Run Time : 13.15
INFO:root:2019-05-11 16:56:36, Epoch : 1, Step : 840, Training Loss : 0.62926, Training Acc : 0.700, Run Time : 2.88
INFO:root:2019-05-11 16:57:07, Epoch : 1, Step : 841, Training Loss : 0.69300, Training Acc : 0.558, Run Time : 30.27
INFO:root:2019-05-11 16:57:22, Epoch : 1, Step : 842, Training Loss : 0.65748, Training Acc : 0.575, Run Time : 14.93
INFO:root:2019-05-11 16:57:50, Epoch : 1, Step : 843, Training Loss : 0.66295, Training Acc : 0.614, Run Time : 28.86
INFO:root:2019-05-11 16:58:14, Epoch : 1, Step : 844, Training Loss : 0.65117, Training Acc : 0.586, Run Time : 23.48
INFO:root:2019-05-11 16:58:37, Epoch : 1, Step : 845, Training Loss : 0.63982, Training Acc : 0.686, Run Time : 22.57
INFO:root:2019-05-11 16:58:41, Epoch : 1, Step : 846, Training Loss : 0.63633, Training Acc : 0.736, Run Time : 4.41
INFO:root:2019-05-11 16:59:02, Epoch : 1, Step : 847, Training Loss : 0.64204, Training Acc : 0.633, Run Time : 21.30
INFO:root:2019-05-11 16:59:50, Epoch : 1, Step : 848, Training Loss : 0.68614, Training Acc : 0.550, Run Time : 47.45
INFO:root:2019-05-11 16:59:57, Epoch : 1, Step : 849, Training Loss : 0.66661, Training Acc : 0.633, Run Time : 7.12
INFO:root:2019-05-11 17:00:23, Epoch : 1, Step : 850, Training Loss : 0.67445, Training Acc : 0.606, Run Time : 25.88
INFO:root:2019-05-11 17:00:29, Epoch : 1, Step : 851, Training Loss : 0.70656, Training Acc : 0.569, Run Time : 6.49
INFO:root:2019-05-11 17:01:10, Epoch : 1, Step : 852, Training Loss : 0.69236, Training Acc : 0.542, Run Time : 41.24
INFO:root:2019-05-11 17:01:26, Epoch : 1, Step : 853, Training Loss : 0.65963, Training Acc : 0.681, Run Time : 15.10
INFO:root:2019-05-11 17:02:14, Epoch : 1, Step : 854, Training Loss : 0.70395, Training Acc : 0.525, Run Time : 48.28
INFO:root:2019-05-11 17:02:23, Epoch : 1, Step : 855, Training Loss : 0.65378, Training Acc : 0.672, Run Time : 9.53
INFO:root:2019-05-11 17:02:58, Epoch : 1, Step : 856, Training Loss : 0.68809, Training Acc : 0.522, Run Time : 34.26
INFO:root:2019-05-11 17:03:32, Epoch : 1, Step : 857, Training Loss : 0.62721, Training Acc : 0.739, Run Time : 34.14
INFO:root:2019-05-11 17:03:59, Epoch : 1, Step : 858, Training Loss : 0.60603, Training Acc : 0.764, Run Time : 27.48
INFO:root:2019-05-11 17:05:00, Epoch : 1, Step : 859, Training Loss : 0.58850, Training Acc : 0.792, Run Time : 60.34
INFO:root:2019-05-11 17:05:29, Epoch : 1, Step : 860, Training Loss : 0.63003, Training Acc : 0.664, Run Time : 29.24
INFO:root:2019-05-11 17:05:48, Epoch : 1, Step : 861, Training Loss : 0.64704, Training Acc : 0.672, Run Time : 18.73
INFO:root:2019-05-11 17:06:06, Epoch : 1, Step : 862, Training Loss : 0.69228, Training Acc : 0.511, Run Time : 18.04
INFO:root:2019-05-11 17:06:57, Epoch : 1, Step : 863, Training Loss : 0.62966, Training Acc : 0.592, Run Time : 51.21
INFO:root:2019-05-11 17:07:24, Epoch : 1, Step : 864, Training Loss : 0.67569, Training Acc : 0.581, Run Time : 27.58
INFO:root:2019-05-11 17:07:41, Epoch : 1, Step : 865, Training Loss : 0.64184, Training Acc : 0.642, Run Time : 16.95
INFO:root:2019-05-11 17:08:08, Epoch : 1, Step : 866, Training Loss : 0.62639, Training Acc : 0.683, Run Time : 26.41
INFO:root:2019-05-11 17:08:23, Epoch : 1, Step : 867, Training Loss : 0.64018, Training Acc : 0.642, Run Time : 15.24
INFO:root:2019-05-11 17:08:41, Epoch : 1, Step : 868, Training Loss : 0.65640, Training Acc : 0.669, Run Time : 17.84
INFO:root:2019-05-11 17:08:44, Epoch : 1, Step : 869, Training Loss : 0.71889, Training Acc : 0.453, Run Time : 2.90
INFO:root:2019-05-11 17:09:24, Epoch : 1, Step : 870, Training Loss : 0.60174, Training Acc : 0.797, Run Time : 40.39
INFO:root:2019-05-11 17:09:28, Epoch : 1, Step : 871, Training Loss : 0.65838, Training Acc : 0.650, Run Time : 3.85
INFO:root:2019-05-11 17:09:53, Epoch : 1, Step : 872, Training Loss : 0.63192, Training Acc : 0.711, Run Time : 25.44
INFO:root:2019-05-11 17:09:58, Epoch : 1, Step : 873, Training Loss : 0.62666, Training Acc : 0.783, Run Time : 4.58
INFO:root:2019-05-11 17:10:54, Epoch : 1, Step : 874, Training Loss : 0.61135, Training Acc : 0.758, Run Time : 55.55
INFO:root:2019-05-11 17:11:00, Epoch : 1, Step : 875, Training Loss : 0.64214, Training Acc : 0.681, Run Time : 6.94
INFO:root:2019-05-11 17:11:36, Epoch : 1, Step : 876, Training Loss : 0.61064, Training Acc : 0.758, Run Time : 35.83
INFO:root:2019-05-11 17:11:42, Epoch : 1, Step : 877, Training Loss : 0.61891, Training Acc : 0.767, Run Time : 5.97
INFO:root:2019-05-11 17:11:45, Epoch : 1, Step : 878, Training Loss : 0.65644, Training Acc : 0.667, Run Time : 2.69
INFO:root:2019-05-11 17:12:14, Epoch : 1, Step : 879, Training Loss : 0.64724, Training Acc : 0.714, Run Time : 28.70
INFO:root:2019-05-11 17:12:16, Epoch : 1, Step : 880, Training Loss : 0.62257, Training Acc : 0.778, Run Time : 2.73
INFO:root:2019-05-11 17:12:50, Epoch : 1, Step : 881, Training Loss : 0.65840, Training Acc : 0.644, Run Time : 33.74
INFO:root:2019-05-11 17:13:08, Epoch : 1, Step : 882, Training Loss : 0.66632, Training Acc : 0.586, Run Time : 17.61
INFO:root:2019-05-11 17:13:28, Epoch : 1, Step : 883, Training Loss : 0.63048, Training Acc : 0.731, Run Time : 20.34
INFO:root:2019-05-11 17:13:50, Epoch : 1, Step : 884, Training Loss : 0.65795, Training Acc : 0.669, Run Time : 21.73
INFO:root:2019-05-11 17:14:01, Epoch : 1, Step : 885, Training Loss : 0.65701, Training Acc : 0.650, Run Time : 10.70
INFO:root:2019-05-11 17:14:21, Epoch : 1, Step : 886, Training Loss : 0.64116, Training Acc : 0.728, Run Time : 20.68
INFO:root:2019-05-11 17:14:48, Epoch : 1, Step : 887, Training Loss : 0.67477, Training Acc : 0.556, Run Time : 27.24
INFO:root:2019-05-11 17:15:08, Epoch : 1, Step : 888, Training Loss : 0.67552, Training Acc : 0.589, Run Time : 19.80
INFO:root:2019-05-11 17:15:39, Epoch : 1, Step : 889, Training Loss : 0.62967, Training Acc : 0.733, Run Time : 30.53
INFO:root:2019-05-11 17:16:21, Epoch : 1, Step : 890, Training Loss : 0.63148, Training Acc : 0.717, Run Time : 41.93
INFO:root:2019-05-11 17:16:31, Epoch : 1, Step : 891, Training Loss : 0.65595, Training Acc : 0.681, Run Time : 9.85
INFO:root:2019-05-11 17:17:00, Epoch : 1, Step : 892, Training Loss : 0.64410, Training Acc : 0.703, Run Time : 29.95
INFO:root:2019-05-11 17:17:30, Epoch : 1, Step : 893, Training Loss : 0.65760, Training Acc : 0.639, Run Time : 29.43
INFO:root:2019-05-11 17:17:44, Epoch : 1, Step : 894, Training Loss : 0.61437, Training Acc : 0.706, Run Time : 14.57
INFO:root:2019-05-11 17:18:04, Epoch : 1, Step : 895, Training Loss : 0.64644, Training Acc : 0.683, Run Time : 19.48
INFO:root:2019-05-11 17:18:31, Epoch : 1, Step : 896, Training Loss : 0.62905, Training Acc : 0.708, Run Time : 26.77
INFO:root:2019-05-11 17:19:22, Epoch : 1, Step : 897, Training Loss : 0.61496, Training Acc : 0.742, Run Time : 51.13
INFO:root:2019-05-11 17:19:30, Epoch : 1, Step : 898, Training Loss : 0.61055, Training Acc : 0.667, Run Time : 7.70
INFO:root:2019-05-11 17:20:01, Epoch : 1, Step : 899, Training Loss : 0.65751, Training Acc : 0.658, Run Time : 31.77
INFO:root:2019-05-11 17:20:37, Epoch : 1, Step : 900, Training Loss : 0.60778, Training Acc : 0.822, Run Time : 35.34
INFO:root:2019-05-11 17:21:59, Epoch : 1, Step : 901, Training Loss : 0.61888, Training Acc : 0.753, Run Time : 82.70
INFO:root:2019-05-11 17:23:03, Epoch : 1, Step : 902, Training Loss : 0.59155, Training Acc : 0.814, Run Time : 63.66
INFO:root:2019-05-11 17:23:42, Epoch : 1, Step : 903, Training Loss : 0.45250, Training Acc : 0.919, Run Time : 38.95
INFO:root:2019-05-11 17:24:34, Epoch : 1, Step : 904, Training Loss : 0.41771, Training Acc : 0.917, Run Time : 52.37
INFO:root:2019-05-11 17:25:17, Epoch : 1, Step : 905, Training Loss : 0.34420, Training Acc : 0.928, Run Time : 42.67
INFO:root:2019-05-11 17:25:24, Epoch : 1, Step : 906, Training Loss : 0.31730, Training Acc : 0.922, Run Time : 6.88
INFO:root:2019-05-11 17:26:16, Epoch : 1, Step : 907, Training Loss : 0.26568, Training Acc : 0.919, Run Time : 52.32
INFO:root:2019-05-11 17:26:41, Epoch : 1, Step : 908, Training Loss : 0.24827, Training Acc : 0.919, Run Time : 24.74
INFO:root:2019-05-11 17:26:50, Epoch : 1, Step : 909, Training Loss : 0.24892, Training Acc : 0.917, Run Time : 9.14
INFO:root:2019-05-11 17:27:28, Epoch : 1, Step : 910, Training Loss : 0.23413, Training Acc : 0.919, Run Time : 38.33
INFO:root:2019-05-11 17:27:35, Epoch : 1, Step : 911, Training Loss : 0.23831, Training Acc : 0.917, Run Time : 6.12
INFO:root:2019-05-11 17:27:38, Epoch : 1, Step : 912, Training Loss : 0.22874, Training Acc : 0.922, Run Time : 3.00
INFO:root:2019-05-11 17:28:35, Epoch : 1, Step : 913, Training Loss : 0.22024, Training Acc : 0.928, Run Time : 57.03
INFO:root:2019-05-11 17:29:25, Epoch : 1, Step : 914, Training Loss : 0.21602, Training Acc : 0.928, Run Time : 50.78
INFO:root:2019-05-11 17:29:33, Epoch : 1, Step : 915, Training Loss : 0.21434, Training Acc : 0.933, Run Time : 7.54
INFO:root:2019-05-11 17:30:20, Epoch : 1, Step : 916, Training Loss : 0.18367, Training Acc : 0.942, Run Time : 46.75
INFO:root:2019-05-11 17:30:39, Epoch : 1, Step : 917, Training Loss : 0.18950, Training Acc : 0.942, Run Time : 19.03
INFO:root:2019-05-11 17:31:09, Epoch : 1, Step : 918, Training Loss : 0.26889, Training Acc : 0.917, Run Time : 30.03
INFO:root:2019-05-11 17:31:32, Epoch : 1, Step : 919, Training Loss : 0.32444, Training Acc : 0.906, Run Time : 23.00
INFO:root:2019-05-11 17:31:51, Epoch : 1, Step : 920, Training Loss : 0.36597, Training Acc : 0.897, Run Time : 19.18
INFO:root:2019-05-11 17:32:14, Epoch : 1, Step : 921, Training Loss : 0.48232, Training Acc : 0.858, Run Time : 22.74
INFO:root:2019-05-11 17:32:21, Epoch : 1, Step : 922, Training Loss : 0.35096, Training Acc : 0.883, Run Time : 7.15
INFO:root:2019-05-11 17:32:55, Epoch : 1, Step : 923, Training Loss : 0.31880, Training Acc : 0.897, Run Time : 34.10
INFO:root:2019-05-11 17:33:17, Epoch : 1, Step : 924, Training Loss : 0.41415, Training Acc : 0.872, Run Time : 22.28
INFO:root:2019-05-11 17:33:23, Epoch : 1, Step : 925, Training Loss : 0.37652, Training Acc : 0.858, Run Time : 5.91
INFO:root:2019-05-11 17:33:52, Epoch : 1, Step : 926, Training Loss : 0.35827, Training Acc : 0.869, Run Time : 29.32
INFO:root:2019-05-11 17:33:55, Epoch : 1, Step : 927, Training Loss : 0.33839, Training Acc : 0.864, Run Time : 2.65
INFO:root:2019-05-11 17:34:24, Epoch : 1, Step : 928, Training Loss : 0.32329, Training Acc : 0.853, Run Time : 28.81
INFO:root:2019-05-11 17:34:26, Epoch : 1, Step : 929, Training Loss : 0.33517, Training Acc : 0.856, Run Time : 2.41
INFO:root:2019-05-11 17:34:29, Epoch : 1, Step : 930, Training Loss : 0.28239, Training Acc : 0.864, Run Time : 3.03
INFO:root:2019-05-11 17:35:09, Epoch : 1, Step : 931, Training Loss : 0.29870, Training Acc : 0.861, Run Time : 39.46
INFO:root:2019-05-11 17:35:17, Epoch : 1, Step : 932, Training Loss : 0.33783, Training Acc : 0.836, Run Time : 8.74
INFO:root:2019-05-11 17:36:42, Epoch : 1, Step : 933, Training Loss : 0.30327, Training Acc : 0.853, Run Time : 84.19
INFO:root:2019-05-11 17:36:51, Epoch : 1, Step : 934, Training Loss : 0.31335, Training Acc : 0.850, Run Time : 9.24
INFO:root:2019-05-11 17:37:51, Epoch : 1, Step : 935, Training Loss : 0.31607, Training Acc : 0.861, Run Time : 59.82
INFO:root:2019-05-11 17:38:02, Epoch : 1, Step : 936, Training Loss : 0.29063, Training Acc : 0.881, Run Time : 11.31
INFO:root:2019-05-11 17:38:28, Epoch : 1, Step : 937, Training Loss : 0.32772, Training Acc : 0.867, Run Time : 26.23
INFO:root:2019-05-11 17:38:34, Epoch : 1, Step : 938, Training Loss : 0.26857, Training Acc : 0.858, Run Time : 5.22
INFO:root:2019-05-11 17:39:03, Epoch : 1, Step : 939, Training Loss : 0.25396, Training Acc : 0.919, Run Time : 29.39
INFO:root:2019-05-11 17:39:07, Epoch : 1, Step : 940, Training Loss : 0.26316, Training Acc : 0.922, Run Time : 3.79
INFO:root:2019-05-11 17:39:28, Epoch : 1, Step : 941, Training Loss : 0.29628, Training Acc : 0.869, Run Time : 21.43
INFO:root:2019-05-11 17:39:44, Epoch : 1, Step : 942, Training Loss : 0.28883, Training Acc : 0.878, Run Time : 15.39
INFO:root:2019-05-11 17:39:54, Epoch : 1, Step : 943, Training Loss : 0.25879, Training Acc : 0.922, Run Time : 10.41
INFO:root:2019-05-11 17:40:57, Epoch : 1, Step : 944, Training Loss : 0.24949, Training Acc : 0.911, Run Time : 63.23
INFO:root:2019-05-11 17:41:17, Epoch : 1, Step : 945, Training Loss : 0.25408, Training Acc : 0.883, Run Time : 19.67
INFO:root:2019-05-11 17:42:20, Epoch : 1, Step : 946, Training Loss : 0.26959, Training Acc : 0.847, Run Time : 63.43
INFO:root:2019-05-11 17:42:24, Epoch : 1, Step : 947, Training Loss : 0.24329, Training Acc : 0.903, Run Time : 3.99
INFO:root:2019-05-11 17:42:44, Epoch : 1, Step : 948, Training Loss : 0.24668, Training Acc : 0.906, Run Time : 20.20
INFO:root:2019-05-11 17:42:46, Epoch : 1, Step : 949, Training Loss : 0.22100, Training Acc : 0.911, Run Time : 1.99
INFO:root:2019-05-11 17:42:50, Epoch : 1, Step : 950, Training Loss : 0.25027, Training Acc : 0.883, Run Time : 3.88
INFO:root:2019-05-11 17:43:18, Epoch : 1, Step : 951, Training Loss : 0.22621, Training Acc : 0.900, Run Time : 27.98
INFO:root:2019-05-11 17:43:54, Epoch : 1, Step : 952, Training Loss : 0.23612, Training Acc : 0.875, Run Time : 35.45
INFO:root:2019-05-11 17:43:56, Epoch : 1, Step : 953, Training Loss : 0.22833, Training Acc : 0.889, Run Time : 2.57
INFO:root:2019-05-11 17:44:01, Epoch : 1, Step : 954, Training Loss : 0.30566, Training Acc : 0.844, Run Time : 5.06
INFO:root:2019-05-11 17:44:07, Epoch : 1, Step : 955, Training Loss : 0.27870, Training Acc : 0.861, Run Time : 5.63
INFO:root:2019-05-11 17:44:36, Epoch : 1, Step : 956, Training Loss : 0.30974, Training Acc : 0.858, Run Time : 28.49
INFO:root:2019-05-11 17:44:51, Epoch : 1, Step : 957, Training Loss : 0.30305, Training Acc : 0.831, Run Time : 15.78
INFO:root:2019-05-11 17:44:58, Epoch : 1, Step : 958, Training Loss : 0.29071, Training Acc : 0.856, Run Time : 6.36
INFO:root:2019-05-11 17:45:21, Epoch : 1, Step : 959, Training Loss : 0.25273, Training Acc : 0.875, Run Time : 23.03
INFO:root:2019-05-11 17:45:31, Epoch : 1, Step : 960, Training Loss : 0.27398, Training Acc : 0.878, Run Time : 9.99
INFO:root:2019-05-11 17:45:36, Epoch : 1, Step : 961, Training Loss : 0.25862, Training Acc : 0.869, Run Time : 5.38
INFO:root:2019-05-11 17:46:06, Epoch : 1, Step : 962, Training Loss : 0.23580, Training Acc : 0.894, Run Time : 30.33
INFO:root:2019-05-11 17:46:22, Epoch : 1, Step : 963, Training Loss : 0.28609, Training Acc : 0.861, Run Time : 15.70
INFO:root:2019-05-11 17:46:58, Epoch : 1, Step : 964, Training Loss : 0.30146, Training Acc : 0.831, Run Time : 36.39
INFO:root:2019-05-11 17:47:06, Epoch : 1, Step : 965, Training Loss : 0.25243, Training Acc : 0.878, Run Time : 7.84
INFO:root:2019-05-11 17:47:37, Epoch : 1, Step : 966, Training Loss : 0.25613, Training Acc : 0.886, Run Time : 30.28
INFO:root:2019-05-11 17:47:52, Epoch : 1, Step : 967, Training Loss : 0.28920, Training Acc : 0.853, Run Time : 15.53
INFO:root:2019-05-11 17:47:59, Epoch : 1, Step : 968, Training Loss : 0.24401, Training Acc : 0.886, Run Time : 7.37
INFO:root:2019-05-11 17:48:23, Epoch : 1, Step : 969, Training Loss : 0.30753, Training Acc : 0.844, Run Time : 23.57
INFO:root:2019-05-11 17:48:27, Epoch : 1, Step : 970, Training Loss : 0.26867, Training Acc : 0.869, Run Time : 3.76
INFO:root:2019-05-11 17:48:53, Epoch : 1, Step : 971, Training Loss : 0.29800, Training Acc : 0.844, Run Time : 25.74
INFO:root:2019-05-11 17:49:03, Epoch : 1, Step : 972, Training Loss : 0.29758, Training Acc : 0.869, Run Time : 10.14
INFO:root:2019-05-11 17:49:32, Epoch : 1, Step : 973, Training Loss : 0.25271, Training Acc : 0.881, Run Time : 29.13
INFO:root:2019-05-11 17:49:47, Epoch : 1, Step : 974, Training Loss : 0.28954, Training Acc : 0.856, Run Time : 15.11
INFO:root:2019-05-11 17:49:54, Epoch : 1, Step : 975, Training Loss : 0.26403, Training Acc : 0.883, Run Time : 6.69
INFO:root:2019-05-11 17:50:15, Epoch : 1, Step : 976, Training Loss : 0.31049, Training Acc : 0.842, Run Time : 21.54
INFO:root:2019-05-11 17:50:35, Epoch : 1, Step : 977, Training Loss : 0.26380, Training Acc : 0.856, Run Time : 19.94
INFO:root:2019-05-11 17:50:57, Epoch : 1, Step : 978, Training Loss : 0.27207, Training Acc : 0.864, Run Time : 22.29
INFO:root:2019-05-11 17:51:07, Epoch : 1, Step : 979, Training Loss : 0.34074, Training Acc : 0.819, Run Time : 10.10
INFO:root:2019-05-11 17:51:32, Epoch : 1, Step : 980, Training Loss : 0.29950, Training Acc : 0.850, Run Time : 24.17
INFO:root:2019-05-11 17:51:54, Epoch : 1, Step : 981, Training Loss : 0.30447, Training Acc : 0.872, Run Time : 22.76
INFO:root:2019-05-11 17:52:31, Epoch : 1, Step : 982, Training Loss : 0.27004, Training Acc : 0.869, Run Time : 36.43
INFO:root:2019-05-11 17:52:47, Epoch : 1, Step : 983, Training Loss : 0.29605, Training Acc : 0.850, Run Time : 16.34
INFO:root:2019-05-11 17:53:05, Epoch : 1, Step : 984, Training Loss : 0.37542, Training Acc : 0.783, Run Time : 17.66
INFO:root:2019-05-11 17:53:45, Epoch : 1, Step : 985, Training Loss : 0.21817, Training Acc : 0.908, Run Time : 39.79
INFO:root:2019-05-11 17:54:22, Epoch : 1, Step : 986, Training Loss : 0.22295, Training Acc : 0.914, Run Time : 37.15
INFO:root:2019-05-11 17:54:29, Epoch : 1, Step : 987, Training Loss : 0.17753, Training Acc : 0.942, Run Time : 7.53
INFO:root:2019-05-11 17:54:50, Epoch : 1, Step : 988, Training Loss : 0.19503, Training Acc : 0.928, Run Time : 21.02
INFO:root:2019-05-11 17:55:18, Epoch : 1, Step : 989, Training Loss : 0.21347, Training Acc : 0.900, Run Time : 27.60
INFO:root:2019-05-11 17:56:03, Epoch : 1, Step : 990, Training Loss : 0.16898, Training Acc : 0.961, Run Time : 44.97
INFO:root:2019-05-11 17:56:52, Epoch : 1, Step : 991, Training Loss : 0.14946, Training Acc : 0.964, Run Time : 49.05
INFO:root:2019-05-11 17:58:14, Epoch : 1, Step : 992, Training Loss : 0.12981, Training Acc : 0.978, Run Time : 81.69
INFO:root:2019-05-11 17:58:55, Epoch : 1, Step : 993, Training Loss : 0.11456, Training Acc : 0.981, Run Time : 41.12
INFO:root:2019-05-11 17:59:11, Epoch : 1, Step : 994, Training Loss : 0.11029, Training Acc : 0.975, Run Time : 15.86
INFO:root:2019-05-11 17:59:24, Epoch : 1, Step : 995, Training Loss : 0.10538, Training Acc : 0.986, Run Time : 13.46
INFO:root:2019-05-11 17:59:38, Epoch : 1, Step : 996, Training Loss : 0.12481, Training Acc : 0.978, Run Time : 13.91
INFO:root:2019-05-11 17:59:40, Epoch : 1, Step : 997, Training Loss : 0.14484, Training Acc : 0.947, Run Time : 2.45
INFO:root:2019-05-11 18:00:09, Epoch : 1, Step : 998, Training Loss : 0.11412, Training Acc : 0.961, Run Time : 28.84
INFO:root:2019-05-11 18:00:25, Epoch : 1, Step : 999, Training Loss : 0.14446, Training Acc : 0.953, Run Time : 15.29
INFO:root:2019-05-11 18:00:59, Epoch : 1, Step : 1000, Training Loss : 0.11958, Training Acc : 0.947, Run Time : 34.33
INFO:root:2019-05-11 18:01:14, Epoch : 1, Step : 1001, Training Loss : 1.32228, Training Acc : 0.603, Run Time : 15.51
INFO:root:2019-05-11 18:01:32, Epoch : 1, Step : 1002, Training Loss : 1.54996, Training Acc : 0.536, Run Time : 17.20
INFO:root:2019-05-11 18:01:40, Epoch : 1, Step : 1003, Training Loss : 1.42031, Training Acc : 0.544, Run Time : 8.04
INFO:root:2019-05-11 18:02:06, Epoch : 1, Step : 1004, Training Loss : 1.41303, Training Acc : 0.514, Run Time : 26.52
INFO:root:2019-05-11 18:02:22, Epoch : 1, Step : 1005, Training Loss : 1.36992, Training Acc : 0.525, Run Time : 16.01
INFO:root:2019-05-11 18:02:25, Epoch : 1, Step : 1006, Training Loss : 1.22892, Training Acc : 0.500, Run Time : 3.06
INFO:root:2019-05-11 18:02:54, Epoch : 1, Step : 1007, Training Loss : 0.92139, Training Acc : 0.542, Run Time : 28.79
INFO:root:2019-05-11 18:03:09, Epoch : 1, Step : 1008, Training Loss : 0.71870, Training Acc : 0.669, Run Time : 15.47
INFO:root:2019-05-11 18:03:20, Epoch : 1, Step : 1009, Training Loss : 0.71728, Training Acc : 0.597, Run Time : 10.18
INFO:root:2019-05-11 18:03:53, Epoch : 1, Step : 1010, Training Loss : 0.63979, Training Acc : 0.669, Run Time : 33.03
INFO:root:2019-05-11 18:04:08, Epoch : 1, Step : 1011, Training Loss : 0.67403, Training Acc : 0.589, Run Time : 14.82
INFO:root:2019-05-11 18:04:23, Epoch : 1, Step : 1012, Training Loss : 0.81982, Training Acc : 0.500, Run Time : 15.40
INFO:root:2019-05-11 18:04:38, Epoch : 1, Step : 1013, Training Loss : 0.77235, Training Acc : 0.544, Run Time : 15.06
INFO:root:2019-05-11 18:04:56, Epoch : 1, Step : 1014, Training Loss : 0.82569, Training Acc : 0.506, Run Time : 17.59
INFO:root:2019-05-11 18:05:02, Epoch : 1, Step : 1015, Training Loss : 0.84280, Training Acc : 0.539, Run Time : 6.65
INFO:root:2019-05-11 18:05:30, Epoch : 1, Step : 1016, Training Loss : 0.83890, Training Acc : 0.489, Run Time : 27.72
INFO:root:2019-05-11 18:05:46, Epoch : 1, Step : 1017, Training Loss : 0.84868, Training Acc : 0.506, Run Time : 15.59
INFO:root:2019-05-11 18:06:01, Epoch : 1, Step : 1018, Training Loss : 0.77914, Training Acc : 0.514, Run Time : 15.24
INFO:root:2019-05-11 18:06:24, Epoch : 1, Step : 1019, Training Loss : 0.78297, Training Acc : 0.522, Run Time : 23.00
INFO:root:2019-05-11 18:06:28, Epoch : 1, Step : 1020, Training Loss : 0.69098, Training Acc : 0.547, Run Time : 4.62
INFO:root:2019-05-11 18:06:38, Epoch : 1, Step : 1021, Training Loss : 0.66168, Training Acc : 0.578, Run Time : 10.00
INFO:root:2019-05-11 18:06:41, Epoch : 1, Step : 1022, Training Loss : 0.66614, Training Acc : 0.619, Run Time : 3.00
INFO:root:2019-05-11 18:07:08, Epoch : 1, Step : 1023, Training Loss : 0.66304, Training Acc : 0.606, Run Time : 27.11
INFO:root:2019-05-11 18:07:42, Epoch : 1, Step : 1024, Training Loss : 0.62098, Training Acc : 0.650, Run Time : 33.01
INFO:root:2019-05-11 18:07:55, Epoch : 1, Step : 1025, Training Loss : 0.67209, Training Acc : 0.633, Run Time : 13.43
INFO:root:2019-05-11 18:08:16, Epoch : 1, Step : 1026, Training Loss : 0.65263, Training Acc : 0.614, Run Time : 21.15
INFO:root:2019-05-11 18:08:40, Epoch : 1, Step : 1027, Training Loss : 0.63779, Training Acc : 0.642, Run Time : 23.99
INFO:root:2019-05-11 18:08:48, Epoch : 1, Step : 1028, Training Loss : 0.65117, Training Acc : 0.606, Run Time : 8.07
INFO:root:2019-05-11 18:09:10, Epoch : 1, Step : 1029, Training Loss : 0.72003, Training Acc : 0.533, Run Time : 22.02
INFO:root:2019-05-11 18:09:22, Epoch : 1, Step : 1030, Training Loss : 0.61813, Training Acc : 0.644, Run Time : 12.29
INFO:root:2019-05-11 18:09:55, Epoch : 1, Step : 1031, Training Loss : 0.63968, Training Acc : 0.606, Run Time : 32.27
INFO:root:2019-05-11 18:09:58, Epoch : 1, Step : 1032, Training Loss : 0.63518, Training Acc : 0.625, Run Time : 3.62
INFO:root:2019-05-11 18:10:18, Epoch : 1, Step : 1033, Training Loss : 0.63725, Training Acc : 0.647, Run Time : 19.75
INFO:root:2019-05-11 18:10:45, Epoch : 1, Step : 1034, Training Loss : 0.58013, Training Acc : 0.711, Run Time : 27.11
INFO:root:2019-05-11 18:10:58, Epoch : 1, Step : 1035, Training Loss : 0.67587, Training Acc : 0.622, Run Time : 12.90
INFO:root:2019-05-11 18:11:25, Epoch : 1, Step : 1036, Training Loss : 0.61925, Training Acc : 0.672, Run Time : 27.20
INFO:root:2019-05-11 18:12:15, Epoch : 1, Step : 1037, Training Loss : 0.60466, Training Acc : 0.664, Run Time : 49.22
INFO:root:2019-05-11 18:12:22, Epoch : 1, Step : 1038, Training Loss : 0.57892, Training Acc : 0.711, Run Time : 7.57
INFO:root:2019-05-11 18:12:47, Epoch : 1, Step : 1039, Training Loss : 0.63221, Training Acc : 0.694, Run Time : 24.85
INFO:root:2019-05-11 18:13:03, Epoch : 1, Step : 1040, Training Loss : 0.60230, Training Acc : 0.697, Run Time : 16.35
INFO:root:2019-05-11 18:13:33, Epoch : 1, Step : 1041, Training Loss : 0.61282, Training Acc : 0.706, Run Time : 29.63
INFO:root:2019-05-11 18:13:40, Epoch : 1, Step : 1042, Training Loss : 0.61301, Training Acc : 0.672, Run Time : 7.03
INFO:root:2019-05-11 18:14:07, Epoch : 1, Step : 1043, Training Loss : 0.56956, Training Acc : 0.706, Run Time : 27.36
INFO:root:2019-05-11 18:14:10, Epoch : 1, Step : 1044, Training Loss : 0.53036, Training Acc : 0.767, Run Time : 2.94
INFO:root:2019-05-11 18:14:29, Epoch : 1, Step : 1045, Training Loss : 0.56483, Training Acc : 0.719, Run Time : 19.16
INFO:root:2019-05-11 18:15:06, Epoch : 1, Step : 1046, Training Loss : 0.60678, Training Acc : 0.650, Run Time : 36.68
INFO:root:2019-05-11 18:15:26, Epoch : 1, Step : 1047, Training Loss : 0.69275, Training Acc : 0.569, Run Time : 20.04
INFO:root:2019-05-11 18:15:51, Epoch : 1, Step : 1048, Training Loss : 0.58454, Training Acc : 0.681, Run Time : 24.77
INFO:root:2019-05-11 18:16:20, Epoch : 1, Step : 1049, Training Loss : 0.64515, Training Acc : 0.636, Run Time : 29.34
INFO:root:2019-05-11 18:16:37, Epoch : 1, Step : 1050, Training Loss : 0.58032, Training Acc : 0.711, Run Time : 17.12
INFO:root:2019-05-11 18:16:44, Epoch : 1, Step : 1051, Training Loss : 0.58956, Training Acc : 0.658, Run Time : 6.23
INFO:root:2019-05-11 18:16:59, Epoch : 1, Step : 1052, Training Loss : 0.58599, Training Acc : 0.783, Run Time : 15.58
INFO:root:2019-05-11 18:17:02, Epoch : 1, Step : 1053, Training Loss : 0.53957, Training Acc : 0.803, Run Time : 2.55
INFO:root:2019-05-11 18:17:31, Epoch : 1, Step : 1054, Training Loss : 0.60015, Training Acc : 0.706, Run Time : 29.11
INFO:root:2019-05-11 18:18:00, Epoch : 1, Step : 1055, Training Loss : 0.53354, Training Acc : 0.772, Run Time : 29.41
INFO:root:2019-05-11 18:18:04, Epoch : 1, Step : 1056, Training Loss : 0.60795, Training Acc : 0.653, Run Time : 4.12
INFO:root:2019-05-11 18:18:19, Epoch : 1, Step : 1057, Training Loss : 0.67574, Training Acc : 0.567, Run Time : 14.59
INFO:root:2019-05-11 18:18:54, Epoch : 1, Step : 1058, Training Loss : 0.66641, Training Acc : 0.567, Run Time : 34.86
INFO:root:2019-05-11 18:19:09, Epoch : 1, Step : 1059, Training Loss : 0.62836, Training Acc : 0.644, Run Time : 14.77
INFO:root:2019-05-11 18:19:25, Epoch : 1, Step : 1060, Training Loss : 0.67173, Training Acc : 0.600, Run Time : 15.99
INFO:root:2019-05-11 18:19:37, Epoch : 1, Step : 1061, Training Loss : 0.59330, Training Acc : 0.783, Run Time : 12.94
INFO:root:2019-05-11 18:20:13, Epoch : 1, Step : 1062, Training Loss : 0.60298, Training Acc : 0.711, Run Time : 35.79
INFO:root:2019-05-11 18:20:34, Epoch : 1, Step : 1063, Training Loss : 0.62351, Training Acc : 0.711, Run Time : 20.41
INFO:root:2019-05-11 18:20:41, Epoch : 1, Step : 1064, Training Loss : 0.59868, Training Acc : 0.753, Run Time : 7.12
INFO:root:2019-05-11 18:21:09, Epoch : 1, Step : 1065, Training Loss : 0.59619, Training Acc : 0.714, Run Time : 28.18
INFO:root:2019-05-11 18:21:31, Epoch : 1, Step : 1066, Training Loss : 0.56942, Training Acc : 0.794, Run Time : 22.08
INFO:root:2019-05-11 18:21:49, Epoch : 1, Step : 1067, Training Loss : 0.61136, Training Acc : 0.697, Run Time : 17.67
INFO:root:2019-05-11 18:21:58, Epoch : 1, Step : 1068, Training Loss : 0.62237, Training Acc : 0.706, Run Time : 9.34
INFO:root:2019-05-11 18:22:21, Epoch : 1, Step : 1069, Training Loss : 0.61886, Training Acc : 0.653, Run Time : 23.41
INFO:root:2019-05-11 18:22:39, Epoch : 1, Step : 1070, Training Loss : 0.59838, Training Acc : 0.769, Run Time : 17.42
INFO:root:2019-05-11 18:22:56, Epoch : 1, Step : 1071, Training Loss : 0.69727, Training Acc : 0.542, Run Time : 17.08
INFO:root:2019-05-11 18:23:15, Epoch : 1, Step : 1072, Training Loss : 0.59242, Training Acc : 0.797, Run Time : 19.38
INFO:root:2019-05-11 18:23:23, Epoch : 1, Step : 1073, Training Loss : 0.54443, Training Acc : 0.867, Run Time : 7.16
INFO:root:2019-05-11 18:23:51, Epoch : 1, Step : 1074, Training Loss : 0.58921, Training Acc : 0.781, Run Time : 28.01
INFO:root:2019-05-11 18:23:58, Epoch : 1, Step : 1075, Training Loss : 0.55161, Training Acc : 0.878, Run Time : 7.73
INFO:root:2019-05-11 18:24:19, Epoch : 1, Step : 1076, Training Loss : 0.57007, Training Acc : 0.786, Run Time : 21.06
INFO:root:2019-05-11 18:24:36, Epoch : 1, Step : 1077, Training Loss : 0.54924, Training Acc : 0.864, Run Time : 16.72
INFO:root:2019-05-11 18:24:52, Epoch : 1, Step : 1078, Training Loss : 0.62666, Training Acc : 0.719, Run Time : 16.44
INFO:root:2019-05-11 18:25:07, Epoch : 1, Step : 1079, Training Loss : 0.54235, Training Acc : 0.850, Run Time : 14.23
INFO:root:2019-05-11 18:25:12, Epoch : 1, Step : 1080, Training Loss : 0.61444, Training Acc : 0.739, Run Time : 5.07
INFO:root:2019-05-11 18:25:14, Epoch : 1, Step : 1081, Training Loss : 0.54235, Training Acc : 0.819, Run Time : 1.97
INFO:root:2019-05-11 18:25:32, Epoch : 1, Step : 1082, Training Loss : 0.51797, Training Acc : 0.808, Run Time : 18.65
INFO:root:2019-05-11 18:25:49, Epoch : 1, Step : 1083, Training Loss : 0.52903, Training Acc : 0.900, Run Time : 17.07
INFO:root:2019-05-11 18:25:53, Epoch : 1, Step : 1084, Training Loss : 0.51194, Training Acc : 0.856, Run Time : 3.61
INFO:root:2019-05-11 18:26:03, Epoch : 1, Step : 1085, Training Loss : 0.47924, Training Acc : 0.911, Run Time : 10.19
INFO:root:2019-05-11 18:26:06, Epoch : 1, Step : 1086, Training Loss : 0.51009, Training Acc : 0.933, Run Time : 2.81
INFO:root:2019-05-11 18:26:25, Epoch : 1, Step : 1087, Training Loss : 0.59273, Training Acc : 0.722, Run Time : 19.39
INFO:root:2019-05-11 18:26:47, Epoch : 1, Step : 1088, Training Loss : 0.55460, Training Acc : 0.814, Run Time : 21.55
INFO:root:2019-05-11 18:27:02, Epoch : 1, Step : 1089, Training Loss : 0.54136, Training Acc : 0.817, Run Time : 15.43
INFO:root:2019-05-11 18:27:19, Epoch : 1, Step : 1090, Training Loss : 0.54378, Training Acc : 0.828, Run Time : 16.61
INFO:root:2019-05-11 18:27:45, Epoch : 1, Step : 1091, Training Loss : 0.57173, Training Acc : 0.822, Run Time : 25.93
INFO:root:2019-05-11 18:27:59, Epoch : 1, Step : 1092, Training Loss : 0.47336, Training Acc : 0.939, Run Time : 14.20
INFO:root:2019-05-11 18:28:33, Epoch : 1, Step : 1093, Training Loss : 0.46315, Training Acc : 0.953, Run Time : 33.98
INFO:root:2019-05-11 18:28:44, Epoch : 1, Step : 1094, Training Loss : 0.41981, Training Acc : 0.983, Run Time : 10.90
INFO:root:2019-05-11 18:29:03, Epoch : 1, Step : 1095, Training Loss : 0.46860, Training Acc : 0.953, Run Time : 19.28
INFO:root:2019-05-11 18:29:43, Epoch : 1, Step : 1096, Training Loss : 0.48535, Training Acc : 0.942, Run Time : 39.34
INFO:root:2019-05-11 18:30:00, Epoch : 1, Step : 1097, Training Loss : 0.46115, Training Acc : 0.933, Run Time : 17.71
INFO:root:2019-05-11 18:30:23, Epoch : 1, Step : 1098, Training Loss : 0.44242, Training Acc : 0.953, Run Time : 22.15
INFO:root:2019-05-11 18:30:39, Epoch : 1, Step : 1099, Training Loss : 0.44799, Training Acc : 0.950, Run Time : 16.66
INFO:root:2019-05-11 18:30:59, Epoch : 1, Step : 1100, Training Loss : 0.44978, Training Acc : 0.944, Run Time : 19.78
INFO:root:2019-05-11 18:31:07, Epoch : 1, Step : 1101, Training Loss : 0.46432, Training Acc : 0.869, Run Time : 7.54
INFO:root:2019-05-11 18:31:43, Epoch : 1, Step : 1102, Training Loss : 0.47458, Training Acc : 0.844, Run Time : 36.16
INFO:root:2019-05-11 18:32:06, Epoch : 1, Step : 1103, Training Loss : 0.45214, Training Acc : 0.819, Run Time : 23.15
INFO:root:2019-05-11 18:32:13, Epoch : 1, Step : 1104, Training Loss : 0.46894, Training Acc : 0.822, Run Time : 7.36
INFO:root:2019-05-11 18:32:21, Epoch : 1, Step : 1105, Training Loss : 0.45976, Training Acc : 0.825, Run Time : 7.98
INFO:root:2019-05-11 18:32:59, Epoch : 1, Step : 1106, Training Loss : 0.47975, Training Acc : 0.783, Run Time : 37.74
INFO:root:2019-05-11 18:33:42, Epoch : 1, Step : 1107, Training Loss : 0.53962, Training Acc : 0.750, Run Time : 43.06
INFO:root:2019-05-11 18:33:57, Epoch : 1, Step : 1108, Training Loss : 0.52529, Training Acc : 0.753, Run Time : 15.17
INFO:root:2019-05-11 18:35:01, Epoch : 1, Step : 1109, Training Loss : 0.57795, Training Acc : 0.728, Run Time : 64.11
INFO:root:2019-05-11 18:35:09, Epoch : 1, Step : 1110, Training Loss : 0.52466, Training Acc : 0.719, Run Time : 8.19
INFO:root:2019-05-11 18:35:39, Epoch : 1, Step : 1111, Training Loss : 0.50808, Training Acc : 0.719, Run Time : 29.82
INFO:root:2019-05-11 18:35:52, Epoch : 1, Step : 1112, Training Loss : 0.51480, Training Acc : 0.767, Run Time : 12.68
INFO:root:2019-05-11 18:36:01, Epoch : 1, Step : 1113, Training Loss : 0.47483, Training Acc : 0.803, Run Time : 8.67
INFO:root:2019-05-11 18:36:18, Epoch : 1, Step : 1114, Training Loss : 0.49952, Training Acc : 0.775, Run Time : 17.45
INFO:root:2019-05-11 18:36:22, Epoch : 1, Step : 1115, Training Loss : 0.48764, Training Acc : 0.767, Run Time : 3.66
INFO:root:2019-05-11 18:36:42, Epoch : 1, Step : 1116, Training Loss : 0.51890, Training Acc : 0.744, Run Time : 20.48
INFO:root:2019-05-11 18:36:56, Epoch : 1, Step : 1117, Training Loss : 0.53194, Training Acc : 0.694, Run Time : 14.27
INFO:root:2019-05-11 18:37:21, Epoch : 1, Step : 1118, Training Loss : 0.50769, Training Acc : 0.767, Run Time : 24.33
INFO:root:2019-05-11 18:37:28, Epoch : 1, Step : 1119, Training Loss : 0.54088, Training Acc : 0.647, Run Time : 7.19
INFO:root:2019-05-11 18:37:51, Epoch : 1, Step : 1120, Training Loss : 0.60860, Training Acc : 0.561, Run Time : 22.68
INFO:root:2019-05-11 18:37:58, Epoch : 1, Step : 1121, Training Loss : 0.58046, Training Acc : 0.611, Run Time : 7.22
INFO:root:2019-05-11 18:38:40, Epoch : 1, Step : 1122, Training Loss : 0.55044, Training Acc : 0.686, Run Time : 42.15
INFO:root:2019-05-11 18:38:48, Epoch : 1, Step : 1123, Training Loss : 0.50494, Training Acc : 0.736, Run Time : 7.84
INFO:root:2019-05-11 18:39:13, Epoch : 1, Step : 1124, Training Loss : 0.53943, Training Acc : 0.683, Run Time : 24.98
INFO:root:2019-05-11 18:39:22, Epoch : 1, Step : 1125, Training Loss : 0.47697, Training Acc : 0.744, Run Time : 9.39
INFO:root:2019-05-11 18:39:55, Epoch : 1, Step : 1126, Training Loss : 0.53267, Training Acc : 0.669, Run Time : 32.53
INFO:root:2019-05-11 18:40:02, Epoch : 1, Step : 1127, Training Loss : 0.52438, Training Acc : 0.686, Run Time : 7.45
INFO:root:2019-05-11 18:40:20, Epoch : 1, Step : 1128, Training Loss : 0.53702, Training Acc : 0.661, Run Time : 17.36
INFO:root:2019-05-11 18:40:23, Epoch : 1, Step : 1129, Training Loss : 0.54742, Training Acc : 0.675, Run Time : 3.39
INFO:root:2019-05-11 18:40:50, Epoch : 1, Step : 1130, Training Loss : 0.59192, Training Acc : 0.639, Run Time : 27.21
INFO:root:2019-05-11 18:41:13, Epoch : 1, Step : 1131, Training Loss : 0.58499, Training Acc : 0.675, Run Time : 22.53
INFO:root:2019-05-11 18:41:15, Epoch : 1, Step : 1132, Training Loss : 0.51936, Training Acc : 0.736, Run Time : 2.61
INFO:root:2019-05-11 18:41:35, Epoch : 1, Step : 1133, Training Loss : 0.59040, Training Acc : 0.717, Run Time : 19.71
INFO:root:2019-05-11 18:41:44, Epoch : 1, Step : 1134, Training Loss : 0.55530, Training Acc : 0.722, Run Time : 9.26
INFO:root:2019-05-11 18:41:47, Epoch : 1, Step : 1135, Training Loss : 0.52801, Training Acc : 0.711, Run Time : 2.63
INFO:root:2019-05-11 18:42:06, Epoch : 1, Step : 1136, Training Loss : 0.49546, Training Acc : 0.772, Run Time : 19.20
INFO:root:2019-05-11 18:42:25, Epoch : 1, Step : 1137, Training Loss : 0.57166, Training Acc : 0.675, Run Time : 18.70
INFO:root:2019-05-11 18:42:49, Epoch : 1, Step : 1138, Training Loss : 0.49313, Training Acc : 0.836, Run Time : 24.43
INFO:root:2019-05-11 18:42:56, Epoch : 1, Step : 1139, Training Loss : 0.49058, Training Acc : 0.828, Run Time : 6.85
INFO:root:2019-05-11 18:43:20, Epoch : 1, Step : 1140, Training Loss : 0.50747, Training Acc : 0.775, Run Time : 24.10
INFO:root:2019-05-11 18:43:25, Epoch : 1, Step : 1141, Training Loss : 0.49986, Training Acc : 0.761, Run Time : 4.95
INFO:root:2019-05-11 18:43:50, Epoch : 1, Step : 1142, Training Loss : 0.53589, Training Acc : 0.731, Run Time : 24.63
INFO:root:2019-05-11 18:43:58, Epoch : 1, Step : 1143, Training Loss : 0.46881, Training Acc : 0.803, Run Time : 8.08
INFO:root:2019-05-11 18:44:20, Epoch : 1, Step : 1144, Training Loss : 0.46684, Training Acc : 0.811, Run Time : 21.76
INFO:root:2019-05-11 18:44:36, Epoch : 1, Step : 1145, Training Loss : 0.51468, Training Acc : 0.694, Run Time : 15.96
INFO:root:2019-05-11 18:44:46, Epoch : 1, Step : 1146, Training Loss : 0.52473, Training Acc : 0.756, Run Time : 10.76
INFO:root:2019-05-11 18:45:24, Epoch : 1, Step : 1147, Training Loss : 0.50129, Training Acc : 0.764, Run Time : 37.49
INFO:root:2019-05-11 18:45:41, Epoch : 1, Step : 1148, Training Loss : 0.69015, Training Acc : 0.519, Run Time : 16.70
INFO:root:2019-05-11 18:46:03, Epoch : 1, Step : 1149, Training Loss : 0.58468, Training Acc : 0.636, Run Time : 22.48
INFO:root:2019-05-11 18:46:41, Epoch : 1, Step : 1150, Training Loss : 0.50211, Training Acc : 0.700, Run Time : 37.71
INFO:root:2019-05-11 18:47:02, Epoch : 1, Step : 1151, Training Loss : 0.59011, Training Acc : 0.672, Run Time : 21.56
INFO:root:2019-05-11 18:47:36, Epoch : 1, Step : 1152, Training Loss : 0.51371, Training Acc : 0.750, Run Time : 33.99
INFO:root:2019-05-11 18:47:43, Epoch : 1, Step : 1153, Training Loss : 0.47483, Training Acc : 0.836, Run Time : 6.67
INFO:root:2019-05-11 18:48:04, Epoch : 1, Step : 1154, Training Loss : 0.53073, Training Acc : 0.736, Run Time : 20.82
INFO:root:2019-05-11 18:48:25, Epoch : 1, Step : 1155, Training Loss : 0.52465, Training Acc : 0.714, Run Time : 21.45
INFO:root:2019-05-11 18:48:43, Epoch : 1, Step : 1156, Training Loss : 0.49640, Training Acc : 0.786, Run Time : 18.21
INFO:root:2019-05-11 18:49:25, Epoch : 1, Step : 1157, Training Loss : 0.51924, Training Acc : 0.794, Run Time : 41.17
INFO:root:2019-05-11 18:50:00, Epoch : 1, Step : 1158, Training Loss : 0.54031, Training Acc : 0.714, Run Time : 34.92
INFO:root:2019-05-11 18:50:08, Epoch : 1, Step : 1159, Training Loss : 0.61177, Training Acc : 0.625, Run Time : 8.76
INFO:root:2019-05-11 18:50:46, Epoch : 1, Step : 1160, Training Loss : 0.59054, Training Acc : 0.661, Run Time : 37.74
INFO:root:2019-05-11 18:51:07, Epoch : 1, Step : 1161, Training Loss : 0.59077, Training Acc : 0.692, Run Time : 21.19
INFO:root:2019-05-11 18:51:15, Epoch : 1, Step : 1162, Training Loss : 0.51438, Training Acc : 0.703, Run Time : 7.76
INFO:root:2019-05-11 18:52:12, Epoch : 1, Step : 1163, Training Loss : 0.61890, Training Acc : 0.639, Run Time : 57.50
INFO:root:2019-05-11 18:52:19, Epoch : 1, Step : 1164, Training Loss : 0.53875, Training Acc : 0.650, Run Time : 6.66
INFO:root:2019-05-11 18:52:52, Epoch : 1, Step : 1165, Training Loss : 0.51507, Training Acc : 0.697, Run Time : 32.41
INFO:root:2019-05-11 18:53:09, Epoch : 1, Step : 1166, Training Loss : 0.47942, Training Acc : 0.814, Run Time : 17.26
INFO:root:2019-05-11 18:53:29, Epoch : 1, Step : 1167, Training Loss : 0.44556, Training Acc : 0.858, Run Time : 19.76
INFO:root:2019-05-11 18:53:36, Epoch : 1, Step : 1168, Training Loss : 0.40177, Training Acc : 0.906, Run Time : 7.33
INFO:root:2019-05-11 18:54:16, Epoch : 1, Step : 1169, Training Loss : 0.36246, Training Acc : 0.917, Run Time : 40.36
INFO:root:2019-05-11 18:54:24, Epoch : 1, Step : 1170, Training Loss : 0.38788, Training Acc : 0.889, Run Time : 8.03
INFO:root:2019-05-11 18:55:00, Epoch : 1, Step : 1171, Training Loss : 0.39237, Training Acc : 0.911, Run Time : 36.06
INFO:root:2019-05-11 18:55:51, Epoch : 1, Step : 1172, Training Loss : 0.42425, Training Acc : 0.908, Run Time : 50.89
INFO:root:2019-05-11 18:56:32, Epoch : 1, Step : 1173, Training Loss : 0.54052, Training Acc : 0.711, Run Time : 41.03
INFO:root:2019-05-11 18:57:12, Epoch : 1, Step : 1174, Training Loss : 0.52059, Training Acc : 0.731, Run Time : 39.29
INFO:root:2019-05-11 18:57:20, Epoch : 1, Step : 1175, Training Loss : 0.41477, Training Acc : 0.850, Run Time : 8.06
INFO:root:2019-05-11 18:58:11, Epoch : 1, Step : 1176, Training Loss : 0.47380, Training Acc : 0.811, Run Time : 51.10
INFO:root:2019-05-11 18:58:56, Epoch : 1, Step : 1177, Training Loss : 0.40126, Training Acc : 0.872, Run Time : 44.96
INFO:root:2019-05-11 18:59:13, Epoch : 1, Step : 1178, Training Loss : 0.33162, Training Acc : 0.917, Run Time : 16.94
INFO:root:2019-05-11 18:59:44, Epoch : 1, Step : 1179, Training Loss : 0.49683, Training Acc : 0.772, Run Time : 31.67
INFO:root:2019-05-11 18:59:57, Epoch : 1, Step : 1180, Training Loss : 0.59615, Training Acc : 0.706, Run Time : 12.48
INFO:root:2019-05-11 19:00:39, Epoch : 1, Step : 1181, Training Loss : 0.59214, Training Acc : 0.650, Run Time : 42.67
INFO:root:2019-05-11 19:01:01, Epoch : 1, Step : 1182, Training Loss : 0.68107, Training Acc : 0.606, Run Time : 21.51
INFO:root:2019-05-11 19:01:14, Epoch : 1, Step : 1183, Training Loss : 0.47932, Training Acc : 0.772, Run Time : 12.92
INFO:root:2019-05-11 19:01:44, Epoch : 1, Step : 1184, Training Loss : 0.43078, Training Acc : 0.817, Run Time : 30.06
INFO:root:2019-05-11 19:01:52, Epoch : 1, Step : 1185, Training Loss : 0.43355, Training Acc : 0.761, Run Time : 7.61
INFO:root:2019-05-11 19:02:33, Epoch : 1, Step : 1186, Training Loss : 0.39937, Training Acc : 0.883, Run Time : 41.27
INFO:root:2019-05-11 19:02:43, Epoch : 1, Step : 1187, Training Loss : 0.43922, Training Acc : 0.808, Run Time : 10.22
INFO:root:2019-05-11 19:03:40, Epoch : 1, Step : 1188, Training Loss : 0.42374, Training Acc : 0.844, Run Time : 56.84
INFO:root:2019-05-11 19:03:48, Epoch : 1, Step : 1189, Training Loss : 0.45853, Training Acc : 0.769, Run Time : 8.25
INFO:root:2019-05-11 19:04:15, Epoch : 1, Step : 1190, Training Loss : 0.37223, Training Acc : 0.867, Run Time : 27.13
INFO:root:2019-05-11 19:04:37, Epoch : 1, Step : 1191, Training Loss : 0.48292, Training Acc : 0.722, Run Time : 21.78
INFO:root:2019-05-11 19:05:12, Epoch : 1, Step : 1192, Training Loss : 0.43394, Training Acc : 0.836, Run Time : 34.61
INFO:root:2019-05-11 19:05:37, Epoch : 1, Step : 1193, Training Loss : 0.40855, Training Acc : 0.839, Run Time : 25.69
INFO:root:2019-05-11 19:07:04, Epoch : 1, Step : 1194, Training Loss : 0.41662, Training Acc : 0.833, Run Time : 86.45
INFO:root:2019-05-11 19:07:53, Epoch : 1, Step : 1195, Training Loss : 0.47424, Training Acc : 0.772, Run Time : 48.75
INFO:root:2019-05-11 19:08:24, Epoch : 1, Step : 1196, Training Loss : 0.45583, Training Acc : 0.761, Run Time : 31.92
INFO:root:2019-05-11 19:09:01, Epoch : 1, Step : 1197, Training Loss : 0.38835, Training Acc : 0.828, Run Time : 36.98
INFO:root:2019-05-11 19:09:33, Epoch : 1, Step : 1198, Training Loss : 0.34263, Training Acc : 0.819, Run Time : 31.40
INFO:root:2019-05-11 19:09:56, Epoch : 1, Step : 1199, Training Loss : 0.32963, Training Acc : 0.819, Run Time : 22.71
INFO:root:2019-05-11 19:10:07, Epoch : 1, Step : 1200, Training Loss : 0.28652, Training Acc : 0.853, Run Time : 11.89
INFO:root:2019-05-11 19:10:25, Epoch : 1, Step : 1201, Training Loss : 0.45509, Training Acc : 0.744, Run Time : 17.32
INFO:root:2019-05-11 19:10:47, Epoch : 1, Step : 1202, Training Loss : 0.55403, Training Acc : 0.725, Run Time : 21.95
INFO:root:2019-05-11 19:10:53, Epoch : 1, Step : 1203, Training Loss : 0.65413, Training Acc : 0.694, Run Time : 6.20
INFO:root:2019-05-11 19:11:02, Epoch : 1, Step : 1204, Training Loss : 0.76092, Training Acc : 0.597, Run Time : 8.70
INFO:root:2019-05-11 19:11:22, Epoch : 1, Step : 1205, Training Loss : 0.66016, Training Acc : 0.592, Run Time : 20.73
INFO:root:2019-05-11 19:12:19, Epoch : 1, Step : 1206, Training Loss : 0.61799, Training Acc : 0.614, Run Time : 56.84
INFO:root:2019-05-11 19:12:38, Epoch : 1, Step : 1207, Training Loss : 0.51316, Training Acc : 0.672, Run Time : 19.07
INFO:root:2019-05-11 19:13:25, Epoch : 1, Step : 1208, Training Loss : 0.56386, Training Acc : 0.622, Run Time : 46.30
INFO:root:2019-05-11 19:13:48, Epoch : 1, Step : 1209, Training Loss : 0.52817, Training Acc : 0.689, Run Time : 23.41
INFO:root:2019-05-11 19:14:31, Epoch : 1, Step : 1210, Training Loss : 0.59136, Training Acc : 0.681, Run Time : 42.75
INFO:root:2019-05-11 19:15:09, Epoch : 1, Step : 1211, Training Loss : 0.51734, Training Acc : 0.742, Run Time : 38.77
INFO:root:2019-05-11 19:15:44, Epoch : 1, Step : 1212, Training Loss : 0.52542, Training Acc : 0.844, Run Time : 34.79
INFO:root:2019-05-11 19:15:55, Epoch : 1, Step : 1213, Training Loss : 0.62846, Training Acc : 0.667, Run Time : 10.77
INFO:root:2019-05-11 19:16:25, Epoch : 1, Step : 1214, Training Loss : 0.59555, Training Acc : 0.725, Run Time : 30.18
INFO:root:2019-05-11 19:17:02, Epoch : 1, Step : 1215, Training Loss : 0.58310, Training Acc : 0.644, Run Time : 36.84
INFO:root:2019-05-11 19:17:12, Epoch : 1, Step : 1216, Training Loss : 0.56993, Training Acc : 0.669, Run Time : 9.66
INFO:root:2019-05-11 19:17:50, Epoch : 1, Step : 1217, Training Loss : 0.53509, Training Acc : 0.733, Run Time : 38.25
INFO:root:2019-05-11 19:18:24, Epoch : 1, Step : 1218, Training Loss : 0.53544, Training Acc : 0.744, Run Time : 34.07
INFO:root:2019-05-11 19:19:19, Epoch : 1, Step : 1219, Training Loss : 0.50266, Training Acc : 0.750, Run Time : 54.89
INFO:root:2019-05-11 19:20:07, Epoch : 1, Step : 1220, Training Loss : 0.52891, Training Acc : 0.758, Run Time : 47.77
INFO:root:2019-05-11 19:20:33, Epoch : 1, Step : 1221, Training Loss : 0.56761, Training Acc : 0.711, Run Time : 26.30
INFO:root:2019-05-11 19:21:25, Epoch : 1, Step : 1222, Training Loss : 0.47349, Training Acc : 0.864, Run Time : 52.47
INFO:root:2019-05-11 19:21:33, Epoch : 1, Step : 1223, Training Loss : 0.46289, Training Acc : 0.864, Run Time : 7.73
INFO:root:2019-05-11 19:22:41, Epoch : 1, Step : 1224, Training Loss : 0.53823, Training Acc : 0.811, Run Time : 67.39
INFO:root:2019-05-11 19:23:19, Epoch : 1, Step : 1225, Training Loss : 0.50204, Training Acc : 0.858, Run Time : 38.07
INFO:root:2019-05-11 19:23:28, Epoch : 1, Step : 1226, Training Loss : 0.48060, Training Acc : 0.844, Run Time : 9.68
INFO:root:2019-05-11 19:24:12, Epoch : 1, Step : 1227, Training Loss : 0.42812, Training Acc : 0.867, Run Time : 44.01
INFO:root:2019-05-11 19:24:52, Epoch : 1, Step : 1228, Training Loss : 0.46578, Training Acc : 0.819, Run Time : 40.10
INFO:root:2019-05-11 19:25:01, Epoch : 1, Step : 1229, Training Loss : 0.38820, Training Acc : 0.842, Run Time : 8.87
INFO:root:2019-05-11 19:25:50, Epoch : 1, Step : 1230, Training Loss : 0.47238, Training Acc : 0.831, Run Time : 49.21
INFO:root:2019-05-11 19:26:56, Epoch : 1, Step : 1231, Training Loss : 0.72349, Training Acc : 0.578, Run Time : 65.76
INFO:root:2019-05-11 19:27:32, Epoch : 1, Step : 1232, Training Loss : 0.61555, Training Acc : 0.594, Run Time : 36.00
INFO:root:2019-05-11 19:27:39, Epoch : 1, Step : 1233, Training Loss : 0.69778, Training Acc : 0.514, Run Time : 6.99
INFO:root:2019-05-11 19:28:00, Epoch : 1, Step : 1234, Training Loss : 0.54226, Training Acc : 0.750, Run Time : 20.59
INFO:root:2019-05-11 19:28:15, Epoch : 1, Step : 1235, Training Loss : 0.58007, Training Acc : 0.653, Run Time : 15.41
INFO:root:2019-05-11 19:28:56, Epoch : 1, Step : 1236, Training Loss : 0.61166, Training Acc : 0.656, Run Time : 40.32
INFO:root:2019-05-11 19:29:15, Epoch : 1, Step : 1237, Training Loss : 0.54454, Training Acc : 0.728, Run Time : 19.75
INFO:root:2019-05-11 19:29:21, Epoch : 1, Step : 1238, Training Loss : 0.56891, Training Acc : 0.711, Run Time : 5.88
INFO:root:2019-05-11 19:29:24, Epoch : 1, Step : 1239, Training Loss : 0.58583, Training Acc : 0.633, Run Time : 3.03
INFO:root:2019-05-11 19:30:06, Epoch : 1, Step : 1240, Training Loss : 0.49681, Training Acc : 0.772, Run Time : 42.27
INFO:root:2019-05-11 19:30:12, Epoch : 1, Step : 1241, Training Loss : 0.52950, Training Acc : 0.717, Run Time : 5.76
INFO:root:2019-05-11 19:30:58, Epoch : 1, Step : 1242, Training Loss : 0.47099, Training Acc : 0.778, Run Time : 45.57
INFO:root:2019-05-11 19:31:15, Epoch : 1, Step : 1243, Training Loss : 0.45836, Training Acc : 0.767, Run Time : 16.73
INFO:root:2019-05-11 19:32:46, Epoch : 1, Step : 1244, Training Loss : 0.54756, Training Acc : 0.731, Run Time : 91.89
INFO:root:2019-05-11 19:32:55, Epoch : 1, Step : 1245, Training Loss : 0.59466, Training Acc : 0.706, Run Time : 8.09
INFO:root:2019-05-11 19:33:55, Epoch : 1, Step : 1246, Training Loss : 0.59493, Training Acc : 0.633, Run Time : 60.11
INFO:root:2019-05-11 19:34:33, Epoch : 1, Step : 1247, Training Loss : 0.49641, Training Acc : 0.756, Run Time : 38.18
INFO:root:2019-05-11 19:35:36, Epoch : 1, Step : 1248, Training Loss : 0.41529, Training Acc : 0.889, Run Time : 63.47
INFO:root:2019-05-11 19:37:07, Epoch : 1, Step : 1249, Training Loss : 0.45237, Training Acc : 0.836, Run Time : 90.63
INFO:root:2019-05-11 19:38:08, Epoch : 1, Step : 1250, Training Loss : 0.45365, Training Acc : 0.869, Run Time : 61.55
INFO:root:2019-05-11 19:38:30, Epoch : 1, Step : 1251, Training Loss : 0.50820, Training Acc : 0.733, Run Time : 21.51
INFO:root:2019-05-11 19:38:59, Epoch : 1, Step : 1252, Training Loss : 0.48459, Training Acc : 0.831, Run Time : 28.78
INFO:root:2019-05-11 19:39:18, Epoch : 1, Step : 1253, Training Loss : 0.47641, Training Acc : 0.864, Run Time : 19.54
INFO:root:2019-05-11 19:40:08, Epoch : 1, Step : 1254, Training Loss : 0.49226, Training Acc : 0.847, Run Time : 49.56
INFO:root:2019-05-11 19:40:17, Epoch : 1, Step : 1255, Training Loss : 0.56761, Training Acc : 0.711, Run Time : 9.37
INFO:root:2019-05-11 19:41:04, Epoch : 1, Step : 1256, Training Loss : 0.46224, Training Acc : 0.878, Run Time : 46.99
INFO:root:2019-05-11 19:41:11, Epoch : 1, Step : 1257, Training Loss : 0.35236, Training Acc : 0.947, Run Time : 7.01
INFO:root:2019-05-11 19:42:39, Epoch : 1, Step : 1258, Training Loss : 0.40291, Training Acc : 0.897, Run Time : 87.81
INFO:root:2019-05-11 19:42:57, Epoch : 1, Step : 1259, Training Loss : 0.43638, Training Acc : 0.864, Run Time : 17.93
INFO:root:2019-05-11 19:43:40, Epoch : 1, Step : 1260, Training Loss : 0.48908, Training Acc : 0.772, Run Time : 43.43
INFO:root:2019-05-11 19:44:05, Epoch : 1, Step : 1261, Training Loss : 0.42709, Training Acc : 0.886, Run Time : 24.73
INFO:root:2019-05-11 19:44:23, Epoch : 1, Step : 1262, Training Loss : 0.48685, Training Acc : 0.772, Run Time : 18.18
INFO:root:2019-05-11 19:44:38, Epoch : 1, Step : 1263, Training Loss : 0.40728, Training Acc : 0.892, Run Time : 14.48
INFO:root:2019-05-11 19:44:57, Epoch : 1, Step : 1264, Training Loss : 0.48067, Training Acc : 0.842, Run Time : 19.70
INFO:root:2019-05-11 19:45:07, Epoch : 1, Step : 1265, Training Loss : 0.43104, Training Acc : 0.914, Run Time : 9.42
INFO:root:2019-05-11 19:45:34, Epoch : 1, Step : 1266, Training Loss : 0.48406, Training Acc : 0.772, Run Time : 26.65
INFO:root:2019-05-11 19:45:43, Epoch : 1, Step : 1267, Training Loss : 0.41043, Training Acc : 0.925, Run Time : 9.04
INFO:root:2019-05-11 19:46:25, Epoch : 1, Step : 1268, Training Loss : 0.43521, Training Acc : 0.897, Run Time : 42.08
INFO:root:2019-05-11 19:46:55, Epoch : 1, Step : 1269, Training Loss : 0.39686, Training Acc : 0.906, Run Time : 30.28
INFO:root:2019-05-11 19:47:05, Epoch : 1, Step : 1270, Training Loss : 0.39761, Training Acc : 0.911, Run Time : 10.08
INFO:root:2019-05-11 19:47:29, Epoch : 1, Step : 1271, Training Loss : 0.40450, Training Acc : 0.897, Run Time : 23.78
INFO:root:2019-05-11 19:47:50, Epoch : 1, Step : 1272, Training Loss : 0.38457, Training Acc : 0.872, Run Time : 21.44
INFO:root:2019-05-11 19:47:53, Epoch : 1, Step : 1273, Training Loss : 0.37298, Training Acc : 0.867, Run Time : 2.99
INFO:root:2019-05-11 19:47:59, Epoch : 1, Step : 1274, Training Loss : 0.46021, Training Acc : 0.775, Run Time : 5.45
INFO:root:2019-05-11 19:48:15, Epoch : 1, Step : 1275, Training Loss : 0.43981, Training Acc : 0.792, Run Time : 16.37
INFO:root:2019-05-11 19:48:35, Epoch : 1, Step : 1276, Training Loss : 0.40921, Training Acc : 0.814, Run Time : 20.38
INFO:root:2019-05-11 19:48:46, Epoch : 1, Step : 1277, Training Loss : 0.33851, Training Acc : 0.906, Run Time : 10.10
INFO:root:2019-05-11 19:49:07, Epoch : 1, Step : 1278, Training Loss : 0.47138, Training Acc : 0.850, Run Time : 21.48
INFO:root:2019-05-11 19:49:40, Epoch : 1, Step : 1279, Training Loss : 0.38797, Training Acc : 0.892, Run Time : 32.82
INFO:root:2019-05-11 19:49:42, Epoch : 1, Step : 1280, Training Loss : 0.40844, Training Acc : 0.886, Run Time : 2.52
INFO:root:2019-05-11 19:50:00, Epoch : 1, Step : 1281, Training Loss : 0.42044, Training Acc : 0.858, Run Time : 17.26
INFO:root:2019-05-11 19:50:03, Epoch : 1, Step : 1282, Training Loss : 0.37062, Training Acc : 0.897, Run Time : 3.43
INFO:root:2019-05-11 19:50:29, Epoch : 1, Step : 1283, Training Loss : 0.38929, Training Acc : 0.914, Run Time : 25.47
INFO:root:2019-05-11 19:51:01, Epoch : 1, Step : 1284, Training Loss : 0.29118, Training Acc : 0.933, Run Time : 32.01
INFO:root:2019-05-11 19:51:18, Epoch : 1, Step : 1285, Training Loss : 0.31715, Training Acc : 0.942, Run Time : 17.80
INFO:root:2019-05-11 19:51:48, Epoch : 1, Step : 1286, Training Loss : 0.35140, Training Acc : 0.922, Run Time : 29.76
INFO:root:2019-05-11 19:52:06, Epoch : 1, Step : 1287, Training Loss : 0.41858, Training Acc : 0.853, Run Time : 18.23
INFO:root:2019-05-11 19:52:35, Epoch : 1, Step : 1288, Training Loss : 0.52472, Training Acc : 0.811, Run Time : 28.25
INFO:root:2019-05-11 19:52:45, Epoch : 1, Step : 1289, Training Loss : 0.43593, Training Acc : 0.875, Run Time : 10.56
INFO:root:2019-05-11 19:53:06, Epoch : 1, Step : 1290, Training Loss : 0.34913, Training Acc : 0.911, Run Time : 20.82
INFO:root:2019-05-11 19:53:29, Epoch : 1, Step : 1291, Training Loss : 0.39148, Training Acc : 0.875, Run Time : 23.51
INFO:root:2019-05-11 19:53:56, Epoch : 1, Step : 1292, Training Loss : 0.39367, Training Acc : 0.900, Run Time : 26.36
INFO:root:2019-05-11 19:54:16, Epoch : 1, Step : 1293, Training Loss : 0.33899, Training Acc : 0.889, Run Time : 19.71
INFO:root:2019-05-11 19:55:02, Epoch : 1, Step : 1294, Training Loss : 0.35068, Training Acc : 0.886, Run Time : 46.05
INFO:root:2019-05-11 19:55:23, Epoch : 1, Step : 1295, Training Loss : 0.33433, Training Acc : 0.878, Run Time : 21.45
INFO:root:2019-05-11 19:55:26, Epoch : 1, Step : 1296, Training Loss : 0.30214, Training Acc : 0.903, Run Time : 3.17
INFO:root:2019-05-11 19:55:52, Epoch : 1, Step : 1297, Training Loss : 0.26003, Training Acc : 0.917, Run Time : 25.36
INFO:root:2019-05-11 19:56:19, Epoch : 1, Step : 1298, Training Loss : 0.31298, Training Acc : 0.819, Run Time : 26.96
INFO:root:2019-05-11 19:56:43, Epoch : 1, Step : 1299, Training Loss : 0.28434, Training Acc : 0.853, Run Time : 24.71
INFO:root:2019-05-11 19:57:06, Epoch : 1, Step : 1300, Training Loss : 0.29797, Training Acc : 0.886, Run Time : 22.80
INFO:root:2019-05-11 19:57:43, Epoch : 1, Step : 1301, Training Loss : 0.67150, Training Acc : 0.672, Run Time : 37.15
INFO:root:2019-05-11 19:58:02, Epoch : 1, Step : 1302, Training Loss : 0.72730, Training Acc : 0.714, Run Time : 19.07
INFO:root:2019-05-11 19:58:11, Epoch : 1, Step : 1303, Training Loss : 0.79574, Training Acc : 0.694, Run Time : 9.11
INFO:root:2019-05-11 19:58:44, Epoch : 1, Step : 1304, Training Loss : 0.94858, Training Acc : 0.544, Run Time : 32.59
INFO:root:2019-05-11 19:59:06, Epoch : 1, Step : 1305, Training Loss : 0.81050, Training Acc : 0.561, Run Time : 22.38
INFO:root:2019-05-11 19:59:13, Epoch : 1, Step : 1306, Training Loss : 0.69905, Training Acc : 0.558, Run Time : 6.99
INFO:root:2019-05-11 19:59:57, Epoch : 1, Step : 1307, Training Loss : 0.80218, Training Acc : 0.611, Run Time : 43.35
INFO:root:2019-05-11 20:00:22, Epoch : 1, Step : 1308, Training Loss : 0.69095, Training Acc : 0.672, Run Time : 24.95
INFO:root:2019-05-11 20:00:50, Epoch : 1, Step : 1309, Training Loss : 0.58762, Training Acc : 0.756, Run Time : 28.21
INFO:root:2019-05-11 20:00:59, Epoch : 1, Step : 1310, Training Loss : 0.61511, Training Acc : 0.703, Run Time : 9.19
INFO:root:2019-05-11 20:01:26, Epoch : 1, Step : 1311, Training Loss : 0.64519, Training Acc : 0.564, Run Time : 27.02
INFO:root:2019-05-11 20:01:35, Epoch : 1, Step : 1312, Training Loss : 0.55306, Training Acc : 0.739, Run Time : 8.75
INFO:root:2019-05-11 20:01:56, Epoch : 1, Step : 1313, Training Loss : 0.61550, Training Acc : 0.678, Run Time : 21.44
INFO:root:2019-05-11 20:02:05, Epoch : 1, Step : 1314, Training Loss : 0.91174, Training Acc : 0.542, Run Time : 9.05
INFO:root:2019-05-11 20:02:37, Epoch : 1, Step : 1315, Training Loss : 0.62896, Training Acc : 0.689, Run Time : 31.31
INFO:root:2019-05-11 20:03:01, Epoch : 1, Step : 1316, Training Loss : 0.81840, Training Acc : 0.519, Run Time : 24.46
INFO:root:2019-05-11 20:03:09, Epoch : 1, Step : 1317, Training Loss : 0.66881, Training Acc : 0.550, Run Time : 7.82
INFO:root:2019-05-11 20:03:39, Epoch : 1, Step : 1318, Training Loss : 0.56621, Training Acc : 0.747, Run Time : 30.46
INFO:root:2019-05-11 20:03:56, Epoch : 1, Step : 1319, Training Loss : 0.57535, Training Acc : 0.678, Run Time : 16.77
INFO:root:2019-05-11 20:04:13, Epoch : 1, Step : 1320, Training Loss : 0.47800, Training Acc : 0.717, Run Time : 17.30
INFO:root:2019-05-11 20:04:30, Epoch : 1, Step : 1321, Training Loss : 0.68758, Training Acc : 0.575, Run Time : 16.41
INFO:root:2019-05-11 20:04:44, Epoch : 1, Step : 1322, Training Loss : 0.46703, Training Acc : 0.739, Run Time : 14.48
INFO:root:2019-05-11 20:05:01, Epoch : 1, Step : 1323, Training Loss : 0.53026, Training Acc : 0.667, Run Time : 16.62
INFO:root:2019-05-11 20:05:04, Epoch : 1, Step : 1324, Training Loss : 0.59428, Training Acc : 0.625, Run Time : 3.54
INFO:root:2019-05-11 20:05:06, Epoch : 1, Step : 1325, Training Loss : 0.56533, Training Acc : 0.636, Run Time : 1.56
INFO:root:2019-05-11 20:05:34, Epoch : 1, Step : 1326, Training Loss : 0.46513, Training Acc : 0.861, Run Time : 28.00
INFO:root:2019-05-11 20:05:37, Epoch : 1, Step : 1327, Training Loss : 0.53925, Training Acc : 0.808, Run Time : 2.68
INFO:root:2019-05-11 20:06:08, Epoch : 1, Step : 1328, Training Loss : 0.59742, Training Acc : 0.739, Run Time : 31.42
INFO:root:2019-05-11 20:06:34, Epoch : 1, Step : 1329, Training Loss : 0.53816, Training Acc : 0.839, Run Time : 26.06
INFO:root:2019-05-11 20:06:52, Epoch : 1, Step : 1330, Training Loss : 0.50868, Training Acc : 0.869, Run Time : 17.85
INFO:root:2019-05-11 20:07:21, Epoch : 1, Step : 1331, Training Loss : 0.49176, Training Acc : 0.864, Run Time : 28.89
INFO:root:2019-05-11 20:07:28, Epoch : 1, Step : 1332, Training Loss : 0.52442, Training Acc : 0.842, Run Time : 6.82
INFO:root:2019-05-11 20:08:19, Epoch : 1, Step : 1333, Training Loss : 0.55636, Training Acc : 0.769, Run Time : 51.29
INFO:root:2019-05-11 20:08:28, Epoch : 1, Step : 1334, Training Loss : 0.63929, Training Acc : 0.600, Run Time : 9.20
INFO:root:2019-05-11 20:08:55, Epoch : 1, Step : 1335, Training Loss : 0.52205, Training Acc : 0.900, Run Time : 27.05
INFO:root:2019-05-11 20:09:03, Epoch : 1, Step : 1336, Training Loss : 0.50305, Training Acc : 0.897, Run Time : 7.41
INFO:root:2019-05-11 20:09:43, Epoch : 1, Step : 1337, Training Loss : 0.40537, Training Acc : 0.956, Run Time : 40.46
INFO:root:2019-05-11 20:09:52, Epoch : 1, Step : 1338, Training Loss : 0.49114, Training Acc : 0.917, Run Time : 8.43
INFO:root:2019-05-11 20:10:29, Epoch : 1, Step : 1339, Training Loss : 0.51164, Training Acc : 0.839, Run Time : 37.77
INFO:root:2019-05-11 20:10:37, Epoch : 1, Step : 1340, Training Loss : 0.50975, Training Acc : 0.856, Run Time : 8.09
INFO:root:2019-05-11 20:11:16, Epoch : 1, Step : 1341, Training Loss : 0.56701, Training Acc : 0.733, Run Time : 38.13
INFO:root:2019-05-11 20:11:38, Epoch : 1, Step : 1342, Training Loss : 0.59605, Training Acc : 0.719, Run Time : 21.97
INFO:root:2019-05-11 20:11:58, Epoch : 1, Step : 1343, Training Loss : 0.44113, Training Acc : 0.947, Run Time : 20.64
INFO:root:2019-05-11 20:12:49, Epoch : 1, Step : 1344, Training Loss : 0.42764, Training Acc : 0.942, Run Time : 51.21
INFO:root:2019-05-11 20:13:31, Epoch : 1, Step : 1345, Training Loss : 0.43442, Training Acc : 0.967, Run Time : 41.26
INFO:root:2019-05-11 20:13:50, Epoch : 1, Step : 1346, Training Loss : 0.50768, Training Acc : 0.794, Run Time : 19.57
INFO:root:2019-05-11 20:14:09, Epoch : 1, Step : 1347, Training Loss : 0.45633, Training Acc : 0.906, Run Time : 18.73
INFO:root:2019-05-11 20:14:37, Epoch : 1, Step : 1348, Training Loss : 0.40123, Training Acc : 0.964, Run Time : 28.46
INFO:root:2019-05-11 20:14:59, Epoch : 1, Step : 1349, Training Loss : 0.37416, Training Acc : 0.961, Run Time : 21.23
INFO:root:2019-05-11 20:17:04, Epoch : 1, Step : 1350, Training Loss : 0.35894, Training Acc : 0.989, Run Time : 125.24
INFO:root:2019-05-11 20:17:31, Epoch : 1, Step : 1351, Training Loss : 0.32366, Training Acc : 0.978, Run Time : 27.28
INFO:root:2019-05-11 20:17:45, Epoch : 1, Step : 1352, Training Loss : 0.43622, Training Acc : 0.972, Run Time : 13.52
INFO:root:2019-05-11 20:18:17, Epoch : 1, Step : 1353, Training Loss : 0.35660, Training Acc : 0.958, Run Time : 32.57
INFO:root:2019-05-11 20:19:00, Epoch : 1, Step : 1354, Training Loss : 0.35287, Training Acc : 0.969, Run Time : 43.04
INFO:root:2019-05-11 20:19:08, Epoch : 1, Step : 1355, Training Loss : 0.37248, Training Acc : 0.961, Run Time : 7.60
INFO:root:2019-05-11 20:19:43, Epoch : 1, Step : 1356, Training Loss : 0.30890, Training Acc : 0.997, Run Time : 35.53
INFO:root:2019-05-11 20:20:32, Epoch : 1, Step : 1357, Training Loss : 0.33318, Training Acc : 0.997, Run Time : 49.09
INFO:root:2019-05-11 20:20:56, Epoch : 1, Step : 1358, Training Loss : 0.32433, Training Acc : 1.000, Run Time : 23.84
INFO:root:2019-05-11 20:21:36, Epoch : 1, Step : 1359, Training Loss : 0.39150, Training Acc : 0.958, Run Time : 39.23
INFO:root:2019-05-11 20:22:24, Epoch : 1, Step : 1360, Training Loss : 0.30209, Training Acc : 0.994, Run Time : 48.05
INFO:root:2019-05-11 20:24:19, Epoch : 1, Step : 1361, Training Loss : 0.37260, Training Acc : 0.972, Run Time : 115.17
INFO:root:2019-05-11 20:25:48, Epoch : 1, Step : 1362, Training Loss : 0.28542, Training Acc : 0.975, Run Time : 89.68
INFO:root:2019-05-11 20:26:06, Epoch : 1, Step : 1363, Training Loss : 0.47018, Training Acc : 0.869, Run Time : 17.28
INFO:root:2019-05-11 20:26:52, Epoch : 1, Step : 1364, Training Loss : 0.31486, Training Acc : 0.989, Run Time : 45.98
INFO:root:2019-05-11 20:27:13, Epoch : 1, Step : 1365, Training Loss : 0.32719, Training Acc : 0.978, Run Time : 21.02
INFO:root:2019-05-11 20:27:36, Epoch : 1, Step : 1366, Training Loss : 0.33641, Training Acc : 0.975, Run Time : 23.52
INFO:root:2019-05-11 20:28:09, Epoch : 1, Step : 1367, Training Loss : 0.27527, Training Acc : 0.989, Run Time : 32.81
INFO:root:2019-05-11 20:28:16, Epoch : 1, Step : 1368, Training Loss : 0.29612, Training Acc : 0.958, Run Time : 6.90
INFO:root:2019-05-11 20:28:43, Epoch : 1, Step : 1369, Training Loss : 0.24101, Training Acc : 0.981, Run Time : 26.53
INFO:root:2019-05-11 20:29:03, Epoch : 1, Step : 1370, Training Loss : 0.30398, Training Acc : 0.986, Run Time : 20.07
INFO:root:2019-05-11 20:29:07, Epoch : 1, Step : 1371, Training Loss : 0.28887, Training Acc : 0.958, Run Time : 4.68
INFO:root:2019-05-11 20:29:32, Epoch : 1, Step : 1372, Training Loss : 0.25890, Training Acc : 0.983, Run Time : 24.32
INFO:root:2019-05-11 20:29:37, Epoch : 1, Step : 1373, Training Loss : 0.32207, Training Acc : 0.958, Run Time : 4.95
INFO:root:2019-05-11 20:30:00, Epoch : 1, Step : 1374, Training Loss : 0.38957, Training Acc : 0.969, Run Time : 23.44
INFO:root:2019-05-11 20:30:07, Epoch : 1, Step : 1375, Training Loss : 0.26112, Training Acc : 0.956, Run Time : 7.29
INFO:root:2019-05-11 20:31:00, Epoch : 1, Step : 1376, Training Loss : 0.31292, Training Acc : 0.947, Run Time : 52.34
INFO:root:2019-05-11 20:31:16, Epoch : 1, Step : 1377, Training Loss : 0.29533, Training Acc : 0.958, Run Time : 16.75
INFO:root:2019-05-11 20:31:29, Epoch : 1, Step : 1378, Training Loss : 0.28612, Training Acc : 0.972, Run Time : 12.75
INFO:root:2019-05-11 20:31:36, Epoch : 1, Step : 1379, Training Loss : 0.28327, Training Acc : 0.953, Run Time : 6.55
INFO:root:2019-05-11 20:31:39, Epoch : 1, Step : 1380, Training Loss : 0.30369, Training Acc : 0.947, Run Time : 3.10
INFO:root:2019-05-11 20:32:08, Epoch : 1, Step : 1381, Training Loss : 0.33821, Training Acc : 0.933, Run Time : 29.01
INFO:root:2019-05-11 20:32:19, Epoch : 1, Step : 1382, Training Loss : 0.32555, Training Acc : 0.942, Run Time : 11.56
INFO:root:2019-05-11 20:32:23, Epoch : 1, Step : 1383, Training Loss : 0.29357, Training Acc : 0.961, Run Time : 3.28
INFO:root:2019-05-11 20:32:47, Epoch : 1, Step : 1384, Training Loss : 0.32990, Training Acc : 0.958, Run Time : 24.52
INFO:root:2019-05-11 20:33:32, Epoch : 1, Step : 1385, Training Loss : 0.26536, Training Acc : 0.975, Run Time : 44.69
INFO:root:2019-05-11 20:34:00, Epoch : 1, Step : 1386, Training Loss : 0.41854, Training Acc : 0.919, Run Time : 28.32
INFO:root:2019-05-11 20:34:22, Epoch : 1, Step : 1387, Training Loss : 0.25617, Training Acc : 0.972, Run Time : 21.75
INFO:root:2019-05-11 20:34:29, Epoch : 1, Step : 1388, Training Loss : 0.28068, Training Acc : 0.967, Run Time : 7.56
INFO:root:2019-05-11 20:34:53, Epoch : 1, Step : 1389, Training Loss : 0.23629, Training Acc : 0.989, Run Time : 23.08
INFO:root:2019-05-11 20:35:09, Epoch : 1, Step : 1390, Training Loss : 0.23449, Training Acc : 0.975, Run Time : 16.95
INFO:root:2019-05-11 20:35:27, Epoch : 1, Step : 1391, Training Loss : 0.33304, Training Acc : 0.961, Run Time : 17.30
INFO:root:2019-05-11 20:35:34, Epoch : 1, Step : 1392, Training Loss : 0.39447, Training Acc : 0.919, Run Time : 7.34
INFO:root:2019-05-11 20:36:00, Epoch : 1, Step : 1393, Training Loss : 0.46780, Training Acc : 0.892, Run Time : 26.30
INFO:root:2019-05-11 20:36:09, Epoch : 1, Step : 1394, Training Loss : 0.37480, Training Acc : 0.939, Run Time : 9.06
INFO:root:2019-05-11 20:36:30, Epoch : 1, Step : 1395, Training Loss : 0.33750, Training Acc : 0.953, Run Time : 21.02
INFO:root:2019-05-11 20:37:01, Epoch : 1, Step : 1396, Training Loss : 0.28335, Training Acc : 0.942, Run Time : 30.87
INFO:root:2019-05-11 20:37:12, Epoch : 1, Step : 1397, Training Loss : 0.27754, Training Acc : 0.942, Run Time : 10.25
INFO:root:2019-05-11 20:37:26, Epoch : 1, Step : 1398, Training Loss : 0.27014, Training Acc : 0.947, Run Time : 14.54
INFO:root:2019-05-11 20:37:29, Epoch : 1, Step : 1399, Training Loss : 0.31551, Training Acc : 0.903, Run Time : 2.61
INFO:root:2019-05-11 20:37:57, Epoch : 1, Step : 1400, Training Loss : 0.39870, Training Acc : 0.819, Run Time : 28.73
INFO:root:2019-05-11 20:38:14, Epoch : 1, Step : 1401, Training Loss : 0.39430, Training Acc : 0.944, Run Time : 16.08
INFO:root:2019-05-11 20:38:23, Epoch : 1, Step : 1402, Training Loss : 0.45500, Training Acc : 0.725, Run Time : 9.51
INFO:root:2019-05-11 20:38:42, Epoch : 1, Step : 1403, Training Loss : 0.49311, Training Acc : 0.842, Run Time : 18.52
INFO:root:2019-05-11 20:39:01, Epoch : 1, Step : 1404, Training Loss : 0.32525, Training Acc : 0.864, Run Time : 19.11
INFO:root:2019-05-11 20:39:03, Epoch : 1, Step : 1405, Training Loss : 0.56732, Training Acc : 0.597, Run Time : 2.34
INFO:root:2019-05-11 20:39:20, Epoch : 1, Step : 1406, Training Loss : 0.38374, Training Acc : 0.714, Run Time : 17.31
INFO:root:2019-05-11 20:39:36, Epoch : 1, Step : 1407, Training Loss : 0.36296, Training Acc : 0.869, Run Time : 15.49
INFO:root:2019-05-11 20:39:45, Epoch : 1, Step : 1408, Training Loss : 0.56813, Training Acc : 0.586, Run Time : 9.40
INFO:root:2019-05-11 20:39:59, Epoch : 1, Step : 1409, Training Loss : 0.41366, Training Acc : 0.886, Run Time : 13.90
INFO:root:2019-05-11 20:40:29, Epoch : 1, Step : 1410, Training Loss : 0.35213, Training Acc : 0.936, Run Time : 29.59
INFO:root:2019-05-11 20:40:32, Epoch : 1, Step : 1411, Training Loss : 0.42319, Training Acc : 0.908, Run Time : 3.12
INFO:root:2019-05-11 20:40:50, Epoch : 1, Step : 1412, Training Loss : 0.30616, Training Acc : 0.931, Run Time : 18.04
INFO:root:2019-05-11 20:41:06, Epoch : 1, Step : 1413, Training Loss : 0.30160, Training Acc : 0.900, Run Time : 16.30
INFO:root:2019-05-11 20:41:20, Epoch : 1, Step : 1414, Training Loss : 0.56677, Training Acc : 0.667, Run Time : 13.97
INFO:root:2019-05-11 20:41:29, Epoch : 1, Step : 1415, Training Loss : 0.42192, Training Acc : 0.708, Run Time : 9.08
INFO:root:2019-05-11 20:41:43, Epoch : 1, Step : 1416, Training Loss : 0.34135, Training Acc : 0.903, Run Time : 13.45
INFO:root:2019-05-11 20:41:45, Epoch : 1, Step : 1417, Training Loss : 0.30837, Training Acc : 0.936, Run Time : 2.71
INFO:root:2019-05-11 20:42:05, Epoch : 1, Step : 1418, Training Loss : 0.55688, Training Acc : 0.778, Run Time : 19.82
INFO:root:2019-05-11 20:42:23, Epoch : 1, Step : 1419, Training Loss : 0.28046, Training Acc : 0.969, Run Time : 17.78
INFO:root:2019-05-11 20:42:32, Epoch : 1, Step : 1420, Training Loss : 0.30892, Training Acc : 0.939, Run Time : 8.54
INFO:root:2019-05-11 20:42:35, Epoch : 1, Step : 1421, Training Loss : 0.30493, Training Acc : 0.944, Run Time : 3.49
INFO:root:2019-05-11 20:42:52, Epoch : 1, Step : 1422, Training Loss : 0.39701, Training Acc : 0.939, Run Time : 17.34
INFO:root:2019-05-11 20:43:06, Epoch : 1, Step : 1423, Training Loss : 0.39427, Training Acc : 0.908, Run Time : 13.73
INFO:root:2019-05-11 20:43:25, Epoch : 1, Step : 1424, Training Loss : 0.22174, Training Acc : 0.950, Run Time : 18.66
INFO:root:2019-05-11 20:43:32, Epoch : 1, Step : 1425, Training Loss : 0.40427, Training Acc : 0.936, Run Time : 7.07
INFO:root:2019-05-11 20:43:56, Epoch : 1, Step : 1426, Training Loss : 0.34639, Training Acc : 0.950, Run Time : 24.51
INFO:root:2019-05-11 20:43:59, Epoch : 1, Step : 1427, Training Loss : 0.28293, Training Acc : 0.956, Run Time : 2.34
INFO:root:2019-05-11 20:44:24, Epoch : 1, Step : 1428, Training Loss : 0.38300, Training Acc : 0.911, Run Time : 25.02
INFO:root:2019-05-11 20:44:38, Epoch : 1, Step : 1429, Training Loss : 0.23518, Training Acc : 0.958, Run Time : 14.74
INFO:root:2019-05-11 20:44:48, Epoch : 1, Step : 1430, Training Loss : 0.19017, Training Acc : 0.967, Run Time : 9.74
INFO:root:2019-05-11 20:45:19, Epoch : 1, Step : 1431, Training Loss : 0.23530, Training Acc : 0.972, Run Time : 30.75
INFO:root:2019-05-11 20:45:42, Epoch : 1, Step : 1432, Training Loss : 0.19749, Training Acc : 0.975, Run Time : 23.05
INFO:root:2019-05-11 20:45:55, Epoch : 1, Step : 1433, Training Loss : 0.18054, Training Acc : 0.992, Run Time : 12.56
INFO:root:2019-05-11 20:46:24, Epoch : 1, Step : 1434, Training Loss : 0.17835, Training Acc : 0.978, Run Time : 29.72
INFO:root:2019-05-11 20:46:34, Epoch : 1, Step : 1435, Training Loss : 0.16405, Training Acc : 0.981, Run Time : 10.08
INFO:root:2019-05-11 20:46:54, Epoch : 1, Step : 1436, Training Loss : 0.20153, Training Acc : 0.967, Run Time : 19.72
INFO:root:2019-05-11 20:47:13, Epoch : 1, Step : 1437, Training Loss : 0.41757, Training Acc : 0.822, Run Time : 19.33
INFO:root:2019-05-11 20:47:29, Epoch : 1, Step : 1438, Training Loss : 0.33620, Training Acc : 0.942, Run Time : 15.27
INFO:root:2019-05-11 20:47:52, Epoch : 1, Step : 1439, Training Loss : 0.27788, Training Acc : 0.956, Run Time : 23.37
INFO:root:2019-05-11 20:47:55, Epoch : 1, Step : 1440, Training Loss : 0.18546, Training Acc : 0.975, Run Time : 2.67
INFO:root:2019-05-11 20:47:58, Epoch : 1, Step : 1441, Training Loss : 0.26692, Training Acc : 0.964, Run Time : 2.86
INFO:root:2019-05-11 20:48:13, Epoch : 1, Step : 1442, Training Loss : 0.30210, Training Acc : 0.967, Run Time : 15.04
INFO:root:2019-05-11 20:48:29, Epoch : 1, Step : 1443, Training Loss : 0.39627, Training Acc : 0.811, Run Time : 16.53
INFO:root:2019-05-11 20:48:46, Epoch : 1, Step : 1444, Training Loss : 0.28124, Training Acc : 0.969, Run Time : 16.82
INFO:root:2019-05-11 20:49:02, Epoch : 1, Step : 1445, Training Loss : 0.36266, Training Acc : 0.894, Run Time : 16.25
INFO:root:2019-05-11 20:49:09, Epoch : 1, Step : 1446, Training Loss : 0.29228, Training Acc : 0.942, Run Time : 6.81
INFO:root:2019-05-11 20:49:44, Epoch : 1, Step : 1447, Training Loss : 0.29264, Training Acc : 0.933, Run Time : 34.95
INFO:root:2019-05-11 20:50:00, Epoch : 1, Step : 1448, Training Loss : 0.18305, Training Acc : 0.978, Run Time : 15.70
INFO:root:2019-05-11 20:50:20, Epoch : 1, Step : 1449, Training Loss : 0.21217, Training Acc : 0.944, Run Time : 20.40
INFO:root:2019-05-11 20:50:30, Epoch : 1, Step : 1450, Training Loss : 0.21022, Training Acc : 0.936, Run Time : 9.42
INFO:root:2019-05-11 20:51:01, Epoch : 1, Step : 1451, Training Loss : 0.27568, Training Acc : 0.906, Run Time : 31.50
INFO:root:2019-05-11 20:51:16, Epoch : 1, Step : 1452, Training Loss : 0.27250, Training Acc : 0.869, Run Time : 14.72
INFO:root:2019-05-11 20:51:51, Epoch : 1, Step : 1453, Training Loss : 0.26691, Training Acc : 0.897, Run Time : 35.20
INFO:root:2019-05-11 20:51:56, Epoch : 1, Step : 1454, Training Loss : 0.26895, Training Acc : 0.883, Run Time : 4.91
INFO:root:2019-05-11 20:52:19, Epoch : 1, Step : 1455, Training Loss : 0.26254, Training Acc : 0.897, Run Time : 22.69
INFO:root:2019-05-11 20:52:48, Epoch : 1, Step : 1456, Training Loss : 0.24578, Training Acc : 0.936, Run Time : 28.97
INFO:root:2019-05-11 20:52:55, Epoch : 1, Step : 1457, Training Loss : 0.27556, Training Acc : 0.889, Run Time : 7.51
INFO:root:2019-05-11 20:53:30, Epoch : 1, Step : 1458, Training Loss : 0.36427, Training Acc : 0.833, Run Time : 34.60
INFO:root:2019-05-11 20:53:37, Epoch : 1, Step : 1459, Training Loss : 0.38211, Training Acc : 0.828, Run Time : 7.74
INFO:root:2019-05-11 20:54:06, Epoch : 1, Step : 1460, Training Loss : 0.26799, Training Acc : 0.858, Run Time : 28.93
INFO:root:2019-05-11 20:54:10, Epoch : 1, Step : 1461, Training Loss : 0.38943, Training Acc : 0.819, Run Time : 3.37
INFO:root:2019-05-11 20:54:29, Epoch : 1, Step : 1462, Training Loss : 0.33823, Training Acc : 0.775, Run Time : 19.74
INFO:root:2019-05-11 20:54:39, Epoch : 1, Step : 1463, Training Loss : 0.49067, Training Acc : 0.675, Run Time : 9.60
INFO:root:2019-05-11 20:55:08, Epoch : 1, Step : 1464, Training Loss : 0.43787, Training Acc : 0.883, Run Time : 29.13
INFO:root:2019-05-11 20:55:26, Epoch : 1, Step : 1465, Training Loss : 0.49275, Training Acc : 0.847, Run Time : 18.00
INFO:root:2019-05-11 20:55:35, Epoch : 1, Step : 1466, Training Loss : 0.49977, Training Acc : 0.797, Run Time : 8.85
INFO:root:2019-05-11 20:55:51, Epoch : 1, Step : 1467, Training Loss : 0.41611, Training Acc : 0.836, Run Time : 16.46
INFO:root:2019-05-11 20:56:12, Epoch : 1, Step : 1468, Training Loss : 0.39298, Training Acc : 0.853, Run Time : 20.32
INFO:root:2019-05-11 20:56:18, Epoch : 1, Step : 1469, Training Loss : 0.36712, Training Acc : 0.897, Run Time : 6.56
INFO:root:2019-05-11 20:56:39, Epoch : 1, Step : 1470, Training Loss : 0.31184, Training Acc : 0.861, Run Time : 20.38
INFO:root:2019-05-11 20:56:50, Epoch : 1, Step : 1471, Training Loss : 0.34065, Training Acc : 0.869, Run Time : 11.06
INFO:root:2019-05-11 20:57:07, Epoch : 1, Step : 1472, Training Loss : 0.33327, Training Acc : 0.883, Run Time : 17.37
INFO:root:2019-05-11 20:57:33, Epoch : 1, Step : 1473, Training Loss : 0.28400, Training Acc : 0.875, Run Time : 25.75
INFO:root:2019-05-11 20:57:37, Epoch : 1, Step : 1474, Training Loss : 0.21839, Training Acc : 0.908, Run Time : 4.15
INFO:root:2019-05-11 20:57:55, Epoch : 1, Step : 1475, Training Loss : 0.23384, Training Acc : 0.931, Run Time : 18.12
INFO:root:2019-05-11 20:58:18, Epoch : 1, Step : 1476, Training Loss : 0.22066, Training Acc : 0.939, Run Time : 22.36
INFO:root:2019-05-11 20:58:21, Epoch : 1, Step : 1477, Training Loss : 0.25932, Training Acc : 0.906, Run Time : 3.49
INFO:root:2019-05-11 20:58:25, Epoch : 1, Step : 1478, Training Loss : 0.18089, Training Acc : 0.936, Run Time : 4.14
INFO:root:2019-05-11 20:58:36, Epoch : 1, Step : 1479, Training Loss : 0.65034, Training Acc : 0.625, Run Time : 10.46
INFO:root:2019-05-11 20:58:53, Epoch : 1, Step : 1480, Training Loss : 0.28620, Training Acc : 0.958, Run Time : 17.34
INFO:root:2019-05-11 20:58:55, Epoch : 1, Step : 1481, Training Loss : 0.17366, Training Acc : 0.978, Run Time : 2.27
INFO:root:2019-05-11 20:59:44, Epoch : 1, Step : 1482, Training Loss : 0.29073, Training Acc : 0.922, Run Time : 48.75
INFO:root:2019-05-11 21:00:40, Epoch : 1, Step : 1483, Training Loss : 0.40173, Training Acc : 0.803, Run Time : 56.18
INFO:root:2019-05-11 21:00:47, Epoch : 1, Step : 1484, Training Loss : 0.27950, Training Acc : 0.947, Run Time : 6.62
INFO:root:2019-05-11 21:01:23, Epoch : 1, Step : 1485, Training Loss : 0.21351, Training Acc : 0.950, Run Time : 36.44
INFO:root:2019-05-11 21:02:02, Epoch : 1, Step : 1486, Training Loss : 0.28848, Training Acc : 0.933, Run Time : 38.69
INFO:root:2019-05-11 21:02:34, Epoch : 1, Step : 1487, Training Loss : 0.41522, Training Acc : 0.861, Run Time : 32.49
INFO:root:2019-05-11 21:03:37, Epoch : 1, Step : 1488, Training Loss : 0.28024, Training Acc : 0.956, Run Time : 62.83
INFO:root:2019-05-11 21:03:41, Epoch : 1, Step : 1489, Training Loss : 0.33501, Training Acc : 0.906, Run Time : 4.21
INFO:root:2019-05-11 21:04:16, Epoch : 1, Step : 1490, Training Loss : 0.32931, Training Acc : 0.906, Run Time : 35.05
INFO:root:2019-05-11 21:04:27, Epoch : 1, Step : 1491, Training Loss : 0.31478, Training Acc : 0.928, Run Time : 10.76
INFO:root:2019-05-11 21:06:21, Epoch : 1, Step : 1492, Training Loss : 0.33321, Training Acc : 0.903, Run Time : 114.27
INFO:root:2019-05-11 21:06:31, Epoch : 1, Step : 1493, Training Loss : 0.35981, Training Acc : 0.872, Run Time : 9.02
INFO:root:2019-05-11 21:07:35, Epoch : 1, Step : 1494, Training Loss : 0.33955, Training Acc : 0.867, Run Time : 64.71
INFO:root:2019-05-11 21:07:51, Epoch : 1, Step : 1495, Training Loss : 0.31793, Training Acc : 0.875, Run Time : 15.49
INFO:root:2019-05-11 21:08:26, Epoch : 1, Step : 1496, Training Loss : 0.26374, Training Acc : 0.911, Run Time : 35.21
INFO:root:2019-05-11 21:08:34, Epoch : 1, Step : 1497, Training Loss : 0.24690, Training Acc : 0.942, Run Time : 8.17
INFO:root:2019-05-11 21:09:09, Epoch : 1, Step : 1498, Training Loss : 0.26729, Training Acc : 0.939, Run Time : 34.80
INFO:root:2019-05-11 21:09:17, Epoch : 1, Step : 1499, Training Loss : 0.26445, Training Acc : 0.917, Run Time : 8.10
INFO:root:2019-05-11 21:09:38, Epoch : 1, Step : 1500, Training Loss : 0.25250, Training Acc : 0.914, Run Time : 20.71
INFO:root:2019-05-11 21:09:43, Epoch : 1, Step : 1501, Training Loss : 1.15779, Training Acc : 0.603, Run Time : 5.28
INFO:root:2019-05-11 21:10:16, Epoch : 1, Step : 1502, Training Loss : 0.78559, Training Acc : 0.531, Run Time : 33.01
INFO:root:2019-05-11 21:10:44, Epoch : 1, Step : 1503, Training Loss : 0.38714, Training Acc : 0.850, Run Time : 28.04
INFO:root:2019-05-11 21:11:08, Epoch : 1, Step : 1504, Training Loss : 0.40175, Training Acc : 0.822, Run Time : 24.36
INFO:root:2019-05-11 21:11:16, Epoch : 1, Step : 1505, Training Loss : 0.53955, Training Acc : 0.697, Run Time : 7.60
INFO:root:2019-05-11 21:12:10, Epoch : 1, Step : 1506, Training Loss : 0.65092, Training Acc : 0.639, Run Time : 53.58
INFO:root:2019-05-11 21:12:33, Epoch : 1, Step : 1507, Training Loss : 0.67728, Training Acc : 0.533, Run Time : 23.23
INFO:root:2019-05-11 21:12:42, Epoch : 1, Step : 1508, Training Loss : 0.25997, Training Acc : 0.961, Run Time : 9.21
INFO:root:2019-05-11 21:13:07, Epoch : 1, Step : 1509, Training Loss : 0.21787, Training Acc : 0.947, Run Time : 25.41
INFO:root:2019-05-11 21:13:26, Epoch : 1, Step : 1510, Training Loss : 0.33337, Training Acc : 0.944, Run Time : 18.20
INFO:root:2019-05-11 21:14:09, Epoch : 1, Step : 1511, Training Loss : 0.39386, Training Acc : 0.850, Run Time : 43.62
INFO:root:2019-05-11 21:14:17, Epoch : 1, Step : 1512, Training Loss : 0.30585, Training Acc : 0.944, Run Time : 8.01
INFO:root:2019-05-11 21:14:37, Epoch : 1, Step : 1513, Training Loss : 0.36258, Training Acc : 0.892, Run Time : 20.07
INFO:root:2019-05-11 21:14:40, Epoch : 1, Step : 1514, Training Loss : 0.34370, Training Acc : 0.958, Run Time : 2.94
INFO:root:2019-05-11 21:15:06, Epoch : 1, Step : 1515, Training Loss : 0.51772, Training Acc : 0.778, Run Time : 25.32
INFO:root:2019-05-11 21:15:14, Epoch : 1, Step : 1516, Training Loss : 0.23746, Training Acc : 0.981, Run Time : 8.88
INFO:root:2019-05-11 21:15:37, Epoch : 1, Step : 1517, Training Loss : 0.29069, Training Acc : 0.986, Run Time : 22.09
INFO:root:2019-05-11 21:15:44, Epoch : 1, Step : 1518, Training Loss : 0.32921, Training Acc : 0.944, Run Time : 6.92
INFO:root:2019-05-11 21:16:09, Epoch : 1, Step : 1519, Training Loss : 0.22624, Training Acc : 0.992, Run Time : 25.00
INFO:root:2019-05-11 21:16:11, Epoch : 1, Step : 1520, Training Loss : 0.29877, Training Acc : 0.928, Run Time : 2.06
INFO:root:2019-05-11 21:16:29, Epoch : 1, Step : 1521, Training Loss : 0.31192, Training Acc : 0.950, Run Time : 18.63
INFO:root:2019-05-11 21:16:32, Epoch : 1, Step : 1522, Training Loss : 0.28848, Training Acc : 0.978, Run Time : 2.43
INFO:root:2019-05-11 21:16:54, Epoch : 1, Step : 1523, Training Loss : 0.33341, Training Acc : 0.936, Run Time : 22.13
INFO:root:2019-05-11 21:17:10, Epoch : 1, Step : 1524, Training Loss : 0.20576, Training Acc : 0.964, Run Time : 16.72
INFO:root:2019-05-11 21:17:38, Epoch : 1, Step : 1525, Training Loss : 0.23193, Training Acc : 0.983, Run Time : 27.08
INFO:root:2019-05-11 21:17:50, Epoch : 1, Step : 1526, Training Loss : 0.37162, Training Acc : 0.808, Run Time : 12.92
INFO:root:2019-05-11 21:18:21, Epoch : 1, Step : 1527, Training Loss : 0.45769, Training Acc : 0.747, Run Time : 30.85
INFO:root:2019-05-11 21:18:29, Epoch : 1, Step : 1528, Training Loss : 0.20984, Training Acc : 0.956, Run Time : 7.38
INFO:root:2019-05-11 21:19:05, Epoch : 1, Step : 1529, Training Loss : 0.29716, Training Acc : 0.936, Run Time : 36.27
INFO:root:2019-05-11 21:19:12, Epoch : 1, Step : 1530, Training Loss : 0.19168, Training Acc : 0.961, Run Time : 7.10
INFO:root:2019-05-11 21:19:53, Epoch : 1, Step : 1531, Training Loss : 0.22710, Training Acc : 0.975, Run Time : 40.74
INFO:root:2019-05-11 21:20:42, Epoch : 1, Step : 1532, Training Loss : 0.20936, Training Acc : 0.972, Run Time : 48.80
INFO:root:2019-05-11 21:20:49, Epoch : 1, Step : 1533, Training Loss : 0.18158, Training Acc : 0.947, Run Time : 7.31
INFO:root:2019-05-11 21:21:33, Epoch : 1, Step : 1534, Training Loss : 0.14514, Training Acc : 0.981, Run Time : 43.80
INFO:root:2019-05-11 21:21:43, Epoch : 1, Step : 1535, Training Loss : 0.24246, Training Acc : 0.983, Run Time : 9.93
INFO:root:2019-05-11 21:22:14, Epoch : 1, Step : 1536, Training Loss : 0.14959, Training Acc : 0.964, Run Time : 31.06
INFO:root:2019-05-11 21:22:39, Epoch : 1, Step : 1537, Training Loss : 0.28331, Training Acc : 0.972, Run Time : 25.70
INFO:root:2019-05-11 21:22:52, Epoch : 1, Step : 1538, Training Loss : 0.22580, Training Acc : 0.986, Run Time : 12.91
INFO:root:2019-05-11 21:23:21, Epoch : 1, Step : 1539, Training Loss : 0.33617, Training Acc : 0.917, Run Time : 29.12
INFO:root:2019-05-11 21:23:41, Epoch : 1, Step : 1540, Training Loss : 0.20377, Training Acc : 0.986, Run Time : 20.00
INFO:root:2019-05-11 21:24:00, Epoch : 1, Step : 1541, Training Loss : 0.20769, Training Acc : 0.986, Run Time : 18.87
INFO:root:2019-05-11 21:25:13, Epoch : 1, Step : 1542, Training Loss : 0.24168, Training Acc : 0.983, Run Time : 72.48
INFO:root:2019-05-11 21:25:53, Epoch : 1, Step : 1543, Training Loss : 0.26266, Training Acc : 0.989, Run Time : 39.76
INFO:root:2019-05-11 21:26:30, Epoch : 1, Step : 1544, Training Loss : 0.28930, Training Acc : 0.964, Run Time : 37.59
INFO:root:2019-05-11 21:26:44, Epoch : 1, Step : 1545, Training Loss : 0.19794, Training Acc : 1.000, Run Time : 13.82
INFO:root:2019-05-11 21:26:59, Epoch : 1, Step : 1546, Training Loss : 0.16518, Training Acc : 1.000, Run Time : 15.21
INFO:root:2019-05-11 21:27:16, Epoch : 1, Step : 1547, Training Loss : 0.20161, Training Acc : 0.986, Run Time : 17.32
INFO:root:2019-05-11 21:27:50, Epoch : 1, Step : 1548, Training Loss : 0.14010, Training Acc : 0.989, Run Time : 33.93
INFO:root:2019-05-11 21:28:25, Epoch : 1, Step : 1549, Training Loss : 0.21090, Training Acc : 0.981, Run Time : 34.90
INFO:root:2019-05-11 21:28:28, Epoch : 1, Step : 1550, Training Loss : 0.19551, Training Acc : 0.986, Run Time : 2.93
INFO:root:2019-05-11 21:28:55, Epoch : 1, Step : 1551, Training Loss : 0.22257, Training Acc : 0.983, Run Time : 26.72
INFO:root:2019-05-11 21:28:57, Epoch : 1, Step : 1552, Training Loss : 0.13824, Training Acc : 0.994, Run Time : 2.26
INFO:root:2019-05-11 21:29:21, Epoch : 1, Step : 1553, Training Loss : 0.12238, Training Acc : 0.994, Run Time : 24.24
INFO:root:2019-05-11 21:29:24, Epoch : 1, Step : 1554, Training Loss : 0.13401, Training Acc : 0.992, Run Time : 2.31
INFO:root:2019-05-11 21:29:43, Epoch : 1, Step : 1555, Training Loss : 0.38122, Training Acc : 0.892, Run Time : 19.56
INFO:root:2019-05-11 21:29:51, Epoch : 1, Step : 1556, Training Loss : 0.16360, Training Acc : 0.994, Run Time : 7.52
INFO:root:2019-05-11 21:29:54, Epoch : 1, Step : 1557, Training Loss : 0.10109, Training Acc : 0.989, Run Time : 2.82
INFO:root:2019-05-11 21:30:13, Epoch : 1, Step : 1558, Training Loss : 0.20007, Training Acc : 0.978, Run Time : 19.22
INFO:root:2019-05-11 21:30:33, Epoch : 1, Step : 1559, Training Loss : 0.27599, Training Acc : 0.978, Run Time : 19.99
INFO:root:2019-05-11 21:31:00, Epoch : 1, Step : 1560, Training Loss : 0.18324, Training Acc : 0.986, Run Time : 27.13
INFO:root:2019-05-11 21:31:10, Epoch : 1, Step : 1561, Training Loss : 0.12912, Training Acc : 0.994, Run Time : 9.95
INFO:root:2019-05-11 21:31:43, Epoch : 1, Step : 1562, Training Loss : 0.25957, Training Acc : 0.967, Run Time : 32.75
INFO:root:2019-05-11 21:32:08, Epoch : 1, Step : 1563, Training Loss : 0.15761, Training Acc : 0.969, Run Time : 25.78
INFO:root:2019-05-11 21:32:17, Epoch : 1, Step : 1564, Training Loss : 0.18608, Training Acc : 0.964, Run Time : 8.68
INFO:root:2019-05-11 21:32:30, Epoch : 1, Step : 1565, Training Loss : 0.14101, Training Acc : 0.986, Run Time : 13.23
INFO:root:2019-05-11 21:32:32, Epoch : 1, Step : 1566, Training Loss : 0.11096, Training Acc : 0.989, Run Time : 1.47
INFO:root:2019-05-11 21:32:50, Epoch : 1, Step : 1567, Training Loss : 0.15079, Training Acc : 0.992, Run Time : 18.39
INFO:root:2019-05-11 21:32:53, Epoch : 1, Step : 1568, Training Loss : 0.28954, Training Acc : 0.956, Run Time : 2.90
INFO:root:2019-05-11 21:33:13, Epoch : 1, Step : 1569, Training Loss : 0.21780, Training Acc : 0.972, Run Time : 19.56
INFO:root:2019-05-11 21:33:48, Epoch : 1, Step : 1570, Training Loss : 0.11257, Training Acc : 0.992, Run Time : 35.65
INFO:root:2019-05-11 21:34:02, Epoch : 1, Step : 1571, Training Loss : 0.17494, Training Acc : 0.956, Run Time : 13.42
INFO:root:2019-05-11 21:34:34, Epoch : 1, Step : 1572, Training Loss : 0.13635, Training Acc : 0.967, Run Time : 32.71
INFO:root:2019-05-11 21:34:55, Epoch : 1, Step : 1573, Training Loss : 0.20085, Training Acc : 0.972, Run Time : 20.78
INFO:root:2019-05-11 21:34:58, Epoch : 1, Step : 1574, Training Loss : 0.16604, Training Acc : 0.986, Run Time : 3.12
INFO:root:2019-05-11 21:35:29, Epoch : 1, Step : 1575, Training Loss : 0.17941, Training Acc : 0.986, Run Time : 30.77
INFO:root:2019-05-11 21:36:01, Epoch : 1, Step : 1576, Training Loss : 0.15927, Training Acc : 0.986, Run Time : 32.18
INFO:root:2019-05-11 21:36:28, Epoch : 1, Step : 1577, Training Loss : 0.12423, Training Acc : 0.997, Run Time : 26.92
INFO:root:2019-05-11 21:36:33, Epoch : 1, Step : 1578, Training Loss : 0.25537, Training Acc : 0.972, Run Time : 4.46
INFO:root:2019-05-11 21:36:57, Epoch : 1, Step : 1579, Training Loss : 0.12150, Training Acc : 0.975, Run Time : 23.97
INFO:root:2019-05-11 21:37:33, Epoch : 1, Step : 1580, Training Loss : 0.15362, Training Acc : 0.981, Run Time : 36.42
INFO:root:2019-05-11 21:38:03, Epoch : 1, Step : 1581, Training Loss : 0.12422, Training Acc : 0.978, Run Time : 29.79
INFO:root:2019-05-11 21:38:06, Epoch : 1, Step : 1582, Training Loss : 0.18007, Training Acc : 0.989, Run Time : 3.33
INFO:root:2019-05-11 21:38:42, Epoch : 1, Step : 1583, Training Loss : 0.18987, Training Acc : 0.981, Run Time : 35.62
INFO:root:2019-05-11 21:39:19, Epoch : 1, Step : 1584, Training Loss : 0.07711, Training Acc : 0.994, Run Time : 37.42
INFO:root:2019-05-11 21:39:30, Epoch : 1, Step : 1585, Training Loss : 0.11927, Training Acc : 0.964, Run Time : 11.19
INFO:root:2019-05-11 21:40:05, Epoch : 1, Step : 1586, Training Loss : 0.12343, Training Acc : 0.986, Run Time : 34.74
INFO:root:2019-05-11 21:40:30, Epoch : 1, Step : 1587, Training Loss : 0.05802, Training Acc : 0.994, Run Time : 24.70
INFO:root:2019-05-11 21:40:37, Epoch : 1, Step : 1588, Training Loss : 0.13466, Training Acc : 0.969, Run Time : 6.68
INFO:root:2019-05-11 21:41:04, Epoch : 1, Step : 1589, Training Loss : 0.12725, Training Acc : 0.964, Run Time : 27.54
INFO:root:2019-05-11 21:41:09, Epoch : 1, Step : 1590, Training Loss : 0.14254, Training Acc : 0.964, Run Time : 5.04
INFO:root:2019-05-11 21:41:35, Epoch : 1, Step : 1591, Training Loss : 1.03107, Training Acc : 0.525, Run Time : 25.48
INFO:root:2019-05-11 21:41:44, Epoch : 1, Step : 1592, Training Loss : 0.87628, Training Acc : 0.564, Run Time : 9.51
INFO:root:2019-05-11 21:42:34, Epoch : 1, Step : 1593, Training Loss : 0.18945, Training Acc : 0.958, Run Time : 50.31
INFO:root:2019-05-11 21:42:42, Epoch : 1, Step : 1594, Training Loss : 0.90832, Training Acc : 0.525, Run Time : 7.77
INFO:root:2019-05-11 21:43:17, Epoch : 1, Step : 1595, Training Loss : 0.49383, Training Acc : 0.558, Run Time : 34.63
INFO:root:2019-05-11 21:43:30, Epoch : 1, Step : 1596, Training Loss : 0.40226, Training Acc : 0.675, Run Time : 13.16
INFO:root:2019-05-11 21:43:49, Epoch : 1, Step : 1597, Training Loss : 0.60446, Training Acc : 0.614, Run Time : 19.47
INFO:root:2019-05-11 21:44:12, Epoch : 1, Step : 1598, Training Loss : 0.49894, Training Acc : 0.606, Run Time : 22.64
INFO:root:2019-05-11 21:44:19, Epoch : 1, Step : 1599, Training Loss : 0.39726, Training Acc : 0.792, Run Time : 7.05
INFO:root:2019-05-11 21:44:53, Epoch : 1, Step : 1600, Training Loss : 0.30485, Training Acc : 0.925, Run Time : 34.31
INFO:root:2019-05-11 21:45:20, Epoch : 1, Step : 1601, Training Loss : 0.37158, Training Acc : 0.839, Run Time : 26.95
INFO:root:2019-05-11 21:45:36, Epoch : 1, Step : 1602, Training Loss : 0.58861, Training Acc : 0.597, Run Time : 15.66
INFO:root:2019-05-11 21:45:53, Epoch : 1, Step : 1603, Training Loss : 0.41336, Training Acc : 0.883, Run Time : 17.32
INFO:root:2019-05-11 21:46:14, Epoch : 1, Step : 1604, Training Loss : 0.22639, Training Acc : 0.950, Run Time : 20.83
INFO:root:2019-05-11 21:46:30, Epoch : 1, Step : 1605, Training Loss : 0.22904, Training Acc : 0.939, Run Time : 15.84
INFO:root:2019-05-11 21:46:40, Epoch : 1, Step : 1606, Training Loss : 0.19420, Training Acc : 0.942, Run Time : 9.48
INFO:root:2019-05-11 21:47:06, Epoch : 1, Step : 1607, Training Loss : 0.23675, Training Acc : 0.939, Run Time : 26.14
INFO:root:2019-05-11 21:47:24, Epoch : 1, Step : 1608, Training Loss : 0.22346, Training Acc : 0.922, Run Time : 18.33
INFO:root:2019-05-11 21:47:37, Epoch : 1, Step : 1609, Training Loss : 0.21422, Training Acc : 0.933, Run Time : 12.56
INFO:root:2019-05-11 21:47:55, Epoch : 1, Step : 1610, Training Loss : 0.25292, Training Acc : 0.914, Run Time : 18.87
INFO:root:2019-05-11 21:48:11, Epoch : 1, Step : 1611, Training Loss : 0.21545, Training Acc : 0.919, Run Time : 15.77
INFO:root:2019-05-11 21:48:18, Epoch : 1, Step : 1612, Training Loss : 0.17151, Training Acc : 0.933, Run Time : 6.47
INFO:root:2019-05-11 21:48:44, Epoch : 1, Step : 1613, Training Loss : 0.30185, Training Acc : 0.892, Run Time : 26.06
INFO:root:2019-05-11 21:49:01, Epoch : 1, Step : 1614, Training Loss : 0.19981, Training Acc : 0.933, Run Time : 17.43
INFO:root:2019-05-11 21:49:26, Epoch : 1, Step : 1615, Training Loss : 0.19667, Training Acc : 0.931, Run Time : 24.92
INFO:root:2019-05-11 21:49:33, Epoch : 1, Step : 1616, Training Loss : 0.19345, Training Acc : 0.953, Run Time : 6.78
INFO:root:2019-05-11 21:50:03, Epoch : 1, Step : 1617, Training Loss : 0.16174, Training Acc : 0.933, Run Time : 30.24
INFO:root:2019-05-11 21:50:46, Epoch : 1, Step : 1618, Training Loss : 0.18179, Training Acc : 0.956, Run Time : 42.51
INFO:root:2019-05-11 21:51:14, Epoch : 1, Step : 1619, Training Loss : 0.19947, Training Acc : 0.933, Run Time : 28.08
INFO:root:2019-05-11 21:51:38, Epoch : 1, Step : 1620, Training Loss : 0.14266, Training Acc : 0.942, Run Time : 23.92
INFO:root:2019-05-11 21:52:08, Epoch : 1, Step : 1621, Training Loss : 0.15263, Training Acc : 0.944, Run Time : 30.80
INFO:root:2019-05-11 21:52:16, Epoch : 1, Step : 1622, Training Loss : 0.16907, Training Acc : 0.944, Run Time : 7.19
INFO:root:2019-05-11 21:52:35, Epoch : 1, Step : 1623, Training Loss : 0.14614, Training Acc : 0.944, Run Time : 19.46
INFO:root:2019-05-11 21:52:54, Epoch : 1, Step : 1624, Training Loss : 0.16203, Training Acc : 0.939, Run Time : 18.70
INFO:root:2019-05-11 21:53:16, Epoch : 1, Step : 1625, Training Loss : 0.21069, Training Acc : 0.939, Run Time : 22.21
INFO:root:2019-05-11 21:53:52, Epoch : 1, Step : 1626, Training Loss : 0.15932, Training Acc : 0.947, Run Time : 36.47
INFO:root:2019-05-11 21:54:29, Epoch : 1, Step : 1627, Training Loss : 0.15596, Training Acc : 0.944, Run Time : 36.59
INFO:root:2019-05-11 21:54:36, Epoch : 1, Step : 1628, Training Loss : 0.14495, Training Acc : 0.950, Run Time : 6.60
INFO:root:2019-05-11 21:55:06, Epoch : 1, Step : 1629, Training Loss : 0.14174, Training Acc : 0.947, Run Time : 30.34
INFO:root:2019-05-11 21:55:23, Epoch : 1, Step : 1630, Training Loss : 0.14100, Training Acc : 0.950, Run Time : 16.58
INFO:root:2019-05-11 21:55:42, Epoch : 1, Step : 1631, Training Loss : 0.20618, Training Acc : 0.928, Run Time : 19.11
INFO:root:2019-05-11 21:56:02, Epoch : 1, Step : 1632, Training Loss : 0.37951, Training Acc : 0.836, Run Time : 19.90
INFO:root:2019-05-11 21:56:04, Epoch : 1, Step : 1633, Training Loss : 0.44698, Training Acc : 0.831, Run Time : 2.72
INFO:root:2019-05-11 21:56:14, Epoch : 1, Step : 1634, Training Loss : 0.37349, Training Acc : 0.836, Run Time : 9.36
INFO:root:2019-05-11 21:56:15, Epoch : 1, Step : 1635, Training Loss : 0.39956, Training Acc : 0.817, Run Time : 1.28
INFO:root:2019-05-11 21:56:17, Epoch : 1, Step : 1636, Training Loss : 0.32894, Training Acc : 0.853, Run Time : 2.43
INFO:root:2019-05-11 21:57:12, Epoch : 1, Step : 1637, Training Loss : 0.33926, Training Acc : 0.856, Run Time : 54.14
INFO:root:2019-05-11 21:57:29, Epoch : 1, Step : 1638, Training Loss : 0.28356, Training Acc : 0.875, Run Time : 17.08
INFO:root:2019-05-11 21:57:49, Epoch : 1, Step : 1639, Training Loss : 0.21669, Training Acc : 0.886, Run Time : 20.84
INFO:root:2019-05-11 21:57:57, Epoch : 1, Step : 1640, Training Loss : 0.24663, Training Acc : 0.886, Run Time : 7.32
INFO:root:2019-05-11 21:58:37, Epoch : 1, Step : 1641, Training Loss : 0.24763, Training Acc : 0.900, Run Time : 39.96
INFO:root:2019-05-11 21:59:23, Epoch : 1, Step : 1642, Training Loss : 0.22312, Training Acc : 0.906, Run Time : 46.02
INFO:root:2019-05-11 21:59:33, Epoch : 1, Step : 1643, Training Loss : 0.22999, Training Acc : 0.892, Run Time : 10.03
INFO:root:2019-05-11 22:00:25, Epoch : 1, Step : 1644, Training Loss : 0.25164, Training Acc : 0.914, Run Time : 51.85
INFO:root:2019-05-11 22:00:31, Epoch : 1, Step : 1645, Training Loss : 0.23751, Training Acc : 0.897, Run Time : 6.61
INFO:root:2019-05-11 22:01:02, Epoch : 1, Step : 1646, Training Loss : 0.21490, Training Acc : 0.903, Run Time : 31.02
INFO:root:2019-05-11 22:01:18, Epoch : 1, Step : 1647, Training Loss : 0.23305, Training Acc : 0.911, Run Time : 15.29
INFO:root:2019-05-11 22:01:45, Epoch : 1, Step : 1648, Training Loss : 0.21134, Training Acc : 0.900, Run Time : 27.67
INFO:root:2019-05-11 22:02:19, Epoch : 1, Step : 1649, Training Loss : 0.22325, Training Acc : 0.892, Run Time : 33.82
INFO:root:2019-05-11 22:02:26, Epoch : 1, Step : 1650, Training Loss : 0.19521, Training Acc : 0.922, Run Time : 7.03
INFO:root:2019-05-11 22:03:04, Epoch : 1, Step : 1651, Training Loss : 0.20082, Training Acc : 0.919, Run Time : 37.75
INFO:root:2019-05-11 22:03:11, Epoch : 1, Step : 1652, Training Loss : 0.20484, Training Acc : 0.911, Run Time : 7.57
INFO:root:2019-05-11 22:03:34, Epoch : 1, Step : 1653, Training Loss : 0.18929, Training Acc : 0.911, Run Time : 22.26
INFO:root:2019-05-11 22:03:55, Epoch : 1, Step : 1654, Training Loss : 0.19493, Training Acc : 0.903, Run Time : 21.81
INFO:root:2019-05-11 22:04:23, Epoch : 1, Step : 1655, Training Loss : 0.17962, Training Acc : 0.906, Run Time : 27.42
INFO:root:2019-05-11 22:04:29, Epoch : 1, Step : 1656, Training Loss : 0.21793, Training Acc : 0.900, Run Time : 6.32
INFO:root:2019-05-11 22:05:05, Epoch : 1, Step : 1657, Training Loss : 0.20572, Training Acc : 0.900, Run Time : 36.34
INFO:root:2019-05-11 22:05:36, Epoch : 1, Step : 1658, Training Loss : 0.18807, Training Acc : 0.906, Run Time : 30.65
INFO:root:2019-05-11 22:06:16, Epoch : 1, Step : 1659, Training Loss : 0.21125, Training Acc : 0.928, Run Time : 40.19
INFO:root:2019-05-11 22:06:24, Epoch : 1, Step : 1660, Training Loss : 0.17287, Training Acc : 0.947, Run Time : 7.55
INFO:root:2019-05-11 22:07:05, Epoch : 1, Step : 1661, Training Loss : 0.20362, Training Acc : 0.914, Run Time : 41.18
INFO:root:2019-05-11 22:07:13, Epoch : 1, Step : 1662, Training Loss : 0.23049, Training Acc : 0.925, Run Time : 8.30
INFO:root:2019-05-11 22:07:48, Epoch : 1, Step : 1663, Training Loss : 0.21110, Training Acc : 0.928, Run Time : 34.32
INFO:root:2019-05-11 22:08:58, Epoch : 1, Step : 1664, Training Loss : 0.19148, Training Acc : 0.911, Run Time : 70.66
INFO:root:2019-05-11 22:09:05, Epoch : 1, Step : 1665, Training Loss : 0.18819, Training Acc : 0.917, Run Time : 7.00
INFO:root:2019-05-11 22:09:37, Epoch : 1, Step : 1666, Training Loss : 0.16969, Training Acc : 0.922, Run Time : 31.26
INFO:root:2019-05-11 22:09:57, Epoch : 1, Step : 1667, Training Loss : 0.17625, Training Acc : 0.908, Run Time : 20.37
INFO:root:2019-05-11 22:10:04, Epoch : 1, Step : 1668, Training Loss : 0.20058, Training Acc : 0.906, Run Time : 6.66
INFO:root:2019-05-11 22:10:40, Epoch : 1, Step : 1669, Training Loss : 0.23761, Training Acc : 0.894, Run Time : 36.18
INFO:root:2019-05-11 22:11:10, Epoch : 1, Step : 1670, Training Loss : 0.22282, Training Acc : 0.892, Run Time : 30.63
INFO:root:2019-05-11 22:11:29, Epoch : 1, Step : 1671, Training Loss : 0.18841, Training Acc : 0.914, Run Time : 18.10
INFO:root:2019-05-11 22:12:02, Epoch : 1, Step : 1672, Training Loss : 0.21542, Training Acc : 0.889, Run Time : 33.18
INFO:root:2019-05-11 22:12:24, Epoch : 1, Step : 1673, Training Loss : 0.24725, Training Acc : 0.886, Run Time : 22.67
INFO:root:2019-05-11 22:12:37, Epoch : 1, Step : 1674, Training Loss : 0.29757, Training Acc : 0.892, Run Time : 12.79
INFO:root:2019-05-11 22:13:00, Epoch : 1, Step : 1675, Training Loss : 0.25620, Training Acc : 0.894, Run Time : 22.55
INFO:root:2019-05-11 22:13:14, Epoch : 1, Step : 1676, Training Loss : 0.30722, Training Acc : 0.878, Run Time : 14.30
INFO:root:2019-05-11 22:14:06, Epoch : 1, Step : 1677, Training Loss : 0.24107, Training Acc : 0.878, Run Time : 52.03
INFO:root:2019-05-11 22:14:15, Epoch : 1, Step : 1678, Training Loss : 0.22175, Training Acc : 0.881, Run Time : 9.39
INFO:root:2019-05-11 22:14:46, Epoch : 1, Step : 1679, Training Loss : 0.25866, Training Acc : 0.897, Run Time : 30.62
INFO:root:2019-05-11 22:14:53, Epoch : 1, Step : 1680, Training Loss : 0.27268, Training Acc : 0.889, Run Time : 6.68
INFO:root:2019-05-11 22:15:22, Epoch : 1, Step : 1681, Training Loss : 0.26450, Training Acc : 0.850, Run Time : 28.99
INFO:root:2019-05-11 22:15:29, Epoch : 1, Step : 1682, Training Loss : 0.24856, Training Acc : 0.892, Run Time : 7.07
INFO:root:2019-05-11 22:16:07, Epoch : 1, Step : 1683, Training Loss : 0.25639, Training Acc : 0.875, Run Time : 37.90
INFO:root:2019-05-11 22:16:32, Epoch : 1, Step : 1684, Training Loss : 0.25628, Training Acc : 0.889, Run Time : 25.28
INFO:root:2019-05-11 22:16:49, Epoch : 1, Step : 1685, Training Loss : 0.22853, Training Acc : 0.889, Run Time : 17.27
INFO:root:2019-05-11 22:17:17, Epoch : 1, Step : 1686, Training Loss : 0.25619, Training Acc : 0.889, Run Time : 27.29
INFO:root:2019-05-11 22:17:25, Epoch : 1, Step : 1687, Training Loss : 0.26045, Training Acc : 0.861, Run Time : 8.57
INFO:root:2019-05-11 22:17:29, Epoch : 1, Step : 1688, Training Loss : 0.23568, Training Acc : 0.864, Run Time : 3.39
INFO:root:2019-05-11 22:17:44, Epoch : 1, Step : 1689, Training Loss : 0.23745, Training Acc : 0.869, Run Time : 15.42
INFO:root:2019-05-11 22:18:03, Epoch : 1, Step : 1690, Training Loss : 0.25923, Training Acc : 0.872, Run Time : 18.59
INFO:root:2019-05-11 22:18:27, Epoch : 1, Step : 1691, Training Loss : 0.20112, Training Acc : 0.897, Run Time : 24.18
INFO:root:2019-05-11 22:18:43, Epoch : 1, Step : 1692, Training Loss : 0.28089, Training Acc : 0.900, Run Time : 16.51
INFO:root:2019-05-11 22:19:07, Epoch : 1, Step : 1693, Training Loss : 0.21402, Training Acc : 0.894, Run Time : 23.41
INFO:root:2019-05-11 22:19:33, Epoch : 1, Step : 1694, Training Loss : 0.18967, Training Acc : 0.919, Run Time : 26.16
INFO:root:2019-05-11 22:19:40, Epoch : 1, Step : 1695, Training Loss : 0.19906, Training Acc : 0.906, Run Time : 6.97
INFO:root:2019-05-11 22:20:13, Epoch : 1, Step : 1696, Training Loss : 0.18554, Training Acc : 0.914, Run Time : 33.57
INFO:root:2019-05-11 22:20:31, Epoch : 1, Step : 1697, Training Loss : 0.19832, Training Acc : 0.897, Run Time : 17.59
INFO:root:2019-05-11 22:20:57, Epoch : 1, Step : 1698, Training Loss : 0.20008, Training Acc : 0.892, Run Time : 25.83
INFO:root:2019-05-11 22:21:05, Epoch : 1, Step : 1699, Training Loss : 0.18718, Training Acc : 0.911, Run Time : 8.33
INFO:root:2019-05-11 22:21:22, Epoch : 1, Step : 1700, Training Loss : 0.19589, Training Acc : 0.889, Run Time : 17.08
INFO:root:2019-05-11 22:21:39, Epoch : 1, Step : 1701, Training Loss : 0.69338, Training Acc : 0.664, Run Time : 17.13
INFO:root:2019-05-11 22:21:56, Epoch : 1, Step : 1702, Training Loss : 0.61748, Training Acc : 0.689, Run Time : 16.75
INFO:root:2019-05-11 22:22:19, Epoch : 1, Step : 1703, Training Loss : 0.64184, Training Acc : 0.689, Run Time : 22.50
INFO:root:2019-05-11 22:22:39, Epoch : 1, Step : 1704, Training Loss : 0.37226, Training Acc : 0.844, Run Time : 20.76
INFO:root:2019-05-11 22:22:46, Epoch : 1, Step : 1705, Training Loss : 0.42137, Training Acc : 0.817, Run Time : 6.72
INFO:root:2019-05-11 22:22:55, Epoch : 1, Step : 1706, Training Loss : 0.50295, Training Acc : 0.764, Run Time : 8.53
INFO:root:2019-05-11 22:23:01, Epoch : 1, Step : 1707, Training Loss : 0.36518, Training Acc : 0.850, Run Time : 6.76
INFO:root:2019-05-11 22:23:34, Epoch : 1, Step : 1708, Training Loss : 0.36344, Training Acc : 0.856, Run Time : 32.26
INFO:root:2019-05-11 22:23:39, Epoch : 1, Step : 1709, Training Loss : 0.17422, Training Acc : 0.942, Run Time : 5.61
INFO:root:2019-05-11 22:23:42, Epoch : 1, Step : 1710, Training Loss : 0.28475, Training Acc : 0.919, Run Time : 3.16
INFO:root:2019-05-11 22:24:03, Epoch : 1, Step : 1711, Training Loss : 0.20338, Training Acc : 0.950, Run Time : 20.97
INFO:root:2019-05-11 22:24:22, Epoch : 1, Step : 1712, Training Loss : 0.38616, Training Acc : 0.781, Run Time : 19.01
INFO:root:2019-05-11 22:24:32, Epoch : 1, Step : 1713, Training Loss : 0.42446, Training Acc : 0.756, Run Time : 9.28
INFO:root:2019-05-11 22:24:58, Epoch : 1, Step : 1714, Training Loss : 0.21795, Training Acc : 0.953, Run Time : 26.39
INFO:root:2019-05-11 22:25:12, Epoch : 1, Step : 1715, Training Loss : 0.37822, Training Acc : 0.842, Run Time : 13.88
INFO:root:2019-05-11 22:25:38, Epoch : 1, Step : 1716, Training Loss : 0.28272, Training Acc : 0.936, Run Time : 26.56
INFO:root:2019-05-11 22:26:03, Epoch : 1, Step : 1717, Training Loss : 0.41572, Training Acc : 0.878, Run Time : 24.81
INFO:root:2019-05-11 22:26:18, Epoch : 1, Step : 1718, Training Loss : 0.20990, Training Acc : 0.964, Run Time : 14.35
INFO:root:2019-05-11 22:26:37, Epoch : 1, Step : 1719, Training Loss : 0.30441, Training Acc : 0.897, Run Time : 18.98
INFO:root:2019-05-11 22:26:50, Epoch : 1, Step : 1720, Training Loss : 0.31162, Training Acc : 0.900, Run Time : 13.61
INFO:root:2019-05-11 22:26:57, Epoch : 1, Step : 1721, Training Loss : 0.34362, Training Acc : 0.850, Run Time : 6.59
INFO:root:2019-05-11 22:27:02, Epoch : 1, Step : 1722, Training Loss : 0.23597, Training Acc : 0.953, Run Time : 4.94
INFO:root:2019-05-11 22:27:17, Epoch : 1, Step : 1723, Training Loss : 0.27588, Training Acc : 0.908, Run Time : 14.99
INFO:root:2019-05-11 22:27:37, Epoch : 1, Step : 1724, Training Loss : 0.18642, Training Acc : 0.931, Run Time : 19.83
INFO:root:2019-05-11 22:28:13, Epoch : 1, Step : 1725, Training Loss : 0.29439, Training Acc : 0.944, Run Time : 36.20
INFO:root:2019-05-11 22:28:19, Epoch : 1, Step : 1726, Training Loss : 0.27924, Training Acc : 0.908, Run Time : 6.41
INFO:root:2019-05-11 22:28:46, Epoch : 1, Step : 1727, Training Loss : 0.28678, Training Acc : 0.950, Run Time : 26.37
INFO:root:2019-05-11 22:29:06, Epoch : 1, Step : 1728, Training Loss : 0.39808, Training Acc : 0.875, Run Time : 20.58
INFO:root:2019-05-11 22:29:14, Epoch : 1, Step : 1729, Training Loss : 0.32302, Training Acc : 0.886, Run Time : 8.13
INFO:root:2019-05-11 22:29:34, Epoch : 1, Step : 1730, Training Loss : 0.23350, Training Acc : 0.978, Run Time : 19.62
INFO:root:2019-05-11 22:30:09, Epoch : 1, Step : 1731, Training Loss : 0.13643, Training Acc : 0.983, Run Time : 35.23
INFO:root:2019-05-11 22:30:15, Epoch : 1, Step : 1732, Training Loss : 0.13236, Training Acc : 0.981, Run Time : 5.83
INFO:root:2019-05-11 22:30:26, Epoch : 1, Step : 1733, Training Loss : 0.12147, Training Acc : 0.978, Run Time : 10.85
INFO:root:2019-05-11 22:30:48, Epoch : 1, Step : 1734, Training Loss : 0.56628, Training Acc : 0.639, Run Time : 22.64
INFO:root:2019-05-11 22:31:08, Epoch : 1, Step : 1735, Training Loss : 0.11370, Training Acc : 0.992, Run Time : 19.72
INFO:root:2019-05-11 22:31:28, Epoch : 1, Step : 1736, Training Loss : 0.14597, Training Acc : 0.967, Run Time : 20.07
INFO:root:2019-05-11 22:31:44, Epoch : 1, Step : 1737, Training Loss : 0.11490, Training Acc : 0.994, Run Time : 15.90
INFO:root:2019-05-11 22:32:07, Epoch : 1, Step : 1738, Training Loss : 0.15956, Training Acc : 0.983, Run Time : 22.90
INFO:root:2019-05-11 22:32:11, Epoch : 1, Step : 1739, Training Loss : 0.14527, Training Acc : 0.981, Run Time : 3.79
INFO:root:2019-05-11 22:32:29, Epoch : 1, Step : 1740, Training Loss : 0.38217, Training Acc : 0.928, Run Time : 18.62
INFO:root:2019-05-11 22:32:44, Epoch : 1, Step : 1741, Training Loss : 0.13989, Training Acc : 0.950, Run Time : 14.96
INFO:root:2019-05-11 22:32:48, Epoch : 1, Step : 1742, Training Loss : 0.13854, Training Acc : 0.956, Run Time : 3.54
INFO:root:2019-05-11 22:33:23, Epoch : 1, Step : 1743, Training Loss : 0.15802, Training Acc : 0.958, Run Time : 35.54
INFO:root:2019-05-11 22:33:58, Epoch : 1, Step : 1744, Training Loss : 0.13200, Training Acc : 0.967, Run Time : 34.45
INFO:root:2019-05-11 22:34:07, Epoch : 1, Step : 1745, Training Loss : 0.12552, Training Acc : 0.989, Run Time : 9.20
INFO:root:2019-05-11 22:34:33, Epoch : 1, Step : 1746, Training Loss : 0.16448, Training Acc : 0.981, Run Time : 25.86
INFO:root:2019-05-11 22:34:48, Epoch : 1, Step : 1747, Training Loss : 0.24770, Training Acc : 0.947, Run Time : 15.50
INFO:root:2019-05-11 22:35:08, Epoch : 1, Step : 1748, Training Loss : 0.15757, Training Acc : 0.981, Run Time : 19.47
INFO:root:2019-05-11 22:35:14, Epoch : 1, Step : 1749, Training Loss : 0.12229, Training Acc : 0.992, Run Time : 5.90
INFO:root:2019-05-11 22:35:51, Epoch : 1, Step : 1750, Training Loss : 0.17624, Training Acc : 0.969, Run Time : 36.94
INFO:root:2019-05-11 22:36:02, Epoch : 1, Step : 1751, Training Loss : 0.13404, Training Acc : 0.986, Run Time : 11.64
INFO:root:2019-05-11 22:36:15, Epoch : 1, Step : 1752, Training Loss : 0.17981, Training Acc : 0.942, Run Time : 12.28
INFO:root:2019-05-11 22:36:18, Epoch : 1, Step : 1753, Training Loss : 0.54474, Training Acc : 0.706, Run Time : 3.23
INFO:root:2019-05-11 22:36:53, Epoch : 1, Step : 1754, Training Loss : 0.14068, Training Acc : 0.942, Run Time : 35.00
INFO:root:2019-05-11 22:37:21, Epoch : 1, Step : 1755, Training Loss : 0.21592, Training Acc : 0.911, Run Time : 28.40
INFO:root:2019-05-11 22:37:28, Epoch : 1, Step : 1756, Training Loss : 0.12397, Training Acc : 0.969, Run Time : 6.90
INFO:root:2019-05-11 22:37:45, Epoch : 1, Step : 1757, Training Loss : 0.14703, Training Acc : 0.953, Run Time : 17.28
INFO:root:2019-05-11 22:38:06, Epoch : 1, Step : 1758, Training Loss : 0.19342, Training Acc : 0.958, Run Time : 20.97
INFO:root:2019-05-11 22:38:25, Epoch : 1, Step : 1759, Training Loss : 0.16345, Training Acc : 0.936, Run Time : 18.30
INFO:root:2019-05-11 22:38:40, Epoch : 1, Step : 1760, Training Loss : 0.17763, Training Acc : 0.961, Run Time : 15.61
INFO:root:2019-05-11 22:39:00, Epoch : 1, Step : 1761, Training Loss : 0.27628, Training Acc : 0.886, Run Time : 19.86
INFO:root:2019-05-11 22:39:26, Epoch : 1, Step : 1762, Training Loss : 0.10180, Training Acc : 0.978, Run Time : 25.33
INFO:root:2019-05-11 22:39:47, Epoch : 1, Step : 1763, Training Loss : 0.21129, Training Acc : 0.939, Run Time : 21.18
INFO:root:2019-05-11 22:40:09, Epoch : 1, Step : 1764, Training Loss : 0.17578, Training Acc : 0.944, Run Time : 22.03
INFO:root:2019-05-11 22:40:24, Epoch : 1, Step : 1765, Training Loss : 0.13136, Training Acc : 0.961, Run Time : 15.53
INFO:root:2019-05-11 22:40:39, Epoch : 1, Step : 1766, Training Loss : 0.16178, Training Acc : 0.956, Run Time : 14.41
INFO:root:2019-05-11 22:40:55, Epoch : 1, Step : 1767, Training Loss : 0.10108, Training Acc : 0.978, Run Time : 16.73
INFO:root:2019-05-11 22:41:09, Epoch : 1, Step : 1768, Training Loss : 0.16288, Training Acc : 0.936, Run Time : 13.77
INFO:root:2019-05-11 22:41:30, Epoch : 1, Step : 1769, Training Loss : 0.08513, Training Acc : 0.992, Run Time : 20.38
INFO:root:2019-05-11 22:41:49, Epoch : 1, Step : 1770, Training Loss : 0.16391, Training Acc : 0.956, Run Time : 19.66
INFO:root:2019-05-11 22:42:11, Epoch : 1, Step : 1771, Training Loss : 0.09913, Training Acc : 0.969, Run Time : 21.50
INFO:root:2019-05-11 22:42:27, Epoch : 1, Step : 1772, Training Loss : 0.10995, Training Acc : 0.983, Run Time : 16.73
INFO:root:2019-05-11 22:42:37, Epoch : 1, Step : 1773, Training Loss : 0.09688, Training Acc : 0.975, Run Time : 9.73
INFO:root:2019-05-11 22:42:58, Epoch : 1, Step : 1774, Training Loss : 0.18236, Training Acc : 0.928, Run Time : 20.77
INFO:root:2019-05-11 22:43:14, Epoch : 1, Step : 1775, Training Loss : 0.15008, Training Acc : 0.953, Run Time : 15.96
INFO:root:2019-05-11 22:43:24, Epoch : 1, Step : 1776, Training Loss : 0.18718, Training Acc : 0.942, Run Time : 10.28
INFO:root:2019-05-11 22:43:45, Epoch : 1, Step : 1777, Training Loss : 0.14049, Training Acc : 0.947, Run Time : 20.32
INFO:root:2019-05-11 22:43:59, Epoch : 1, Step : 1778, Training Loss : 0.12714, Training Acc : 0.956, Run Time : 14.03
INFO:root:2019-05-11 22:44:17, Epoch : 1, Step : 1779, Training Loss : 0.10107, Training Acc : 0.967, Run Time : 18.40
INFO:root:2019-05-11 22:44:42, Epoch : 1, Step : 1780, Training Loss : 0.16989, Training Acc : 0.936, Run Time : 25.25
INFO:root:2019-05-11 22:45:03, Epoch : 1, Step : 1781, Training Loss : 0.29906, Training Acc : 0.906, Run Time : 20.67
INFO:root:2019-05-11 22:45:19, Epoch : 1, Step : 1782, Training Loss : 0.22464, Training Acc : 0.928, Run Time : 16.15
INFO:root:2019-05-11 22:45:50, Epoch : 1, Step : 1783, Training Loss : 0.24704, Training Acc : 0.886, Run Time : 31.46
INFO:root:2019-05-11 22:46:10, Epoch : 1, Step : 1784, Training Loss : 0.29093, Training Acc : 0.839, Run Time : 19.22
INFO:root:2019-05-11 22:46:33, Epoch : 1, Step : 1785, Training Loss : 0.21313, Training Acc : 0.903, Run Time : 23.69
INFO:root:2019-05-11 22:46:56, Epoch : 1, Step : 1786, Training Loss : 0.22762, Training Acc : 0.881, Run Time : 22.14
INFO:root:2019-05-11 22:47:03, Epoch : 1, Step : 1787, Training Loss : 0.13118, Training Acc : 0.956, Run Time : 7.27
INFO:root:2019-05-11 22:47:44, Epoch : 1, Step : 1788, Training Loss : 0.21539, Training Acc : 0.914, Run Time : 41.13
INFO:root:2019-05-11 22:48:01, Epoch : 1, Step : 1789, Training Loss : 0.15784, Training Acc : 0.942, Run Time : 17.37
INFO:root:2019-05-11 22:48:25, Epoch : 1, Step : 1790, Training Loss : 0.20874, Training Acc : 0.914, Run Time : 23.72
INFO:root:2019-05-11 22:48:43, Epoch : 1, Step : 1791, Training Loss : 0.14217, Training Acc : 0.961, Run Time : 18.35
INFO:root:2019-05-11 22:48:51, Epoch : 1, Step : 1792, Training Loss : 0.15780, Training Acc : 0.964, Run Time : 7.28
INFO:root:2019-05-11 22:49:23, Epoch : 1, Step : 1793, Training Loss : 0.12777, Training Acc : 0.972, Run Time : 31.88
INFO:root:2019-05-11 22:50:10, Epoch : 1, Step : 1794, Training Loss : 0.12620, Training Acc : 0.967, Run Time : 47.63
INFO:root:2019-05-11 22:50:40, Epoch : 1, Step : 1795, Training Loss : 0.11675, Training Acc : 0.967, Run Time : 30.15
INFO:root:2019-05-11 22:51:07, Epoch : 1, Step : 1796, Training Loss : 0.14638, Training Acc : 0.961, Run Time : 26.93
INFO:root:2019-05-11 22:51:14, Epoch : 1, Step : 1797, Training Loss : 0.18284, Training Acc : 0.944, Run Time : 7.20
INFO:root:2019-05-11 22:51:46, Epoch : 1, Step : 1798, Training Loss : 0.18790, Training Acc : 0.933, Run Time : 31.42
INFO:root:2019-05-11 22:51:53, Epoch : 1, Step : 1799, Training Loss : 0.25654, Training Acc : 0.892, Run Time : 6.73
INFO:root:2019-05-11 22:52:15, Epoch : 1, Step : 1800, Training Loss : 0.45845, Training Acc : 0.719, Run Time : 22.28
INFO:root:2019-05-11 22:52:38, Epoch : 1, Step : 1801, Training Loss : 0.46164, Training Acc : 0.761, Run Time : 23.27
INFO:root:2019-05-11 22:52:52, Epoch : 1, Step : 1802, Training Loss : 0.39390, Training Acc : 0.747, Run Time : 13.55
INFO:root:2019-05-11 22:53:23, Epoch : 1, Step : 1803, Training Loss : 0.98511, Training Acc : 0.411, Run Time : 31.35
INFO:root:2019-05-11 22:53:40, Epoch : 1, Step : 1804, Training Loss : 0.96288, Training Acc : 0.431, Run Time : 16.96
INFO:root:2019-05-11 22:53:57, Epoch : 1, Step : 1805, Training Loss : 0.32846, Training Acc : 0.817, Run Time : 17.31
INFO:root:2019-05-11 22:54:00, Epoch : 1, Step : 1806, Training Loss : 0.49648, Training Acc : 0.697, Run Time : 3.18
INFO:root:2019-05-11 22:54:17, Epoch : 1, Step : 1807, Training Loss : 0.39125, Training Acc : 0.778, Run Time : 16.78
INFO:root:2019-05-11 22:54:35, Epoch : 1, Step : 1808, Training Loss : 0.28227, Training Acc : 0.881, Run Time : 17.82
INFO:root:2019-05-11 22:54:37, Epoch : 1, Step : 1809, Training Loss : 0.30077, Training Acc : 0.864, Run Time : 1.86
INFO:root:2019-05-11 22:54:59, Epoch : 1, Step : 1810, Training Loss : 0.29635, Training Acc : 0.894, Run Time : 22.26
INFO:root:2019-05-11 22:55:03, Epoch : 1, Step : 1811, Training Loss : 0.43797, Training Acc : 0.819, Run Time : 3.73
INFO:root:2019-05-11 22:55:21, Epoch : 1, Step : 1812, Training Loss : 0.31360, Training Acc : 0.881, Run Time : 18.23
INFO:root:2019-05-11 22:55:35, Epoch : 1, Step : 1813, Training Loss : 0.38954, Training Acc : 0.847, Run Time : 14.06
INFO:root:2019-05-11 22:55:54, Epoch : 1, Step : 1814, Training Loss : 0.31599, Training Acc : 0.883, Run Time : 18.29
INFO:root:2019-05-11 22:56:00, Epoch : 1, Step : 1815, Training Loss : 0.46759, Training Acc : 0.778, Run Time : 6.51
INFO:root:2019-05-11 22:56:26, Epoch : 1, Step : 1816, Training Loss : 0.52986, Training Acc : 0.708, Run Time : 25.89
INFO:root:2019-05-11 22:56:41, Epoch : 1, Step : 1817, Training Loss : 0.34669, Training Acc : 0.875, Run Time : 15.45
INFO:root:2019-05-11 22:56:48, Epoch : 1, Step : 1818, Training Loss : 0.31126, Training Acc : 0.914, Run Time : 6.49
INFO:root:2019-05-11 22:56:50, Epoch : 1, Step : 1819, Training Loss : 0.50703, Training Acc : 0.736, Run Time : 2.61
INFO:root:2019-05-11 22:57:15, Epoch : 1, Step : 1820, Training Loss : 0.27517, Training Acc : 0.914, Run Time : 24.87
INFO:root:2019-05-11 22:57:22, Epoch : 1, Step : 1821, Training Loss : 0.25178, Training Acc : 0.914, Run Time : 6.84
INFO:root:2019-05-11 22:57:40, Epoch : 1, Step : 1822, Training Loss : 0.34065, Training Acc : 0.889, Run Time : 17.34
INFO:root:2019-05-11 22:58:05, Epoch : 1, Step : 1823, Training Loss : 0.23568, Training Acc : 0.928, Run Time : 25.58
INFO:root:2019-05-11 22:58:25, Epoch : 1, Step : 1824, Training Loss : 0.28447, Training Acc : 0.919, Run Time : 20.26
INFO:root:2019-05-11 22:58:49, Epoch : 1, Step : 1825, Training Loss : 0.35098, Training Acc : 0.892, Run Time : 23.50
INFO:root:2019-05-11 22:59:03, Epoch : 1, Step : 1826, Training Loss : 0.30030, Training Acc : 0.919, Run Time : 14.43
INFO:root:2019-05-11 22:59:07, Epoch : 1, Step : 1827, Training Loss : 0.24643, Training Acc : 0.933, Run Time : 3.48
INFO:root:2019-05-11 22:59:27, Epoch : 1, Step : 1828, Training Loss : 0.26289, Training Acc : 0.953, Run Time : 19.80
INFO:root:2019-05-11 22:59:44, Epoch : 1, Step : 1829, Training Loss : 0.18733, Training Acc : 0.967, Run Time : 17.23
INFO:root:2019-05-11 22:59:51, Epoch : 1, Step : 1830, Training Loss : 0.17822, Training Acc : 0.978, Run Time : 6.92
INFO:root:2019-05-11 23:00:23, Epoch : 1, Step : 1831, Training Loss : 0.22175, Training Acc : 0.939, Run Time : 32.21
INFO:root:2019-05-11 23:00:35, Epoch : 1, Step : 1832, Training Loss : 0.16577, Training Acc : 0.950, Run Time : 11.68
INFO:root:2019-05-11 23:00:54, Epoch : 1, Step : 1833, Training Loss : 0.21521, Training Acc : 0.956, Run Time : 19.20
INFO:root:2019-05-11 23:01:11, Epoch : 1, Step : 1834, Training Loss : 0.38364, Training Acc : 0.867, Run Time : 16.72
INFO:root:2019-05-11 23:01:31, Epoch : 1, Step : 1835, Training Loss : 1.20511, Training Acc : 0.481, Run Time : 20.65
INFO:root:2019-05-11 23:01:37, Epoch : 1, Step : 1836, Training Loss : 0.41868, Training Acc : 0.875, Run Time : 5.60
INFO:root:2019-05-11 23:01:41, Epoch : 1, Step : 1837, Training Loss : 0.28843, Training Acc : 0.928, Run Time : 4.30
INFO:root:2019-05-11 23:01:47, Epoch : 1, Step : 1838, Training Loss : 0.40121, Training Acc : 0.772, Run Time : 6.24
INFO:root:2019-05-11 23:02:07, Epoch : 1, Step : 1839, Training Loss : 0.47640, Training Acc : 0.658, Run Time : 19.41
INFO:root:2019-05-11 23:02:28, Epoch : 1, Step : 1840, Training Loss : 0.19567, Training Acc : 0.956, Run Time : 21.42
INFO:root:2019-05-11 23:02:35, Epoch : 1, Step : 1841, Training Loss : 0.21893, Training Acc : 0.939, Run Time : 7.13
INFO:root:2019-05-11 23:03:04, Epoch : 1, Step : 1842, Training Loss : 0.23731, Training Acc : 0.983, Run Time : 28.30
INFO:root:2019-05-11 23:03:07, Epoch : 1, Step : 1843, Training Loss : 0.72059, Training Acc : 0.550, Run Time : 3.80
INFO:root:2019-05-11 23:03:24, Epoch : 1, Step : 1844, Training Loss : 0.65724, Training Acc : 0.553, Run Time : 16.84
INFO:root:2019-05-11 23:03:53, Epoch : 1, Step : 1845, Training Loss : 0.55755, Training Acc : 0.658, Run Time : 28.79
INFO:root:2019-05-11 23:03:59, Epoch : 1, Step : 1846, Training Loss : 0.51558, Training Acc : 0.736, Run Time : 5.69
INFO:root:2019-05-11 23:04:40, Epoch : 1, Step : 1847, Training Loss : 0.31007, Training Acc : 0.958, Run Time : 41.09
INFO:root:2019-05-11 23:04:46, Epoch : 1, Step : 1848, Training Loss : 0.32521, Training Acc : 0.944, Run Time : 6.12
INFO:root:2019-05-11 23:05:12, Epoch : 1, Step : 1849, Training Loss : 0.37231, Training Acc : 0.817, Run Time : 26.42
INFO:root:2019-05-11 23:05:31, Epoch : 1, Step : 1850, Training Loss : 0.52045, Training Acc : 0.594, Run Time : 18.67
INFO:root:2019-05-11 23:05:47, Epoch : 1, Step : 1851, Training Loss : 0.68458, Training Acc : 0.558, Run Time : 16.41
INFO:root:2019-05-11 23:05:51, Epoch : 1, Step : 1852, Training Loss : 0.30206, Training Acc : 0.883, Run Time : 3.25
INFO:root:2019-05-11 23:06:17, Epoch : 1, Step : 1853, Training Loss : 0.25007, Training Acc : 0.950, Run Time : 26.15
INFO:root:2019-05-11 23:06:34, Epoch : 1, Step : 1854, Training Loss : 0.25789, Training Acc : 0.925, Run Time : 17.56
INFO:root:2019-05-11 23:07:07, Epoch : 1, Step : 1855, Training Loss : 0.24927, Training Acc : 0.936, Run Time : 32.77
INFO:root:2019-05-11 23:07:14, Epoch : 1, Step : 1856, Training Loss : 0.42368, Training Acc : 0.881, Run Time : 6.54
INFO:root:2019-05-11 23:07:50, Epoch : 1, Step : 1857, Training Loss : 0.23030, Training Acc : 0.947, Run Time : 36.39
INFO:root:2019-05-11 23:08:04, Epoch : 1, Step : 1858, Training Loss : 0.23095, Training Acc : 0.969, Run Time : 13.66
INFO:root:2019-05-11 23:08:32, Epoch : 1, Step : 1859, Training Loss : 0.27569, Training Acc : 0.944, Run Time : 28.08
INFO:root:2019-05-11 23:08:37, Epoch : 1, Step : 1860, Training Loss : 0.33249, Training Acc : 0.933, Run Time : 5.65
INFO:root:2019-05-11 23:08:40, Epoch : 1, Step : 1861, Training Loss : 0.24956, Training Acc : 0.939, Run Time : 2.88
INFO:root:2019-05-11 23:08:57, Epoch : 1, Step : 1862, Training Loss : 0.18293, Training Acc : 0.958, Run Time : 17.06
INFO:root:2019-05-11 23:09:24, Epoch : 1, Step : 1863, Training Loss : 0.20190, Training Acc : 0.950, Run Time : 26.88
INFO:root:2019-05-11 23:09:32, Epoch : 1, Step : 1864, Training Loss : 0.26985, Training Acc : 0.908, Run Time : 7.66
INFO:root:2019-05-11 23:09:58, Epoch : 1, Step : 1865, Training Loss : 0.14352, Training Acc : 0.972, Run Time : 26.13
INFO:root:2019-05-11 23:10:12, Epoch : 1, Step : 1866, Training Loss : 0.34815, Training Acc : 0.931, Run Time : 14.42
INFO:root:2019-05-11 23:10:27, Epoch : 1, Step : 1867, Training Loss : 0.25458, Training Acc : 0.922, Run Time : 14.64
INFO:root:2019-05-11 23:10:46, Epoch : 1, Step : 1868, Training Loss : 0.22801, Training Acc : 0.953, Run Time : 18.81
INFO:root:2019-05-11 23:11:13, Epoch : 1, Step : 1869, Training Loss : 0.45056, Training Acc : 0.658, Run Time : 27.21
INFO:root:2019-05-11 23:11:31, Epoch : 1, Step : 1870, Training Loss : 0.18287, Training Acc : 0.969, Run Time : 17.92
INFO:root:2019-05-11 23:11:53, Epoch : 1, Step : 1871, Training Loss : 0.17535, Training Acc : 0.967, Run Time : 22.42
INFO:root:2019-05-11 23:11:58, Epoch : 1, Step : 1872, Training Loss : 0.18822, Training Acc : 0.989, Run Time : 4.30
INFO:root:2019-05-11 23:12:16, Epoch : 1, Step : 1873, Training Loss : 0.17324, Training Acc : 0.956, Run Time : 18.13
INFO:root:2019-05-11 23:12:33, Epoch : 1, Step : 1874, Training Loss : 0.18328, Training Acc : 0.961, Run Time : 17.30
INFO:root:2019-05-11 23:12:36, Epoch : 1, Step : 1875, Training Loss : 0.20985, Training Acc : 0.956, Run Time : 3.03
INFO:root:2019-05-11 23:12:42, Epoch : 1, Step : 1876, Training Loss : 0.32738, Training Acc : 0.917, Run Time : 6.13
INFO:root:2019-05-11 23:13:07, Epoch : 1, Step : 1877, Training Loss : 1.00646, Training Acc : 0.453, Run Time : 24.67
INFO:root:2019-05-11 23:13:10, Epoch : 1, Step : 1878, Training Loss : 0.26068, Training Acc : 0.936, Run Time : 2.96
INFO:root:2019-05-11 23:13:31, Epoch : 1, Step : 1879, Training Loss : 0.33089, Training Acc : 0.914, Run Time : 21.01
INFO:root:2019-05-11 23:13:39, Epoch : 1, Step : 1880, Training Loss : 0.49839, Training Acc : 0.742, Run Time : 7.79
INFO:root:2019-05-11 23:13:53, Epoch : 1, Step : 1881, Training Loss : 0.26811, Training Acc : 0.908, Run Time : 14.67
INFO:root:2019-05-11 23:14:12, Epoch : 1, Step : 1882, Training Loss : 0.33456, Training Acc : 0.900, Run Time : 18.92
INFO:root:2019-05-11 23:14:32, Epoch : 1, Step : 1883, Training Loss : 0.37997, Training Acc : 0.856, Run Time : 19.56
INFO:root:2019-05-11 23:14:48, Epoch : 1, Step : 1884, Training Loss : 0.34423, Training Acc : 0.806, Run Time : 15.63
INFO:root:2019-05-11 23:14:55, Epoch : 1, Step : 1885, Training Loss : 0.35405, Training Acc : 0.847, Run Time : 7.20
INFO:root:2019-05-11 23:15:19, Epoch : 1, Step : 1886, Training Loss : 0.42058, Training Acc : 0.714, Run Time : 23.98
INFO:root:2019-05-11 23:15:41, Epoch : 1, Step : 1887, Training Loss : 0.43780, Training Acc : 0.689, Run Time : 22.66
INFO:root:2019-05-11 23:16:13, Epoch : 1, Step : 1888, Training Loss : 0.45500, Training Acc : 0.664, Run Time : 31.11
INFO:root:2019-05-11 23:16:20, Epoch : 1, Step : 1889, Training Loss : 0.48678, Training Acc : 0.678, Run Time : 7.20
INFO:root:2019-05-11 23:16:43, Epoch : 1, Step : 1890, Training Loss : 0.35324, Training Acc : 0.764, Run Time : 23.03
INFO:root:2019-05-11 23:17:01, Epoch : 1, Step : 1891, Training Loss : 0.32319, Training Acc : 0.836, Run Time : 18.25
INFO:root:2019-05-11 23:17:15, Epoch : 1, Step : 1892, Training Loss : 0.33921, Training Acc : 0.853, Run Time : 13.81
INFO:root:2019-05-11 23:17:30, Epoch : 1, Step : 1893, Training Loss : 0.34995, Training Acc : 0.831, Run Time : 14.78
INFO:root:2019-05-11 23:17:44, Epoch : 1, Step : 1894, Training Loss : 0.32163, Training Acc : 0.861, Run Time : 14.51
INFO:root:2019-05-11 23:18:08, Epoch : 1, Step : 1895, Training Loss : 0.39201, Training Acc : 0.828, Run Time : 24.23
INFO:root:2019-05-11 23:18:36, Epoch : 1, Step : 1896, Training Loss : 0.36977, Training Acc : 0.833, Run Time : 27.24
INFO:root:2019-05-11 23:18:42, Epoch : 1, Step : 1897, Training Loss : 0.41799, Training Acc : 0.803, Run Time : 6.06
INFO:root:2019-05-11 23:19:03, Epoch : 1, Step : 1898, Training Loss : 0.45652, Training Acc : 0.783, Run Time : 21.54
INFO:root:2019-05-11 23:19:20, Epoch : 1, Step : 1899, Training Loss : 0.48026, Training Acc : 0.778, Run Time : 16.96
INFO:root:2019-05-11 23:19:46, Epoch : 1, Step : 1900, Training Loss : 0.66780, Training Acc : 0.642, Run Time : 25.97
INFO:root:2019-05-11 23:20:06, Epoch : 1, Step : 1901, Training Loss : 0.42623, Training Acc : 0.836, Run Time : 20.13
INFO:root:2019-05-11 23:20:13, Epoch : 1, Step : 1902, Training Loss : 0.37085, Training Acc : 0.917, Run Time : 6.56
INFO:root:2019-05-11 23:20:33, Epoch : 1, Step : 1903, Training Loss : 0.52233, Training Acc : 0.675, Run Time : 20.32
INFO:root:2019-05-11 23:20:51, Epoch : 1, Step : 1904, Training Loss : 0.38970, Training Acc : 0.797, Run Time : 18.09
INFO:root:2019-05-11 23:21:06, Epoch : 1, Step : 1905, Training Loss : 0.34018, Training Acc : 0.881, Run Time : 15.04
INFO:root:2019-05-11 23:21:28, Epoch : 1, Step : 1906, Training Loss : 0.30573, Training Acc : 0.908, Run Time : 21.42
INFO:root:2019-05-11 23:21:43, Epoch : 1, Step : 1907, Training Loss : 0.34059, Training Acc : 0.839, Run Time : 14.89
INFO:root:2019-05-11 23:22:08, Epoch : 1, Step : 1908, Training Loss : 0.51456, Training Acc : 0.656, Run Time : 25.01
INFO:root:2019-05-11 23:22:22, Epoch : 1, Step : 1909, Training Loss : 0.31694, Training Acc : 0.897, Run Time : 14.59
INFO:root:2019-05-11 23:22:30, Epoch : 1, Step : 1910, Training Loss : 0.46416, Training Acc : 0.872, Run Time : 7.78
INFO:root:2019-05-11 23:22:33, Epoch : 1, Step : 1911, Training Loss : 0.30227, Training Acc : 0.856, Run Time : 2.61
INFO:root:2019-05-11 23:22:44, Epoch : 1, Step : 1912, Training Loss : 0.42489, Training Acc : 0.839, Run Time : 11.83
INFO:root:2019-05-11 23:22:48, Epoch : 1, Step : 1913, Training Loss : 0.34346, Training Acc : 0.858, Run Time : 3.29
INFO:root:2019-05-11 23:23:02, Epoch : 1, Step : 1914, Training Loss : 0.32282, Training Acc : 0.869, Run Time : 14.24
INFO:root:2019-05-11 23:23:14, Epoch : 1, Step : 1915, Training Loss : 0.31547, Training Acc : 0.861, Run Time : 11.62
INFO:root:2019-05-11 23:23:47, Epoch : 1, Step : 1916, Training Loss : 0.33651, Training Acc : 0.881, Run Time : 33.39
INFO:root:2019-05-11 23:24:02, Epoch : 1, Step : 1917, Training Loss : 0.30539, Training Acc : 0.914, Run Time : 15.13
INFO:root:2019-05-11 23:24:17, Epoch : 1, Step : 1918, Training Loss : 0.25794, Training Acc : 0.900, Run Time : 15.37
INFO:root:2019-05-11 23:24:36, Epoch : 1, Step : 1919, Training Loss : 0.32945, Training Acc : 0.844, Run Time : 18.50
INFO:root:2019-05-11 23:24:55, Epoch : 1, Step : 1920, Training Loss : 0.29354, Training Acc : 0.856, Run Time : 19.04
INFO:root:2019-05-11 23:25:02, Epoch : 1, Step : 1921, Training Loss : 0.27885, Training Acc : 0.908, Run Time : 6.78
INFO:root:2019-05-11 23:25:21, Epoch : 1, Step : 1922, Training Loss : 0.30332, Training Acc : 0.911, Run Time : 19.73
INFO:root:2019-05-11 23:25:36, Epoch : 1, Step : 1923, Training Loss : 0.25266, Training Acc : 0.939, Run Time : 14.34
INFO:root:2019-05-11 23:25:51, Epoch : 1, Step : 1924, Training Loss : 0.29805, Training Acc : 0.883, Run Time : 15.21
INFO:root:2019-05-11 23:26:00, Epoch : 1, Step : 1925, Training Loss : 0.32416, Training Acc : 0.822, Run Time : 8.66
INFO:root:2019-05-11 23:26:02, Epoch : 1, Step : 1926, Training Loss : 0.33291, Training Acc : 0.811, Run Time : 2.28
INFO:root:2019-05-11 23:26:18, Epoch : 1, Step : 1927, Training Loss : 0.21503, Training Acc : 0.931, Run Time : 16.14
INFO:root:2019-05-11 23:26:35, Epoch : 1, Step : 1928, Training Loss : 0.23014, Training Acc : 0.925, Run Time : 16.48
INFO:root:2019-05-11 23:26:39, Epoch : 1, Step : 1929, Training Loss : 0.31285, Training Acc : 0.881, Run Time : 4.12
INFO:root:2019-05-11 23:27:00, Epoch : 1, Step : 1930, Training Loss : 0.39251, Training Acc : 0.783, Run Time : 20.98
INFO:root:2019-05-11 23:27:07, Epoch : 1, Step : 1931, Training Loss : 0.27020, Training Acc : 0.908, Run Time : 6.94
INFO:root:2019-05-11 23:27:22, Epoch : 1, Step : 1932, Training Loss : 0.34235, Training Acc : 0.839, Run Time : 15.36
INFO:root:2019-05-11 23:27:41, Epoch : 1, Step : 1933, Training Loss : 0.42804, Training Acc : 0.803, Run Time : 18.64
INFO:root:2019-05-11 23:27:50, Epoch : 1, Step : 1934, Training Loss : 0.24671, Training Acc : 0.917, Run Time : 9.20
INFO:root:2019-05-11 23:28:13, Epoch : 1, Step : 1935, Training Loss : 0.46886, Training Acc : 0.808, Run Time : 23.36
INFO:root:2019-05-11 23:28:16, Epoch : 1, Step : 1936, Training Loss : 0.18803, Training Acc : 0.933, Run Time : 3.09
INFO:root:2019-05-11 23:28:33, Epoch : 1, Step : 1937, Training Loss : 0.16097, Training Acc : 0.975, Run Time : 17.08
INFO:root:2019-05-11 23:28:52, Epoch : 1, Step : 1938, Training Loss : 0.20256, Training Acc : 0.933, Run Time : 18.84
INFO:root:2019-05-11 23:28:58, Epoch : 1, Step : 1939, Training Loss : 0.13558, Training Acc : 0.950, Run Time : 5.95
INFO:root:2019-05-11 23:29:23, Epoch : 1, Step : 1940, Training Loss : 0.21319, Training Acc : 0.922, Run Time : 25.26
INFO:root:2019-05-11 23:29:29, Epoch : 1, Step : 1941, Training Loss : 0.24594, Training Acc : 0.906, Run Time : 5.29
INFO:root:2019-05-11 23:29:36, Epoch : 1, Step : 1942, Training Loss : 0.21447, Training Acc : 0.922, Run Time : 7.32
INFO:root:2019-05-11 23:29:42, Epoch : 1, Step : 1943, Training Loss : 0.21552, Training Acc : 0.914, Run Time : 6.41
INFO:root:2019-05-11 23:30:07, Epoch : 1, Step : 1944, Training Loss : 0.28764, Training Acc : 0.872, Run Time : 24.12
INFO:root:2019-05-11 23:30:09, Epoch : 1, Step : 1945, Training Loss : 0.27859, Training Acc : 0.900, Run Time : 2.22
INFO:root:2019-05-11 23:30:26, Epoch : 1, Step : 1946, Training Loss : 0.35133, Training Acc : 0.756, Run Time : 16.94
INFO:root:2019-05-11 23:30:39, Epoch : 1, Step : 1947, Training Loss : 0.28736, Training Acc : 0.867, Run Time : 13.46
INFO:root:2019-05-11 23:30:56, Epoch : 1, Step : 1948, Training Loss : 0.20120, Training Acc : 0.947, Run Time : 16.96
INFO:root:2019-05-11 23:31:14, Epoch : 1, Step : 1949, Training Loss : 0.13070, Training Acc : 0.956, Run Time : 17.99
INFO:root:2019-05-11 23:31:22, Epoch : 1, Step : 1950, Training Loss : 0.15363, Training Acc : 0.942, Run Time : 7.43
INFO:root:2019-05-11 23:31:44, Epoch : 1, Step : 1951, Training Loss : 0.34920, Training Acc : 0.808, Run Time : 22.21
INFO:root:2019-05-11 23:31:53, Epoch : 1, Step : 1952, Training Loss : 0.29954, Training Acc : 0.869, Run Time : 9.56
INFO:root:2019-05-11 23:32:05, Epoch : 1, Step : 1953, Training Loss : 0.17519, Training Acc : 0.950, Run Time : 11.43
INFO:root:2019-05-11 23:32:24, Epoch : 1, Step : 1954, Training Loss : 0.28730, Training Acc : 0.869, Run Time : 19.58
INFO:root:2019-05-11 23:32:42, Epoch : 1, Step : 1955, Training Loss : 0.24356, Training Acc : 0.903, Run Time : 17.92
INFO:root:2019-05-11 23:33:11, Epoch : 1, Step : 1956, Training Loss : 0.21315, Training Acc : 0.917, Run Time : 29.09
INFO:root:2019-05-11 23:33:19, Epoch : 1, Step : 1957, Training Loss : 0.18801, Training Acc : 0.961, Run Time : 7.34
INFO:root:2019-05-11 23:33:41, Epoch : 1, Step : 1958, Training Loss : 0.50813, Training Acc : 0.697, Run Time : 22.37
INFO:root:2019-05-11 23:33:58, Epoch : 1, Step : 1959, Training Loss : 0.14869, Training Acc : 0.939, Run Time : 17.24
INFO:root:2019-05-11 23:34:10, Epoch : 1, Step : 1960, Training Loss : 0.35775, Training Acc : 0.853, Run Time : 11.47
INFO:root:2019-05-11 23:34:20, Epoch : 1, Step : 1961, Training Loss : 0.48864, Training Acc : 0.708, Run Time : 9.95
INFO:root:2019-05-11 23:34:22, Epoch : 1, Step : 1962, Training Loss : 0.50703, Training Acc : 0.719, Run Time : 1.97
INFO:root:2019-05-11 23:34:50, Epoch : 1, Step : 1963, Training Loss : 0.33047, Training Acc : 0.936, Run Time : 28.50
INFO:root:2019-05-11 23:35:15, Epoch : 1, Step : 1964, Training Loss : 0.75054, Training Acc : 0.842, Run Time : 25.30
INFO:root:2019-05-11 23:35:22, Epoch : 1, Step : 1965, Training Loss : 0.91013, Training Acc : 0.683, Run Time : 6.13
INFO:root:2019-05-11 23:35:25, Epoch : 1, Step : 1966, Training Loss : 0.33098, Training Acc : 0.906, Run Time : 2.93
INFO:root:2019-05-11 23:35:43, Epoch : 1, Step : 1967, Training Loss : 0.33181, Training Acc : 0.875, Run Time : 18.76
INFO:root:2019-05-11 23:36:00, Epoch : 1, Step : 1968, Training Loss : 0.37481, Training Acc : 0.908, Run Time : 16.86
INFO:root:2019-05-11 23:36:16, Epoch : 1, Step : 1969, Training Loss : 0.54301, Training Acc : 0.731, Run Time : 15.69
INFO:root:2019-05-11 23:36:30, Epoch : 1, Step : 1970, Training Loss : 0.36088, Training Acc : 0.919, Run Time : 13.70
INFO:root:2019-05-11 23:36:42, Epoch : 1, Step : 1971, Training Loss : 0.47516, Training Acc : 0.822, Run Time : 12.79
INFO:root:2019-05-11 23:36:57, Epoch : 1, Step : 1972, Training Loss : 0.40787, Training Acc : 0.875, Run Time : 14.93
INFO:root:2019-05-11 23:37:16, Epoch : 1, Step : 1973, Training Loss : 0.61794, Training Acc : 0.589, Run Time : 19.05
INFO:root:2019-05-11 23:37:25, Epoch : 1, Step : 1974, Training Loss : 0.22994, Training Acc : 0.983, Run Time : 8.63
INFO:root:2019-05-11 23:37:51, Epoch : 1, Step : 1975, Training Loss : 0.25375, Training Acc : 0.989, Run Time : 26.41
INFO:root:2019-05-11 23:38:08, Epoch : 1, Step : 1976, Training Loss : 0.38852, Training Acc : 0.825, Run Time : 16.96
INFO:root:2019-05-11 23:38:33, Epoch : 1, Step : 1977, Training Loss : 0.21190, Training Acc : 0.978, Run Time : 25.08
INFO:root:2019-05-11 23:38:37, Epoch : 1, Step : 1978, Training Loss : 0.48548, Training Acc : 0.700, Run Time : 3.46
INFO:root:2019-05-11 23:39:09, Epoch : 1, Step : 1979, Training Loss : 0.18642, Training Acc : 0.983, Run Time : 32.32
INFO:root:2019-05-11 23:39:15, Epoch : 1, Step : 1980, Training Loss : 0.05406, Training Acc : 0.997, Run Time : 5.84
INFO:root:2019-05-11 23:39:38, Epoch : 1, Step : 1981, Training Loss : 0.13222, Training Acc : 0.967, Run Time : 22.97
INFO:root:2019-05-11 23:39:42, Epoch : 1, Step : 1982, Training Loss : 0.21985, Training Acc : 0.928, Run Time : 4.25
INFO:root:2019-05-11 23:40:06, Epoch : 1, Step : 1983, Training Loss : 0.08434, Training Acc : 0.981, Run Time : 23.66
INFO:root:2019-05-11 23:40:51, Epoch : 1, Step : 1984, Training Loss : 0.22814, Training Acc : 0.928, Run Time : 45.15
INFO:root:2019-05-11 23:41:18, Epoch : 1, Step : 1985, Training Loss : 0.18687, Training Acc : 0.942, Run Time : 26.83
INFO:root:2019-05-11 23:41:36, Epoch : 1, Step : 1986, Training Loss : 0.14284, Training Acc : 0.958, Run Time : 17.83
INFO:root:2019-05-11 23:41:52, Epoch : 1, Step : 1987, Training Loss : 0.11484, Training Acc : 0.972, Run Time : 16.25
INFO:root:2019-05-11 23:42:07, Epoch : 1, Step : 1988, Training Loss : 0.13648, Training Acc : 0.983, Run Time : 15.12
INFO:root:2019-05-11 23:42:23, Epoch : 1, Step : 1989, Training Loss : 0.09452, Training Acc : 0.978, Run Time : 16.21
INFO:root:2019-05-11 23:42:38, Epoch : 1, Step : 1990, Training Loss : 0.31704, Training Acc : 0.911, Run Time : 14.87
INFO:root:2019-05-11 23:42:58, Epoch : 1, Step : 1991, Training Loss : 0.34260, Training Acc : 0.928, Run Time : 20.28
INFO:root:2019-05-11 23:43:17, Epoch : 1, Step : 1992, Training Loss : 0.15948, Training Acc : 0.961, Run Time : 18.53
INFO:root:2019-05-11 23:43:23, Epoch : 1, Step : 1993, Training Loss : 0.17585, Training Acc : 0.936, Run Time : 6.44
INFO:root:2019-05-11 23:43:45, Epoch : 1, Step : 1994, Training Loss : 0.33888, Training Acc : 0.922, Run Time : 21.09
INFO:root:2019-05-11 23:44:05, Epoch : 1, Step : 1995, Training Loss : 0.20645, Training Acc : 0.928, Run Time : 20.28
INFO:root:2019-05-11 23:44:09, Epoch : 1, Step : 1996, Training Loss : 0.19321, Training Acc : 0.975, Run Time : 3.80
INFO:root:2019-05-11 23:44:28, Epoch : 1, Step : 1997, Training Loss : 0.25314, Training Acc : 0.961, Run Time : 19.54
INFO:root:2019-05-11 23:44:31, Epoch : 1, Step : 1998, Training Loss : 0.15944, Training Acc : 0.967, Run Time : 3.01
INFO:root:2019-05-11 23:44:52, Epoch : 1, Step : 1999, Training Loss : 0.47974, Training Acc : 0.667, Run Time : 21.24
INFO:root:2019-05-11 23:45:05, Epoch : 1, Step : 2000, Training Loss : 0.36946, Training Acc : 0.858, Run Time : 12.95
INFO:root:2019-05-11 23:45:24, Epoch : 1, Step : 2001, Training Loss : 0.27036, Training Acc : 0.867, Run Time : 18.25
INFO:root:2019-05-11 23:45:38, Epoch : 1, Step : 2002, Training Loss : 0.30293, Training Acc : 0.864, Run Time : 14.37
INFO:root:2019-05-11 23:45:53, Epoch : 1, Step : 2003, Training Loss : 0.74011, Training Acc : 0.542, Run Time : 14.97
INFO:root:2019-05-11 23:46:06, Epoch : 1, Step : 2004, Training Loss : 0.52835, Training Acc : 0.694, Run Time : 13.55
INFO:root:2019-05-11 23:46:08, Epoch : 1, Step : 2005, Training Loss : 0.41513, Training Acc : 0.725, Run Time : 1.87
INFO:root:2019-05-11 23:46:25, Epoch : 1, Step : 2006, Training Loss : 0.24327, Training Acc : 0.900, Run Time : 16.95
INFO:root:2019-05-11 23:46:39, Epoch : 1, Step : 2007, Training Loss : 0.32702, Training Acc : 0.844, Run Time : 14.11
INFO:root:2019-05-11 23:46:50, Epoch : 1, Step : 2008, Training Loss : 0.28369, Training Acc : 0.869, Run Time : 10.70
INFO:root:2019-05-11 23:46:53, Epoch : 1, Step : 2009, Training Loss : 0.70281, Training Acc : 0.581, Run Time : 2.78
INFO:root:2019-05-11 23:47:02, Epoch : 1, Step : 2010, Training Loss : 0.25811, Training Acc : 0.906, Run Time : 9.05
INFO:root:2019-05-11 23:47:26, Epoch : 1, Step : 2011, Training Loss : 0.19271, Training Acc : 0.911, Run Time : 23.90
INFO:root:2019-05-11 23:47:33, Epoch : 1, Step : 2012, Training Loss : 0.20613, Training Acc : 0.917, Run Time : 7.68
INFO:root:2019-05-11 23:47:52, Epoch : 1, Step : 2013, Training Loss : 0.13531, Training Acc : 0.953, Run Time : 18.50
INFO:root:2019-05-11 23:48:10, Epoch : 1, Step : 2014, Training Loss : 0.22350, Training Acc : 0.906, Run Time : 17.93
INFO:root:2019-05-11 23:48:16, Epoch : 1, Step : 2015, Training Loss : 0.17701, Training Acc : 0.947, Run Time : 6.07
INFO:root:2019-05-11 23:48:31, Epoch : 1, Step : 2016, Training Loss : 0.19679, Training Acc : 0.922, Run Time : 14.92
INFO:root:2019-05-11 23:48:34, Epoch : 1, Step : 2017, Training Loss : 0.18259, Training Acc : 0.925, Run Time : 3.53
INFO:root:2019-05-11 23:48:55, Epoch : 1, Step : 2018, Training Loss : 0.25020, Training Acc : 0.906, Run Time : 20.93
INFO:root:2019-05-11 23:49:07, Epoch : 1, Step : 2019, Training Loss : 0.27053, Training Acc : 0.878, Run Time : 11.38
INFO:root:2019-05-11 23:49:26, Epoch : 1, Step : 2020, Training Loss : 0.31227, Training Acc : 0.892, Run Time : 19.23
INFO:root:2019-05-11 23:49:44, Epoch : 1, Step : 2021, Training Loss : 0.56441, Training Acc : 0.669, Run Time : 18.19
INFO:root:2019-05-11 23:49:59, Epoch : 1, Step : 2022, Training Loss : 0.35845, Training Acc : 0.861, Run Time : 14.77
INFO:root:2019-05-11 23:50:11, Epoch : 1, Step : 2023, Training Loss : 0.34003, Training Acc : 0.861, Run Time : 12.47
INFO:root:2019-05-11 23:50:35, Epoch : 1, Step : 2024, Training Loss : 0.21313, Training Acc : 0.917, Run Time : 23.89
INFO:root:2019-05-11 23:50:41, Epoch : 1, Step : 2025, Training Loss : 0.30922, Training Acc : 0.881, Run Time : 6.10
INFO:root:2019-05-11 23:51:00, Epoch : 1, Step : 2026, Training Loss : 0.26356, Training Acc : 0.883, Run Time : 19.08
INFO:root:2019-05-11 23:51:03, Epoch : 1, Step : 2027, Training Loss : 0.31501, Training Acc : 0.878, Run Time : 2.86
INFO:root:2019-05-11 23:51:23, Epoch : 1, Step : 2028, Training Loss : 0.51488, Training Acc : 0.769, Run Time : 19.30
INFO:root:2019-05-11 23:51:27, Epoch : 1, Step : 2029, Training Loss : 0.31341, Training Acc : 0.850, Run Time : 4.65
INFO:root:2019-05-11 23:51:36, Epoch : 1, Step : 2030, Training Loss : 0.26734, Training Acc : 0.914, Run Time : 8.94
INFO:root:2019-05-11 23:51:38, Epoch : 1, Step : 2031, Training Loss : 0.23661, Training Acc : 0.900, Run Time : 2.15
INFO:root:2019-05-11 23:52:02, Epoch : 1, Step : 2032, Training Loss : 0.24451, Training Acc : 0.908, Run Time : 23.44
INFO:root:2019-05-11 23:52:18, Epoch : 1, Step : 2033, Training Loss : 0.26479, Training Acc : 0.903, Run Time : 16.07
INFO:root:2019-05-11 23:52:33, Epoch : 1, Step : 2034, Training Loss : 0.25257, Training Acc : 0.886, Run Time : 15.50
INFO:root:2019-05-11 23:52:51, Epoch : 1, Step : 2035, Training Loss : 0.21095, Training Acc : 0.917, Run Time : 17.78
INFO:root:2019-05-11 23:53:06, Epoch : 1, Step : 2036, Training Loss : 0.32292, Training Acc : 0.869, Run Time : 14.93
INFO:root:2019-05-11 23:53:23, Epoch : 1, Step : 2037, Training Loss : 0.21116, Training Acc : 0.936, Run Time : 17.04
INFO:root:2019-05-11 23:53:35, Epoch : 1, Step : 2038, Training Loss : 0.16485, Training Acc : 0.933, Run Time : 12.03
INFO:root:2019-05-11 23:53:41, Epoch : 1, Step : 2039, Training Loss : 0.25163, Training Acc : 0.914, Run Time : 5.88
INFO:root:2019-05-11 23:53:48, Epoch : 1, Step : 2040, Training Loss : 0.25569, Training Acc : 0.914, Run Time : 6.69
INFO:root:2019-05-11 23:53:50, Epoch : 1, Step : 2041, Training Loss : 0.21287, Training Acc : 0.928, Run Time : 2.58
INFO:root:2019-05-11 23:54:04, Epoch : 1, Step : 2042, Training Loss : 0.16600, Training Acc : 0.947, Run Time : 13.54
INFO:root:2019-05-11 23:54:17, Epoch : 1, Step : 2043, Training Loss : 0.18181, Training Acc : 0.964, Run Time : 12.97
INFO:root:2019-05-11 23:54:19, Epoch : 1, Step : 2044, Training Loss : 0.22236, Training Acc : 0.922, Run Time : 2.39
INFO:root:2019-05-11 23:54:39, Epoch : 1, Step : 2045, Training Loss : 0.15570, Training Acc : 0.942, Run Time : 19.26
INFO:root:2019-05-11 23:54:49, Epoch : 1, Step : 2046, Training Loss : 0.20815, Training Acc : 0.939, Run Time : 10.51
INFO:root:2019-05-11 23:54:59, Epoch : 1, Step : 2047, Training Loss : 0.09799, Training Acc : 0.975, Run Time : 9.62
INFO:root:2019-05-11 23:55:02, Epoch : 1, Step : 2048, Training Loss : 0.17643, Training Acc : 0.958, Run Time : 2.91
INFO:root:2019-05-11 23:55:20, Epoch : 1, Step : 2049, Training Loss : 0.22298, Training Acc : 0.944, Run Time : 18.08
INFO:root:2019-05-11 23:55:33, Epoch : 1, Step : 2050, Training Loss : 0.66810, Training Acc : 0.597, Run Time : 13.48
INFO:root:2019-05-11 23:55:47, Epoch : 1, Step : 2051, Training Loss : 0.16171, Training Acc : 0.936, Run Time : 13.53
INFO:root:2019-05-11 23:55:54, Epoch : 1, Step : 2052, Training Loss : 0.11828, Training Acc : 0.961, Run Time : 7.48
INFO:root:2019-05-11 23:56:11, Epoch : 1, Step : 2053, Training Loss : 0.14968, Training Acc : 0.947, Run Time : 17.33
INFO:root:2019-05-11 23:56:24, Epoch : 1, Step : 2054, Training Loss : 0.19834, Training Acc : 0.925, Run Time : 12.50
INFO:root:2019-05-11 23:56:38, Epoch : 1, Step : 2055, Training Loss : 0.52834, Training Acc : 0.617, Run Time : 14.04
INFO:root:2019-05-11 23:57:05, Epoch : 1, Step : 2056, Training Loss : 0.35174, Training Acc : 0.850, Run Time : 27.21
INFO:root:2019-05-11 23:57:12, Epoch : 1, Step : 2057, Training Loss : 0.46315, Training Acc : 0.692, Run Time : 6.97
INFO:root:2019-05-11 23:57:38, Epoch : 1, Step : 2058, Training Loss : 0.26637, Training Acc : 0.939, Run Time : 26.04
INFO:root:2019-05-11 23:57:54, Epoch : 1, Step : 2059, Training Loss : 0.21790, Training Acc : 0.942, Run Time : 16.29
INFO:root:2019-05-11 23:58:13, Epoch : 1, Step : 2060, Training Loss : 0.52517, Training Acc : 0.639, Run Time : 18.09
INFO:root:2019-05-11 23:58:21, Epoch : 1, Step : 2061, Training Loss : 0.38217, Training Acc : 0.889, Run Time : 7.99
INFO:root:2019-05-11 23:58:40, Epoch : 1, Step : 2062, Training Loss : 0.20762, Training Acc : 0.933, Run Time : 19.53
INFO:root:2019-05-11 23:58:47, Epoch : 1, Step : 2063, Training Loss : 0.21334, Training Acc : 0.922, Run Time : 7.10
INFO:root:2019-05-11 23:59:01, Epoch : 1, Step : 2064, Training Loss : 0.14795, Training Acc : 0.947, Run Time : 13.32
INFO:root:2019-05-11 23:59:03, Epoch : 1, Step : 2065, Training Loss : 0.15819, Training Acc : 0.944, Run Time : 2.86
INFO:root:2019-05-11 23:59:18, Epoch : 1, Step : 2066, Training Loss : 0.38745, Training Acc : 0.786, Run Time : 14.51
INFO:root:2019-05-11 23:59:38, Epoch : 1, Step : 2067, Training Loss : 0.27218, Training Acc : 0.903, Run Time : 19.64
INFO:root:2019-05-11 23:59:53, Epoch : 1, Step : 2068, Training Loss : 0.12186, Training Acc : 0.961, Run Time : 15.73
INFO:root:2019-05-12 00:00:14, Epoch : 1, Step : 2069, Training Loss : 0.14496, Training Acc : 0.953, Run Time : 20.78
INFO:root:2019-05-12 00:00:20, Epoch : 1, Step : 2070, Training Loss : 0.14262, Training Acc : 0.950, Run Time : 5.72
INFO:root:2019-05-12 00:00:43, Epoch : 1, Step : 2071, Training Loss : 0.29018, Training Acc : 0.931, Run Time : 23.22
INFO:root:2019-05-12 00:00:48, Epoch : 1, Step : 2072, Training Loss : 0.12989, Training Acc : 0.975, Run Time : 5.47
INFO:root:2019-05-12 00:00:51, Epoch : 1, Step : 2073, Training Loss : 0.13115, Training Acc : 0.944, Run Time : 2.55
INFO:root:2019-05-12 00:01:12, Epoch : 1, Step : 2074, Training Loss : 0.19040, Training Acc : 0.936, Run Time : 20.89
INFO:root:2019-05-12 00:01:21, Epoch : 1, Step : 2075, Training Loss : 0.33840, Training Acc : 0.844, Run Time : 9.12
INFO:root:2019-05-12 00:01:27, Epoch : 1, Step : 2076, Training Loss : 0.27125, Training Acc : 0.908, Run Time : 5.61
INFO:root:2019-05-12 00:01:40, Epoch : 1, Step : 2077, Training Loss : 0.54012, Training Acc : 0.689, Run Time : 13.62
INFO:root:2019-05-12 00:01:53, Epoch : 1, Step : 2078, Training Loss : 0.17451, Training Acc : 0.975, Run Time : 12.77
INFO:root:2019-05-12 00:01:56, Epoch : 1, Step : 2079, Training Loss : 0.57982, Training Acc : 0.686, Run Time : 3.25
INFO:root:2019-05-12 00:02:11, Epoch : 1, Step : 2080, Training Loss : 0.40312, Training Acc : 0.881, Run Time : 14.39
INFO:root:2019-05-12 00:02:24, Epoch : 1, Step : 2081, Training Loss : 0.40615, Training Acc : 0.858, Run Time : 13.77
INFO:root:2019-05-12 00:02:39, Epoch : 1, Step : 2082, Training Loss : 0.31888, Training Acc : 0.878, Run Time : 14.30
INFO:root:2019-05-12 00:02:42, Epoch : 1, Step : 2083, Training Loss : 0.40371, Training Acc : 0.872, Run Time : 3.26
INFO:root:2019-05-12 00:02:46, Epoch : 1, Step : 2084, Training Loss : 0.19749, Training Acc : 0.931, Run Time : 3.97
INFO:root:2019-05-12 00:02:59, Epoch : 1, Step : 2085, Training Loss : 0.45681, Training Acc : 0.631, Run Time : 13.04
INFO:root:2019-05-12 00:03:05, Epoch : 1, Step : 2086, Training Loss : 0.15957, Training Acc : 0.947, Run Time : 5.80
INFO:root:2019-05-12 00:03:07, Epoch : 1, Step : 2087, Training Loss : 0.24746, Training Acc : 0.925, Run Time : 2.55
INFO:root:2019-05-12 00:03:30, Epoch : 1, Step : 2088, Training Loss : 0.50142, Training Acc : 0.697, Run Time : 22.47
INFO:root:2019-05-12 00:03:37, Epoch : 1, Step : 2089, Training Loss : 0.16456, Training Acc : 0.969, Run Time : 7.22
INFO:root:2019-05-12 00:03:59, Epoch : 1, Step : 2090, Training Loss : 0.20205, Training Acc : 0.958, Run Time : 22.09
INFO:root:2019-05-12 00:04:05, Epoch : 1, Step : 2091, Training Loss : 0.16395, Training Acc : 0.967, Run Time : 6.06
INFO:root:2019-05-12 00:04:35, Epoch : 1, Step : 2092, Training Loss : 0.09796, Training Acc : 0.986, Run Time : 29.63
INFO:root:2019-05-12 00:04:41, Epoch : 1, Step : 2093, Training Loss : 0.11126, Training Acc : 0.986, Run Time : 6.01
INFO:root:2019-05-12 00:05:04, Epoch : 1, Step : 2094, Training Loss : 0.11099, Training Acc : 0.972, Run Time : 23.61
INFO:root:2019-05-12 00:05:21, Epoch : 1, Step : 2095, Training Loss : 0.14565, Training Acc : 0.961, Run Time : 16.94
INFO:root:2019-05-12 00:05:30, Epoch : 1, Step : 2096, Training Loss : 0.12164, Training Acc : 0.961, Run Time : 8.48
INFO:root:2019-05-12 00:05:51, Epoch : 1, Step : 2097, Training Loss : 0.18195, Training Acc : 0.914, Run Time : 21.64
INFO:root:2019-05-12 00:06:11, Epoch : 1, Step : 2098, Training Loss : 0.18064, Training Acc : 0.917, Run Time : 19.23
INFO:root:2019-05-12 00:06:18, Epoch : 1, Step : 2099, Training Loss : 0.24327, Training Acc : 0.889, Run Time : 7.38
INFO:root:2019-05-12 00:06:44, Epoch : 1, Step : 2100, Training Loss : 0.53322, Training Acc : 0.822, Run Time : 25.67
INFO:root:2019-05-12 00:07:03, Epoch : 1, Step : 2101, Training Loss : 0.33665, Training Acc : 0.797, Run Time : 18.80
INFO:root:2019-05-12 00:07:10, Epoch : 1, Step : 2102, Training Loss : 0.33465, Training Acc : 0.761, Run Time : 7.76
INFO:root:2019-05-12 00:07:31, Epoch : 1, Step : 2103, Training Loss : 0.51281, Training Acc : 0.706, Run Time : 20.27
INFO:root:2019-05-12 00:07:47, Epoch : 1, Step : 2104, Training Loss : 0.20208, Training Acc : 0.925, Run Time : 16.68
INFO:root:2019-05-12 00:07:52, Epoch : 1, Step : 2105, Training Loss : 0.22958, Training Acc : 0.917, Run Time : 5.12
INFO:root:2019-05-12 00:07:54, Epoch : 1, Step : 2106, Training Loss : 0.29022, Training Acc : 0.897, Run Time : 2.05
INFO:root:2019-05-12 00:08:10, Epoch : 1, Step : 2107, Training Loss : 0.21393, Training Acc : 0.881, Run Time : 15.77
INFO:root:2019-05-12 00:08:15, Epoch : 1, Step : 2108, Training Loss : 0.19993, Training Acc : 0.886, Run Time : 4.79
INFO:root:2019-05-12 00:08:17, Epoch : 1, Step : 2109, Training Loss : 0.21733, Training Acc : 0.931, Run Time : 1.79
INFO:root:2019-05-12 00:08:43, Epoch : 1, Step : 2110, Training Loss : 0.26851, Training Acc : 0.900, Run Time : 26.19
INFO:root:2019-05-12 00:08:48, Epoch : 1, Step : 2111, Training Loss : 0.13667, Training Acc : 0.950, Run Time : 5.39
INFO:root:2019-05-12 00:09:07, Epoch : 1, Step : 2112, Training Loss : 0.17311, Training Acc : 0.919, Run Time : 18.99
INFO:root:2019-05-12 00:09:12, Epoch : 1, Step : 2113, Training Loss : 0.18070, Training Acc : 0.936, Run Time : 4.21
INFO:root:2019-05-12 00:09:28, Epoch : 1, Step : 2114, Training Loss : 0.15614, Training Acc : 0.942, Run Time : 16.45
INFO:root:2019-05-12 00:09:48, Epoch : 1, Step : 2115, Training Loss : 0.16597, Training Acc : 0.956, Run Time : 19.76
INFO:root:2019-05-12 00:09:55, Epoch : 1, Step : 2116, Training Loss : 0.19075, Training Acc : 0.933, Run Time : 7.61
INFO:root:2019-05-12 00:10:16, Epoch : 1, Step : 2117, Training Loss : 0.17289, Training Acc : 0.933, Run Time : 20.62
INFO:root:2019-05-12 00:10:29, Epoch : 1, Step : 2118, Training Loss : 0.22253, Training Acc : 0.908, Run Time : 12.72
INFO:root:2019-05-12 00:10:42, Epoch : 1, Step : 2119, Training Loss : 0.18192, Training Acc : 0.922, Run Time : 13.24
INFO:root:2019-05-12 00:11:01, Epoch : 1, Step : 2120, Training Loss : 0.24791, Training Acc : 0.922, Run Time : 18.51
INFO:root:2019-05-12 00:11:07, Epoch : 1, Step : 2121, Training Loss : 0.20307, Training Acc : 0.911, Run Time : 6.22
INFO:root:2019-05-12 00:11:34, Epoch : 1, Step : 2122, Training Loss : 0.17294, Training Acc : 0.925, Run Time : 27.41
INFO:root:2019-05-12 00:12:03, Epoch : 1, Step : 2123, Training Loss : 0.26209, Training Acc : 0.911, Run Time : 28.42
INFO:root:2019-05-12 00:12:16, Epoch : 1, Step : 2124, Training Loss : 0.18362, Training Acc : 0.928, Run Time : 13.39
INFO:root:2019-05-12 00:12:29, Epoch : 1, Step : 2125, Training Loss : 0.21459, Training Acc : 0.903, Run Time : 12.75
INFO:root:2019-05-12 00:12:44, Epoch : 1, Step : 2126, Training Loss : 0.22856, Training Acc : 0.914, Run Time : 15.19
INFO:root:2019-05-12 00:12:59, Epoch : 1, Step : 2127, Training Loss : 0.15736, Training Acc : 0.933, Run Time : 15.55
INFO:root:2019-05-12 00:13:02, Epoch : 1, Step : 2128, Training Loss : 0.15622, Training Acc : 0.936, Run Time : 2.99
INFO:root:2019-05-12 00:13:17, Epoch : 1, Step : 2129, Training Loss : 0.17632, Training Acc : 0.931, Run Time : 14.65
INFO:root:2019-05-12 00:13:19, Epoch : 1, Step : 2130, Training Loss : 0.13923, Training Acc : 0.958, Run Time : 2.15
INFO:root:2019-05-12 00:13:47, Epoch : 1, Step : 2131, Training Loss : 0.18614, Training Acc : 0.931, Run Time : 27.29
INFO:root:2019-05-12 00:14:06, Epoch : 1, Step : 2132, Training Loss : 0.16627, Training Acc : 0.956, Run Time : 19.84
INFO:root:2019-05-12 00:14:23, Epoch : 1, Step : 2133, Training Loss : 0.23957, Training Acc : 0.928, Run Time : 16.14
INFO:root:2019-05-12 00:14:29, Epoch : 1, Step : 2134, Training Loss : 0.20704, Training Acc : 0.939, Run Time : 6.34
INFO:root:2019-05-12 00:14:49, Epoch : 1, Step : 2135, Training Loss : 0.20050, Training Acc : 0.933, Run Time : 20.29
INFO:root:2019-05-12 00:15:02, Epoch : 1, Step : 2136, Training Loss : 0.20664, Training Acc : 0.936, Run Time : 13.07
INFO:root:2019-05-12 00:15:09, Epoch : 1, Step : 2137, Training Loss : 0.19182, Training Acc : 0.944, Run Time : 7.08
INFO:root:2019-05-12 00:15:30, Epoch : 1, Step : 2138, Training Loss : 0.24285, Training Acc : 0.942, Run Time : 20.58
INFO:root:2019-05-12 00:15:35, Epoch : 1, Step : 2139, Training Loss : 0.13867, Training Acc : 0.972, Run Time : 5.30
INFO:root:2019-05-12 00:15:53, Epoch : 1, Step : 2140, Training Loss : 0.17427, Training Acc : 0.958, Run Time : 17.74
INFO:root:2019-05-12 00:16:03, Epoch : 1, Step : 2141, Training Loss : 0.26456, Training Acc : 0.933, Run Time : 9.87
INFO:root:2019-05-12 00:16:24, Epoch : 1, Step : 2142, Training Loss : 0.24506, Training Acc : 0.942, Run Time : 20.75
INFO:root:2019-05-12 00:16:29, Epoch : 1, Step : 2143, Training Loss : 0.15387, Training Acc : 0.972, Run Time : 5.89
INFO:root:2019-05-12 00:16:52, Epoch : 1, Step : 2144, Training Loss : 0.27771, Training Acc : 0.906, Run Time : 23.03
INFO:root:2019-05-12 00:17:15, Epoch : 1, Step : 2145, Training Loss : 0.20485, Training Acc : 0.928, Run Time : 22.40
INFO:root:2019-05-12 00:17:31, Epoch : 1, Step : 2146, Training Loss : 0.24659, Training Acc : 0.914, Run Time : 15.95
INFO:root:2019-05-12 00:17:37, Epoch : 1, Step : 2147, Training Loss : 0.16282, Training Acc : 0.942, Run Time : 6.57
INFO:root:2019-05-12 00:17:54, Epoch : 1, Step : 2148, Training Loss : 0.17920, Training Acc : 0.950, Run Time : 16.93
INFO:root:2019-05-12 00:18:02, Epoch : 1, Step : 2149, Training Loss : 0.22034, Training Acc : 0.933, Run Time : 7.62
INFO:root:2019-05-12 00:18:18, Epoch : 1, Step : 2150, Training Loss : 0.18426, Training Acc : 0.942, Run Time : 16.47
INFO:root:2019-05-12 00:18:32, Epoch : 1, Step : 2151, Training Loss : 0.26843, Training Acc : 0.919, Run Time : 13.34
INFO:root:2019-05-12 00:18:45, Epoch : 1, Step : 2152, Training Loss : 0.19477, Training Acc : 0.936, Run Time : 13.05
INFO:root:2019-05-12 00:18:59, Epoch : 1, Step : 2153, Training Loss : 0.25674, Training Acc : 0.917, Run Time : 13.94
INFO:root:2019-05-12 00:19:01, Epoch : 1, Step : 2154, Training Loss : 0.25366, Training Acc : 0.897, Run Time : 2.25
INFO:root:2019-05-12 00:19:11, Epoch : 1, Step : 2155, Training Loss : 0.22582, Training Acc : 0.933, Run Time : 10.46
INFO:root:2019-05-12 00:19:14, Epoch : 1, Step : 2156, Training Loss : 0.20270, Training Acc : 0.953, Run Time : 2.20
INFO:root:2019-05-12 00:19:30, Epoch : 1, Step : 2157, Training Loss : 0.22695, Training Acc : 0.931, Run Time : 16.55
INFO:root:2019-05-12 00:19:33, Epoch : 1, Step : 2158, Training Loss : 0.22710, Training Acc : 0.933, Run Time : 3.14
INFO:root:2019-05-12 00:19:50, Epoch : 1, Step : 2159, Training Loss : 0.21688, Training Acc : 0.928, Run Time : 16.73
INFO:root:2019-05-12 00:20:02, Epoch : 1, Step : 2160, Training Loss : 0.25747, Training Acc : 0.911, Run Time : 11.75
INFO:root:2019-05-12 00:20:08, Epoch : 1, Step : 2161, Training Loss : 0.24105, Training Acc : 0.936, Run Time : 6.67
INFO:root:2019-05-12 00:20:26, Epoch : 1, Step : 2162, Training Loss : 0.22647, Training Acc : 0.944, Run Time : 17.34
INFO:root:2019-05-12 00:20:33, Epoch : 1, Step : 2163, Training Loss : 0.18248, Training Acc : 0.953, Run Time : 7.37
INFO:root:2019-05-12 00:20:50, Epoch : 1, Step : 2164, Training Loss : 0.30548, Training Acc : 0.911, Run Time : 16.99
INFO:root:2019-05-12 00:21:03, Epoch : 1, Step : 2165, Training Loss : 0.23149, Training Acc : 0.939, Run Time : 12.95
INFO:root:2019-05-12 00:21:06, Epoch : 1, Step : 2166, Training Loss : 0.28106, Training Acc : 0.911, Run Time : 2.45
INFO:root:2019-05-12 00:21:16, Epoch : 1, Step : 2167, Training Loss : 0.24138, Training Acc : 0.925, Run Time : 10.67
INFO:root:2019-05-12 00:21:19, Epoch : 1, Step : 2168, Training Loss : 0.18409, Training Acc : 0.947, Run Time : 2.47
INFO:root:2019-05-12 00:21:32, Epoch : 1, Step : 2169, Training Loss : 0.25051, Training Acc : 0.931, Run Time : 13.62
INFO:root:2019-05-12 00:21:43, Epoch : 1, Step : 2170, Training Loss : 0.28693, Training Acc : 0.906, Run Time : 10.70
INFO:root:2019-05-12 00:21:46, Epoch : 1, Step : 2171, Training Loss : 0.16485, Training Acc : 0.950, Run Time : 3.20
INFO:root:2019-05-12 00:21:58, Epoch : 1, Step : 2172, Training Loss : 0.22617, Training Acc : 0.928, Run Time : 11.62
INFO:root:2019-05-12 00:22:11, Epoch : 1, Step : 2173, Training Loss : 0.23193, Training Acc : 0.931, Run Time : 12.81
INFO:root:2019-05-12 00:22:25, Epoch : 1, Step : 2174, Training Loss : 0.22028, Training Acc : 0.950, Run Time : 14.26
INFO:root:2019-05-12 00:22:39, Epoch : 1, Step : 2175, Training Loss : 0.18686, Training Acc : 0.953, Run Time : 14.52
INFO:root:2019-05-12 00:22:46, Epoch : 1, Step : 2176, Training Loss : 0.23811, Training Acc : 0.928, Run Time : 6.09
INFO:root:2019-05-12 00:22:52, Epoch : 1, Step : 2177, Training Loss : 0.28012, Training Acc : 0.911, Run Time : 6.50
INFO:root:2019-05-12 00:22:56, Epoch : 1, Step : 2178, Training Loss : 0.41981, Training Acc : 0.881, Run Time : 4.11
INFO:root:2019-05-12 00:23:17, Epoch : 1, Step : 2179, Training Loss : 0.74767, Training Acc : 0.769, Run Time : 20.86
INFO:root:2019-05-12 00:23:22, Epoch : 1, Step : 2180, Training Loss : 0.34129, Training Acc : 0.894, Run Time : 5.00
INFO:root:2019-05-12 00:23:24, Epoch : 1, Step : 2181, Training Loss : 0.27931, Training Acc : 0.914, Run Time : 2.48
INFO:root:2019-05-12 00:23:47, Epoch : 1, Step : 2182, Training Loss : 0.27160, Training Acc : 0.908, Run Time : 22.06
INFO:root:2019-05-12 00:24:07, Epoch : 1, Step : 2183, Training Loss : 0.23072, Training Acc : 0.928, Run Time : 20.37
INFO:root:2019-05-12 00:24:12, Epoch : 1, Step : 2184, Training Loss : 0.25728, Training Acc : 0.894, Run Time : 5.12
INFO:root:2019-05-12 00:24:18, Epoch : 1, Step : 2185, Training Loss : 0.34613, Training Acc : 0.894, Run Time : 5.94
INFO:root:2019-05-12 00:24:24, Epoch : 1, Step : 2186, Training Loss : 0.23772, Training Acc : 0.897, Run Time : 5.78
INFO:root:2019-05-12 00:24:41, Epoch : 1, Step : 2187, Training Loss : 0.23951, Training Acc : 0.894, Run Time : 16.78
INFO:root:2019-05-12 00:24:44, Epoch : 1, Step : 2188, Training Loss : 0.35077, Training Acc : 0.861, Run Time : 3.02
INFO:root:2019-05-12 00:24:49, Epoch : 1, Step : 2189, Training Loss : 0.30953, Training Acc : 0.828, Run Time : 5.72
INFO:root:2019-05-12 00:24:52, Epoch : 1, Step : 2190, Training Loss : 0.30377, Training Acc : 0.817, Run Time : 2.65
INFO:root:2019-05-12 00:25:05, Epoch : 1, Step : 2191, Training Loss : 0.29356, Training Acc : 0.836, Run Time : 13.26
INFO:root:2019-05-12 00:25:19, Epoch : 1, Step : 2192, Training Loss : 0.38669, Training Acc : 0.792, Run Time : 13.65
INFO:root:2019-05-12 00:25:31, Epoch : 1, Step : 2193, Training Loss : 0.35300, Training Acc : 0.797, Run Time : 11.76
INFO:root:2019-05-12 00:25:44, Epoch : 1, Step : 2194, Training Loss : 0.58074, Training Acc : 0.756, Run Time : 13.82
INFO:root:2019-05-12 00:26:03, Epoch : 1, Step : 2195, Training Loss : 0.25731, Training Acc : 0.894, Run Time : 18.28
INFO:root:2019-05-12 00:26:10, Epoch : 1, Step : 2196, Training Loss : 0.24686, Training Acc : 0.892, Run Time : 6.85
INFO:root:2019-05-12 00:26:25, Epoch : 1, Step : 2197, Training Loss : 0.30805, Training Acc : 0.861, Run Time : 15.34
INFO:root:2019-05-12 00:26:34, Epoch : 1, Step : 2198, Training Loss : 0.32291, Training Acc : 0.842, Run Time : 8.74
INFO:root:2019-05-12 00:26:37, Epoch : 1, Step : 2199, Training Loss : 0.26518, Training Acc : 0.892, Run Time : 3.38
INFO:root:2019-05-12 00:26:49, Epoch : 1, Step : 2200, Training Loss : 0.24816, Training Acc : 0.886, Run Time : 11.93
INFO:root:2019-05-12 00:26:51, Epoch : 1, Step : 2201, Training Loss : 0.46841, Training Acc : 0.717, Run Time : 2.10
INFO:root:2019-05-12 00:26:58, Epoch : 1, Step : 2202, Training Loss : 0.42624, Training Acc : 0.758, Run Time : 7.16
INFO:root:2019-05-12 00:27:00, Epoch : 1, Step : 2203, Training Loss : 0.40689, Training Acc : 0.772, Run Time : 1.83
INFO:root:2019-05-12 00:27:02, Epoch : 1, Step : 2204, Training Loss : 0.27288, Training Acc : 0.856, Run Time : 1.53
INFO:root:2019-05-12 00:27:10, Epoch : 1, Step : 2205, Training Loss : 0.29926, Training Acc : 0.814, Run Time : 8.12
INFO:root:2019-05-12 00:27:11, Epoch : 1, Step : 2206, Training Loss : 0.32864, Training Acc : 0.800, Run Time : 1.71
INFO:root:2019-05-12 00:27:14, Epoch : 1, Step : 2207, Training Loss : 0.40354, Training Acc : 0.778, Run Time : 2.16
INFO:root:2019-05-12 00:27:23, Epoch : 1, Step : 2208, Training Loss : 0.38881, Training Acc : 0.789, Run Time : 8.97
INFO:root:2019-05-12 00:27:24, Epoch : 1, Step : 2209, Training Loss : 0.38860, Training Acc : 0.814, Run Time : 1.60
INFO:root:2019-05-12 00:27:34, Epoch : 1, Step : 2210, Training Loss : 0.36434, Training Acc : 0.831, Run Time : 10.33
INFO:root:2019-05-12 00:27:36, Epoch : 1, Step : 2211, Training Loss : 0.27264, Training Acc : 0.861, Run Time : 1.22
INFO:root:2019-05-12 00:27:50, Epoch : 1, Step : 2212, Training Loss : 0.27568, Training Acc : 0.842, Run Time : 14.56
INFO:root:2019-05-12 00:27:51, Epoch : 1, Step : 2213, Training Loss : 0.27932, Training Acc : 0.872, Run Time : 1.16
INFO:root:2019-05-12 00:27:53, Epoch : 1, Step : 2214, Training Loss : 0.17881, Training Acc : 0.922, Run Time : 1.98
INFO:root:2019-05-12 00:28:04, Epoch : 1, Step : 2215, Training Loss : 0.17138, Training Acc : 0.942, Run Time : 10.19
INFO:root:2019-05-12 00:28:05, Epoch : 1, Step : 2216, Training Loss : 0.25743, Training Acc : 0.931, Run Time : 1.24
INFO:root:2019-05-12 00:28:07, Epoch : 1, Step : 2217, Training Loss : 0.20657, Training Acc : 0.894, Run Time : 2.35
INFO:root:2019-05-12 00:28:17, Epoch : 1, Step : 2218, Training Loss : 0.26673, Training Acc : 0.883, Run Time : 10.33
INFO:root:2019-05-12 00:28:19, Epoch : 1, Step : 2219, Training Loss : 0.24140, Training Acc : 0.889, Run Time : 1.35
INFO:root:2019-05-12 00:28:20, Epoch : 1, Step : 2220, Training Loss : 0.31076, Training Acc : 0.861, Run Time : 1.13
INFO:root:2019-05-12 00:28:22, Epoch : 1, Step : 2221, Training Loss : 0.33309, Training Acc : 0.828, Run Time : 1.94
INFO:root:2019-05-12 00:28:23, Epoch : 1, Step : 2222, Training Loss : 0.26845, Training Acc : 0.856, Run Time : 1.42
INFO:root:2019-05-12 00:28:25, Epoch : 1, Step : 2223, Training Loss : 0.26434, Training Acc : 0.922, Run Time : 1.82
INFO:root:2019-05-12 00:28:35, Epoch : 1, Step : 2224, Training Loss : 0.24635, Training Acc : 0.889, Run Time : 10.00
INFO:root:2019-05-12 00:28:37, Epoch : 1, Step : 2225, Training Loss : 0.28642, Training Acc : 0.867, Run Time : 1.49
INFO:root:2019-05-12 00:28:39, Epoch : 1, Step : 2226, Training Loss : 0.40305, Training Acc : 0.853, Run Time : 2.16
INFO:root:2019-05-12 00:28:48, Epoch : 1, Step : 2227, Training Loss : 0.35237, Training Acc : 0.767, Run Time : 9.10
INFO:root:2019-05-12 00:28:49, Epoch : 1, Step : 2228, Training Loss : 0.27712, Training Acc : 0.886, Run Time : 1.12
INFO:root:2019-05-12 00:28:50, Epoch : 1, Step : 2229, Training Loss : 0.26513, Training Acc : 0.881, Run Time : 1.15
INFO:root:2019-05-12 00:28:51, Epoch : 1, Step : 2230, Training Loss : 0.26566, Training Acc : 0.883, Run Time : 1.23
INFO:root:2019-05-12 00:28:53, Epoch : 1, Step : 2231, Training Loss : 0.43062, Training Acc : 0.803, Run Time : 1.66
INFO:root:2019-05-12 00:29:02, Epoch : 1, Step : 2232, Training Loss : 0.33772, Training Acc : 0.881, Run Time : 8.82
INFO:root:2019-05-12 00:29:03, Epoch : 1, Step : 2233, Training Loss : 0.23239, Training Acc : 0.897, Run Time : 1.37
INFO:root:2019-05-12 00:29:06, Epoch : 1, Step : 2234, Training Loss : 0.26500, Training Acc : 0.844, Run Time : 2.78
INFO:root:2019-05-12 00:29:16, Epoch : 1, Step : 2235, Training Loss : 0.28023, Training Acc : 0.856, Run Time : 10.34
INFO:root:2019-05-12 00:29:18, Epoch : 1, Step : 2236, Training Loss : 0.34780, Training Acc : 0.831, Run Time : 1.17
INFO:root:2019-05-12 00:29:29, Epoch : 1, Step : 2237, Training Loss : 0.54104, Training Acc : 0.797, Run Time : 11.38
INFO:root:2019-05-12 00:29:31, Epoch : 1, Step : 2238, Training Loss : 0.40264, Training Acc : 0.858, Run Time : 1.61
INFO:root:2019-05-12 00:29:33, Epoch : 1, Step : 2239, Training Loss : 0.35714, Training Acc : 0.875, Run Time : 2.56
INFO:root:2019-05-12 00:29:41, Epoch : 1, Step : 2240, Training Loss : 0.13780, Training Acc : 0.956, Run Time : 7.86
INFO:root:2019-05-12 00:29:42, Epoch : 1, Step : 2241, Training Loss : 0.20687, Training Acc : 0.931, Run Time : 1.32
INFO:root:2019-05-12 00:29:44, Epoch : 1, Step : 2242, Training Loss : 0.23276, Training Acc : 0.903, Run Time : 2.12
INFO:root:2019-05-12 00:29:54, Epoch : 1, Step : 2243, Training Loss : 0.29981, Training Acc : 0.883, Run Time : 9.18
INFO:root:2019-05-12 00:29:55, Epoch : 1, Step : 2244, Training Loss : 0.29359, Training Acc : 0.914, Run Time : 1.41
INFO:root:2019-05-12 00:29:57, Epoch : 1, Step : 2245, Training Loss : 0.25664, Training Acc : 0.900, Run Time : 1.99
INFO:root:2019-05-12 00:30:06, Epoch : 1, Step : 2246, Training Loss : 0.35704, Training Acc : 0.867, Run Time : 9.38
INFO:root:2019-05-12 00:30:08, Epoch : 1, Step : 2247, Training Loss : 0.21669, Training Acc : 0.908, Run Time : 1.17
INFO:root:2019-05-12 00:30:14, Epoch : 1, Step : 2248, Training Loss : 0.20648, Training Acc : 0.906, Run Time : 6.88
INFO:root:2019-05-12 00:30:26, Epoch : 1, Step : 2249, Training Loss : 0.25128, Training Acc : 0.908, Run Time : 11.24
INFO:root:2019-05-12 00:30:28, Epoch : 1, Step : 2250, Training Loss : 0.23478, Training Acc : 0.889, Run Time : 2.04
INFO:root:2019-05-12 00:30:30, Epoch : 1, Step : 2251, Training Loss : 0.24081, Training Acc : 0.906, Run Time : 2.19
INFO:root:2019-05-12 00:30:33, Epoch : 1, Step : 2252, Training Loss : 0.26087, Training Acc : 0.922, Run Time : 2.66
INFO:root:2019-05-12 00:30:34, Epoch : 1, Step : 2253, Training Loss : 0.33989, Training Acc : 0.853, Run Time : 1.18
INFO:root:2019-05-12 00:30:35, Epoch : 1, Step : 2254, Training Loss : 0.21526, Training Acc : 0.925, Run Time : 1.42
INFO:root:2019-05-12 00:30:37, Epoch : 1, Step : 2255, Training Loss : 0.26083, Training Acc : 0.931, Run Time : 2.30
INFO:root:2019-05-12 00:30:45, Epoch : 1, Step : 2256, Training Loss : 0.34322, Training Acc : 0.814, Run Time : 7.65
INFO:root:2019-05-12 00:30:49, Epoch : 1, Step : 2257, Training Loss : 0.20302, Training Acc : 0.925, Run Time : 3.79
INFO:root:2019-05-12 00:31:00, Epoch : 1, Step : 2258, Training Loss : 0.32355, Training Acc : 0.797, Run Time : 11.46
INFO:root:2019-05-12 00:31:01, Epoch : 1, Step : 2259, Training Loss : 0.17871, Training Acc : 0.936, Run Time : 1.14
INFO:root:2019-05-12 00:31:04, Epoch : 1, Step : 2260, Training Loss : 0.21362, Training Acc : 0.903, Run Time : 2.32
INFO:root:2019-05-12 00:31:12, Epoch : 1, Step : 2261, Training Loss : 0.19857, Training Acc : 0.919, Run Time : 8.13
INFO:root:2019-05-12 00:31:14, Epoch : 1, Step : 2262, Training Loss : 0.21096, Training Acc : 0.931, Run Time : 2.58
INFO:root:2019-05-12 00:31:16, Epoch : 1, Step : 2263, Training Loss : 0.19017, Training Acc : 0.922, Run Time : 1.70
INFO:root:2019-05-12 00:31:26, Epoch : 1, Step : 2264, Training Loss : 0.32235, Training Acc : 0.883, Run Time : 10.15
INFO:root:2019-05-12 00:31:28, Epoch : 1, Step : 2265, Training Loss : 0.32902, Training Acc : 0.842, Run Time : 1.17
INFO:root:2019-05-12 00:31:29, Epoch : 1, Step : 2266, Training Loss : 0.22060, Training Acc : 0.892, Run Time : 1.15
INFO:root:2019-05-12 00:31:31, Epoch : 1, Step : 2267, Training Loss : 0.24372, Training Acc : 0.922, Run Time : 2.71
INFO:root:2019-05-12 00:31:42, Epoch : 1, Step : 2268, Training Loss : 0.32242, Training Acc : 0.869, Run Time : 10.91
INFO:root:2019-05-12 00:31:44, Epoch : 1, Step : 2269, Training Loss : 0.64488, Training Acc : 0.683, Run Time : 1.49
INFO:root:2019-05-12 00:31:51, Epoch : 1, Step : 2270, Training Loss : 0.37224, Training Acc : 0.803, Run Time : 6.93
INFO:root:2019-05-12 00:31:52, Epoch : 1, Step : 2271, Training Loss : 0.28690, Training Acc : 0.861, Run Time : 1.24
INFO:root:2019-05-12 00:31:53, Epoch : 1, Step : 2272, Training Loss : 0.59023, Training Acc : 0.714, Run Time : 1.13
INFO:root:2019-05-12 00:31:57, Epoch : 1, Step : 2273, Training Loss : 0.38004, Training Acc : 0.847, Run Time : 4.41
INFO:root:2019-05-12 00:32:04, Epoch : 1, Step : 2274, Training Loss : 0.23856, Training Acc : 0.903, Run Time : 6.05
INFO:root:2019-05-12 00:32:05, Epoch : 1, Step : 2275, Training Loss : 0.69732, Training Acc : 0.586, Run Time : 1.13
INFO:root:2019-05-12 00:32:08, Epoch : 1, Step : 2276, Training Loss : 0.22344, Training Acc : 0.897, Run Time : 3.51
INFO:root:2019-05-12 00:32:11, Epoch : 1, Step : 2277, Training Loss : 0.26464, Training Acc : 0.872, Run Time : 2.70
INFO:root:2019-05-12 00:32:12, Epoch : 1, Step : 2278, Training Loss : 0.56332, Training Acc : 0.678, Run Time : 1.22
INFO:root:2019-05-12 00:32:14, Epoch : 1, Step : 2279, Training Loss : 0.55625, Training Acc : 0.731, Run Time : 2.25
INFO:root:2019-05-12 00:32:23, Epoch : 1, Step : 2280, Training Loss : 0.37944, Training Acc : 0.783, Run Time : 8.98
INFO:root:2019-05-12 00:32:24, Epoch : 1, Step : 2281, Training Loss : 0.27082, Training Acc : 0.842, Run Time : 1.14
INFO:root:2019-05-12 00:32:35, Epoch : 1, Step : 2282, Training Loss : 0.28955, Training Acc : 0.836, Run Time : 10.08
INFO:root:2019-05-12 00:32:37, Epoch : 1, Step : 2283, Training Loss : 0.27406, Training Acc : 0.828, Run Time : 2.32
INFO:root:2019-05-12 00:32:46, Epoch : 1, Step : 2284, Training Loss : 0.23782, Training Acc : 0.892, Run Time : 8.75
INFO:root:2019-05-12 00:32:47, Epoch : 1, Step : 2285, Training Loss : 0.25082, Training Acc : 0.897, Run Time : 1.54
INFO:root:2019-05-12 00:32:55, Epoch : 1, Step : 2286, Training Loss : 0.22330, Training Acc : 0.908, Run Time : 7.79
INFO:root:2019-05-12 00:33:01, Epoch : 1, Step : 2287, Training Loss : 0.30603, Training Acc : 0.872, Run Time : 5.63
INFO:root:2019-05-12 00:33:03, Epoch : 1, Step : 2288, Training Loss : 0.23052, Training Acc : 0.872, Run Time : 2.05
INFO:root:2019-05-12 00:33:12, Epoch : 1, Step : 2289, Training Loss : 0.25032, Training Acc : 0.878, Run Time : 9.47
INFO:root:2019-05-12 00:33:14, Epoch : 1, Step : 2290, Training Loss : 0.25223, Training Acc : 0.864, Run Time : 2.08
INFO:root:2019-05-12 00:33:25, Epoch : 1, Step : 2291, Training Loss : 0.23054, Training Acc : 0.881, Run Time : 10.33
INFO:root:2019-05-12 00:33:27, Epoch : 1, Step : 2292, Training Loss : 0.24144, Training Acc : 0.906, Run Time : 2.82
INFO:root:2019-05-12 00:33:38, Epoch : 1, Step : 2293, Training Loss : 0.26070, Training Acc : 0.867, Run Time : 10.73
INFO:root:2019-05-12 00:33:39, Epoch : 1, Step : 2294, Training Loss : 0.21864, Training Acc : 0.922, Run Time : 1.17
INFO:root:2019-05-12 00:33:51, Epoch : 1, Step : 2295, Training Loss : 0.62676, Training Acc : 0.656, Run Time : 12.04
INFO:root:2019-05-12 00:33:53, Epoch : 1, Step : 2296, Training Loss : 0.62756, Training Acc : 0.650, Run Time : 1.54
INFO:root:2019-05-12 00:33:55, Epoch : 1, Step : 2297, Training Loss : 0.50644, Training Acc : 0.700, Run Time : 2.07
INFO:root:2019-05-12 00:33:57, Epoch : 1, Step : 2298, Training Loss : 0.34908, Training Acc : 0.856, Run Time : 1.72
INFO:root:2019-05-12 00:34:05, Epoch : 1, Step : 2299, Training Loss : 0.32649, Training Acc : 0.875, Run Time : 8.03
INFO:root:2019-05-12 00:34:07, Epoch : 1, Step : 2300, Training Loss : 0.25179, Training Acc : 0.917, Run Time : 2.32
INFO:root:2019-05-12 00:34:15, Epoch : 1, Step : 2301, Training Loss : 0.71944, Training Acc : 0.858, Run Time : 7.55
INFO:root:2019-05-12 00:34:17, Epoch : 1, Step : 2302, Training Loss : 0.89299, Training Acc : 0.833, Run Time : 2.71
INFO:root:2019-05-12 00:34:29, Epoch : 1, Step : 2303, Training Loss : 0.85067, Training Acc : 0.831, Run Time : 11.38
INFO:root:2019-05-12 00:34:30, Epoch : 1, Step : 2304, Training Loss : 0.94074, Training Acc : 0.800, Run Time : 1.16
INFO:root:2019-05-12 00:34:31, Epoch : 1, Step : 2305, Training Loss : 0.92626, Training Acc : 0.794, Run Time : 1.60
INFO:root:2019-05-12 00:34:39, Epoch : 1, Step : 2306, Training Loss : 0.70703, Training Acc : 0.806, Run Time : 7.69
INFO:root:2019-05-12 00:34:41, Epoch : 1, Step : 2307, Training Loss : 0.64600, Training Acc : 0.792, Run Time : 1.94
INFO:root:2019-05-12 00:34:49, Epoch : 1, Step : 2308, Training Loss : 0.51788, Training Acc : 0.800, Run Time : 8.19
INFO:root:2019-05-12 00:34:50, Epoch : 1, Step : 2309, Training Loss : 0.48366, Training Acc : 0.772, Run Time : 1.15
INFO:root:2019-05-12 00:34:52, Epoch : 1, Step : 2310, Training Loss : 0.50204, Training Acc : 0.758, Run Time : 1.70
INFO:root:2019-05-12 00:35:02, Epoch : 1, Step : 2311, Training Loss : 0.46597, Training Acc : 0.750, Run Time : 10.10
INFO:root:2019-05-12 00:35:03, Epoch : 1, Step : 2312, Training Loss : 0.50319, Training Acc : 0.747, Run Time : 1.23
INFO:root:2019-05-12 00:35:13, Epoch : 1, Step : 2313, Training Loss : 0.44192, Training Acc : 0.811, Run Time : 9.41
INFO:root:2019-05-12 00:35:14, Epoch : 1, Step : 2314, Training Loss : 0.44716, Training Acc : 0.817, Run Time : 1.32
INFO:root:2019-05-12 00:35:16, Epoch : 1, Step : 2315, Training Loss : 0.61007, Training Acc : 0.650, Run Time : 2.19
INFO:root:2019-05-12 00:35:25, Epoch : 1, Step : 2316, Training Loss : 0.57471, Training Acc : 0.678, Run Time : 8.35
INFO:root:2019-05-12 00:35:27, Epoch : 1, Step : 2317, Training Loss : 0.40515, Training Acc : 0.839, Run Time : 2.78
INFO:root:2019-05-12 00:35:35, Epoch : 1, Step : 2318, Training Loss : 0.50621, Training Acc : 0.733, Run Time : 7.87
INFO:root:2019-05-12 00:35:37, Epoch : 1, Step : 2319, Training Loss : 0.40335, Training Acc : 0.850, Run Time : 1.22
INFO:root:2019-05-12 00:35:39, Epoch : 1, Step : 2320, Training Loss : 0.41360, Training Acc : 0.781, Run Time : 2.85
INFO:root:2019-05-12 00:35:46, Epoch : 1, Step : 2321, Training Loss : 0.45374, Training Acc : 0.819, Run Time : 6.72
INFO:root:2019-05-12 00:35:47, Epoch : 1, Step : 2322, Training Loss : 0.47793, Training Acc : 0.764, Run Time : 1.13
INFO:root:2019-05-12 00:35:58, Epoch : 1, Step : 2323, Training Loss : 0.30315, Training Acc : 0.881, Run Time : 10.63
INFO:root:2019-05-12 00:35:59, Epoch : 1, Step : 2324, Training Loss : 0.33948, Training Acc : 0.867, Run Time : 1.55
INFO:root:2019-05-12 00:36:10, Epoch : 1, Step : 2325, Training Loss : 0.35587, Training Acc : 0.875, Run Time : 10.71
INFO:root:2019-05-12 00:36:12, Epoch : 1, Step : 2326, Training Loss : 0.36495, Training Acc : 0.825, Run Time : 2.06
INFO:root:2019-05-12 00:36:22, Epoch : 1, Step : 2327, Training Loss : 0.51849, Training Acc : 0.714, Run Time : 9.62
INFO:root:2019-05-12 00:36:24, Epoch : 1, Step : 2328, Training Loss : 0.31416, Training Acc : 0.897, Run Time : 1.71
INFO:root:2019-05-12 00:36:33, Epoch : 1, Step : 2329, Training Loss : 0.36346, Training Acc : 0.850, Run Time : 9.77
INFO:root:2019-05-12 00:36:35, Epoch : 1, Step : 2330, Training Loss : 0.38456, Training Acc : 0.847, Run Time : 2.07
INFO:root:2019-05-12 00:36:47, Epoch : 1, Step : 2331, Training Loss : 0.58836, Training Acc : 0.664, Run Time : 11.44
INFO:root:2019-05-12 00:36:48, Epoch : 1, Step : 2332, Training Loss : 0.47420, Training Acc : 0.825, Run Time : 1.14
INFO:root:2019-05-12 00:36:49, Epoch : 1, Step : 2333, Training Loss : 0.44753, Training Acc : 0.794, Run Time : 1.31
INFO:root:2019-05-12 00:36:58, Epoch : 1, Step : 2334, Training Loss : 0.45126, Training Acc : 0.839, Run Time : 8.77
INFO:root:2019-05-12 00:37:00, Epoch : 1, Step : 2335, Training Loss : 0.48820, Training Acc : 0.781, Run Time : 1.67
INFO:root:2019-05-12 00:37:10, Epoch : 1, Step : 2336, Training Loss : 0.34365, Training Acc : 0.886, Run Time : 10.22
INFO:root:2019-05-12 00:37:11, Epoch : 1, Step : 2337, Training Loss : 0.51605, Training Acc : 0.797, Run Time : 1.30
INFO:root:2019-05-12 00:37:23, Epoch : 1, Step : 2338, Training Loss : 0.61369, Training Acc : 0.636, Run Time : 11.38
INFO:root:2019-05-12 00:37:24, Epoch : 1, Step : 2339, Training Loss : 0.36231, Training Acc : 0.794, Run Time : 1.30
INFO:root:2019-05-12 00:37:35, Epoch : 1, Step : 2340, Training Loss : 0.36825, Training Acc : 0.853, Run Time : 11.17
INFO:root:2019-05-12 00:37:37, Epoch : 1, Step : 2341, Training Loss : 0.35629, Training Acc : 0.767, Run Time : 2.06
INFO:root:2019-05-12 00:37:47, Epoch : 1, Step : 2342, Training Loss : 0.57606, Training Acc : 0.644, Run Time : 10.11
INFO:root:2019-05-12 00:37:49, Epoch : 1, Step : 2343, Training Loss : 0.44223, Training Acc : 0.725, Run Time : 1.47
INFO:root:2019-05-12 00:37:51, Epoch : 1, Step : 2344, Training Loss : 0.32929, Training Acc : 0.833, Run Time : 2.28
INFO:root:2019-05-12 00:38:00, Epoch : 1, Step : 2345, Training Loss : 0.30183, Training Acc : 0.917, Run Time : 8.73
INFO:root:2019-05-12 00:38:01, Epoch : 1, Step : 2346, Training Loss : 0.33106, Training Acc : 0.881, Run Time : 1.14
INFO:root:2019-05-12 00:38:03, Epoch : 1, Step : 2347, Training Loss : 0.41248, Training Acc : 0.856, Run Time : 1.69
INFO:root:2019-05-12 00:38:12, Epoch : 1, Step : 2348, Training Loss : 0.30541, Training Acc : 0.908, Run Time : 9.46
INFO:root:2019-05-12 00:38:13, Epoch : 1, Step : 2349, Training Loss : 0.27042, Training Acc : 0.917, Run Time : 1.48
INFO:root:2019-05-12 00:38:15, Epoch : 1, Step : 2350, Training Loss : 0.33045, Training Acc : 0.894, Run Time : 1.17
INFO:root:2019-05-12 00:38:23, Epoch : 1, Step : 2351, Training Loss : 0.20765, Training Acc : 0.922, Run Time : 8.05
INFO:root:2019-05-12 00:38:24, Epoch : 1, Step : 2352, Training Loss : 0.27244, Training Acc : 0.894, Run Time : 1.34
INFO:root:2019-05-12 00:38:26, Epoch : 1, Step : 2353, Training Loss : 0.35783, Training Acc : 0.892, Run Time : 2.34
INFO:root:2019-05-12 00:38:35, Epoch : 1, Step : 2354, Training Loss : 0.28983, Training Acc : 0.881, Run Time : 9.04
INFO:root:2019-05-12 00:38:37, Epoch : 1, Step : 2355, Training Loss : 0.57935, Training Acc : 0.647, Run Time : 1.15
INFO:root:2019-05-12 00:38:39, Epoch : 1, Step : 2356, Training Loss : 0.44973, Training Acc : 0.786, Run Time : 2.30
INFO:root:2019-05-12 00:38:48, Epoch : 1, Step : 2357, Training Loss : 0.34446, Training Acc : 0.892, Run Time : 8.81
INFO:root:2019-05-12 00:38:49, Epoch : 1, Step : 2358, Training Loss : 0.31164, Training Acc : 0.886, Run Time : 1.13
INFO:root:2019-05-12 00:39:00, Epoch : 1, Step : 2359, Training Loss : 0.57017, Training Acc : 0.639, Run Time : 10.91
INFO:root:2019-05-12 00:39:01, Epoch : 1, Step : 2360, Training Loss : 0.51150, Training Acc : 0.711, Run Time : 1.61
INFO:root:2019-05-12 00:39:13, Epoch : 1, Step : 2361, Training Loss : 0.23561, Training Acc : 0.936, Run Time : 11.83
INFO:root:2019-05-12 00:39:16, Epoch : 1, Step : 2362, Training Loss : 0.30135, Training Acc : 0.867, Run Time : 2.79
INFO:root:2019-05-12 00:39:28, Epoch : 1, Step : 2363, Training Loss : 0.40246, Training Acc : 0.833, Run Time : 11.89
INFO:root:2019-05-12 00:39:30, Epoch : 1, Step : 2364, Training Loss : 0.29958, Training Acc : 0.922, Run Time : 1.84
INFO:root:2019-05-12 00:39:32, Epoch : 1, Step : 2365, Training Loss : 0.38335, Training Acc : 0.775, Run Time : 2.59
INFO:root:2019-05-12 00:39:41, Epoch : 1, Step : 2366, Training Loss : 0.38212, Training Acc : 0.844, Run Time : 9.17
INFO:root:2019-05-12 00:39:43, Epoch : 1, Step : 2367, Training Loss : 0.35347, Training Acc : 0.914, Run Time : 1.74
INFO:root:2019-05-12 00:39:54, Epoch : 1, Step : 2368, Training Loss : 0.35675, Training Acc : 0.883, Run Time : 10.65
INFO:root:2019-05-12 00:39:56, Epoch : 1, Step : 2369, Training Loss : 0.26111, Training Acc : 0.933, Run Time : 2.49
INFO:root:2019-05-12 00:40:05, Epoch : 1, Step : 2370, Training Loss : 0.27784, Training Acc : 0.911, Run Time : 9.06
INFO:root:2019-05-12 00:40:07, Epoch : 1, Step : 2371, Training Loss : 0.24530, Training Acc : 0.903, Run Time : 1.14
INFO:root:2019-05-12 00:40:08, Epoch : 1, Step : 2372, Training Loss : 0.32372, Training Acc : 0.878, Run Time : 1.75
INFO:root:2019-05-12 00:40:19, Epoch : 1, Step : 2373, Training Loss : 0.27477, Training Acc : 0.883, Run Time : 11.05
INFO:root:2019-05-12 00:40:21, Epoch : 1, Step : 2374, Training Loss : 0.32529, Training Acc : 0.850, Run Time : 1.31
INFO:root:2019-05-12 00:40:26, Epoch : 1, Step : 2375, Training Loss : 0.36566, Training Acc : 0.847, Run Time : 5.56
INFO:root:2019-05-12 00:40:29, Epoch : 1, Step : 2376, Training Loss : 0.91795, Training Acc : 0.481, Run Time : 2.68
INFO:root:2019-05-12 00:40:32, Epoch : 1, Step : 2377, Training Loss : 0.40805, Training Acc : 0.847, Run Time : 2.73
INFO:root:2019-05-12 00:40:34, Epoch : 1, Step : 2378, Training Loss : 0.35904, Training Acc : 0.836, Run Time : 2.23
INFO:root:2019-05-12 00:40:35, Epoch : 1, Step : 2379, Training Loss : 0.59903, Training Acc : 0.694, Run Time : 1.14
INFO:root:2019-05-12 00:40:37, Epoch : 1, Step : 2380, Training Loss : 0.53098, Training Acc : 0.697, Run Time : 1.54
INFO:root:2019-05-12 00:40:45, Epoch : 1, Step : 2381, Training Loss : 0.38699, Training Acc : 0.825, Run Time : 8.15
INFO:root:2019-05-12 00:40:47, Epoch : 1, Step : 2382, Training Loss : 0.30370, Training Acc : 0.861, Run Time : 2.51
INFO:root:2019-05-12 00:40:58, Epoch : 1, Step : 2383, Training Loss : 0.29634, Training Acc : 0.850, Run Time : 11.18
INFO:root:2019-05-12 00:40:59, Epoch : 1, Step : 2384, Training Loss : 0.27275, Training Acc : 0.867, Run Time : 1.13
INFO:root:2019-05-12 00:41:01, Epoch : 1, Step : 2385, Training Loss : 0.43331, Training Acc : 0.789, Run Time : 1.13
INFO:root:2019-05-12 00:41:02, Epoch : 1, Step : 2386, Training Loss : 0.30927, Training Acc : 0.869, Run Time : 1.13
INFO:root:2019-05-12 00:41:04, Epoch : 1, Step : 2387, Training Loss : 0.38106, Training Acc : 0.836, Run Time : 2.04
INFO:root:2019-05-12 00:41:13, Epoch : 1, Step : 2388, Training Loss : 0.32616, Training Acc : 0.850, Run Time : 9.48
INFO:root:2019-05-12 00:41:14, Epoch : 1, Step : 2389, Training Loss : 0.34170, Training Acc : 0.869, Run Time : 1.23
INFO:root:2019-05-12 00:41:17, Epoch : 1, Step : 2390, Training Loss : 0.31015, Training Acc : 0.869, Run Time : 2.41
INFO:root:2019-05-12 00:41:26, Epoch : 1, Step : 2391, Training Loss : 0.39982, Training Acc : 0.828, Run Time : 9.17
INFO:root:2019-05-12 00:41:27, Epoch : 1, Step : 2392, Training Loss : 0.35339, Training Acc : 0.903, Run Time : 1.35
INFO:root:2019-05-12 00:41:29, Epoch : 1, Step : 2393, Training Loss : 0.25930, Training Acc : 0.906, Run Time : 2.08
INFO:root:2019-05-12 00:41:39, Epoch : 1, Step : 2394, Training Loss : 0.31112, Training Acc : 0.831, Run Time : 9.59
INFO:root:2019-05-12 00:41:40, Epoch : 1, Step : 2395, Training Loss : 0.21655, Training Acc : 0.947, Run Time : 1.24
INFO:root:2019-05-12 00:41:51, Epoch : 1, Step : 2396, Training Loss : 0.36829, Training Acc : 0.914, Run Time : 10.19
INFO:root:2019-05-12 00:41:52, Epoch : 1, Step : 2397, Training Loss : 0.30205, Training Acc : 0.900, Run Time : 1.69
INFO:root:2019-05-12 00:42:03, Epoch : 1, Step : 2398, Training Loss : 0.26942, Training Acc : 0.875, Run Time : 10.99
INFO:root:2019-05-12 00:42:05, Epoch : 1, Step : 2399, Training Loss : 0.33345, Training Acc : 0.836, Run Time : 1.46
INFO:root:2019-05-12 00:42:13, Epoch : 1, Step : 2400, Training Loss : 0.33065, Training Acc : 0.850, Run Time : 8.61
INFO:root:2019-05-12 00:42:17, Epoch : 1, Step : 2401, Training Loss : 0.94194, Training Acc : 0.661, Run Time : 4.20
INFO:root:2019-05-12 00:42:19, Epoch : 1, Step : 2402, Training Loss : 1.12411, Training Acc : 0.553, Run Time : 1.23
INFO:root:2019-05-12 00:42:24, Epoch : 1, Step : 2403, Training Loss : 1.00316, Training Acc : 0.558, Run Time : 5.06
INFO:root:2019-05-12 00:42:28, Epoch : 1, Step : 2404, Training Loss : 1.00587, Training Acc : 0.581, Run Time : 4.45
INFO:root:2019-05-12 00:42:31, Epoch : 1, Step : 2405, Training Loss : 1.07460, Training Acc : 0.472, Run Time : 2.60
INFO:root:2019-05-12 00:42:41, Epoch : 1, Step : 2406, Training Loss : 1.42161, Training Acc : 0.225, Run Time : 10.07
INFO:root:2019-05-12 00:42:43, Epoch : 1, Step : 2407, Training Loss : 0.93372, Training Acc : 0.508, Run Time : 1.93
INFO:root:2019-05-12 00:42:55, Epoch : 1, Step : 2408, Training Loss : 0.59870, Training Acc : 0.653, Run Time : 12.56
INFO:root:2019-05-12 00:42:57, Epoch : 1, Step : 2409, Training Loss : 0.85109, Training Acc : 0.547, Run Time : 1.20
INFO:root:2019-05-12 00:43:07, Epoch : 1, Step : 2410, Training Loss : 0.82335, Training Acc : 0.544, Run Time : 10.00
INFO:root:2019-05-12 00:43:08, Epoch : 1, Step : 2411, Training Loss : 0.40976, Training Acc : 0.844, Run Time : 1.49
INFO:root:2019-05-12 00:43:19, Epoch : 1, Step : 2412, Training Loss : 0.48888, Training Acc : 0.814, Run Time : 10.95
INFO:root:2019-05-12 00:43:20, Epoch : 1, Step : 2413, Training Loss : 0.33234, Training Acc : 0.911, Run Time : 1.19
INFO:root:2019-05-12 00:43:31, Epoch : 1, Step : 2414, Training Loss : 0.54708, Training Acc : 0.703, Run Time : 10.85
INFO:root:2019-05-12 00:43:32, Epoch : 1, Step : 2415, Training Loss : 0.47359, Training Acc : 0.753, Run Time : 1.45
INFO:root:2019-05-12 00:43:34, Epoch : 1, Step : 2416, Training Loss : 0.46488, Training Acc : 0.806, Run Time : 1.13
INFO:root:2019-05-12 00:43:44, Epoch : 1, Step : 2417, Training Loss : 0.43022, Training Acc : 0.808, Run Time : 10.62
INFO:root:2019-05-12 00:43:45, Epoch : 1, Step : 2418, Training Loss : 0.43304, Training Acc : 0.875, Run Time : 1.18
INFO:root:2019-05-12 00:43:54, Epoch : 1, Step : 2419, Training Loss : 0.56997, Training Acc : 0.694, Run Time : 8.07
INFO:root:2019-05-12 00:43:55, Epoch : 1, Step : 2420, Training Loss : 0.39316, Training Acc : 0.833, Run Time : 1.36
INFO:root:2019-05-12 00:44:03, Epoch : 1, Step : 2421, Training Loss : 0.28953, Training Acc : 0.964, Run Time : 8.22
INFO:root:2019-05-12 00:44:05, Epoch : 1, Step : 2422, Training Loss : 0.38749, Training Acc : 0.850, Run Time : 2.32
INFO:root:2019-05-12 00:44:15, Epoch : 1, Step : 2423, Training Loss : 0.38934, Training Acc : 0.836, Run Time : 9.61
INFO:root:2019-05-12 00:44:17, Epoch : 1, Step : 2424, Training Loss : 0.64639, Training Acc : 0.564, Run Time : 2.45
INFO:root:2019-05-12 00:44:28, Epoch : 1, Step : 2425, Training Loss : 0.37157, Training Acc : 0.908, Run Time : 10.15
INFO:root:2019-05-12 00:44:29, Epoch : 1, Step : 2426, Training Loss : 0.29308, Training Acc : 0.942, Run Time : 1.59
INFO:root:2019-05-12 00:44:41, Epoch : 1, Step : 2427, Training Loss : 0.28441, Training Acc : 0.967, Run Time : 11.71
INFO:root:2019-05-12 00:44:42, Epoch : 1, Step : 2428, Training Loss : 0.15385, Training Acc : 0.978, Run Time : 1.29
INFO:root:2019-05-12 00:44:45, Epoch : 1, Step : 2429, Training Loss : 0.84665, Training Acc : 0.439, Run Time : 2.34
INFO:root:2019-05-12 00:44:55, Epoch : 1, Step : 2430, Training Loss : 0.33839, Training Acc : 0.958, Run Time : 10.74
INFO:root:2019-05-12 00:44:58, Epoch : 1, Step : 2431, Training Loss : 0.59537, Training Acc : 0.561, Run Time : 2.43
INFO:root:2019-05-12 00:45:03, Epoch : 1, Step : 2432, Training Loss : 0.32841, Training Acc : 0.933, Run Time : 5.06
INFO:root:2019-05-12 00:45:04, Epoch : 1, Step : 2433, Training Loss : 0.33097, Training Acc : 0.950, Run Time : 1.16
INFO:root:2019-05-12 00:45:16, Epoch : 1, Step : 2434, Training Loss : 0.31888, Training Acc : 0.931, Run Time : 12.09
INFO:root:2019-05-12 00:45:17, Epoch : 1, Step : 2435, Training Loss : 0.36902, Training Acc : 0.931, Run Time : 1.41
INFO:root:2019-05-12 00:45:20, Epoch : 1, Step : 2436, Training Loss : 0.40095, Training Acc : 0.906, Run Time : 2.50
INFO:root:2019-05-12 00:45:28, Epoch : 1, Step : 2437, Training Loss : 0.48425, Training Acc : 0.839, Run Time : 7.66
INFO:root:2019-05-12 00:45:29, Epoch : 1, Step : 2438, Training Loss : 0.47261, Training Acc : 0.772, Run Time : 1.25
INFO:root:2019-05-12 00:45:31, Epoch : 1, Step : 2439, Training Loss : 0.36955, Training Acc : 0.847, Run Time : 2.16
INFO:root:2019-05-12 00:45:41, Epoch : 1, Step : 2440, Training Loss : 0.72606, Training Acc : 0.581, Run Time : 9.86
INFO:root:2019-05-12 00:45:42, Epoch : 1, Step : 2441, Training Loss : 0.52824, Training Acc : 0.703, Run Time : 1.13
INFO:root:2019-05-12 00:45:45, Epoch : 1, Step : 2442, Training Loss : 0.28820, Training Acc : 0.922, Run Time : 2.68
INFO:root:2019-05-12 00:45:55, Epoch : 1, Step : 2443, Training Loss : 0.40853, Training Acc : 0.828, Run Time : 10.61
INFO:root:2019-05-12 00:45:57, Epoch : 1, Step : 2444, Training Loss : 0.48203, Training Acc : 0.822, Run Time : 1.80
INFO:root:2019-05-12 00:46:06, Epoch : 1, Step : 2445, Training Loss : 0.16287, Training Acc : 0.978, Run Time : 9.38
INFO:root:2019-05-12 00:46:08, Epoch : 1, Step : 2446, Training Loss : 0.21650, Training Acc : 0.983, Run Time : 1.37
INFO:root:2019-05-12 00:46:09, Epoch : 1, Step : 2447, Training Loss : 0.32013, Training Acc : 0.906, Run Time : 1.15
INFO:root:2019-05-12 00:46:11, Epoch : 1, Step : 2448, Training Loss : 0.31349, Training Acc : 0.956, Run Time : 1.92
INFO:root:2019-05-12 00:46:22, Epoch : 1, Step : 2449, Training Loss : 0.24040, Training Acc : 0.981, Run Time : 10.81
INFO:root:2019-05-12 00:46:23, Epoch : 1, Step : 2450, Training Loss : 0.21494, Training Acc : 0.981, Run Time : 1.63
INFO:root:2019-05-12 00:46:26, Epoch : 1, Step : 2451, Training Loss : 0.25325, Training Acc : 0.958, Run Time : 2.52
INFO:root:2019-05-12 00:46:34, Epoch : 1, Step : 2452, Training Loss : 0.24189, Training Acc : 0.981, Run Time : 8.00
INFO:root:2019-05-12 00:46:35, Epoch : 1, Step : 2453, Training Loss : 0.20917, Training Acc : 0.947, Run Time : 1.28
INFO:root:2019-05-12 00:46:37, Epoch : 1, Step : 2454, Training Loss : 0.31135, Training Acc : 0.950, Run Time : 2.36
INFO:root:2019-05-12 00:46:46, Epoch : 1, Step : 2455, Training Loss : 0.19959, Training Acc : 0.992, Run Time : 8.46
INFO:root:2019-05-12 00:46:47, Epoch : 1, Step : 2456, Training Loss : 0.29814, Training Acc : 0.964, Run Time : 1.16
INFO:root:2019-05-12 00:46:50, Epoch : 1, Step : 2457, Training Loss : 0.42713, Training Acc : 0.858, Run Time : 2.59
INFO:root:2019-05-12 00:46:53, Epoch : 1, Step : 2458, Training Loss : 0.37369, Training Acc : 0.878, Run Time : 2.92
INFO:root:2019-05-12 00:46:54, Epoch : 1, Step : 2459, Training Loss : 0.24152, Training Acc : 0.931, Run Time : 1.38
INFO:root:2019-05-12 00:47:03, Epoch : 1, Step : 2460, Training Loss : 0.48001, Training Acc : 0.683, Run Time : 8.70
INFO:root:2019-05-12 00:47:04, Epoch : 1, Step : 2461, Training Loss : 0.71015, Training Acc : 0.569, Run Time : 1.14
INFO:root:2019-05-12 00:47:05, Epoch : 1, Step : 2462, Training Loss : 0.31387, Training Acc : 0.886, Run Time : 1.42
INFO:root:2019-05-12 00:47:07, Epoch : 1, Step : 2463, Training Loss : 0.26724, Training Acc : 0.936, Run Time : 2.05
INFO:root:2019-05-12 00:47:18, Epoch : 1, Step : 2464, Training Loss : 0.19968, Training Acc : 0.942, Run Time : 10.26
INFO:root:2019-05-12 00:47:19, Epoch : 1, Step : 2465, Training Loss : 0.20172, Training Acc : 0.944, Run Time : 1.17
INFO:root:2019-05-12 00:47:21, Epoch : 1, Step : 2466, Training Loss : 0.26627, Training Acc : 0.953, Run Time : 2.48
INFO:root:2019-05-12 00:47:31, Epoch : 1, Step : 2467, Training Loss : 0.25785, Training Acc : 0.958, Run Time : 10.10
INFO:root:2019-05-12 00:47:33, Epoch : 1, Step : 2468, Training Loss : 0.83658, Training Acc : 0.564, Run Time : 1.34
INFO:root:2019-05-12 00:47:34, Epoch : 1, Step : 2469, Training Loss : 0.46084, Training Acc : 0.747, Run Time : 1.82
INFO:root:2019-05-12 00:47:44, Epoch : 1, Step : 2470, Training Loss : 0.23864, Training Acc : 0.942, Run Time : 9.38
INFO:root:2019-05-12 00:47:45, Epoch : 1, Step : 2471, Training Loss : 0.25701, Training Acc : 0.928, Run Time : 1.27
INFO:root:2019-05-12 00:47:55, Epoch : 1, Step : 2472, Training Loss : 0.53505, Training Acc : 0.642, Run Time : 10.33
INFO:root:2019-05-12 00:47:57, Epoch : 1, Step : 2473, Training Loss : 0.34778, Training Acc : 0.822, Run Time : 1.53
INFO:root:2019-05-12 00:48:08, Epoch : 1, Step : 2474, Training Loss : 0.28109, Training Acc : 0.900, Run Time : 10.99
INFO:root:2019-05-12 00:48:09, Epoch : 1, Step : 2475, Training Loss : 0.19045, Training Acc : 0.967, Run Time : 1.28
INFO:root:2019-05-12 00:48:11, Epoch : 1, Step : 2476, Training Loss : 0.44169, Training Acc : 0.861, Run Time : 1.31
INFO:root:2019-05-12 00:48:12, Epoch : 1, Step : 2477, Training Loss : 0.28306, Training Acc : 0.950, Run Time : 1.72
INFO:root:2019-05-12 00:48:14, Epoch : 1, Step : 2478, Training Loss : 0.25652, Training Acc : 0.953, Run Time : 1.27
INFO:root:2019-05-12 00:48:23, Epoch : 1, Step : 2479, Training Loss : 0.20667, Training Acc : 0.961, Run Time : 9.24
INFO:root:2019-05-12 00:48:24, Epoch : 1, Step : 2480, Training Loss : 0.34107, Training Acc : 0.972, Run Time : 1.22
INFO:root:2019-05-12 00:48:27, Epoch : 1, Step : 2481, Training Loss : 0.24915, Training Acc : 0.956, Run Time : 2.76
INFO:root:2019-05-12 00:48:34, Epoch : 1, Step : 2482, Training Loss : 0.30643, Training Acc : 0.936, Run Time : 7.52
INFO:root:2019-05-12 00:48:37, Epoch : 1, Step : 2483, Training Loss : 0.30362, Training Acc : 0.917, Run Time : 2.76
INFO:root:2019-05-12 00:48:47, Epoch : 1, Step : 2484, Training Loss : 0.28829, Training Acc : 0.942, Run Time : 10.34
INFO:root:2019-05-12 00:48:49, Epoch : 1, Step : 2485, Training Loss : 0.38218, Training Acc : 0.894, Run Time : 1.12
INFO:root:2019-05-12 00:48:50, Epoch : 1, Step : 2486, Training Loss : 0.28124, Training Acc : 0.914, Run Time : 1.15
INFO:root:2019-05-12 00:48:51, Epoch : 1, Step : 2487, Training Loss : 0.29552, Training Acc : 0.897, Run Time : 1.81
INFO:root:2019-05-12 00:49:01, Epoch : 1, Step : 2488, Training Loss : 0.37262, Training Acc : 0.869, Run Time : 9.93
INFO:root:2019-05-12 00:49:03, Epoch : 1, Step : 2489, Training Loss : 0.49935, Training Acc : 0.692, Run Time : 1.12
INFO:root:2019-05-12 00:49:05, Epoch : 1, Step : 2490, Training Loss : 0.64425, Training Acc : 0.628, Run Time : 2.25
INFO:root:2019-05-12 00:49:14, Epoch : 1, Step : 2491, Training Loss : 0.40872, Training Acc : 0.792, Run Time : 9.72
INFO:root:2019-05-12 00:49:16, Epoch : 1, Step : 2492, Training Loss : 0.51286, Training Acc : 0.700, Run Time : 1.17
INFO:root:2019-05-12 00:49:18, Epoch : 1, Step : 2493, Training Loss : 0.39501, Training Acc : 0.878, Run Time : 2.06
INFO:root:2019-05-12 00:49:19, Epoch : 1, Step : 2494, Training Loss : 0.53052, Training Acc : 0.817, Run Time : 1.17
INFO:root:2019-05-12 00:49:20, Epoch : 1, Step : 2495, Training Loss : 0.38597, Training Acc : 0.881, Run Time : 1.14
INFO:root:2019-05-12 00:49:21, Epoch : 1, Step : 2496, Training Loss : 0.70468, Training Acc : 0.558, Run Time : 1.28
INFO:root:2019-05-12 00:49:33, Epoch : 1, Step : 2497, Training Loss : 0.32223, Training Acc : 0.897, Run Time : 11.27
INFO:root:2019-05-12 00:49:34, Epoch : 1, Step : 2498, Training Loss : 0.44978, Training Acc : 0.831, Run Time : 1.89
INFO:root:2019-05-12 00:49:45, Epoch : 1, Step : 2499, Training Loss : 0.72901, Training Acc : 0.564, Run Time : 10.61
INFO:root:2019-05-12 00:49:47, Epoch : 1, Step : 2500, Training Loss : 1.16495, Training Acc : 0.425, Run Time : 1.88
INFO:root:2019-05-12 00:49:59, Epoch : 1, Step : 2501, Training Loss : 0.43123, Training Acc : 0.867, Run Time : 12.34
INFO:root:2019-05-12 00:50:01, Epoch : 1, Step : 2502, Training Loss : 0.49010, Training Acc : 0.694, Run Time : 1.33
INFO:root:2019-05-12 00:50:07, Epoch : 1, Step : 2503, Training Loss : 0.31400, Training Acc : 0.942, Run Time : 6.31
INFO:root:2019-05-12 00:50:08, Epoch : 1, Step : 2504, Training Loss : 0.85829, Training Acc : 0.292, Run Time : 1.56
INFO:root:2019-05-12 00:50:10, Epoch : 1, Step : 2505, Training Loss : 0.66709, Training Acc : 0.528, Run Time : 1.64
INFO:root:2019-05-12 00:50:19, Epoch : 1, Step : 2506, Training Loss : 0.85367, Training Acc : 0.442, Run Time : 8.95
INFO:root:2019-05-12 00:50:20, Epoch : 1, Step : 2507, Training Loss : 1.04983, Training Acc : 0.378, Run Time : 1.14
INFO:root:2019-05-12 00:50:23, Epoch : 1, Step : 2508, Training Loss : 0.64833, Training Acc : 0.636, Run Time : 2.60
INFO:root:2019-05-12 00:50:34, Epoch : 1, Step : 2509, Training Loss : 0.69344, Training Acc : 0.614, Run Time : 10.73
INFO:root:2019-05-12 00:50:35, Epoch : 1, Step : 2510, Training Loss : 0.51219, Training Acc : 0.767, Run Time : 1.14
INFO:root:2019-05-12 00:50:38, Epoch : 1, Step : 2511, Training Loss : 0.72154, Training Acc : 0.558, Run Time : 3.45
INFO:root:2019-05-12 00:50:44, Epoch : 1, Step : 2512, Training Loss : 0.54370, Training Acc : 0.744, Run Time : 5.53
INFO:root:2019-05-12 00:50:46, Epoch : 1, Step : 2513, Training Loss : 0.39248, Training Acc : 0.850, Run Time : 2.29
INFO:root:2019-05-12 00:50:56, Epoch : 1, Step : 2514, Training Loss : 0.67387, Training Acc : 0.572, Run Time : 10.26
INFO:root:2019-05-12 00:50:58, Epoch : 1, Step : 2515, Training Loss : 0.73170, Training Acc : 0.606, Run Time : 1.78
INFO:root:2019-05-12 00:51:07, Epoch : 1, Step : 2516, Training Loss : 0.88013, Training Acc : 0.467, Run Time : 9.39
INFO:root:2019-05-12 00:51:09, Epoch : 1, Step : 2517, Training Loss : 0.54576, Training Acc : 0.753, Run Time : 1.38
INFO:root:2019-05-12 00:51:10, Epoch : 1, Step : 2518, Training Loss : 0.43990, Training Acc : 0.753, Run Time : 1.68
INFO:root:2019-05-12 00:51:21, Epoch : 1, Step : 2519, Training Loss : 0.35984, Training Acc : 0.900, Run Time : 10.59
INFO:root:2019-05-12 00:51:22, Epoch : 1, Step : 2520, Training Loss : 0.30379, Training Acc : 0.894, Run Time : 1.12
INFO:root:2019-05-12 00:51:32, Epoch : 1, Step : 2521, Training Loss : 0.38364, Training Acc : 0.867, Run Time : 9.84
INFO:root:2019-05-12 00:51:34, Epoch : 1, Step : 2522, Training Loss : 0.40126, Training Acc : 0.703, Run Time : 1.80
INFO:root:2019-05-12 00:51:35, Epoch : 1, Step : 2523, Training Loss : 0.62591, Training Acc : 0.617, Run Time : 1.36
INFO:root:2019-05-12 00:51:44, Epoch : 1, Step : 2524, Training Loss : 0.50579, Training Acc : 0.822, Run Time : 8.92
INFO:root:2019-05-12 00:51:45, Epoch : 1, Step : 2525, Training Loss : 1.02924, Training Acc : 0.250, Run Time : 1.28
INFO:root:2019-05-12 00:51:49, Epoch : 1, Step : 2526, Training Loss : 0.42684, Training Acc : 0.894, Run Time : 3.65
INFO:root:2019-05-12 00:51:53, Epoch : 1, Step : 2527, Training Loss : 0.35465, Training Acc : 0.839, Run Time : 3.72
INFO:root:2019-05-12 00:51:55, Epoch : 1, Step : 2528, Training Loss : 0.31109, Training Acc : 0.942, Run Time : 2.19
INFO:root:2019-05-12 00:52:05, Epoch : 1, Step : 2529, Training Loss : 0.35268, Training Acc : 0.858, Run Time : 10.09
INFO:root:2019-05-12 00:52:07, Epoch : 1, Step : 2530, Training Loss : 0.41941, Training Acc : 0.789, Run Time : 1.76
INFO:root:2019-05-12 00:52:16, Epoch : 1, Step : 2531, Training Loss : 0.12286, Training Acc : 0.994, Run Time : 9.68
INFO:root:2019-05-12 00:52:18, Epoch : 1, Step : 2532, Training Loss : 0.34784, Training Acc : 0.919, Run Time : 1.29
INFO:root:2019-05-12 00:52:28, Epoch : 1, Step : 2533, Training Loss : 0.63803, Training Acc : 0.536, Run Time : 10.26
INFO:root:2019-05-12 00:52:29, Epoch : 1, Step : 2534, Training Loss : 0.36181, Training Acc : 0.908, Run Time : 1.33
INFO:root:2019-05-12 00:52:32, Epoch : 1, Step : 2535, Training Loss : 1.06527, Training Acc : 0.489, Run Time : 2.16
INFO:root:2019-05-12 00:52:41, Epoch : 1, Step : 2536, Training Loss : 0.28268, Training Acc : 0.961, Run Time : 9.52
INFO:root:2019-05-12 00:52:42, Epoch : 1, Step : 2537, Training Loss : 0.18259, Training Acc : 0.986, Run Time : 1.21
INFO:root:2019-05-12 00:52:43, Epoch : 1, Step : 2538, Training Loss : 0.39264, Training Acc : 0.964, Run Time : 1.16
INFO:root:2019-05-12 00:52:47, Epoch : 1, Step : 2539, Training Loss : 0.66816, Training Acc : 0.542, Run Time : 3.33
INFO:root:2019-05-12 00:52:54, Epoch : 1, Step : 2540, Training Loss : 0.14566, Training Acc : 1.000, Run Time : 7.77
INFO:root:2019-05-12 00:52:57, Epoch : 1, Step : 2541, Training Loss : 0.17690, Training Acc : 1.000, Run Time : 2.25
INFO:root:2019-05-12 00:53:04, Epoch : 1, Step : 2542, Training Loss : 0.19221, Training Acc : 0.992, Run Time : 7.51
INFO:root:2019-05-12 00:53:07, Epoch : 1, Step : 2543, Training Loss : 0.31003, Training Acc : 0.986, Run Time : 2.47
INFO:root:2019-05-12 00:53:17, Epoch : 1, Step : 2544, Training Loss : 0.46674, Training Acc : 0.900, Run Time : 10.45
INFO:root:2019-05-12 00:53:18, Epoch : 1, Step : 2545, Training Loss : 0.45558, Training Acc : 0.811, Run Time : 1.30
INFO:root:2019-05-12 00:53:20, Epoch : 1, Step : 2546, Training Loss : 0.79703, Training Acc : 0.517, Run Time : 1.89
INFO:root:2019-05-12 00:53:30, Epoch : 1, Step : 2547, Training Loss : 1.01488, Training Acc : 0.611, Run Time : 9.63
INFO:root:2019-05-12 00:53:31, Epoch : 1, Step : 2548, Training Loss : 0.53155, Training Acc : 0.686, Run Time : 1.12
INFO:root:2019-05-12 00:53:33, Epoch : 1, Step : 2549, Training Loss : 0.51832, Training Acc : 0.794, Run Time : 1.66
INFO:root:2019-05-12 00:53:43, Epoch : 1, Step : 2550, Training Loss : 0.50220, Training Acc : 0.722, Run Time : 10.65
INFO:root:2019-05-12 00:53:45, Epoch : 1, Step : 2551, Training Loss : 0.43164, Training Acc : 0.850, Run Time : 1.12
INFO:root:2019-05-12 00:53:47, Epoch : 1, Step : 2552, Training Loss : 0.39830, Training Acc : 0.842, Run Time : 2.78
INFO:root:2019-05-12 00:53:57, Epoch : 1, Step : 2553, Training Loss : 0.34365, Training Acc : 0.864, Run Time : 10.13
INFO:root:2019-05-12 00:53:59, Epoch : 1, Step : 2554, Training Loss : 0.60270, Training Acc : 0.667, Run Time : 1.57
INFO:root:2019-05-12 00:54:08, Epoch : 1, Step : 2555, Training Loss : 0.37580, Training Acc : 0.878, Run Time : 9.21
INFO:root:2019-05-12 00:54:09, Epoch : 1, Step : 2556, Training Loss : 0.35158, Training Acc : 0.897, Run Time : 1.21
INFO:root:2019-05-12 00:54:11, Epoch : 1, Step : 2557, Training Loss : 0.30417, Training Acc : 0.903, Run Time : 2.02
INFO:root:2019-05-12 00:54:22, Epoch : 1, Step : 2558, Training Loss : 0.61759, Training Acc : 0.628, Run Time : 10.17
INFO:root:2019-05-12 00:54:23, Epoch : 1, Step : 2559, Training Loss : 0.54433, Training Acc : 0.739, Run Time : 1.75
INFO:root:2019-05-12 00:54:31, Epoch : 1, Step : 2560, Training Loss : 0.78134, Training Acc : 0.486, Run Time : 8.10
INFO:root:2019-05-12 00:54:33, Epoch : 1, Step : 2561, Training Loss : 0.63169, Training Acc : 0.664, Run Time : 1.35
INFO:root:2019-05-12 00:54:34, Epoch : 1, Step : 2562, Training Loss : 1.03658, Training Acc : 0.372, Run Time : 1.38
INFO:root:2019-05-12 00:54:44, Epoch : 1, Step : 2563, Training Loss : 0.67144, Training Acc : 0.581, Run Time : 10.01
INFO:root:2019-05-12 00:54:46, Epoch : 1, Step : 2564, Training Loss : 0.60179, Training Acc : 0.694, Run Time : 1.83
INFO:root:2019-05-12 00:54:56, Epoch : 1, Step : 2565, Training Loss : 0.70332, Training Acc : 0.581, Run Time : 10.39
INFO:root:2019-05-12 00:54:58, Epoch : 1, Step : 2566, Training Loss : 0.79874, Training Acc : 0.447, Run Time : 1.17
INFO:root:2019-05-12 00:54:59, Epoch : 1, Step : 2567, Training Loss : 0.58374, Training Acc : 0.656, Run Time : 1.14
INFO:root:2019-05-12 00:55:10, Epoch : 1, Step : 2568, Training Loss : 0.94579, Training Acc : 0.458, Run Time : 11.23
INFO:root:2019-05-12 00:55:11, Epoch : 1, Step : 2569, Training Loss : 0.34779, Training Acc : 0.894, Run Time : 1.15
INFO:root:2019-05-12 00:55:21, Epoch : 1, Step : 2570, Training Loss : 0.37034, Training Acc : 0.900, Run Time : 10.21
INFO:root:2019-05-12 00:55:23, Epoch : 1, Step : 2571, Training Loss : 0.42159, Training Acc : 0.742, Run Time : 1.73
INFO:root:2019-05-12 00:55:31, Epoch : 1, Step : 2572, Training Loss : 0.53170, Training Acc : 0.797, Run Time : 8.38
INFO:root:2019-05-12 00:55:33, Epoch : 1, Step : 2573, Training Loss : 0.24585, Training Acc : 0.917, Run Time : 1.69
INFO:root:2019-05-12 00:55:43, Epoch : 1, Step : 2574, Training Loss : 0.48796, Training Acc : 0.836, Run Time : 9.57
INFO:root:2019-05-12 00:55:44, Epoch : 1, Step : 2575, Training Loss : 0.34933, Training Acc : 0.881, Run Time : 1.33
INFO:root:2019-05-12 00:55:55, Epoch : 1, Step : 2576, Training Loss : 0.38424, Training Acc : 0.872, Run Time : 10.46
INFO:root:2019-05-12 00:55:57, Epoch : 1, Step : 2577, Training Loss : 0.32746, Training Acc : 0.881, Run Time : 2.27
INFO:root:2019-05-12 00:56:06, Epoch : 1, Step : 2578, Training Loss : 0.31489, Training Acc : 0.856, Run Time : 8.77
INFO:root:2019-05-12 00:56:07, Epoch : 1, Step : 2579, Training Loss : 0.50263, Training Acc : 0.772, Run Time : 1.89
INFO:root:2019-05-12 00:56:19, Epoch : 1, Step : 2580, Training Loss : 0.59305, Training Acc : 0.656, Run Time : 11.57
INFO:root:2019-05-12 00:56:20, Epoch : 1, Step : 2581, Training Loss : 0.36557, Training Acc : 0.850, Run Time : 1.28
INFO:root:2019-05-12 00:56:23, Epoch : 1, Step : 2582, Training Loss : 0.53695, Training Acc : 0.742, Run Time : 2.72
INFO:root:2019-05-12 00:56:24, Epoch : 1, Step : 2583, Training Loss : 0.39669, Training Acc : 0.839, Run Time : 1.36
INFO:root:2019-05-12 00:56:26, Epoch : 1, Step : 2584, Training Loss : 0.32812, Training Acc : 0.908, Run Time : 2.08
INFO:root:2019-05-12 00:56:34, Epoch : 1, Step : 2585, Training Loss : 0.39831, Training Acc : 0.850, Run Time : 7.92
INFO:root:2019-05-12 00:56:35, Epoch : 1, Step : 2586, Training Loss : 0.35644, Training Acc : 0.864, Run Time : 1.13
INFO:root:2019-05-12 00:56:47, Epoch : 1, Step : 2587, Training Loss : 0.31014, Training Acc : 0.886, Run Time : 11.57
INFO:root:2019-05-12 00:56:48, Epoch : 1, Step : 2588, Training Loss : 0.31917, Training Acc : 0.894, Run Time : 1.38
INFO:root:2019-05-12 00:56:50, Epoch : 1, Step : 2589, Training Loss : 0.71359, Training Acc : 0.567, Run Time : 1.15
INFO:root:2019-05-12 00:57:00, Epoch : 1, Step : 2590, Training Loss : 0.37574, Training Acc : 0.881, Run Time : 10.76
INFO:root:2019-05-12 00:57:02, Epoch : 1, Step : 2591, Training Loss : 0.38119, Training Acc : 0.819, Run Time : 1.92
INFO:root:2019-05-12 00:57:04, Epoch : 1, Step : 2592, Training Loss : 0.44201, Training Acc : 0.756, Run Time : 2.04
INFO:root:2019-05-12 00:57:14, Epoch : 1, Step : 2593, Training Loss : 0.32865, Training Acc : 0.853, Run Time : 9.22
INFO:root:2019-05-12 00:57:15, Epoch : 1, Step : 2594, Training Loss : 0.45056, Training Acc : 0.783, Run Time : 1.20
INFO:root:2019-05-12 00:57:25, Epoch : 1, Step : 2595, Training Loss : 0.32680, Training Acc : 0.842, Run Time : 10.73
INFO:root:2019-05-12 00:57:27, Epoch : 1, Step : 2596, Training Loss : 0.72301, Training Acc : 0.564, Run Time : 1.83
INFO:root:2019-05-12 00:57:39, Epoch : 1, Step : 2597, Training Loss : 0.46257, Training Acc : 0.772, Run Time : 11.32
INFO:root:2019-05-12 00:57:40, Epoch : 1, Step : 2598, Training Loss : 0.31704, Training Acc : 0.906, Run Time : 1.55
INFO:root:2019-05-12 00:57:41, Epoch : 1, Step : 2599, Training Loss : 0.34366, Training Acc : 0.889, Run Time : 1.20
INFO:root:2019-05-12 00:57:47, Epoch : 1, Step : 2600, Training Loss : 0.41707, Training Acc : 0.844, Run Time : 6.10
INFO:root:2019-05-12 00:57:53, Epoch : 1, Step : 2601, Training Loss : 0.59087, Training Acc : 0.739, Run Time : 5.13
INFO:root:2019-05-12 00:57:55, Epoch : 1, Step : 2602, Training Loss : 0.56048, Training Acc : 0.728, Run Time : 2.14
INFO:root:2019-05-12 00:58:08, Epoch : 1, Step : 2603, Training Loss : 0.43309, Training Acc : 0.836, Run Time : 12.98
INFO:root:2019-05-12 00:58:09, Epoch : 1, Step : 2604, Training Loss : 0.47114, Training Acc : 0.742, Run Time : 1.36
INFO:root:2019-05-12 00:58:10, Epoch : 1, Step : 2605, Training Loss : 0.45314, Training Acc : 0.778, Run Time : 1.30
INFO:root:2019-05-12 00:58:12, Epoch : 1, Step : 2606, Training Loss : 0.73826, Training Acc : 0.603, Run Time : 1.95
INFO:root:2019-05-12 00:58:14, Epoch : 1, Step : 2607, Training Loss : 0.44965, Training Acc : 0.794, Run Time : 1.34
INFO:root:2019-05-12 00:58:24, Epoch : 1, Step : 2608, Training Loss : 0.43107, Training Acc : 0.803, Run Time : 10.46
INFO:root:2019-05-12 00:58:25, Epoch : 1, Step : 2609, Training Loss : 0.35538, Training Acc : 0.867, Run Time : 1.17
INFO:root:2019-05-12 00:58:32, Epoch : 1, Step : 2610, Training Loss : 0.48374, Training Acc : 0.742, Run Time : 6.61
INFO:root:2019-05-12 00:58:38, Epoch : 1, Step : 2611, Training Loss : 0.62636, Training Acc : 0.667, Run Time : 6.02
INFO:root:2019-05-12 00:58:39, Epoch : 1, Step : 2612, Training Loss : 0.47853, Training Acc : 0.767, Run Time : 1.13
INFO:root:2019-05-12 00:58:41, Epoch : 1, Step : 2613, Training Loss : 0.50502, Training Acc : 0.781, Run Time : 1.86
INFO:root:2019-05-12 00:58:52, Epoch : 1, Step : 2614, Training Loss : 0.72369, Training Acc : 0.583, Run Time : 10.81
INFO:root:2019-05-12 00:58:53, Epoch : 1, Step : 2615, Training Loss : 0.43068, Training Acc : 0.836, Run Time : 1.14
INFO:root:2019-05-12 00:59:03, Epoch : 1, Step : 2616, Training Loss : 0.42521, Training Acc : 0.844, Run Time : 9.67
INFO:root:2019-05-12 00:59:04, Epoch : 1, Step : 2617, Training Loss : 0.59311, Training Acc : 0.697, Run Time : 1.66
INFO:root:2019-05-12 00:59:13, Epoch : 1, Step : 2618, Training Loss : 0.51332, Training Acc : 0.744, Run Time : 9.11
INFO:root:2019-05-12 00:59:15, Epoch : 1, Step : 2619, Training Loss : 0.47413, Training Acc : 0.781, Run Time : 1.24
INFO:root:2019-05-12 00:59:17, Epoch : 1, Step : 2620, Training Loss : 0.42623, Training Acc : 0.789, Run Time : 2.37
INFO:root:2019-05-12 00:59:28, Epoch : 1, Step : 2621, Training Loss : 0.47338, Training Acc : 0.775, Run Time : 11.12
INFO:root:2019-05-12 00:59:29, Epoch : 1, Step : 2622, Training Loss : 0.37546, Training Acc : 0.833, Run Time : 1.35
INFO:root:2019-05-12 00:59:31, Epoch : 1, Step : 2623, Training Loss : 0.26141, Training Acc : 0.906, Run Time : 1.17
INFO:root:2019-05-12 00:59:42, Epoch : 1, Step : 2624, Training Loss : 0.49896, Training Acc : 0.767, Run Time : 11.55
INFO:root:2019-05-12 00:59:44, Epoch : 1, Step : 2625, Training Loss : 0.90547, Training Acc : 0.558, Run Time : 2.13
INFO:root:2019-05-12 00:59:55, Epoch : 1, Step : 2626, Training Loss : 0.90257, Training Acc : 0.522, Run Time : 10.54
INFO:root:2019-05-12 00:59:57, Epoch : 1, Step : 2627, Training Loss : 0.84231, Training Acc : 0.528, Run Time : 2.46
INFO:root:2019-05-12 01:00:08, Epoch : 1, Step : 2628, Training Loss : 0.36150, Training Acc : 0.881, Run Time : 10.30
INFO:root:2019-05-12 01:00:09, Epoch : 1, Step : 2629, Training Loss : 0.59347, Training Acc : 0.611, Run Time : 1.61
INFO:root:2019-05-12 01:00:17, Epoch : 1, Step : 2630, Training Loss : 0.72871, Training Acc : 0.578, Run Time : 8.09
INFO:root:2019-05-12 01:00:19, Epoch : 1, Step : 2631, Training Loss : 0.67390, Training Acc : 0.608, Run Time : 1.26
INFO:root:2019-05-12 01:00:23, Epoch : 1, Step : 2632, Training Loss : 0.31430, Training Acc : 0.839, Run Time : 4.17
INFO:root:2019-05-12 01:00:30, Epoch : 1, Step : 2633, Training Loss : 0.53934, Training Acc : 0.644, Run Time : 7.24
INFO:root:2019-05-12 01:00:32, Epoch : 1, Step : 2634, Training Loss : 0.56504, Training Acc : 0.639, Run Time : 1.80
INFO:root:2019-05-12 01:00:42, Epoch : 1, Step : 2635, Training Loss : 0.44420, Training Acc : 0.814, Run Time : 10.66
INFO:root:2019-05-12 01:00:44, Epoch : 1, Step : 2636, Training Loss : 0.38906, Training Acc : 0.889, Run Time : 1.20
INFO:root:2019-05-12 01:00:46, Epoch : 1, Step : 2637, Training Loss : 0.40898, Training Acc : 0.858, Run Time : 1.96
INFO:root:2019-05-12 01:00:54, Epoch : 1, Step : 2638, Training Loss : 0.56005, Training Acc : 0.656, Run Time : 8.93
INFO:root:2019-05-12 01:00:56, Epoch : 1, Step : 2639, Training Loss : 0.42961, Training Acc : 0.814, Run Time : 1.12
INFO:root:2019-05-12 01:01:07, Epoch : 1, Step : 2640, Training Loss : 0.27923, Training Acc : 0.942, Run Time : 11.00
INFO:root:2019-05-12 01:01:08, Epoch : 1, Step : 2641, Training Loss : 0.33048, Training Acc : 0.931, Run Time : 1.25
INFO:root:2019-05-12 01:01:09, Epoch : 1, Step : 2642, Training Loss : 0.52248, Training Acc : 0.686, Run Time : 1.63
INFO:root:2019-05-12 01:01:23, Epoch : 1, Step : 2643, Training Loss : 0.17748, Training Acc : 0.969, Run Time : 14.02
INFO:root:2019-05-12 01:01:25, Epoch : 1, Step : 2644, Training Loss : 0.31379, Training Acc : 0.947, Run Time : 1.80
INFO:root:2019-05-12 01:01:37, Epoch : 1, Step : 2645, Training Loss : 0.28366, Training Acc : 0.878, Run Time : 11.74
INFO:root:2019-05-12 01:01:38, Epoch : 1, Step : 2646, Training Loss : 0.21677, Training Acc : 0.953, Run Time : 1.13
INFO:root:2019-05-12 01:01:41, Epoch : 1, Step : 2647, Training Loss : 0.19788, Training Acc : 0.956, Run Time : 2.36
INFO:root:2019-05-12 01:01:51, Epoch : 1, Step : 2648, Training Loss : 0.34158, Training Acc : 0.836, Run Time : 10.49
INFO:root:2019-05-12 01:01:52, Epoch : 1, Step : 2649, Training Loss : 0.29209, Training Acc : 0.908, Run Time : 1.31
INFO:root:2019-05-12 01:01:57, Epoch : 1, Step : 2650, Training Loss : 0.40505, Training Acc : 0.892, Run Time : 5.01
INFO:root:2019-05-12 01:02:02, Epoch : 1, Step : 2651, Training Loss : 0.59467, Training Acc : 0.683, Run Time : 5.01
INFO:root:2019-05-12 01:02:07, Epoch : 1, Step : 2652, Training Loss : 0.76517, Training Acc : 0.514, Run Time : 4.48
INFO:root:2019-05-12 01:02:12, Epoch : 1, Step : 2653, Training Loss : 0.41679, Training Acc : 0.903, Run Time : 5.16
INFO:root:2019-05-12 01:02:13, Epoch : 1, Step : 2654, Training Loss : 0.63989, Training Acc : 0.628, Run Time : 1.41
INFO:root:2019-05-12 01:02:24, Epoch : 1, Step : 2655, Training Loss : 0.40759, Training Acc : 0.875, Run Time : 10.40
INFO:root:2019-05-12 01:02:25, Epoch : 1, Step : 2656, Training Loss : 0.53025, Training Acc : 0.806, Run Time : 1.22
INFO:root:2019-05-12 01:02:28, Epoch : 1, Step : 2657, Training Loss : 0.42305, Training Acc : 0.794, Run Time : 2.58
INFO:root:2019-05-12 01:02:37, Epoch : 1, Step : 2658, Training Loss : 0.35265, Training Acc : 0.842, Run Time : 9.90
INFO:root:2019-05-12 01:02:39, Epoch : 1, Step : 2659, Training Loss : 0.56354, Training Acc : 0.633, Run Time : 1.28
INFO:root:2019-05-12 01:02:42, Epoch : 1, Step : 2660, Training Loss : 0.33581, Training Acc : 0.847, Run Time : 2.99
INFO:root:2019-05-12 01:02:49, Epoch : 1, Step : 2661, Training Loss : 0.43945, Training Acc : 0.750, Run Time : 7.04
INFO:root:2019-05-12 01:02:50, Epoch : 1, Step : 2662, Training Loss : 0.39284, Training Acc : 0.775, Run Time : 1.66
INFO:root:2019-05-12 01:03:00, Epoch : 1, Step : 2663, Training Loss : 0.69935, Training Acc : 0.578, Run Time : 9.42
INFO:root:2019-05-12 01:03:01, Epoch : 1, Step : 2664, Training Loss : 0.44886, Training Acc : 0.806, Run Time : 1.23
INFO:root:2019-05-12 01:03:11, Epoch : 1, Step : 2665, Training Loss : 0.38289, Training Acc : 0.789, Run Time : 9.68
INFO:root:2019-05-12 01:03:13, Epoch : 1, Step : 2666, Training Loss : 0.35638, Training Acc : 0.794, Run Time : 1.97
INFO:root:2019-05-12 01:03:14, Epoch : 1, Step : 2667, Training Loss : 0.33633, Training Acc : 0.858, Run Time : 1.51
INFO:root:2019-05-12 01:03:23, Epoch : 1, Step : 2668, Training Loss : 0.34310, Training Acc : 0.822, Run Time : 8.73
INFO:root:2019-05-12 01:03:25, Epoch : 1, Step : 2669, Training Loss : 0.46563, Training Acc : 0.756, Run Time : 1.56
INFO:root:2019-05-12 01:03:34, Epoch : 1, Step : 2670, Training Loss : 0.78139, Training Acc : 0.489, Run Time : 9.84
INFO:root:2019-05-12 01:03:37, Epoch : 1, Step : 2671, Training Loss : 0.39418, Training Acc : 0.825, Run Time : 2.33
INFO:root:2019-05-12 01:03:47, Epoch : 1, Step : 2672, Training Loss : 0.38607, Training Acc : 0.844, Run Time : 10.51
INFO:root:2019-05-12 01:03:48, Epoch : 1, Step : 2673, Training Loss : 0.33974, Training Acc : 0.858, Run Time : 1.15
INFO:root:2019-05-12 01:03:51, Epoch : 1, Step : 2674, Training Loss : 0.42751, Training Acc : 0.756, Run Time : 2.92
INFO:root:2019-05-12 01:03:57, Epoch : 1, Step : 2675, Training Loss : 0.42062, Training Acc : 0.722, Run Time : 5.61
INFO:root:2019-05-12 01:03:59, Epoch : 1, Step : 2676, Training Loss : 0.48518, Training Acc : 0.678, Run Time : 1.72
INFO:root:2019-05-12 01:04:10, Epoch : 1, Step : 2677, Training Loss : 0.46994, Training Acc : 0.764, Run Time : 11.81
INFO:root:2019-05-12 01:04:12, Epoch : 1, Step : 2678, Training Loss : 0.59767, Training Acc : 0.722, Run Time : 1.14
INFO:root:2019-05-12 01:04:14, Epoch : 1, Step : 2679, Training Loss : 0.52857, Training Acc : 0.722, Run Time : 2.76
INFO:root:2019-05-12 01:04:24, Epoch : 1, Step : 2680, Training Loss : 0.60765, Training Acc : 0.661, Run Time : 9.47
INFO:root:2019-05-12 01:04:26, Epoch : 1, Step : 2681, Training Loss : 0.58942, Training Acc : 0.683, Run Time : 1.72
INFO:root:2019-05-12 01:04:36, Epoch : 1, Step : 2682, Training Loss : 0.53310, Training Acc : 0.689, Run Time : 10.67
INFO:root:2019-05-12 01:04:37, Epoch : 1, Step : 2683, Training Loss : 0.64986, Training Acc : 0.711, Run Time : 1.21
INFO:root:2019-05-12 01:04:39, Epoch : 1, Step : 2684, Training Loss : 0.56151, Training Acc : 0.728, Run Time : 1.60
INFO:root:2019-05-12 01:04:50, Epoch : 1, Step : 2685, Training Loss : 0.44159, Training Acc : 0.764, Run Time : 11.45
INFO:root:2019-05-12 01:04:52, Epoch : 1, Step : 2686, Training Loss : 0.40264, Training Acc : 0.811, Run Time : 1.15
INFO:root:2019-05-12 01:04:53, Epoch : 1, Step : 2687, Training Loss : 0.49393, Training Acc : 0.733, Run Time : 1.84
INFO:root:2019-05-12 01:04:56, Epoch : 1, Step : 2688, Training Loss : 0.41400, Training Acc : 0.783, Run Time : 3.00
INFO:root:2019-05-12 01:05:05, Epoch : 1, Step : 2689, Training Loss : 0.45587, Training Acc : 0.783, Run Time : 8.37
INFO:root:2019-05-12 01:05:07, Epoch : 1, Step : 2690, Training Loss : 0.48290, Training Acc : 0.744, Run Time : 2.20
INFO:root:2019-05-12 01:05:17, Epoch : 1, Step : 2691, Training Loss : 0.45413, Training Acc : 0.781, Run Time : 9.54
INFO:root:2019-05-12 01:05:19, Epoch : 1, Step : 2692, Training Loss : 0.68745, Training Acc : 0.592, Run Time : 2.24
INFO:root:2019-05-12 01:05:27, Epoch : 1, Step : 2693, Training Loss : 0.37561, Training Acc : 0.764, Run Time : 8.53
INFO:root:2019-05-12 01:05:28, Epoch : 1, Step : 2694, Training Loss : 0.41201, Training Acc : 0.753, Run Time : 1.14
INFO:root:2019-05-12 01:05:30, Epoch : 1, Step : 2695, Training Loss : 0.45329, Training Acc : 0.747, Run Time : 1.14
INFO:root:2019-05-12 01:05:33, Epoch : 1, Step : 2696, Training Loss : 0.40705, Training Acc : 0.817, Run Time : 3.74
INFO:root:2019-05-12 01:05:40, Epoch : 1, Step : 2697, Training Loss : 0.49014, Training Acc : 0.744, Run Time : 6.31
INFO:root:2019-05-12 01:05:41, Epoch : 1, Step : 2698, Training Loss : 0.45681, Training Acc : 0.778, Run Time : 1.75
INFO:root:2019-05-12 01:05:52, Epoch : 1, Step : 2699, Training Loss : 0.43362, Training Acc : 0.806, Run Time : 11.09
INFO:root:2019-05-12 01:05:54, Epoch : 1, Step : 2700, Training Loss : 0.37482, Training Acc : 0.839, Run Time : 1.72
INFO:root:2019-05-12 01:06:08, Epoch : 1, Step : 2701, Training Loss : 0.42373, Training Acc : 0.811, Run Time : 13.57
INFO:root:2019-05-12 01:06:10, Epoch : 1, Step : 2702, Training Loss : 0.39856, Training Acc : 0.789, Run Time : 1.87
INFO:root:2019-05-12 01:06:13, Epoch : 1, Step : 2703, Training Loss : 0.39655, Training Acc : 0.803, Run Time : 3.21
INFO:root:2019-05-12 01:06:22, Epoch : 1, Step : 2704, Training Loss : 0.40671, Training Acc : 0.819, Run Time : 8.70
INFO:root:2019-05-12 01:06:24, Epoch : 1, Step : 2705, Training Loss : 0.42885, Training Acc : 0.806, Run Time : 2.49
INFO:root:2019-05-12 01:06:35, Epoch : 1, Step : 2706, Training Loss : 0.45835, Training Acc : 0.786, Run Time : 10.97
INFO:root:2019-05-12 01:06:37, Epoch : 1, Step : 2707, Training Loss : 0.28441, Training Acc : 0.864, Run Time : 2.34
INFO:root:2019-05-12 01:06:48, Epoch : 1, Step : 2708, Training Loss : 0.31809, Training Acc : 0.831, Run Time : 10.83
INFO:root:2019-05-12 01:06:51, Epoch : 1, Step : 2709, Training Loss : 0.31938, Training Acc : 0.831, Run Time : 2.62
INFO:root:2019-05-12 01:07:01, Epoch : 1, Step : 2710, Training Loss : 0.36562, Training Acc : 0.808, Run Time : 10.01
INFO:root:2019-05-12 01:07:02, Epoch : 1, Step : 2711, Training Loss : 0.27550, Training Acc : 0.842, Run Time : 1.55
INFO:root:2019-05-12 01:07:15, Epoch : 1, Step : 2712, Training Loss : 0.29456, Training Acc : 0.853, Run Time : 12.25
INFO:root:2019-05-12 01:07:18, Epoch : 1, Step : 2713, Training Loss : 0.37407, Training Acc : 0.806, Run Time : 2.93
INFO:root:2019-05-12 01:07:27, Epoch : 1, Step : 2714, Training Loss : 0.48751, Training Acc : 0.719, Run Time : 9.37
INFO:root:2019-05-12 01:07:28, Epoch : 1, Step : 2715, Training Loss : 0.30295, Training Acc : 0.850, Run Time : 1.15
INFO:root:2019-05-12 01:07:31, Epoch : 1, Step : 2716, Training Loss : 0.32069, Training Acc : 0.856, Run Time : 3.13
INFO:root:2019-05-12 01:07:39, Epoch : 1, Step : 2717, Training Loss : 0.19385, Training Acc : 0.908, Run Time : 7.65
INFO:root:2019-05-12 01:07:40, Epoch : 1, Step : 2718, Training Loss : 0.19728, Training Acc : 0.919, Run Time : 1.46
INFO:root:2019-05-12 01:07:51, Epoch : 1, Step : 2719, Training Loss : 0.19672, Training Acc : 0.925, Run Time : 10.20
INFO:root:2019-05-12 01:07:52, Epoch : 1, Step : 2720, Training Loss : 0.37726, Training Acc : 0.886, Run Time : 1.71
INFO:root:2019-05-12 01:08:03, Epoch : 1, Step : 2721, Training Loss : 0.64267, Training Acc : 0.786, Run Time : 10.77
INFO:root:2019-05-12 01:08:04, Epoch : 1, Step : 2722, Training Loss : 0.76419, Training Acc : 0.769, Run Time : 1.37
INFO:root:2019-05-12 01:08:14, Epoch : 1, Step : 2723, Training Loss : 0.57514, Training Acc : 0.797, Run Time : 9.55
INFO:root:2019-05-12 01:08:15, Epoch : 1, Step : 2724, Training Loss : 0.39953, Training Acc : 0.811, Run Time : 1.22
INFO:root:2019-05-12 01:08:18, Epoch : 1, Step : 2725, Training Loss : 0.50322, Training Acc : 0.811, Run Time : 2.43
INFO:root:2019-05-12 01:08:26, Epoch : 1, Step : 2726, Training Loss : 0.23374, Training Acc : 0.897, Run Time : 8.86
INFO:root:2019-05-12 01:08:28, Epoch : 1, Step : 2727, Training Loss : 0.34675, Training Acc : 0.842, Run Time : 1.33
INFO:root:2019-05-12 01:08:29, Epoch : 1, Step : 2728, Training Loss : 0.40596, Training Acc : 0.811, Run Time : 1.71
INFO:root:2019-05-12 01:08:38, Epoch : 1, Step : 2729, Training Loss : 0.37723, Training Acc : 0.856, Run Time : 9.01
INFO:root:2019-05-12 01:08:40, Epoch : 1, Step : 2730, Training Loss : 0.36361, Training Acc : 0.856, Run Time : 1.55
INFO:root:2019-05-12 01:08:42, Epoch : 1, Step : 2731, Training Loss : 0.34023, Training Acc : 0.858, Run Time : 1.76
INFO:root:2019-05-12 01:08:43, Epoch : 1, Step : 2732, Training Loss : 0.25930, Training Acc : 0.892, Run Time : 1.58
INFO:root:2019-05-12 01:08:45, Epoch : 1, Step : 2733, Training Loss : 0.30408, Training Acc : 0.872, Run Time : 1.74
INFO:root:2019-05-12 01:08:51, Epoch : 1, Step : 2734, Training Loss : 0.24655, Training Acc : 0.900, Run Time : 5.53
INFO:root:2019-05-12 01:08:52, Epoch : 1, Step : 2735, Training Loss : 0.20070, Training Acc : 0.928, Run Time : 1.27
INFO:root:2019-05-12 01:08:53, Epoch : 1, Step : 2736, Training Loss : 0.26092, Training Acc : 0.869, Run Time : 1.53
INFO:root:2019-05-12 01:09:04, Epoch : 1, Step : 2737, Training Loss : 0.22669, Training Acc : 0.931, Run Time : 10.13
INFO:root:2019-05-12 01:09:05, Epoch : 1, Step : 2738, Training Loss : 0.25896, Training Acc : 0.867, Run Time : 1.15
INFO:root:2019-05-12 01:09:14, Epoch : 1, Step : 2739, Training Loss : 0.21158, Training Acc : 0.928, Run Time : 9.62
INFO:root:2019-05-12 01:09:16, Epoch : 1, Step : 2740, Training Loss : 0.17836, Training Acc : 0.944, Run Time : 1.26
INFO:root:2019-05-12 01:09:27, Epoch : 1, Step : 2741, Training Loss : 0.17307, Training Acc : 0.931, Run Time : 11.71
INFO:root:2019-05-12 01:09:28, Epoch : 1, Step : 2742, Training Loss : 0.25008, Training Acc : 0.875, Run Time : 1.14
INFO:root:2019-05-12 01:09:39, Epoch : 1, Step : 2743, Training Loss : 0.29206, Training Acc : 0.869, Run Time : 10.30
INFO:root:2019-05-12 01:09:40, Epoch : 1, Step : 2744, Training Loss : 0.96755, Training Acc : 0.783, Run Time : 1.23
INFO:root:2019-05-12 01:09:42, Epoch : 1, Step : 2745, Training Loss : 1.02035, Training Acc : 0.778, Run Time : 1.59
INFO:root:2019-05-12 01:09:55, Epoch : 1, Step : 2746, Training Loss : 1.06131, Training Acc : 0.700, Run Time : 13.42
INFO:root:2019-05-12 01:09:57, Epoch : 1, Step : 2747, Training Loss : 0.99051, Training Acc : 0.683, Run Time : 2.19
INFO:root:2019-05-12 01:10:07, Epoch : 1, Step : 2748, Training Loss : 0.81255, Training Acc : 0.681, Run Time : 9.96
INFO:root:2019-05-12 01:10:09, Epoch : 1, Step : 2749, Training Loss : 0.53448, Training Acc : 0.792, Run Time : 1.65
INFO:root:2019-05-12 01:10:11, Epoch : 1, Step : 2750, Training Loss : 0.54036, Training Acc : 0.764, Run Time : 2.63
INFO:root:2019-05-12 01:10:13, Epoch : 1, Step : 2751, Training Loss : 0.54516, Training Acc : 0.756, Run Time : 1.55
INFO:root:2019-05-12 01:10:19, Epoch : 1, Step : 2752, Training Loss : 0.45803, Training Acc : 0.783, Run Time : 6.41
INFO:root:2019-05-12 01:10:21, Epoch : 1, Step : 2753, Training Loss : 0.44704, Training Acc : 0.825, Run Time : 1.79
INFO:root:2019-05-12 01:10:31, Epoch : 1, Step : 2754, Training Loss : 0.39237, Training Acc : 0.825, Run Time : 9.77
INFO:root:2019-05-12 01:10:33, Epoch : 1, Step : 2755, Training Loss : 0.50731, Training Acc : 0.803, Run Time : 2.07
INFO:root:2019-05-12 01:10:43, Epoch : 1, Step : 2756, Training Loss : 0.44619, Training Acc : 0.825, Run Time : 9.86
INFO:root:2019-05-12 01:10:44, Epoch : 1, Step : 2757, Training Loss : 0.55304, Training Acc : 0.772, Run Time : 1.51
INFO:root:2019-05-12 01:10:47, Epoch : 1, Step : 2758, Training Loss : 0.33566, Training Acc : 0.900, Run Time : 2.32
INFO:root:2019-05-12 01:10:54, Epoch : 1, Step : 2759, Training Loss : 0.68867, Training Acc : 0.719, Run Time : 7.71
INFO:root:2019-05-12 01:10:56, Epoch : 1, Step : 2760, Training Loss : 0.34364, Training Acc : 0.853, Run Time : 1.23
INFO:root:2019-05-12 01:11:08, Epoch : 1, Step : 2761, Training Loss : 0.38704, Training Acc : 0.808, Run Time : 11.93
INFO:root:2019-05-12 01:11:09, Epoch : 1, Step : 2762, Training Loss : 0.41038, Training Acc : 0.803, Run Time : 1.14
INFO:root:2019-05-12 01:11:21, Epoch : 1, Step : 2763, Training Loss : 0.61185, Training Acc : 0.664, Run Time : 12.11
INFO:root:2019-05-12 01:11:22, Epoch : 1, Step : 2764, Training Loss : 0.60487, Training Acc : 0.744, Run Time : 1.12
INFO:root:2019-05-12 01:11:31, Epoch : 1, Step : 2765, Training Loss : 0.31292, Training Acc : 0.858, Run Time : 9.08
INFO:root:2019-05-12 01:11:33, Epoch : 1, Step : 2766, Training Loss : 0.46057, Training Acc : 0.783, Run Time : 2.19
INFO:root:2019-05-12 01:11:45, Epoch : 1, Step : 2767, Training Loss : 0.53458, Training Acc : 0.719, Run Time : 11.45
INFO:root:2019-05-12 01:11:48, Epoch : 1, Step : 2768, Training Loss : 0.59781, Training Acc : 0.756, Run Time : 2.83
INFO:root:2019-05-12 01:11:57, Epoch : 1, Step : 2769, Training Loss : 0.43607, Training Acc : 0.792, Run Time : 9.95
INFO:root:2019-05-12 01:11:59, Epoch : 1, Step : 2770, Training Loss : 0.58472, Training Acc : 0.692, Run Time : 1.16
INFO:root:2019-05-12 01:12:09, Epoch : 1, Step : 2771, Training Loss : 0.42918, Training Acc : 0.806, Run Time : 10.56
INFO:root:2019-05-12 01:12:10, Epoch : 1, Step : 2772, Training Loss : 0.55647, Training Acc : 0.675, Run Time : 1.16
INFO:root:2019-05-12 01:12:19, Epoch : 1, Step : 2773, Training Loss : 0.44556, Training Acc : 0.767, Run Time : 9.05
INFO:root:2019-05-12 01:12:21, Epoch : 1, Step : 2774, Training Loss : 0.56757, Training Acc : 0.706, Run Time : 1.20
INFO:root:2019-05-12 01:12:30, Epoch : 1, Step : 2775, Training Loss : 0.44030, Training Acc : 0.772, Run Time : 9.75
INFO:root:2019-05-12 01:12:32, Epoch : 1, Step : 2776, Training Loss : 0.27778, Training Acc : 0.867, Run Time : 1.37
INFO:root:2019-05-12 01:12:33, Epoch : 1, Step : 2777, Training Loss : 0.24068, Training Acc : 0.897, Run Time : 1.75
INFO:root:2019-05-12 01:12:35, Epoch : 1, Step : 2778, Training Loss : 0.24431, Training Acc : 0.908, Run Time : 2.00
INFO:root:2019-05-12 01:12:41, Epoch : 1, Step : 2779, Training Loss : 0.30371, Training Acc : 0.889, Run Time : 5.88
INFO:root:2019-05-12 01:12:43, Epoch : 1, Step : 2780, Training Loss : 0.40162, Training Acc : 0.794, Run Time : 1.18
INFO:root:2019-05-12 01:12:46, Epoch : 1, Step : 2781, Training Loss : 0.41206, Training Acc : 0.758, Run Time : 3.13
INFO:root:2019-05-12 01:12:52, Epoch : 1, Step : 2782, Training Loss : 0.39127, Training Acc : 0.778, Run Time : 6.47
INFO:root:2019-05-12 01:12:54, Epoch : 1, Step : 2783, Training Loss : 0.29806, Training Acc : 0.864, Run Time : 2.15
INFO:root:2019-05-12 01:13:04, Epoch : 1, Step : 2784, Training Loss : 0.36055, Training Acc : 0.797, Run Time : 10.15
INFO:root:2019-05-12 01:13:08, Epoch : 1, Step : 2785, Training Loss : 0.39404, Training Acc : 0.831, Run Time : 3.15
INFO:root:2019-05-12 01:13:17, Epoch : 1, Step : 2786, Training Loss : 0.35554, Training Acc : 0.875, Run Time : 9.58
INFO:root:2019-05-12 01:13:18, Epoch : 1, Step : 2787, Training Loss : 0.30738, Training Acc : 0.878, Run Time : 1.28
INFO:root:2019-05-12 01:13:20, Epoch : 1, Step : 2788, Training Loss : 0.39121, Training Acc : 0.822, Run Time : 1.30
INFO:root:2019-05-12 01:13:30, Epoch : 1, Step : 2789, Training Loss : 0.32905, Training Acc : 0.858, Run Time : 10.68
INFO:root:2019-05-12 01:13:32, Epoch : 1, Step : 2790, Training Loss : 0.37772, Training Acc : 0.872, Run Time : 1.17
INFO:root:2019-05-12 01:13:34, Epoch : 1, Step : 2791, Training Loss : 0.30476, Training Acc : 0.878, Run Time : 2.16
INFO:root:2019-05-12 01:13:45, Epoch : 1, Step : 2792, Training Loss : 0.28358, Training Acc : 0.897, Run Time : 10.79
INFO:root:2019-05-12 01:13:47, Epoch : 1, Step : 2793, Training Loss : 0.22875, Training Acc : 0.917, Run Time : 2.89
INFO:root:2019-05-12 01:14:01, Epoch : 1, Step : 2794, Training Loss : 0.28950, Training Acc : 0.894, Run Time : 14.02
INFO:root:2019-05-12 01:14:03, Epoch : 1, Step : 2795, Training Loss : 0.37953, Training Acc : 0.847, Run Time : 1.37
INFO:root:2019-05-12 01:14:17, Epoch : 1, Step : 2796, Training Loss : 0.29432, Training Acc : 0.889, Run Time : 14.00
INFO:root:2019-05-12 01:14:18, Epoch : 1, Step : 2797, Training Loss : 0.30027, Training Acc : 0.881, Run Time : 1.19
INFO:root:2019-05-12 01:14:19, Epoch : 1, Step : 2798, Training Loss : 0.37115, Training Acc : 0.847, Run Time : 1.17
INFO:root:2019-05-12 01:14:32, Epoch : 1, Step : 2799, Training Loss : 0.31001, Training Acc : 0.875, Run Time : 12.55
INFO:root:2019-05-12 01:14:34, Epoch : 1, Step : 2800, Training Loss : 0.38537, Training Acc : 0.836, Run Time : 1.98
INFO:root:2019-05-12 01:14:45, Epoch : 1, Step : 2801, Training Loss : 0.50610, Training Acc : 0.739, Run Time : 11.21
INFO:root:2019-05-12 01:14:47, Epoch : 1, Step : 2802, Training Loss : 0.58817, Training Acc : 0.708, Run Time : 2.02
INFO:root:2019-05-12 01:14:52, Epoch : 1, Step : 2803, Training Loss : 0.86714, Training Acc : 0.664, Run Time : 5.29
INFO:root:2019-05-12 01:14:55, Epoch : 1, Step : 2804, Training Loss : 0.76467, Training Acc : 0.689, Run Time : 2.72
INFO:root:2019-05-12 01:15:04, Epoch : 1, Step : 2805, Training Loss : 0.42937, Training Acc : 0.731, Run Time : 9.03
INFO:root:2019-05-12 01:15:07, Epoch : 1, Step : 2806, Training Loss : 0.42338, Training Acc : 0.822, Run Time : 2.55
INFO:root:2019-05-12 01:15:18, Epoch : 1, Step : 2807, Training Loss : 0.51800, Training Acc : 0.769, Run Time : 11.08
INFO:root:2019-05-12 01:15:19, Epoch : 1, Step : 2808, Training Loss : 0.58827, Training Acc : 0.781, Run Time : 1.14
INFO:root:2019-05-12 01:15:20, Epoch : 1, Step : 2809, Training Loss : 0.89919, Training Acc : 0.581, Run Time : 1.33
INFO:root:2019-05-12 01:15:23, Epoch : 1, Step : 2810, Training Loss : 0.92423, Training Acc : 0.556, Run Time : 2.73
INFO:root:2019-05-12 01:15:24, Epoch : 1, Step : 2811, Training Loss : 0.65143, Training Acc : 0.658, Run Time : 1.15
INFO:root:2019-05-12 01:15:32, Epoch : 1, Step : 2812, Training Loss : 0.58670, Training Acc : 0.753, Run Time : 8.30
INFO:root:2019-05-12 01:15:34, Epoch : 1, Step : 2813, Training Loss : 0.56376, Training Acc : 0.722, Run Time : 1.58
INFO:root:2019-05-12 01:15:36, Epoch : 1, Step : 2814, Training Loss : 0.42303, Training Acc : 0.792, Run Time : 2.45
INFO:root:2019-05-12 01:15:44, Epoch : 1, Step : 2815, Training Loss : 0.52574, Training Acc : 0.742, Run Time : 7.84
INFO:root:2019-05-12 01:15:47, Epoch : 1, Step : 2816, Training Loss : 0.55798, Training Acc : 0.733, Run Time : 2.62
INFO:root:2019-05-12 01:15:59, Epoch : 1, Step : 2817, Training Loss : 0.42468, Training Acc : 0.781, Run Time : 12.22
INFO:root:2019-05-12 01:16:00, Epoch : 1, Step : 2818, Training Loss : 0.54968, Training Acc : 0.678, Run Time : 1.14
INFO:root:2019-05-12 01:16:03, Epoch : 1, Step : 2819, Training Loss : 0.35233, Training Acc : 0.850, Run Time : 2.72
INFO:root:2019-05-12 01:16:12, Epoch : 1, Step : 2820, Training Loss : 0.37551, Training Acc : 0.828, Run Time : 9.07
INFO:root:2019-05-12 01:16:14, Epoch : 1, Step : 2821, Training Loss : 0.40972, Training Acc : 0.819, Run Time : 1.92
INFO:root:2019-05-12 01:16:26, Epoch : 1, Step : 2822, Training Loss : 0.45902, Training Acc : 0.814, Run Time : 12.47
INFO:root:2019-05-12 01:16:28, Epoch : 1, Step : 2823, Training Loss : 0.40184, Training Acc : 0.792, Run Time : 1.49
INFO:root:2019-05-12 01:16:29, Epoch : 1, Step : 2824, Training Loss : 0.38656, Training Acc : 0.853, Run Time : 1.56
INFO:root:2019-05-12 01:16:40, Epoch : 1, Step : 2825, Training Loss : 0.28948, Training Acc : 0.936, Run Time : 11.01
INFO:root:2019-05-12 01:16:44, Epoch : 1, Step : 2826, Training Loss : 0.43943, Training Acc : 0.828, Run Time : 3.99
INFO:root:2019-05-12 01:16:46, Epoch : 1, Step : 2827, Training Loss : 0.30745, Training Acc : 0.925, Run Time : 1.34
INFO:root:2019-05-12 01:16:54, Epoch : 1, Step : 2828, Training Loss : 0.44888, Training Acc : 0.842, Run Time : 8.40
INFO:root:2019-05-12 01:16:56, Epoch : 1, Step : 2829, Training Loss : 0.44910, Training Acc : 0.864, Run Time : 2.18
INFO:root:2019-05-12 01:17:06, Epoch : 1, Step : 2830, Training Loss : 0.50090, Training Acc : 0.819, Run Time : 9.42
INFO:root:2019-05-12 01:17:09, Epoch : 1, Step : 2831, Training Loss : 0.38300, Training Acc : 0.875, Run Time : 3.17
INFO:root:2019-05-12 01:17:17, Epoch : 1, Step : 2832, Training Loss : 0.40062, Training Acc : 0.886, Run Time : 8.57
INFO:root:2019-05-12 01:17:19, Epoch : 1, Step : 2833, Training Loss : 0.24054, Training Acc : 0.942, Run Time : 1.83
INFO:root:2019-05-12 01:17:28, Epoch : 1, Step : 2834, Training Loss : 0.29035, Training Acc : 0.911, Run Time : 9.02
INFO:root:2019-05-12 01:17:30, Epoch : 1, Step : 2835, Training Loss : 0.29423, Training Acc : 0.889, Run Time : 2.25
INFO:root:2019-05-12 01:17:42, Epoch : 1, Step : 2836, Training Loss : 0.32927, Training Acc : 0.844, Run Time : 11.58
INFO:root:2019-05-12 01:17:44, Epoch : 1, Step : 2837, Training Loss : 0.30765, Training Acc : 0.894, Run Time : 1.55
INFO:root:2019-05-12 01:17:46, Epoch : 1, Step : 2838, Training Loss : 0.37519, Training Acc : 0.833, Run Time : 2.60
INFO:root:2019-05-12 01:17:57, Epoch : 1, Step : 2839, Training Loss : 0.34190, Training Acc : 0.844, Run Time : 11.07
INFO:root:2019-05-12 01:17:58, Epoch : 1, Step : 2840, Training Loss : 0.40776, Training Acc : 0.786, Run Time : 1.12
INFO:root:2019-05-12 01:18:10, Epoch : 1, Step : 2841, Training Loss : 0.34399, Training Acc : 0.903, Run Time : 11.72
INFO:root:2019-05-12 01:18:11, Epoch : 1, Step : 2842, Training Loss : 0.28310, Training Acc : 0.919, Run Time : 1.29
INFO:root:2019-05-12 01:18:24, Epoch : 1, Step : 2843, Training Loss : 0.38119, Training Acc : 0.858, Run Time : 12.86
INFO:root:2019-05-12 01:18:25, Epoch : 1, Step : 2844, Training Loss : 0.29554, Training Acc : 0.928, Run Time : 1.14
INFO:root:2019-05-12 01:18:36, Epoch : 1, Step : 2845, Training Loss : 0.29725, Training Acc : 0.886, Run Time : 10.13
INFO:root:2019-05-12 01:18:38, Epoch : 1, Step : 2846, Training Loss : 0.47424, Training Acc : 0.836, Run Time : 2.11
INFO:root:2019-05-12 01:18:49, Epoch : 1, Step : 2847, Training Loss : 0.30590, Training Acc : 0.903, Run Time : 11.06
INFO:root:2019-05-12 01:18:50, Epoch : 1, Step : 2848, Training Loss : 0.40805, Training Acc : 0.853, Run Time : 1.12
INFO:root:2019-05-12 01:18:51, Epoch : 1, Step : 2849, Training Loss : 0.39459, Training Acc : 0.864, Run Time : 1.16
INFO:root:2019-05-12 01:18:53, Epoch : 1, Step : 2850, Training Loss : 0.19095, Training Acc : 0.936, Run Time : 2.41
INFO:root:2019-05-12 01:19:01, Epoch : 1, Step : 2851, Training Loss : 0.19771, Training Acc : 0.953, Run Time : 7.66
INFO:root:2019-05-12 01:19:03, Epoch : 1, Step : 2852, Training Loss : 0.34534, Training Acc : 0.928, Run Time : 2.13
INFO:root:2019-05-12 01:19:05, Epoch : 1, Step : 2853, Training Loss : 0.24943, Training Acc : 0.947, Run Time : 2.13
INFO:root:2019-05-12 01:19:13, Epoch : 1, Step : 2854, Training Loss : 0.24974, Training Acc : 0.919, Run Time : 7.49
INFO:root:2019-05-12 01:19:14, Epoch : 1, Step : 2855, Training Loss : 0.17336, Training Acc : 0.956, Run Time : 1.14
INFO:root:2019-05-12 01:19:17, Epoch : 1, Step : 2856, Training Loss : 0.20685, Training Acc : 0.961, Run Time : 2.56
INFO:root:2019-05-12 01:19:25, Epoch : 1, Step : 2857, Training Loss : 0.25826, Training Acc : 0.911, Run Time : 8.49
INFO:root:2019-05-12 01:19:27, Epoch : 1, Step : 2858, Training Loss : 0.18208, Training Acc : 0.947, Run Time : 2.06
INFO:root:2019-05-12 01:19:39, Epoch : 1, Step : 2859, Training Loss : 0.27994, Training Acc : 0.922, Run Time : 11.75
INFO:root:2019-05-12 01:19:40, Epoch : 1, Step : 2860, Training Loss : 0.23268, Training Acc : 0.944, Run Time : 1.13
INFO:root:2019-05-12 01:19:53, Epoch : 1, Step : 2861, Training Loss : 0.31630, Training Acc : 0.922, Run Time : 13.03
INFO:root:2019-05-12 01:19:55, Epoch : 1, Step : 2862, Training Loss : 0.23283, Training Acc : 0.925, Run Time : 1.67
INFO:root:2019-05-12 01:20:04, Epoch : 1, Step : 2863, Training Loss : 0.21639, Training Acc : 0.950, Run Time : 9.80
INFO:root:2019-05-12 01:20:06, Epoch : 1, Step : 2864, Training Loss : 0.30742, Training Acc : 0.947, Run Time : 1.21
INFO:root:2019-05-12 01:20:14, Epoch : 1, Step : 2865, Training Loss : 0.25775, Training Acc : 0.939, Run Time : 8.83
INFO:root:2019-05-12 01:20:16, Epoch : 1, Step : 2866, Training Loss : 0.31724, Training Acc : 0.897, Run Time : 1.12
INFO:root:2019-05-12 01:20:26, Epoch : 1, Step : 2867, Training Loss : 0.23707, Training Acc : 0.942, Run Time : 10.87
INFO:root:2019-05-12 01:20:28, Epoch : 1, Step : 2868, Training Loss : 0.26701, Training Acc : 0.903, Run Time : 1.51
INFO:root:2019-05-12 01:20:37, Epoch : 1, Step : 2869, Training Loss : 0.48319, Training Acc : 0.872, Run Time : 9.06
INFO:root:2019-05-12 01:20:39, Epoch : 1, Step : 2870, Training Loss : 0.27516, Training Acc : 0.919, Run Time : 2.17
INFO:root:2019-05-12 01:20:50, Epoch : 1, Step : 2871, Training Loss : 0.32794, Training Acc : 0.919, Run Time : 10.58
INFO:root:2019-05-12 01:20:51, Epoch : 1, Step : 2872, Training Loss : 0.37249, Training Acc : 0.897, Run Time : 1.15
INFO:root:2019-05-12 01:20:53, Epoch : 1, Step : 2873, Training Loss : 0.47584, Training Acc : 0.842, Run Time : 2.44
INFO:root:2019-05-12 01:21:04, Epoch : 1, Step : 2874, Training Loss : 0.54781, Training Acc : 0.761, Run Time : 10.39
INFO:root:2019-05-12 01:21:05, Epoch : 1, Step : 2875, Training Loss : 0.42522, Training Acc : 0.783, Run Time : 1.32
INFO:root:2019-05-12 01:21:07, Epoch : 1, Step : 2876, Training Loss : 0.41108, Training Acc : 0.892, Run Time : 2.37
INFO:root:2019-05-12 01:21:17, Epoch : 1, Step : 2877, Training Loss : 0.36842, Training Acc : 0.850, Run Time : 9.81
INFO:root:2019-05-12 01:21:20, Epoch : 1, Step : 2878, Training Loss : 0.54933, Training Acc : 0.739, Run Time : 2.27
INFO:root:2019-05-12 01:21:29, Epoch : 1, Step : 2879, Training Loss : 0.33569, Training Acc : 0.933, Run Time : 9.13
INFO:root:2019-05-12 01:21:30, Epoch : 1, Step : 2880, Training Loss : 0.25909, Training Acc : 0.944, Run Time : 1.33
INFO:root:2019-05-12 01:21:32, Epoch : 1, Step : 2881, Training Loss : 0.24884, Training Acc : 0.944, Run Time : 2.46
INFO:root:2019-05-12 01:21:41, Epoch : 1, Step : 2882, Training Loss : 0.26448, Training Acc : 0.953, Run Time : 8.69
INFO:root:2019-05-12 01:21:43, Epoch : 1, Step : 2883, Training Loss : 0.22832, Training Acc : 0.953, Run Time : 2.20
INFO:root:2019-05-12 01:21:53, Epoch : 1, Step : 2884, Training Loss : 0.35741, Training Acc : 0.869, Run Time : 9.29
INFO:root:2019-05-12 01:21:54, Epoch : 1, Step : 2885, Training Loss : 0.32437, Training Acc : 0.894, Run Time : 1.16
INFO:root:2019-05-12 01:21:55, Epoch : 1, Step : 2886, Training Loss : 0.33701, Training Acc : 0.897, Run Time : 1.40
INFO:root:2019-05-12 01:22:08, Epoch : 1, Step : 2887, Training Loss : 0.29475, Training Acc : 0.917, Run Time : 12.90
INFO:root:2019-05-12 01:22:10, Epoch : 1, Step : 2888, Training Loss : 0.41648, Training Acc : 0.836, Run Time : 2.05
INFO:root:2019-05-12 01:22:13, Epoch : 1, Step : 2889, Training Loss : 0.36280, Training Acc : 0.881, Run Time : 2.39
INFO:root:2019-05-12 01:22:20, Epoch : 1, Step : 2890, Training Loss : 0.34758, Training Acc : 0.917, Run Time : 7.08
INFO:root:2019-05-12 01:22:21, Epoch : 1, Step : 2891, Training Loss : 0.37165, Training Acc : 0.906, Run Time : 1.43
INFO:root:2019-05-12 01:22:23, Epoch : 1, Step : 2892, Training Loss : 0.26092, Training Acc : 0.942, Run Time : 2.24
INFO:root:2019-05-12 01:22:32, Epoch : 1, Step : 2893, Training Loss : 0.20921, Training Acc : 0.939, Run Time : 8.43
INFO:root:2019-05-12 01:22:33, Epoch : 1, Step : 2894, Training Loss : 0.21334, Training Acc : 0.958, Run Time : 1.23
INFO:root:2019-05-12 01:22:44, Epoch : 1, Step : 2895, Training Loss : 0.15559, Training Acc : 0.975, Run Time : 10.91
INFO:root:2019-05-12 01:22:45, Epoch : 1, Step : 2896, Training Loss : 0.31051, Training Acc : 0.956, Run Time : 1.31
INFO:root:2019-05-12 01:22:47, Epoch : 1, Step : 2897, Training Loss : 0.29416, Training Acc : 0.969, Run Time : 2.06
INFO:root:2019-05-12 01:22:57, Epoch : 1, Step : 2898, Training Loss : 0.20852, Training Acc : 0.961, Run Time : 10.08
INFO:root:2019-05-12 01:22:58, Epoch : 1, Step : 2899, Training Loss : 0.30877, Training Acc : 0.906, Run Time : 1.13
INFO:root:2019-05-12 01:23:01, Epoch : 1, Step : 2900, Training Loss : 0.36074, Training Acc : 0.825, Run Time : 2.34
INFO:root:2019-05-12 01:23:03, Epoch : 1, Step : 2901, Training Loss : 0.42224, Training Acc : 0.725, Run Time : 2.02
INFO:root:2019-05-12 01:23:04, Epoch : 1, Step : 2902, Training Loss : 0.28217, Training Acc : 0.889, Run Time : 1.24
INFO:root:2019-05-12 01:23:06, Epoch : 1, Step : 2903, Training Loss : 0.44314, Training Acc : 0.689, Run Time : 1.57
INFO:root:2019-05-12 01:23:17, Epoch : 1, Step : 2904, Training Loss : 0.36354, Training Acc : 0.739, Run Time : 11.06
INFO:root:2019-05-12 01:23:18, Epoch : 1, Step : 2905, Training Loss : 0.26607, Training Acc : 0.872, Run Time : 1.25
INFO:root:2019-05-12 01:23:19, Epoch : 1, Step : 2906, Training Loss : 0.28381, Training Acc : 0.867, Run Time : 1.53
INFO:root:2019-05-12 01:23:31, Epoch : 1, Step : 2907, Training Loss : 0.29088, Training Acc : 0.850, Run Time : 11.34
INFO:root:2019-05-12 01:23:32, Epoch : 1, Step : 2908, Training Loss : 0.34480, Training Acc : 0.861, Run Time : 1.62
INFO:root:2019-05-12 01:23:34, Epoch : 1, Step : 2909, Training Loss : 0.33901, Training Acc : 0.853, Run Time : 2.00
INFO:root:2019-05-12 01:23:41, Epoch : 1, Step : 2910, Training Loss : 0.39531, Training Acc : 0.856, Run Time : 6.79
INFO:root:2019-05-12 01:23:43, Epoch : 1, Step : 2911, Training Loss : 0.27767, Training Acc : 0.861, Run Time : 1.32
INFO:root:2019-05-12 01:23:45, Epoch : 1, Step : 2912, Training Loss : 0.34348, Training Acc : 0.844, Run Time : 2.11
INFO:root:2019-05-12 01:23:46, Epoch : 1, Step : 2913, Training Loss : 0.42591, Training Acc : 0.847, Run Time : 1.80
INFO:root:2019-05-12 01:23:54, Epoch : 1, Step : 2914, Training Loss : 0.34055, Training Acc : 0.847, Run Time : 7.23
INFO:root:2019-05-12 01:23:55, Epoch : 1, Step : 2915, Training Loss : 0.37421, Training Acc : 0.842, Run Time : 1.72
INFO:root:2019-05-12 01:24:09, Epoch : 1, Step : 2916, Training Loss : 0.37265, Training Acc : 0.847, Run Time : 13.42
INFO:root:2019-05-12 01:24:10, Epoch : 1, Step : 2917, Training Loss : 0.28680, Training Acc : 0.842, Run Time : 1.33
INFO:root:2019-05-12 01:24:12, Epoch : 1, Step : 2918, Training Loss : 0.29298, Training Acc : 0.847, Run Time : 1.73
INFO:root:2019-05-12 01:24:21, Epoch : 1, Step : 2919, Training Loss : 0.28961, Training Acc : 0.844, Run Time : 8.78
INFO:root:2019-05-12 01:24:22, Epoch : 1, Step : 2920, Training Loss : 0.29490, Training Acc : 0.842, Run Time : 1.64
INFO:root:2019-05-12 01:24:32, Epoch : 1, Step : 2921, Training Loss : 0.29372, Training Acc : 0.825, Run Time : 9.79
INFO:root:2019-05-12 01:24:34, Epoch : 1, Step : 2922, Training Loss : 0.33924, Training Acc : 0.806, Run Time : 1.71
INFO:root:2019-05-12 01:24:35, Epoch : 1, Step : 2923, Training Loss : 0.33443, Training Acc : 0.839, Run Time : 1.16
INFO:root:2019-05-12 01:24:37, Epoch : 1, Step : 2924, Training Loss : 0.34373, Training Acc : 0.792, Run Time : 1.67
INFO:root:2019-05-12 01:24:47, Epoch : 1, Step : 2925, Training Loss : 0.32363, Training Acc : 0.822, Run Time : 10.23
INFO:root:2019-05-12 01:24:48, Epoch : 1, Step : 2926, Training Loss : 0.31112, Training Acc : 0.817, Run Time : 1.12
INFO:root:2019-05-12 01:24:49, Epoch : 1, Step : 2927, Training Loss : 0.32369, Training Acc : 0.836, Run Time : 1.15
INFO:root:2019-05-12 01:24:50, Epoch : 1, Step : 2928, Training Loss : 0.33647, Training Acc : 0.814, Run Time : 1.26
INFO:root:2019-05-12 01:24:52, Epoch : 1, Step : 2929, Training Loss : 0.28641, Training Acc : 0.850, Run Time : 1.34
INFO:root:2019-05-12 01:24:55, Epoch : 1, Step : 2930, Training Loss : 0.28221, Training Acc : 0.847, Run Time : 3.44
INFO:root:2019-05-12 01:25:00, Epoch : 1, Step : 2931, Training Loss : 0.29864, Training Acc : 0.811, Run Time : 4.82
INFO:root:2019-05-12 01:25:02, Epoch : 1, Step : 2932, Training Loss : 0.28511, Training Acc : 0.853, Run Time : 1.86
INFO:root:2019-05-12 01:25:12, Epoch : 1, Step : 2933, Training Loss : 0.28979, Training Acc : 0.850, Run Time : 10.10
INFO:root:2019-05-12 01:25:14, Epoch : 1, Step : 2934, Training Loss : 0.28794, Training Acc : 0.856, Run Time : 2.12
INFO:root:2019-05-12 01:25:23, Epoch : 1, Step : 2935, Training Loss : 0.29754, Training Acc : 0.828, Run Time : 8.78
INFO:root:2019-05-12 01:25:24, Epoch : 1, Step : 2936, Training Loss : 0.28550, Training Acc : 0.850, Run Time : 1.34
INFO:root:2019-05-12 01:25:33, Epoch : 1, Step : 2937, Training Loss : 0.29747, Training Acc : 0.850, Run Time : 8.68
INFO:root:2019-05-12 01:25:34, Epoch : 1, Step : 2938, Training Loss : 0.28647, Training Acc : 0.856, Run Time : 1.30
INFO:root:2019-05-12 01:25:44, Epoch : 1, Step : 2939, Training Loss : 0.29757, Training Acc : 0.850, Run Time : 9.70
INFO:root:2019-05-12 01:25:45, Epoch : 1, Step : 2940, Training Loss : 0.33933, Training Acc : 0.819, Run Time : 1.27
INFO:root:2019-05-12 01:25:48, Epoch : 1, Step : 2941, Training Loss : 0.33859, Training Acc : 0.797, Run Time : 2.47
INFO:root:2019-05-12 01:25:57, Epoch : 1, Step : 2942, Training Loss : 0.30421, Training Acc : 0.828, Run Time : 9.02
INFO:root:2019-05-12 01:25:58, Epoch : 1, Step : 2943, Training Loss : 0.30365, Training Acc : 0.858, Run Time : 1.15
INFO:root:2019-05-12 01:26:00, Epoch : 1, Step : 2944, Training Loss : 0.35348, Training Acc : 0.808, Run Time : 1.70
INFO:root:2019-05-12 01:26:09, Epoch : 1, Step : 2945, Training Loss : 0.31892, Training Acc : 0.825, Run Time : 9.67
INFO:root:2019-05-12 01:26:10, Epoch : 1, Step : 2946, Training Loss : 0.30991, Training Acc : 0.839, Run Time : 1.12
INFO:root:2019-05-12 01:26:13, Epoch : 1, Step : 2947, Training Loss : 0.29910, Training Acc : 0.833, Run Time : 2.25
INFO:root:2019-05-12 01:26:23, Epoch : 1, Step : 2948, Training Loss : 0.29050, Training Acc : 0.833, Run Time : 10.40
INFO:root:2019-05-12 01:26:24, Epoch : 1, Step : 2949, Training Loss : 0.31031, Training Acc : 0.833, Run Time : 1.17
INFO:root:2019-05-12 01:26:34, Epoch : 1, Step : 2950, Training Loss : 0.31242, Training Acc : 0.828, Run Time : 9.83
INFO:root:2019-05-12 01:26:37, Epoch : 1, Step : 2951, Training Loss : 0.28679, Training Acc : 0.861, Run Time : 2.90
INFO:root:2019-05-12 01:26:44, Epoch : 1, Step : 2952, Training Loss : 0.29441, Training Acc : 0.856, Run Time : 6.77
INFO:root:2019-05-12 01:26:45, Epoch : 1, Step : 2953, Training Loss : 0.29166, Training Acc : 0.858, Run Time : 1.18
INFO:root:2019-05-12 01:26:47, Epoch : 1, Step : 2954, Training Loss : 0.34991, Training Acc : 0.850, Run Time : 2.36
INFO:root:2019-05-12 01:26:57, Epoch : 1, Step : 2955, Training Loss : 0.60439, Training Acc : 0.778, Run Time : 9.42
INFO:root:2019-05-12 01:26:58, Epoch : 1, Step : 2956, Training Loss : 0.93076, Training Acc : 0.628, Run Time : 1.60
INFO:root:2019-05-12 01:27:08, Epoch : 1, Step : 2957, Training Loss : 0.78415, Training Acc : 0.656, Run Time : 9.87
INFO:root:2019-05-12 01:27:10, Epoch : 1, Step : 2958, Training Loss : 0.73437, Training Acc : 0.661, Run Time : 1.76
INFO:root:2019-05-12 01:27:18, Epoch : 1, Step : 2959, Training Loss : 0.79624, Training Acc : 0.686, Run Time : 8.01
INFO:root:2019-05-12 01:27:19, Epoch : 1, Step : 2960, Training Loss : 0.79093, Training Acc : 0.603, Run Time : 1.26
INFO:root:2019-05-12 01:27:21, Epoch : 1, Step : 2961, Training Loss : 0.76546, Training Acc : 0.533, Run Time : 1.75
INFO:root:2019-05-12 01:27:30, Epoch : 1, Step : 2962, Training Loss : 0.84075, Training Acc : 0.550, Run Time : 9.39
INFO:root:2019-05-12 01:27:31, Epoch : 1, Step : 2963, Training Loss : 0.67859, Training Acc : 0.603, Run Time : 1.19
INFO:root:2019-05-12 01:27:41, Epoch : 1, Step : 2964, Training Loss : 0.71401, Training Acc : 0.647, Run Time : 9.34
INFO:root:2019-05-12 01:27:43, Epoch : 1, Step : 2965, Training Loss : 0.73304, Training Acc : 0.589, Run Time : 1.74
INFO:root:2019-05-12 01:27:44, Epoch : 1, Step : 2966, Training Loss : 0.74211, Training Acc : 0.592, Run Time : 1.76
INFO:root:2019-05-12 01:27:53, Epoch : 1, Step : 2967, Training Loss : 0.68084, Training Acc : 0.642, Run Time : 9.10
INFO:root:2019-05-12 01:27:55, Epoch : 1, Step : 2968, Training Loss : 0.58270, Training Acc : 0.669, Run Time : 1.40
INFO:root:2019-05-12 01:27:57, Epoch : 1, Step : 2969, Training Loss : 0.56065, Training Acc : 0.703, Run Time : 2.12
INFO:root:2019-05-12 01:28:05, Epoch : 1, Step : 2970, Training Loss : 0.46627, Training Acc : 0.731, Run Time : 8.46
INFO:root:2019-05-12 01:28:07, Epoch : 1, Step : 2971, Training Loss : 0.67287, Training Acc : 0.644, Run Time : 1.17
INFO:root:2019-05-12 01:28:08, Epoch : 1, Step : 2972, Training Loss : 0.50035, Training Acc : 0.758, Run Time : 1.63
INFO:root:2019-05-12 01:28:18, Epoch : 1, Step : 2973, Training Loss : 0.42789, Training Acc : 0.822, Run Time : 9.50
INFO:root:2019-05-12 01:28:19, Epoch : 1, Step : 2974, Training Loss : 0.33913, Training Acc : 0.856, Run Time : 1.70
INFO:root:2019-05-12 01:28:30, Epoch : 1, Step : 2975, Training Loss : 0.36023, Training Acc : 0.847, Run Time : 10.72
INFO:root:2019-05-12 01:28:31, Epoch : 1, Step : 2976, Training Loss : 0.31036, Training Acc : 0.861, Run Time : 1.32
INFO:root:2019-05-12 01:28:42, Epoch : 1, Step : 2977, Training Loss : 0.37158, Training Acc : 0.864, Run Time : 10.12
INFO:root:2019-05-12 01:28:43, Epoch : 1, Step : 2978, Training Loss : 0.37506, Training Acc : 0.858, Run Time : 1.48
INFO:root:2019-05-12 01:28:45, Epoch : 1, Step : 2979, Training Loss : 0.40074, Training Acc : 0.864, Run Time : 1.68
INFO:root:2019-05-12 01:28:54, Epoch : 1, Step : 2980, Training Loss : 0.35261, Training Acc : 0.869, Run Time : 9.17
INFO:root:2019-05-12 01:28:56, Epoch : 1, Step : 2981, Training Loss : 0.39749, Training Acc : 0.858, Run Time : 1.81
INFO:root:2019-05-12 01:29:06, Epoch : 1, Step : 2982, Training Loss : 0.39344, Training Acc : 0.864, Run Time : 10.41
INFO:root:2019-05-12 01:29:07, Epoch : 1, Step : 2983, Training Loss : 0.39333, Training Acc : 0.869, Run Time : 1.16
INFO:root:2019-05-12 01:29:08, Epoch : 1, Step : 2984, Training Loss : 0.35235, Training Acc : 0.875, Run Time : 1.15
INFO:root:2019-05-12 01:29:10, Epoch : 1, Step : 2985, Training Loss : 0.40817, Training Acc : 0.856, Run Time : 1.17
INFO:root:2019-05-12 01:29:17, Epoch : 1, Step : 2986, Training Loss : 0.38128, Training Acc : 0.858, Run Time : 7.90
INFO:root:2019-05-12 01:29:19, Epoch : 1, Step : 2987, Training Loss : 0.39276, Training Acc : 0.856, Run Time : 1.56
INFO:root:2019-05-12 01:29:28, Epoch : 1, Step : 2988, Training Loss : 0.31950, Training Acc : 0.856, Run Time : 8.60
INFO:root:2019-05-12 01:29:30, Epoch : 1, Step : 2989, Training Loss : 0.25750, Training Acc : 0.867, Run Time : 2.03
INFO:root:2019-05-12 01:29:39, Epoch : 1, Step : 2990, Training Loss : 0.25481, Training Acc : 0.872, Run Time : 9.12
INFO:root:2019-05-12 01:29:41, Epoch : 1, Step : 2991, Training Loss : 0.23212, Training Acc : 0.881, Run Time : 1.91
INFO:root:2019-05-12 01:29:52, Epoch : 1, Step : 2992, Training Loss : 0.18726, Training Acc : 0.906, Run Time : 11.79
INFO:root:2019-05-12 01:29:54, Epoch : 1, Step : 2993, Training Loss : 0.26710, Training Acc : 0.908, Run Time : 1.49
INFO:root:2019-05-12 01:29:55, Epoch : 1, Step : 2994, Training Loss : 0.26041, Training Acc : 0.917, Run Time : 1.12
INFO:root:2019-05-12 01:29:57, Epoch : 1, Step : 2995, Training Loss : 0.26743, Training Acc : 0.917, Run Time : 2.26
INFO:root:2019-05-12 01:30:08, Epoch : 1, Step : 2996, Training Loss : 0.25179, Training Acc : 0.861, Run Time : 10.20
INFO:root:2019-05-12 01:30:09, Epoch : 1, Step : 2997, Training Loss : 0.23763, Training Acc : 0.939, Run Time : 1.45
INFO:root:2019-05-12 01:30:11, Epoch : 1, Step : 2998, Training Loss : 0.27944, Training Acc : 0.894, Run Time : 2.38
INFO:root:2019-05-12 01:30:21, Epoch : 1, Step : 2999, Training Loss : 0.24042, Training Acc : 0.914, Run Time : 9.21
INFO:root:2019-05-12 01:30:22, Epoch : 1, Step : 3000, Training Loss : 0.24457, Training Acc : 0.919, Run Time : 1.13
INFO:root:2019-05-12 01:30:32, Epoch : 1, Step : 3001, Training Loss : 0.85219, Training Acc : 0.650, Run Time : 9.83
INFO:root:2019-05-12 01:30:33, Epoch : 1, Step : 3002, Training Loss : 0.86803, Training Acc : 0.633, Run Time : 1.66
INFO:root:2019-05-12 01:30:35, Epoch : 1, Step : 3003, Training Loss : 0.50328, Training Acc : 0.792, Run Time : 1.40
INFO:root:2019-05-12 01:30:43, Epoch : 1, Step : 3004, Training Loss : 0.48333, Training Acc : 0.842, Run Time : 8.41
INFO:root:2019-05-12 01:30:44, Epoch : 1, Step : 3005, Training Loss : 0.41190, Training Acc : 0.825, Run Time : 1.29
INFO:root:2019-05-12 01:30:46, Epoch : 1, Step : 3006, Training Loss : 0.31542, Training Acc : 0.867, Run Time : 1.85
INFO:root:2019-05-12 01:30:54, Epoch : 1, Step : 3007, Training Loss : 0.37869, Training Acc : 0.847, Run Time : 8.18
INFO:root:2019-05-12 01:30:57, Epoch : 1, Step : 3008, Training Loss : 0.31931, Training Acc : 0.864, Run Time : 2.79
INFO:root:2019-05-12 01:31:08, Epoch : 1, Step : 3009, Training Loss : 0.38685, Training Acc : 0.867, Run Time : 11.35
INFO:root:2019-05-12 01:31:10, Epoch : 1, Step : 3010, Training Loss : 0.27457, Training Acc : 0.881, Run Time : 1.70
INFO:root:2019-05-12 01:31:21, Epoch : 1, Step : 3011, Training Loss : 0.30017, Training Acc : 0.889, Run Time : 11.24
INFO:root:2019-05-12 01:31:23, Epoch : 1, Step : 3012, Training Loss : 0.54460, Training Acc : 0.719, Run Time : 1.17
INFO:root:2019-05-12 01:31:25, Epoch : 1, Step : 3013, Training Loss : 0.26803, Training Acc : 0.878, Run Time : 2.29
INFO:root:2019-05-12 01:31:34, Epoch : 1, Step : 3014, Training Loss : 0.53856, Training Acc : 0.722, Run Time : 8.98
INFO:root:2019-05-12 01:31:35, Epoch : 1, Step : 3015, Training Loss : 0.50574, Training Acc : 0.742, Run Time : 1.44
INFO:root:2019-05-12 01:31:46, Epoch : 1, Step : 3016, Training Loss : 0.34367, Training Acc : 0.892, Run Time : 10.71
INFO:root:2019-05-12 01:31:47, Epoch : 1, Step : 3017, Training Loss : 0.21987, Training Acc : 0.947, Run Time : 1.15
INFO:root:2019-05-12 01:31:58, Epoch : 1, Step : 3018, Training Loss : 0.37044, Training Acc : 0.833, Run Time : 10.95
INFO:root:2019-05-12 01:31:59, Epoch : 1, Step : 3019, Training Loss : 0.24778, Training Acc : 0.936, Run Time : 1.12
INFO:root:2019-05-12 01:32:01, Epoch : 1, Step : 3020, Training Loss : 0.36654, Training Acc : 0.858, Run Time : 1.53
INFO:root:2019-05-12 01:32:13, Epoch : 1, Step : 3021, Training Loss : 0.39115, Training Acc : 0.881, Run Time : 12.12
INFO:root:2019-05-12 01:32:14, Epoch : 1, Step : 3022, Training Loss : 0.30572, Training Acc : 0.906, Run Time : 1.21
INFO:root:2019-05-12 01:32:17, Epoch : 1, Step : 3023, Training Loss : 0.34032, Training Acc : 0.919, Run Time : 2.67
INFO:root:2019-05-12 01:32:27, Epoch : 1, Step : 3024, Training Loss : 0.21199, Training Acc : 0.950, Run Time : 10.49
INFO:root:2019-05-12 01:32:29, Epoch : 1, Step : 3025, Training Loss : 0.24884, Training Acc : 0.942, Run Time : 1.60
INFO:root:2019-05-12 01:32:39, Epoch : 1, Step : 3026, Training Loss : 0.18716, Training Acc : 0.942, Run Time : 10.35
INFO:root:2019-05-12 01:32:40, Epoch : 1, Step : 3027, Training Loss : 0.19278, Training Acc : 0.947, Run Time : 1.14
INFO:root:2019-05-12 01:32:50, Epoch : 1, Step : 3028, Training Loss : 0.24472, Training Acc : 0.936, Run Time : 9.90
INFO:root:2019-05-12 01:32:52, Epoch : 1, Step : 3029, Training Loss : 0.24546, Training Acc : 0.903, Run Time : 1.67
INFO:root:2019-05-12 01:33:01, Epoch : 1, Step : 3030, Training Loss : 0.29240, Training Acc : 0.881, Run Time : 9.07
INFO:root:2019-05-12 01:33:02, Epoch : 1, Step : 3031, Training Loss : 0.38046, Training Acc : 0.853, Run Time : 1.39
INFO:root:2019-05-12 01:33:04, Epoch : 1, Step : 3032, Training Loss : 0.32974, Training Acc : 0.894, Run Time : 1.78
INFO:root:2019-05-12 01:33:13, Epoch : 1, Step : 3033, Training Loss : 0.27050, Training Acc : 0.944, Run Time : 8.63
INFO:root:2019-05-12 01:33:14, Epoch : 1, Step : 3034, Training Loss : 0.59259, Training Acc : 0.711, Run Time : 1.67
INFO:root:2019-05-12 01:33:23, Epoch : 1, Step : 3035, Training Loss : 0.16590, Training Acc : 0.933, Run Time : 8.78
INFO:root:2019-05-12 01:33:25, Epoch : 1, Step : 3036, Training Loss : 0.21051, Training Acc : 0.906, Run Time : 1.68
INFO:root:2019-05-12 01:33:34, Epoch : 1, Step : 3037, Training Loss : 0.33182, Training Acc : 0.842, Run Time : 8.80
INFO:root:2019-05-12 01:33:35, Epoch : 1, Step : 3038, Training Loss : 0.30176, Training Acc : 0.819, Run Time : 1.24
INFO:root:2019-05-12 01:33:38, Epoch : 1, Step : 3039, Training Loss : 0.27308, Training Acc : 0.883, Run Time : 2.97
INFO:root:2019-05-12 01:33:48, Epoch : 1, Step : 3040, Training Loss : 0.26464, Training Acc : 0.900, Run Time : 10.39
INFO:root:2019-05-12 01:33:50, Epoch : 1, Step : 3041, Training Loss : 0.45193, Training Acc : 0.708, Run Time : 1.85
INFO:root:2019-05-12 01:34:02, Epoch : 1, Step : 3042, Training Loss : 0.22414, Training Acc : 0.925, Run Time : 11.64
INFO:root:2019-05-12 01:34:03, Epoch : 1, Step : 3043, Training Loss : 0.38360, Training Acc : 0.797, Run Time : 1.19
INFO:root:2019-05-12 01:34:12, Epoch : 1, Step : 3044, Training Loss : 0.21303, Training Acc : 0.892, Run Time : 8.72
INFO:root:2019-05-12 01:34:14, Epoch : 1, Step : 3045, Training Loss : 0.23603, Training Acc : 0.897, Run Time : 1.89
INFO:root:2019-05-12 01:34:21, Epoch : 1, Step : 3046, Training Loss : 0.17644, Training Acc : 0.942, Run Time : 7.77
INFO:root:2019-05-12 01:34:23, Epoch : 1, Step : 3047, Training Loss : 0.19512, Training Acc : 0.931, Run Time : 1.70
INFO:root:2019-05-12 01:34:34, Epoch : 1, Step : 3048, Training Loss : 0.28628, Training Acc : 0.914, Run Time : 11.13
INFO:root:2019-05-12 01:34:36, Epoch : 1, Step : 3049, Training Loss : 0.27921, Training Acc : 0.886, Run Time : 1.34
INFO:root:2019-05-12 01:34:48, Epoch : 1, Step : 3050, Training Loss : 0.16830, Training Acc : 0.942, Run Time : 12.04
INFO:root:2019-05-12 01:34:50, Epoch : 1, Step : 3051, Training Loss : 0.39666, Training Acc : 0.836, Run Time : 2.93
INFO:root:2019-05-12 01:35:01, Epoch : 1, Step : 3052, Training Loss : 0.21144, Training Acc : 0.942, Run Time : 10.03
INFO:root:2019-05-12 01:35:02, Epoch : 1, Step : 3053, Training Loss : 0.34319, Training Acc : 0.858, Run Time : 1.13
INFO:root:2019-05-12 01:35:04, Epoch : 1, Step : 3054, Training Loss : 0.90835, Training Acc : 0.508, Run Time : 2.20
INFO:root:2019-05-12 01:35:12, Epoch : 1, Step : 3055, Training Loss : 0.22733, Training Acc : 0.942, Run Time : 7.98
INFO:root:2019-05-12 01:35:14, Epoch : 1, Step : 3056, Training Loss : 0.14281, Training Acc : 0.956, Run Time : 1.75
INFO:root:2019-05-12 01:35:23, Epoch : 1, Step : 3057, Training Loss : 0.12902, Training Acc : 0.956, Run Time : 9.64
INFO:root:2019-05-12 01:35:24, Epoch : 1, Step : 3058, Training Loss : 0.28434, Training Acc : 0.892, Run Time : 1.23
INFO:root:2019-05-12 01:35:27, Epoch : 1, Step : 3059, Training Loss : 0.46465, Training Acc : 0.678, Run Time : 2.95
INFO:root:2019-05-12 01:35:37, Epoch : 1, Step : 3060, Training Loss : 0.25107, Training Acc : 0.900, Run Time : 9.78
INFO:root:2019-05-12 01:35:39, Epoch : 1, Step : 3061, Training Loss : 0.34960, Training Acc : 0.861, Run Time : 1.32
INFO:root:2019-05-12 01:35:41, Epoch : 1, Step : 3062, Training Loss : 0.18011, Training Acc : 0.917, Run Time : 2.09
INFO:root:2019-05-12 01:35:49, Epoch : 1, Step : 3063, Training Loss : 0.30288, Training Acc : 0.872, Run Time : 8.78
INFO:root:2019-05-12 01:35:51, Epoch : 1, Step : 3064, Training Loss : 0.19022, Training Acc : 0.933, Run Time : 1.12
INFO:root:2019-05-12 01:35:52, Epoch : 1, Step : 3065, Training Loss : 0.23370, Training Acc : 0.911, Run Time : 1.15
INFO:root:2019-05-12 01:35:53, Epoch : 1, Step : 3066, Training Loss : 0.27614, Training Acc : 0.908, Run Time : 1.82
INFO:root:2019-05-12 01:36:03, Epoch : 1, Step : 3067, Training Loss : 0.17856, Training Acc : 0.956, Run Time : 9.39
INFO:root:2019-05-12 01:36:04, Epoch : 1, Step : 3068, Training Loss : 0.14954, Training Acc : 0.981, Run Time : 1.56
INFO:root:2019-05-12 01:36:13, Epoch : 1, Step : 3069, Training Loss : 0.21449, Training Acc : 0.936, Run Time : 8.87
INFO:root:2019-05-12 01:36:15, Epoch : 1, Step : 3070, Training Loss : 0.32884, Training Acc : 0.872, Run Time : 1.24
INFO:root:2019-05-12 01:36:25, Epoch : 1, Step : 3071, Training Loss : 0.22787, Training Acc : 0.931, Run Time : 10.31
INFO:root:2019-05-12 01:36:27, Epoch : 1, Step : 3072, Training Loss : 0.14976, Training Acc : 0.961, Run Time : 2.07
INFO:root:2019-05-12 01:36:35, Epoch : 1, Step : 3073, Training Loss : 0.17963, Training Acc : 0.958, Run Time : 8.31
INFO:root:2019-05-12 01:36:37, Epoch : 1, Step : 3074, Training Loss : 0.21308, Training Acc : 0.925, Run Time : 1.97
INFO:root:2019-05-12 01:36:43, Epoch : 1, Step : 3075, Training Loss : 0.51386, Training Acc : 0.833, Run Time : 5.87
INFO:root:2019-05-12 01:36:45, Epoch : 1, Step : 3076, Training Loss : 0.29973, Training Acc : 0.919, Run Time : 2.00
INFO:root:2019-05-12 01:36:54, Epoch : 1, Step : 3077, Training Loss : 0.27116, Training Acc : 0.911, Run Time : 9.07
INFO:root:2019-05-12 01:36:55, Epoch : 1, Step : 3078, Training Loss : 0.18205, Training Acc : 0.947, Run Time : 1.13
INFO:root:2019-05-12 01:36:59, Epoch : 1, Step : 3079, Training Loss : 0.17533, Training Acc : 0.947, Run Time : 3.64
INFO:root:2019-05-12 01:37:03, Epoch : 1, Step : 3080, Training Loss : 0.24113, Training Acc : 0.928, Run Time : 4.34
INFO:root:2019-05-12 01:37:05, Epoch : 1, Step : 3081, Training Loss : 0.20107, Training Acc : 0.944, Run Time : 1.56
INFO:root:2019-05-12 01:37:11, Epoch : 1, Step : 3082, Training Loss : 0.32229, Training Acc : 0.903, Run Time : 6.15
INFO:root:2019-05-12 01:37:12, Epoch : 1, Step : 3083, Training Loss : 0.23598, Training Acc : 0.914, Run Time : 1.14
INFO:root:2019-05-12 01:37:22, Epoch : 1, Step : 3084, Training Loss : 0.20040, Training Acc : 0.928, Run Time : 10.08
INFO:root:2019-05-12 01:37:23, Epoch : 1, Step : 3085, Training Loss : 0.16964, Training Acc : 0.956, Run Time : 1.22
INFO:root:2019-05-12 01:37:25, Epoch : 1, Step : 3086, Training Loss : 0.17628, Training Acc : 0.969, Run Time : 1.58
INFO:root:2019-05-12 01:37:35, Epoch : 1, Step : 3087, Training Loss : 0.13343, Training Acc : 0.967, Run Time : 10.42
INFO:root:2019-05-12 01:37:37, Epoch : 1, Step : 3088, Training Loss : 0.25013, Training Acc : 0.931, Run Time : 1.18
INFO:root:2019-05-12 01:37:42, Epoch : 1, Step : 3089, Training Loss : 0.34366, Training Acc : 0.900, Run Time : 5.68
INFO:root:2019-05-12 01:37:44, Epoch : 1, Step : 3090, Training Loss : 0.23010, Training Acc : 0.944, Run Time : 1.52
INFO:root:2019-05-12 01:37:49, Epoch : 1, Step : 3091, Training Loss : 0.18110, Training Acc : 0.936, Run Time : 5.55
INFO:root:2019-05-12 01:37:51, Epoch : 1, Step : 3092, Training Loss : 0.18438, Training Acc : 0.969, Run Time : 1.95
INFO:root:2019-05-12 01:37:53, Epoch : 1, Step : 3093, Training Loss : 0.17032, Training Acc : 0.961, Run Time : 1.60
INFO:root:2019-05-12 01:38:03, Epoch : 1, Step : 3094, Training Loss : 0.28262, Training Acc : 0.919, Run Time : 10.61
INFO:root:2019-05-12 01:38:05, Epoch : 1, Step : 3095, Training Loss : 0.20551, Training Acc : 0.928, Run Time : 1.18
INFO:root:2019-05-12 01:38:15, Epoch : 1, Step : 3096, Training Loss : 0.23124, Training Acc : 0.942, Run Time : 10.79
INFO:root:2019-05-12 01:38:17, Epoch : 1, Step : 3097, Training Loss : 0.11830, Training Acc : 0.956, Run Time : 1.45
INFO:root:2019-05-12 01:38:19, Epoch : 1, Step : 3098, Training Loss : 0.15105, Training Acc : 0.933, Run Time : 2.17
INFO:root:2019-05-12 01:38:25, Epoch : 1, Step : 3099, Training Loss : 0.22040, Training Acc : 0.958, Run Time : 6.13
INFO:root:2019-05-12 01:38:27, Epoch : 1, Step : 3100, Training Loss : 0.46953, Training Acc : 0.839, Run Time : 1.86
INFO:root:2019-05-12 01:38:38, Epoch : 1, Step : 3101, Training Loss : 0.57448, Training Acc : 0.733, Run Time : 10.62
INFO:root:2019-05-12 01:38:39, Epoch : 1, Step : 3102, Training Loss : 0.77271, Training Acc : 0.628, Run Time : 1.59
INFO:root:2019-05-12 01:38:50, Epoch : 1, Step : 3103, Training Loss : 0.52260, Training Acc : 0.703, Run Time : 10.41
INFO:root:2019-05-12 01:38:51, Epoch : 1, Step : 3104, Training Loss : 0.60780, Training Acc : 0.675, Run Time : 1.30
INFO:root:2019-05-12 01:38:53, Epoch : 1, Step : 3105, Training Loss : 0.29694, Training Acc : 0.836, Run Time : 1.75
INFO:root:2019-05-12 01:39:02, Epoch : 1, Step : 3106, Training Loss : 0.47369, Training Acc : 0.803, Run Time : 9.17
INFO:root:2019-05-12 01:39:03, Epoch : 1, Step : 3107, Training Loss : 0.44692, Training Acc : 0.822, Run Time : 1.18
INFO:root:2019-05-12 01:39:05, Epoch : 1, Step : 3108, Training Loss : 0.73066, Training Acc : 0.803, Run Time : 2.16
INFO:root:2019-05-12 01:39:14, Epoch : 1, Step : 3109, Training Loss : 0.34033, Training Acc : 0.764, Run Time : 8.97
INFO:root:2019-05-12 01:39:17, Epoch : 1, Step : 3110, Training Loss : 0.45359, Training Acc : 0.772, Run Time : 2.44
INFO:root:2019-05-12 01:39:24, Epoch : 1, Step : 3111, Training Loss : 0.35138, Training Acc : 0.764, Run Time : 7.77
INFO:root:2019-05-12 01:39:26, Epoch : 1, Step : 3112, Training Loss : 0.33732, Training Acc : 0.764, Run Time : 1.13
INFO:root:2019-05-12 01:39:34, Epoch : 1, Step : 3113, Training Loss : 0.39583, Training Acc : 0.744, Run Time : 8.45
INFO:root:2019-05-12 01:39:35, Epoch : 1, Step : 3114, Training Loss : 0.28435, Training Acc : 0.903, Run Time : 1.25
INFO:root:2019-05-12 01:39:37, Epoch : 1, Step : 3115, Training Loss : 0.32244, Training Acc : 0.881, Run Time : 2.14
INFO:root:2019-05-12 01:39:47, Epoch : 1, Step : 3116, Training Loss : 0.24545, Training Acc : 0.925, Run Time : 9.58
INFO:root:2019-05-12 01:39:48, Epoch : 1, Step : 3117, Training Loss : 0.35328, Training Acc : 0.869, Run Time : 1.39
INFO:root:2019-05-12 01:39:50, Epoch : 1, Step : 3118, Training Loss : 0.72715, Training Acc : 0.614, Run Time : 1.59
INFO:root:2019-05-12 01:40:00, Epoch : 1, Step : 3119, Training Loss : 0.36132, Training Acc : 0.881, Run Time : 9.58
INFO:root:2019-05-12 01:40:02, Epoch : 1, Step : 3120, Training Loss : 0.20097, Training Acc : 0.925, Run Time : 1.97
INFO:root:2019-05-12 01:40:12, Epoch : 1, Step : 3121, Training Loss : 0.21161, Training Acc : 0.914, Run Time : 10.20
INFO:root:2019-05-12 01:40:13, Epoch : 1, Step : 3122, Training Loss : 0.29493, Training Acc : 0.856, Run Time : 1.25
INFO:root:2019-05-12 01:40:14, Epoch : 1, Step : 3123, Training Loss : 0.25570, Training Acc : 0.872, Run Time : 1.25
INFO:root:2019-05-12 01:40:15, Epoch : 1, Step : 3124, Training Loss : 0.26532, Training Acc : 0.908, Run Time : 1.17
INFO:root:2019-05-12 01:40:25, Epoch : 1, Step : 3125, Training Loss : 0.22865, Training Acc : 0.900, Run Time : 9.67
INFO:root:2019-05-12 01:40:27, Epoch : 1, Step : 3126, Training Loss : 0.26031, Training Acc : 0.922, Run Time : 1.93
INFO:root:2019-05-12 01:40:34, Epoch : 1, Step : 3127, Training Loss : 0.27336, Training Acc : 0.883, Run Time : 7.48
INFO:root:2019-05-12 01:40:36, Epoch : 1, Step : 3128, Training Loss : 0.27879, Training Acc : 0.869, Run Time : 1.14
INFO:root:2019-05-12 01:40:45, Epoch : 1, Step : 3129, Training Loss : 0.31075, Training Acc : 0.856, Run Time : 9.07
INFO:root:2019-05-12 01:40:47, Epoch : 1, Step : 3130, Training Loss : 0.22592, Training Acc : 0.892, Run Time : 2.04
INFO:root:2019-05-12 01:40:55, Epoch : 1, Step : 3131, Training Loss : 0.25622, Training Acc : 0.889, Run Time : 8.01
INFO:root:2019-05-12 01:40:57, Epoch : 1, Step : 3132, Training Loss : 0.32664, Training Acc : 0.839, Run Time : 2.12
INFO:root:2019-05-12 01:41:05, Epoch : 1, Step : 3133, Training Loss : 0.39138, Training Acc : 0.781, Run Time : 8.28
INFO:root:2019-05-12 01:41:07, Epoch : 1, Step : 3134, Training Loss : 0.26423, Training Acc : 0.889, Run Time : 1.92
INFO:root:2019-05-12 01:41:17, Epoch : 1, Step : 3135, Training Loss : 0.21140, Training Acc : 0.922, Run Time : 10.08
INFO:root:2019-05-12 01:41:19, Epoch : 1, Step : 3136, Training Loss : 0.27928, Training Acc : 0.864, Run Time : 1.47
INFO:root:2019-05-12 01:41:22, Epoch : 1, Step : 3137, Training Loss : 0.20233, Training Acc : 0.925, Run Time : 3.66
INFO:root:2019-05-12 01:41:29, Epoch : 1, Step : 3138, Training Loss : 0.23620, Training Acc : 0.908, Run Time : 7.14
INFO:root:2019-05-12 01:41:31, Epoch : 1, Step : 3139, Training Loss : 0.23412, Training Acc : 0.894, Run Time : 1.47
INFO:root:2019-05-12 01:41:40, Epoch : 1, Step : 3140, Training Loss : 0.28715, Training Acc : 0.900, Run Time : 9.31
INFO:root:2019-05-12 01:41:42, Epoch : 1, Step : 3141, Training Loss : 0.28069, Training Acc : 0.892, Run Time : 1.68
INFO:root:2019-05-12 01:41:52, Epoch : 1, Step : 3142, Training Loss : 0.19469, Training Acc : 0.914, Run Time : 10.24
INFO:root:2019-05-12 01:41:53, Epoch : 1, Step : 3143, Training Loss : 0.25323, Training Acc : 0.900, Run Time : 1.27
INFO:root:2019-05-12 01:41:55, Epoch : 1, Step : 3144, Training Loss : 0.29920, Training Acc : 0.900, Run Time : 1.25
INFO:root:2019-05-12 01:42:05, Epoch : 1, Step : 3145, Training Loss : 0.32129, Training Acc : 0.878, Run Time : 10.10
INFO:root:2019-05-12 01:42:07, Epoch : 1, Step : 3146, Training Loss : 0.31364, Training Acc : 0.883, Run Time : 2.28
INFO:root:2019-05-12 01:42:15, Epoch : 1, Step : 3147, Training Loss : 0.27240, Training Acc : 0.894, Run Time : 8.45
INFO:root:2019-05-12 01:42:17, Epoch : 1, Step : 3148, Training Loss : 0.26187, Training Acc : 0.908, Run Time : 1.15
INFO:root:2019-05-12 01:42:26, Epoch : 1, Step : 3149, Training Loss : 0.31203, Training Acc : 0.875, Run Time : 8.92
INFO:root:2019-05-12 01:42:27, Epoch : 1, Step : 3150, Training Loss : 0.34080, Training Acc : 0.894, Run Time : 1.29
INFO:root:2019-05-12 01:42:28, Epoch : 1, Step : 3151, Training Loss : 0.28136, Training Acc : 0.889, Run Time : 1.16
INFO:root:2019-05-12 01:42:29, Epoch : 1, Step : 3152, Training Loss : 0.28565, Training Acc : 0.886, Run Time : 1.49
INFO:root:2019-05-12 01:42:37, Epoch : 1, Step : 3153, Training Loss : 0.30298, Training Acc : 0.892, Run Time : 7.94
INFO:root:2019-05-12 01:42:39, Epoch : 1, Step : 3154, Training Loss : 0.21137, Training Acc : 0.928, Run Time : 1.26
INFO:root:2019-05-12 01:42:40, Epoch : 1, Step : 3155, Training Loss : 0.32159, Training Acc : 0.892, Run Time : 1.56
INFO:root:2019-05-12 01:42:47, Epoch : 1, Step : 3156, Training Loss : 0.31138, Training Acc : 0.900, Run Time : 6.32
INFO:root:2019-05-12 01:42:48, Epoch : 1, Step : 3157, Training Loss : 0.32623, Training Acc : 0.903, Run Time : 1.45
INFO:root:2019-05-12 01:42:52, Epoch : 1, Step : 3158, Training Loss : 0.33376, Training Acc : 0.894, Run Time : 4.21
INFO:root:2019-05-12 01:42:54, Epoch : 1, Step : 3159, Training Loss : 0.33580, Training Acc : 0.892, Run Time : 1.66
INFO:root:2019-05-12 01:43:01, Epoch : 1, Step : 3160, Training Loss : 0.42813, Training Acc : 0.825, Run Time : 6.69
INFO:root:2019-05-12 01:43:02, Epoch : 1, Step : 3161, Training Loss : 0.35052, Training Acc : 0.881, Run Time : 1.15
INFO:root:2019-05-12 01:43:09, Epoch : 1, Step : 3162, Training Loss : 0.37623, Training Acc : 0.875, Run Time : 7.61
INFO:root:2019-05-12 01:43:10, Epoch : 1, Step : 3163, Training Loss : 0.35813, Training Acc : 0.867, Run Time : 1.15
INFO:root:2019-05-12 01:43:12, Epoch : 1, Step : 3164, Training Loss : 0.38260, Training Acc : 0.861, Run Time : 1.19
INFO:root:2019-05-12 01:43:15, Epoch : 1, Step : 3165, Training Loss : 0.35151, Training Acc : 0.900, Run Time : 3.67
INFO:root:2019-05-12 01:43:17, Epoch : 1, Step : 3166, Training Loss : 0.44257, Training Acc : 0.833, Run Time : 1.71
INFO:root:2019-05-12 01:43:21, Epoch : 1, Step : 3167, Training Loss : 0.44783, Training Acc : 0.808, Run Time : 3.89
INFO:root:2019-05-12 01:43:22, Epoch : 1, Step : 3168, Training Loss : 0.37178, Training Acc : 0.853, Run Time : 1.18
INFO:root:2019-05-12 01:43:24, Epoch : 1, Step : 3169, Training Loss : 0.34767, Training Acc : 0.861, Run Time : 1.74
INFO:root:2019-05-12 01:43:28, Epoch : 1, Step : 3170, Training Loss : 0.35560, Training Acc : 0.869, Run Time : 4.20
INFO:root:2019-05-12 01:43:30, Epoch : 1, Step : 3171, Training Loss : 0.37971, Training Acc : 0.872, Run Time : 1.75
INFO:root:2019-05-12 01:43:36, Epoch : 1, Step : 3172, Training Loss : 0.25980, Training Acc : 0.922, Run Time : 6.44
INFO:root:2019-05-12 01:43:37, Epoch : 1, Step : 3173, Training Loss : 0.30352, Training Acc : 0.883, Run Time : 1.16
INFO:root:2019-05-12 01:43:39, Epoch : 1, Step : 3174, Training Loss : 0.18936, Training Acc : 0.950, Run Time : 1.35
INFO:root:2019-05-12 01:43:44, Epoch : 1, Step : 3175, Training Loss : 0.29005, Training Acc : 0.883, Run Time : 5.23
INFO:root:2019-05-12 01:43:46, Epoch : 1, Step : 3176, Training Loss : 0.27135, Training Acc : 0.900, Run Time : 1.60
INFO:root:2019-05-12 01:43:50, Epoch : 1, Step : 3177, Training Loss : 0.27889, Training Acc : 0.903, Run Time : 4.14
INFO:root:2019-05-12 01:43:51, Epoch : 1, Step : 3178, Training Loss : 0.26627, Training Acc : 0.897, Run Time : 1.14
INFO:root:2019-05-12 01:43:52, Epoch : 1, Step : 3179, Training Loss : 0.25627, Training Acc : 0.911, Run Time : 1.52
INFO:root:2019-05-12 01:43:59, Epoch : 1, Step : 3180, Training Loss : 0.25293, Training Acc : 0.900, Run Time : 6.18
INFO:root:2019-05-12 01:44:00, Epoch : 1, Step : 3181, Training Loss : 0.23523, Training Acc : 0.900, Run Time : 1.37
INFO:root:2019-05-12 01:44:01, Epoch : 1, Step : 3182, Training Loss : 0.28584, Training Acc : 0.897, Run Time : 1.40
INFO:root:2019-05-12 01:44:08, Epoch : 1, Step : 3183, Training Loss : 0.20778, Training Acc : 0.936, Run Time : 6.36
INFO:root:2019-05-12 01:44:09, Epoch : 1, Step : 3184, Training Loss : 0.36101, Training Acc : 0.844, Run Time : 1.13
INFO:root:2019-05-12 01:44:10, Epoch : 1, Step : 3185, Training Loss : 0.28230, Training Acc : 0.892, Run Time : 1.14
INFO:root:2019-05-12 01:44:13, Epoch : 1, Step : 3186, Training Loss : 0.26241, Training Acc : 0.894, Run Time : 3.26
INFO:root:2019-05-12 01:44:15, Epoch : 1, Step : 3187, Training Loss : 0.28760, Training Acc : 0.861, Run Time : 1.32
INFO:root:2019-05-12 01:44:17, Epoch : 1, Step : 3188, Training Loss : 0.32591, Training Acc : 0.867, Run Time : 2.18
INFO:root:2019-05-12 01:44:18, Epoch : 1, Step : 3189, Training Loss : 0.29796, Training Acc : 0.875, Run Time : 1.29
INFO:root:2019-05-12 01:44:19, Epoch : 1, Step : 3190, Training Loss : 0.28324, Training Acc : 0.867, Run Time : 1.17
INFO:root:2019-05-12 01:44:25, Epoch : 1, Step : 3191, Training Loss : 0.23884, Training Acc : 0.883, Run Time : 5.69
INFO:root:2019-05-12 01:44:27, Epoch : 1, Step : 3192, Training Loss : 0.31024, Training Acc : 0.842, Run Time : 1.81
INFO:root:2019-05-12 01:44:32, Epoch : 1, Step : 3193, Training Loss : 0.25971, Training Acc : 0.889, Run Time : 5.73
INFO:root:2019-05-12 01:44:34, Epoch : 1, Step : 3194, Training Loss : 0.29537, Training Acc : 0.861, Run Time : 1.32
INFO:root:2019-05-12 01:44:39, Epoch : 1, Step : 3195, Training Loss : 0.26754, Training Acc : 0.878, Run Time : 4.83
INFO:root:2019-05-12 01:44:40, Epoch : 1, Step : 3196, Training Loss : 0.29585, Training Acc : 0.842, Run Time : 1.14
INFO:root:2019-05-12 01:44:42, Epoch : 1, Step : 3197, Training Loss : 0.30299, Training Acc : 0.808, Run Time : 2.06
INFO:root:2019-05-12 01:44:48, Epoch : 1, Step : 3198, Training Loss : 0.29701, Training Acc : 0.831, Run Time : 5.89
INFO:root:2019-05-12 01:44:49, Epoch : 1, Step : 3199, Training Loss : 0.28612, Training Acc : 0.842, Run Time : 1.12
INFO:root:2019-05-12 01:44:50, Epoch : 1, Step : 3200, Training Loss : 0.31672, Training Acc : 0.914, Run Time : 1.17
INFO:root:2019-05-12 01:44:51, Epoch : 1, Step : 3201, Training Loss : 0.57986, Training Acc : 0.664, Run Time : 1.48
INFO:root:2019-05-12 01:44:53, Epoch : 1, Step : 3202, Training Loss : 0.92137, Training Acc : 0.614, Run Time : 2.08
INFO:root:2019-05-12 01:44:55, Epoch : 1, Step : 3203, Training Loss : 0.81012, Training Acc : 0.622, Run Time : 1.39
INFO:root:2019-05-12 01:44:59, Epoch : 1, Step : 3204, Training Loss : 0.37399, Training Acc : 0.853, Run Time : 3.85
INFO:root:2019-05-12 01:45:00, Epoch : 1, Step : 3205, Training Loss : 0.38785, Training Acc : 0.828, Run Time : 1.31
INFO:root:2019-05-12 01:45:02, Epoch : 1, Step : 3206, Training Loss : 0.29576, Training Acc : 0.917, Run Time : 2.33
INFO:root:2019-05-12 01:45:09, Epoch : 1, Step : 3207, Training Loss : 0.77628, Training Acc : 0.586, Run Time : 6.40
INFO:root:2019-05-12 01:45:10, Epoch : 1, Step : 3208, Training Loss : 0.46635, Training Acc : 0.717, Run Time : 1.22
INFO:root:2019-05-12 01:45:11, Epoch : 1, Step : 3209, Training Loss : 0.47268, Training Acc : 0.736, Run Time : 1.14
INFO:root:2019-05-12 01:45:12, Epoch : 1, Step : 3210, Training Loss : 0.65690, Training Acc : 0.636, Run Time : 1.25
INFO:root:2019-05-12 01:45:17, Epoch : 1, Step : 3211, Training Loss : 0.50534, Training Acc : 0.722, Run Time : 4.87
INFO:root:2019-05-12 01:45:18, Epoch : 1, Step : 3212, Training Loss : 0.68704, Training Acc : 0.597, Run Time : 1.12
INFO:root:2019-05-12 01:45:21, Epoch : 1, Step : 3213, Training Loss : 0.43881, Training Acc : 0.806, Run Time : 2.57
INFO:root:2019-05-12 01:45:25, Epoch : 1, Step : 3214, Training Loss : 0.53399, Training Acc : 0.761, Run Time : 3.86
INFO:root:2019-05-12 01:45:26, Epoch : 1, Step : 3215, Training Loss : 0.51851, Training Acc : 0.728, Run Time : 1.42
INFO:root:2019-05-12 01:45:31, Epoch : 1, Step : 3216, Training Loss : 0.22947, Training Acc : 0.936, Run Time : 5.17
INFO:root:2019-05-12 01:45:33, Epoch : 1, Step : 3217, Training Loss : 0.47247, Training Acc : 0.758, Run Time : 1.23
INFO:root:2019-05-12 01:45:34, Epoch : 1, Step : 3218, Training Loss : 0.51456, Training Acc : 0.758, Run Time : 1.36
INFO:root:2019-05-12 01:45:39, Epoch : 1, Step : 3219, Training Loss : 0.43164, Training Acc : 0.797, Run Time : 4.84
INFO:root:2019-05-12 01:45:40, Epoch : 1, Step : 3220, Training Loss : 0.37058, Training Acc : 0.822, Run Time : 1.21
INFO:root:2019-05-12 01:45:41, Epoch : 1, Step : 3221, Training Loss : 0.32224, Training Acc : 0.869, Run Time : 1.16
INFO:root:2019-05-12 01:45:45, Epoch : 1, Step : 3222, Training Loss : 0.22631, Training Acc : 0.942, Run Time : 3.59
INFO:root:2019-05-12 01:45:46, Epoch : 1, Step : 3223, Training Loss : 0.35801, Training Acc : 0.861, Run Time : 1.37
INFO:root:2019-05-12 01:45:48, Epoch : 1, Step : 3224, Training Loss : 0.43795, Training Acc : 0.800, Run Time : 1.53
INFO:root:2019-05-12 01:45:49, Epoch : 1, Step : 3225, Training Loss : 0.30131, Training Acc : 0.878, Run Time : 1.64
INFO:root:2019-05-12 01:45:57, Epoch : 1, Step : 3226, Training Loss : 0.25339, Training Acc : 0.919, Run Time : 7.41
INFO:root:2019-05-12 01:45:58, Epoch : 1, Step : 3227, Training Loss : 0.31768, Training Acc : 0.908, Run Time : 1.31
INFO:root:2019-05-12 01:45:59, Epoch : 1, Step : 3228, Training Loss : 0.33915, Training Acc : 0.875, Run Time : 1.31
INFO:root:2019-05-12 01:46:01, Epoch : 1, Step : 3229, Training Loss : 0.31913, Training Acc : 0.900, Run Time : 2.08
INFO:root:2019-05-12 01:46:05, Epoch : 1, Step : 3230, Training Loss : 0.31305, Training Acc : 0.919, Run Time : 3.65
INFO:root:2019-05-12 01:46:07, Epoch : 1, Step : 3231, Training Loss : 0.27249, Training Acc : 0.919, Run Time : 1.87
INFO:root:2019-05-12 01:46:11, Epoch : 1, Step : 3232, Training Loss : 0.51239, Training Acc : 0.781, Run Time : 3.85
INFO:root:2019-05-12 01:46:12, Epoch : 1, Step : 3233, Training Loss : 0.35842, Training Acc : 0.919, Run Time : 1.14
INFO:root:2019-05-12 01:46:14, Epoch : 1, Step : 3234, Training Loss : 0.50597, Training Acc : 0.767, Run Time : 1.63
INFO:root:2019-05-12 01:46:21, Epoch : 1, Step : 3235, Training Loss : 0.60966, Training Acc : 0.647, Run Time : 7.38
INFO:root:2019-05-12 01:46:22, Epoch : 1, Step : 3236, Training Loss : 0.27449, Training Acc : 0.956, Run Time : 1.25
INFO:root:2019-05-12 01:46:23, Epoch : 1, Step : 3237, Training Loss : 0.46959, Training Acc : 0.806, Run Time : 1.31
INFO:root:2019-05-12 01:46:30, Epoch : 1, Step : 3238, Training Loss : 0.33613, Training Acc : 0.872, Run Time : 6.20
INFO:root:2019-05-12 01:46:31, Epoch : 1, Step : 3239, Training Loss : 0.31451, Training Acc : 0.922, Run Time : 1.56
INFO:root:2019-05-12 01:46:32, Epoch : 1, Step : 3240, Training Loss : 0.37217, Training Acc : 0.850, Run Time : 1.12
INFO:root:2019-05-12 01:46:40, Epoch : 1, Step : 3241, Training Loss : 0.41250, Training Acc : 0.850, Run Time : 7.26
INFO:root:2019-05-12 01:46:41, Epoch : 1, Step : 3242, Training Loss : 0.24950, Training Acc : 0.922, Run Time : 1.28
INFO:root:2019-05-12 01:46:42, Epoch : 1, Step : 3243, Training Loss : 0.27687, Training Acc : 0.925, Run Time : 1.15
INFO:root:2019-05-12 01:46:44, Epoch : 1, Step : 3244, Training Loss : 0.24739, Training Acc : 0.922, Run Time : 1.71
INFO:root:2019-05-12 01:46:49, Epoch : 1, Step : 3245, Training Loss : 0.26585, Training Acc : 0.950, Run Time : 5.21
INFO:root:2019-05-12 01:46:50, Epoch : 1, Step : 3246, Training Loss : 0.31124, Training Acc : 0.881, Run Time : 1.51
INFO:root:2019-05-12 01:46:58, Epoch : 1, Step : 3247, Training Loss : 0.51546, Training Acc : 0.728, Run Time : 7.11
INFO:root:2019-05-12 01:46:59, Epoch : 1, Step : 3248, Training Loss : 0.49551, Training Acc : 0.761, Run Time : 1.35
INFO:root:2019-05-12 01:47:01, Epoch : 1, Step : 3249, Training Loss : 0.33524, Training Acc : 0.886, Run Time : 1.63
INFO:root:2019-05-12 01:47:07, Epoch : 1, Step : 3250, Training Loss : 0.38728, Training Acc : 0.833, Run Time : 6.41
INFO:root:2019-05-12 01:47:08, Epoch : 1, Step : 3251, Training Loss : 0.40992, Training Acc : 0.844, Run Time : 1.32
INFO:root:2019-05-12 01:47:10, Epoch : 1, Step : 3252, Training Loss : 0.25643, Training Acc : 0.914, Run Time : 1.40
INFO:root:2019-05-12 01:47:14, Epoch : 1, Step : 3253, Training Loss : 0.32558, Training Acc : 0.903, Run Time : 4.12
INFO:root:2019-05-12 01:47:16, Epoch : 1, Step : 3254, Training Loss : 0.29671, Training Acc : 0.869, Run Time : 2.09
INFO:root:2019-05-12 01:47:19, Epoch : 1, Step : 3255, Training Loss : 0.34546, Training Acc : 0.894, Run Time : 3.45
INFO:root:2019-05-12 01:47:21, Epoch : 1, Step : 3256, Training Loss : 0.31857, Training Acc : 0.900, Run Time : 1.83
INFO:root:2019-05-12 01:47:24, Epoch : 1, Step : 3257, Training Loss : 0.24673, Training Acc : 0.922, Run Time : 2.40
INFO:root:2019-05-12 01:47:25, Epoch : 1, Step : 3258, Training Loss : 0.40880, Training Acc : 0.844, Run Time : 1.42
INFO:root:2019-05-12 01:47:31, Epoch : 1, Step : 3259, Training Loss : 0.50472, Training Acc : 0.781, Run Time : 5.52
INFO:root:2019-05-12 01:47:32, Epoch : 1, Step : 3260, Training Loss : 0.23510, Training Acc : 0.914, Run Time : 1.25
INFO:root:2019-05-12 01:47:33, Epoch : 1, Step : 3261, Training Loss : 0.21577, Training Acc : 0.942, Run Time : 1.24
INFO:root:2019-05-12 01:47:40, Epoch : 1, Step : 3262, Training Loss : 0.36942, Training Acc : 0.797, Run Time : 7.13
INFO:root:2019-05-12 01:47:41, Epoch : 1, Step : 3263, Training Loss : 0.21544, Training Acc : 0.919, Run Time : 1.15
INFO:root:2019-05-12 01:47:42, Epoch : 1, Step : 3264, Training Loss : 0.25006, Training Acc : 0.944, Run Time : 1.16
INFO:root:2019-05-12 01:47:51, Epoch : 1, Step : 3265, Training Loss : 0.36699, Training Acc : 0.814, Run Time : 8.58
INFO:root:2019-05-12 01:47:53, Epoch : 1, Step : 3266, Training Loss : 0.14319, Training Acc : 0.947, Run Time : 1.56
INFO:root:2019-05-12 01:47:57, Epoch : 1, Step : 3267, Training Loss : 0.20085, Training Acc : 0.936, Run Time : 4.32
INFO:root:2019-05-12 01:47:58, Epoch : 1, Step : 3268, Training Loss : 0.15786, Training Acc : 0.961, Run Time : 1.34
INFO:root:2019-05-12 01:48:04, Epoch : 1, Step : 3269, Training Loss : 0.07074, Training Acc : 0.992, Run Time : 5.66
INFO:root:2019-05-12 01:48:05, Epoch : 1, Step : 3270, Training Loss : 0.06885, Training Acc : 0.989, Run Time : 1.33
INFO:root:2019-05-12 01:48:13, Epoch : 1, Step : 3271, Training Loss : 0.10028, Training Acc : 0.969, Run Time : 7.76
INFO:root:2019-05-12 01:48:14, Epoch : 1, Step : 3272, Training Loss : 0.13446, Training Acc : 0.958, Run Time : 1.26
INFO:root:2019-05-12 01:48:20, Epoch : 1, Step : 3273, Training Loss : 0.20835, Training Acc : 0.933, Run Time : 5.50
INFO:root:2019-05-12 01:48:21, Epoch : 1, Step : 3274, Training Loss : 0.11351, Training Acc : 0.956, Run Time : 1.57
INFO:root:2019-05-12 01:48:23, Epoch : 1, Step : 3275, Training Loss : 0.07778, Training Acc : 0.983, Run Time : 1.20
INFO:root:2019-05-12 01:48:24, Epoch : 1, Step : 3276, Training Loss : 0.14179, Training Acc : 0.967, Run Time : 1.59
INFO:root:2019-05-12 01:48:28, Epoch : 1, Step : 3277, Training Loss : 0.07262, Training Acc : 1.000, Run Time : 3.87
INFO:root:2019-05-12 01:48:30, Epoch : 1, Step : 3278, Training Loss : 0.08280, Training Acc : 0.994, Run Time : 1.64
INFO:root:2019-05-12 01:48:33, Epoch : 1, Step : 3279, Training Loss : 0.13750, Training Acc : 0.950, Run Time : 3.14
INFO:root:2019-05-12 01:48:34, Epoch : 1, Step : 3280, Training Loss : 0.15432, Training Acc : 0.975, Run Time : 1.24
INFO:root:2019-05-12 01:48:43, Epoch : 1, Step : 3281, Training Loss : 0.17174, Training Acc : 0.936, Run Time : 8.48
INFO:root:2019-05-12 01:48:44, Epoch : 1, Step : 3282, Training Loss : 0.24444, Training Acc : 0.925, Run Time : 1.27
INFO:root:2019-05-12 01:48:51, Epoch : 1, Step : 3283, Training Loss : 0.38515, Training Acc : 0.817, Run Time : 6.84
INFO:root:2019-05-12 01:48:52, Epoch : 1, Step : 3284, Training Loss : 0.25248, Training Acc : 0.886, Run Time : 1.85
INFO:root:2019-05-12 01:49:01, Epoch : 1, Step : 3285, Training Loss : 0.14493, Training Acc : 0.944, Run Time : 8.39
INFO:root:2019-05-12 01:49:03, Epoch : 1, Step : 3286, Training Loss : 0.19055, Training Acc : 0.922, Run Time : 1.65
INFO:root:2019-05-12 01:49:07, Epoch : 1, Step : 3287, Training Loss : 0.20015, Training Acc : 0.914, Run Time : 4.58
INFO:root:2019-05-12 01:49:08, Epoch : 1, Step : 3288, Training Loss : 0.20506, Training Acc : 0.942, Run Time : 1.13
INFO:root:2019-05-12 01:49:14, Epoch : 1, Step : 3289, Training Loss : 0.15460, Training Acc : 0.964, Run Time : 6.20
INFO:root:2019-05-12 01:49:16, Epoch : 1, Step : 3290, Training Loss : 0.14025, Training Acc : 0.967, Run Time : 1.81
INFO:root:2019-05-12 01:49:19, Epoch : 1, Step : 3291, Training Loss : 0.23623, Training Acc : 0.908, Run Time : 3.18
INFO:root:2019-05-12 01:49:21, Epoch : 1, Step : 3292, Training Loss : 0.21761, Training Acc : 0.939, Run Time : 1.14
INFO:root:2019-05-12 01:49:22, Epoch : 1, Step : 3293, Training Loss : 0.18591, Training Acc : 0.953, Run Time : 1.22
INFO:root:2019-05-12 01:49:23, Epoch : 1, Step : 3294, Training Loss : 0.20747, Training Acc : 0.947, Run Time : 1.51
INFO:root:2019-05-12 01:49:25, Epoch : 1, Step : 3295, Training Loss : 0.20345, Training Acc : 0.939, Run Time : 1.79
INFO:root:2019-05-12 01:49:28, Epoch : 1, Step : 3296, Training Loss : 0.16096, Training Acc : 0.956, Run Time : 3.08
INFO:root:2019-05-12 01:49:29, Epoch : 1, Step : 3297, Training Loss : 0.23503, Training Acc : 0.903, Run Time : 1.19
INFO:root:2019-05-12 01:49:35, Epoch : 1, Step : 3298, Training Loss : 0.18128, Training Acc : 0.936, Run Time : 5.42
INFO:root:2019-05-12 01:49:36, Epoch : 1, Step : 3299, Training Loss : 0.08548, Training Acc : 0.981, Run Time : 1.68
INFO:root:2019-05-12 01:49:45, Epoch : 1, Step : 3300, Training Loss : 0.26363, Training Acc : 0.872, Run Time : 8.32
INFO:root:2019-05-12 01:49:46, Epoch : 1, Step : 3301, Training Loss : 1.06471, Training Acc : 0.611, Run Time : 1.47
INFO:root:2019-05-12 01:49:47, Epoch : 1, Step : 3302, Training Loss : 1.35699, Training Acc : 0.578, Run Time : 1.14
INFO:root:2019-05-12 01:49:49, Epoch : 1, Step : 3303, Training Loss : 1.33510, Training Acc : 0.469, Run Time : 1.56
INFO:root:2019-05-12 01:49:51, Epoch : 1, Step : 3304, Training Loss : 0.85371, Training Acc : 0.483, Run Time : 2.10
INFO:root:2019-05-12 01:49:53, Epoch : 1, Step : 3305, Training Loss : 1.10425, Training Acc : 0.350, Run Time : 2.07
INFO:root:2019-05-12 01:49:55, Epoch : 1, Step : 3306, Training Loss : 0.65293, Training Acc : 0.644, Run Time : 1.66
INFO:root:2019-05-12 01:49:57, Epoch : 1, Step : 3307, Training Loss : 1.59078, Training Acc : 0.500, Run Time : 1.86
INFO:root:2019-05-12 01:50:03, Epoch : 1, Step : 3308, Training Loss : 0.98109, Training Acc : 0.606, Run Time : 6.04
INFO:root:2019-05-12 01:50:04, Epoch : 1, Step : 3309, Training Loss : 1.40100, Training Acc : 0.597, Run Time : 1.71
INFO:root:2019-05-12 01:50:14, Epoch : 1, Step : 3310, Training Loss : 0.79173, Training Acc : 0.572, Run Time : 9.94
INFO:root:2019-05-12 01:50:16, Epoch : 1, Step : 3311, Training Loss : 0.31007, Training Acc : 0.883, Run Time : 1.72
INFO:root:2019-05-12 01:50:22, Epoch : 1, Step : 3312, Training Loss : 0.88645, Training Acc : 0.486, Run Time : 6.21
INFO:root:2019-05-12 01:50:23, Epoch : 1, Step : 3313, Training Loss : 0.71739, Training Acc : 0.575, Run Time : 1.15
INFO:root:2019-05-12 01:50:25, Epoch : 1, Step : 3314, Training Loss : 0.36249, Training Acc : 0.869, Run Time : 2.05
INFO:root:2019-05-12 01:50:29, Epoch : 1, Step : 3315, Training Loss : 1.02511, Training Acc : 0.572, Run Time : 3.72
INFO:root:2019-05-12 01:50:30, Epoch : 1, Step : 3316, Training Loss : 0.28144, Training Acc : 0.900, Run Time : 1.13
INFO:root:2019-05-12 01:50:32, Epoch : 1, Step : 3317, Training Loss : 0.61222, Training Acc : 0.533, Run Time : 1.39
INFO:root:2019-05-12 01:50:37, Epoch : 1, Step : 3318, Training Loss : 0.34410, Training Acc : 0.869, Run Time : 5.34
INFO:root:2019-05-12 01:50:39, Epoch : 1, Step : 3319, Training Loss : 0.64710, Training Acc : 0.544, Run Time : 1.56
INFO:root:2019-05-12 01:50:43, Epoch : 1, Step : 3320, Training Loss : 0.31797, Training Acc : 0.897, Run Time : 4.85
INFO:root:2019-05-12 01:50:45, Epoch : 1, Step : 3321, Training Loss : 0.28398, Training Acc : 0.958, Run Time : 1.44
INFO:root:2019-05-12 01:50:52, Epoch : 1, Step : 3322, Training Loss : 0.30594, Training Acc : 0.917, Run Time : 7.53
INFO:root:2019-05-12 01:50:54, Epoch : 1, Step : 3323, Training Loss : 0.26199, Training Acc : 0.981, Run Time : 1.16
INFO:root:2019-05-12 01:50:56, Epoch : 1, Step : 3324, Training Loss : 0.34794, Training Acc : 0.825, Run Time : 2.19
INFO:root:2019-05-12 01:51:01, Epoch : 1, Step : 3325, Training Loss : 0.29261, Training Acc : 0.881, Run Time : 4.86
INFO:root:2019-05-12 01:51:02, Epoch : 1, Step : 3326, Training Loss : 0.41907, Training Acc : 0.781, Run Time : 1.73
INFO:root:2019-05-12 01:51:09, Epoch : 1, Step : 3327, Training Loss : 0.24376, Training Acc : 0.969, Run Time : 6.73
INFO:root:2019-05-12 01:51:11, Epoch : 1, Step : 3328, Training Loss : 0.23450, Training Acc : 0.981, Run Time : 1.94
INFO:root:2019-05-12 01:51:18, Epoch : 1, Step : 3329, Training Loss : 0.57716, Training Acc : 0.622, Run Time : 6.49
INFO:root:2019-05-12 01:51:19, Epoch : 1, Step : 3330, Training Loss : 0.38247, Training Acc : 0.933, Run Time : 1.78
INFO:root:2019-05-12 01:51:27, Epoch : 1, Step : 3331, Training Loss : 0.27619, Training Acc : 0.967, Run Time : 7.84
INFO:root:2019-05-12 01:51:29, Epoch : 1, Step : 3332, Training Loss : 0.42259, Training Acc : 0.844, Run Time : 1.57
INFO:root:2019-05-12 01:51:35, Epoch : 1, Step : 3333, Training Loss : 0.46858, Training Acc : 0.861, Run Time : 6.56
INFO:root:2019-05-12 01:51:37, Epoch : 1, Step : 3334, Training Loss : 0.22708, Training Acc : 0.967, Run Time : 1.39
INFO:root:2019-05-12 01:51:44, Epoch : 1, Step : 3335, Training Loss : 0.72484, Training Acc : 0.592, Run Time : 7.65
INFO:root:2019-05-12 01:51:46, Epoch : 1, Step : 3336, Training Loss : 0.39300, Training Acc : 0.786, Run Time : 1.21
INFO:root:2019-05-12 01:51:51, Epoch : 1, Step : 3337, Training Loss : 0.22540, Training Acc : 0.969, Run Time : 5.52
INFO:root:2019-05-12 01:51:52, Epoch : 1, Step : 3338, Training Loss : 0.41151, Training Acc : 0.858, Run Time : 1.22
INFO:root:2019-05-12 01:51:54, Epoch : 1, Step : 3339, Training Loss : 0.20681, Training Acc : 0.961, Run Time : 1.24
INFO:root:2019-05-12 01:52:00, Epoch : 1, Step : 3340, Training Loss : 0.13421, Training Acc : 0.997, Run Time : 6.62
INFO:root:2019-05-12 01:52:02, Epoch : 1, Step : 3341, Training Loss : 0.31549, Training Acc : 0.969, Run Time : 1.51
INFO:root:2019-05-12 01:52:04, Epoch : 1, Step : 3342, Training Loss : 0.30595, Training Acc : 0.958, Run Time : 2.04
INFO:root:2019-05-12 01:52:08, Epoch : 1, Step : 3343, Training Loss : 0.18659, Training Acc : 0.994, Run Time : 4.04
INFO:root:2019-05-12 01:52:09, Epoch : 1, Step : 3344, Training Loss : 0.37472, Training Acc : 0.872, Run Time : 1.32
INFO:root:2019-05-12 01:52:11, Epoch : 1, Step : 3345, Training Loss : 0.19635, Training Acc : 0.969, Run Time : 1.46
INFO:root:2019-05-12 01:52:12, Epoch : 1, Step : 3346, Training Loss : 0.74808, Training Acc : 0.494, Run Time : 1.15
INFO:root:2019-05-12 01:52:16, Epoch : 1, Step : 3347, Training Loss : 0.19047, Training Acc : 0.981, Run Time : 3.88
INFO:root:2019-05-12 01:52:17, Epoch : 1, Step : 3348, Training Loss : 0.50866, Training Acc : 0.633, Run Time : 1.39
INFO:root:2019-05-12 01:52:19, Epoch : 1, Step : 3349, Training Loss : 0.21074, Training Acc : 0.964, Run Time : 1.67
INFO:root:2019-05-12 01:52:20, Epoch : 1, Step : 3350, Training Loss : 0.40805, Training Acc : 0.775, Run Time : 1.14
INFO:root:2019-05-12 01:52:21, Epoch : 1, Step : 3351, Training Loss : 0.21994, Training Acc : 0.950, Run Time : 1.30
INFO:root:2019-05-12 01:52:26, Epoch : 1, Step : 3352, Training Loss : 0.37036, Training Acc : 0.869, Run Time : 5.04
INFO:root:2019-05-12 01:52:27, Epoch : 1, Step : 3353, Training Loss : 0.29290, Training Acc : 0.967, Run Time : 1.14
INFO:root:2019-05-12 01:52:28, Epoch : 1, Step : 3354, Training Loss : 0.33892, Training Acc : 0.928, Run Time : 1.16
INFO:root:2019-05-12 01:52:30, Epoch : 1, Step : 3355, Training Loss : 0.29390, Training Acc : 0.961, Run Time : 1.42
INFO:root:2019-05-12 01:52:36, Epoch : 1, Step : 3356, Training Loss : 0.44999, Training Acc : 0.717, Run Time : 6.22
INFO:root:2019-05-12 01:52:37, Epoch : 1, Step : 3357, Training Loss : 0.32790, Training Acc : 0.944, Run Time : 1.34
INFO:root:2019-05-12 01:52:39, Epoch : 1, Step : 3358, Training Loss : 0.19611, Training Acc : 0.953, Run Time : 1.16
INFO:root:2019-05-12 01:52:45, Epoch : 1, Step : 3359, Training Loss : 0.57234, Training Acc : 0.533, Run Time : 6.80
INFO:root:2019-05-12 01:52:47, Epoch : 1, Step : 3360, Training Loss : 0.57204, Training Acc : 0.761, Run Time : 2.04
INFO:root:2019-05-12 01:52:56, Epoch : 1, Step : 3361, Training Loss : 0.24995, Training Acc : 0.978, Run Time : 8.81
INFO:root:2019-05-12 01:52:57, Epoch : 1, Step : 3362, Training Loss : 0.42105, Training Acc : 0.664, Run Time : 1.28
INFO:root:2019-05-12 01:53:00, Epoch : 1, Step : 3363, Training Loss : 0.41693, Training Acc : 0.914, Run Time : 2.14
INFO:root:2019-05-12 01:53:04, Epoch : 1, Step : 3364, Training Loss : 0.22480, Training Acc : 0.972, Run Time : 3.94
INFO:root:2019-05-12 01:53:05, Epoch : 1, Step : 3365, Training Loss : 0.24660, Training Acc : 0.992, Run Time : 1.30
INFO:root:2019-05-12 01:53:06, Epoch : 1, Step : 3366, Training Loss : 0.48016, Training Acc : 0.725, Run Time : 1.61
INFO:root:2019-05-12 01:53:11, Epoch : 1, Step : 3367, Training Loss : 0.24950, Training Acc : 0.969, Run Time : 4.81
INFO:root:2019-05-12 01:53:12, Epoch : 1, Step : 3368, Training Loss : 0.24199, Training Acc : 0.978, Run Time : 1.20
INFO:root:2019-05-12 01:53:19, Epoch : 1, Step : 3369, Training Loss : 0.58437, Training Acc : 0.544, Run Time : 6.47
INFO:root:2019-05-12 01:53:20, Epoch : 1, Step : 3370, Training Loss : 0.54437, Training Acc : 0.600, Run Time : 1.35
INFO:root:2019-05-12 01:53:25, Epoch : 1, Step : 3371, Training Loss : 0.62660, Training Acc : 0.519, Run Time : 4.43
INFO:root:2019-05-12 01:53:26, Epoch : 1, Step : 3372, Training Loss : 0.47678, Training Acc : 0.631, Run Time : 1.53
INFO:root:2019-05-12 01:53:29, Epoch : 1, Step : 3373, Training Loss : 0.50721, Training Acc : 0.586, Run Time : 2.66
INFO:root:2019-05-12 01:53:31, Epoch : 1, Step : 3374, Training Loss : 0.78517, Training Acc : 0.389, Run Time : 2.15
INFO:root:2019-05-12 01:53:33, Epoch : 1, Step : 3375, Training Loss : 0.37925, Training Acc : 0.867, Run Time : 1.56
INFO:root:2019-05-12 01:53:34, Epoch : 1, Step : 3376, Training Loss : 0.21103, Training Acc : 0.983, Run Time : 1.35
INFO:root:2019-05-12 01:53:37, Epoch : 1, Step : 3377, Training Loss : 0.35442, Training Acc : 0.925, Run Time : 2.55
INFO:root:2019-05-12 01:53:42, Epoch : 1, Step : 3378, Training Loss : 0.91001, Training Acc : 0.494, Run Time : 5.73
INFO:root:2019-05-12 01:53:43, Epoch : 1, Step : 3379, Training Loss : 0.51252, Training Acc : 0.683, Run Time : 1.20
INFO:root:2019-05-12 01:53:51, Epoch : 1, Step : 3380, Training Loss : 0.20717, Training Acc : 0.958, Run Time : 7.69
INFO:root:2019-05-12 01:53:52, Epoch : 1, Step : 3381, Training Loss : 0.49782, Training Acc : 0.889, Run Time : 1.32
INFO:root:2019-05-12 01:53:54, Epoch : 1, Step : 3382, Training Loss : 0.24375, Training Acc : 0.975, Run Time : 1.59
INFO:root:2019-05-12 01:53:59, Epoch : 1, Step : 3383, Training Loss : 0.39269, Training Acc : 0.875, Run Time : 5.44
INFO:root:2019-05-12 01:54:01, Epoch : 1, Step : 3384, Training Loss : 0.28319, Training Acc : 0.978, Run Time : 1.97
INFO:root:2019-05-12 01:54:03, Epoch : 1, Step : 3385, Training Loss : 0.39255, Training Acc : 0.942, Run Time : 1.65
INFO:root:2019-05-12 01:54:05, Epoch : 1, Step : 3386, Training Loss : 0.28876, Training Acc : 0.972, Run Time : 1.43
INFO:root:2019-05-12 01:54:10, Epoch : 1, Step : 3387, Training Loss : 0.13351, Training Acc : 0.978, Run Time : 5.41
INFO:root:2019-05-12 01:54:11, Epoch : 1, Step : 3388, Training Loss : 0.18970, Training Acc : 0.972, Run Time : 1.12
INFO:root:2019-05-12 01:54:12, Epoch : 1, Step : 3389, Training Loss : 0.10423, Training Acc : 1.000, Run Time : 1.23
INFO:root:2019-05-12 01:54:14, Epoch : 1, Step : 3390, Training Loss : 0.15806, Training Acc : 1.000, Run Time : 1.87
INFO:root:2019-05-12 01:54:18, Epoch : 1, Step : 3391, Training Loss : 0.25294, Training Acc : 0.975, Run Time : 4.14
INFO:root:2019-05-12 01:54:19, Epoch : 1, Step : 3392, Training Loss : 0.15032, Training Acc : 0.978, Run Time : 1.15
INFO:root:2019-05-12 01:54:22, Epoch : 1, Step : 3393, Training Loss : 0.25330, Training Acc : 0.936, Run Time : 2.98
INFO:root:2019-05-12 01:54:26, Epoch : 1, Step : 3394, Training Loss : 0.45264, Training Acc : 0.864, Run Time : 3.94
INFO:root:2019-05-12 01:54:28, Epoch : 1, Step : 3395, Training Loss : 0.41456, Training Acc : 0.842, Run Time : 1.40
INFO:root:2019-05-12 01:54:29, Epoch : 1, Step : 3396, Training Loss : 0.33429, Training Acc : 0.911, Run Time : 1.70
INFO:root:2019-05-12 01:54:35, Epoch : 1, Step : 3397, Training Loss : 0.44349, Training Acc : 0.914, Run Time : 5.29
INFO:root:2019-05-12 01:54:36, Epoch : 1, Step : 3398, Training Loss : 0.29486, Training Acc : 0.886, Run Time : 1.63
INFO:root:2019-05-12 01:54:41, Epoch : 1, Step : 3399, Training Loss : 0.22983, Training Acc : 0.950, Run Time : 5.02
INFO:root:2019-05-12 01:54:43, Epoch : 1, Step : 3400, Training Loss : 0.49620, Training Acc : 0.797, Run Time : 1.42
INFO:root:2019-05-12 01:54:50, Epoch : 1, Step : 3401, Training Loss : 0.21135, Training Acc : 0.928, Run Time : 6.92
INFO:root:2019-05-12 01:54:51, Epoch : 1, Step : 3402, Training Loss : 0.23448, Training Acc : 0.922, Run Time : 1.34
INFO:root:2019-05-12 01:54:57, Epoch : 1, Step : 3403, Training Loss : 0.23217, Training Acc : 0.933, Run Time : 5.56
INFO:root:2019-05-12 01:54:58, Epoch : 1, Step : 3404, Training Loss : 0.33665, Training Acc : 0.922, Run Time : 1.19
INFO:root:2019-05-12 01:54:59, Epoch : 1, Step : 3405, Training Loss : 0.19946, Training Acc : 0.928, Run Time : 1.44
INFO:root:2019-05-12 01:55:04, Epoch : 1, Step : 3406, Training Loss : 0.23012, Training Acc : 0.925, Run Time : 4.57
INFO:root:2019-05-12 01:55:05, Epoch : 1, Step : 3407, Training Loss : 0.22260, Training Acc : 0.922, Run Time : 1.59
INFO:root:2019-05-12 01:55:11, Epoch : 1, Step : 3408, Training Loss : 0.24529, Training Acc : 0.922, Run Time : 5.12
INFO:root:2019-05-12 01:55:12, Epoch : 1, Step : 3409, Training Loss : 0.24806, Training Acc : 0.922, Run Time : 1.15
INFO:root:2019-05-12 01:55:18, Epoch : 1, Step : 3410, Training Loss : 0.20299, Training Acc : 0.925, Run Time : 5.87
INFO:root:2019-05-12 01:55:20, Epoch : 1, Step : 3411, Training Loss : 0.23750, Training Acc : 0.922, Run Time : 2.58
INFO:root:2019-05-12 01:55:27, Epoch : 1, Step : 3412, Training Loss : 0.22650, Training Acc : 0.911, Run Time : 6.69
INFO:root:2019-05-12 01:55:29, Epoch : 1, Step : 3413, Training Loss : 0.20774, Training Acc : 0.928, Run Time : 2.14
INFO:root:2019-05-12 01:55:34, Epoch : 1, Step : 3414, Training Loss : 0.19152, Training Acc : 0.922, Run Time : 4.93
INFO:root:2019-05-12 01:55:35, Epoch : 1, Step : 3415, Training Loss : 0.18255, Training Acc : 0.917, Run Time : 1.14
INFO:root:2019-05-12 01:55:37, Epoch : 1, Step : 3416, Training Loss : 0.23915, Training Acc : 0.914, Run Time : 1.90
INFO:root:2019-05-12 01:55:45, Epoch : 1, Step : 3417, Training Loss : 0.36413, Training Acc : 0.869, Run Time : 7.61
INFO:root:2019-05-12 01:55:46, Epoch : 1, Step : 3418, Training Loss : 0.22953, Training Acc : 0.897, Run Time : 1.55
INFO:root:2019-05-12 01:55:52, Epoch : 1, Step : 3419, Training Loss : 0.16567, Training Acc : 0.928, Run Time : 5.41
INFO:root:2019-05-12 01:55:53, Epoch : 1, Step : 3420, Training Loss : 0.18896, Training Acc : 0.914, Run Time : 1.46
INFO:root:2019-05-12 01:56:00, Epoch : 1, Step : 3421, Training Loss : 0.14482, Training Acc : 0.939, Run Time : 7.13
INFO:root:2019-05-12 01:56:02, Epoch : 1, Step : 3422, Training Loss : 0.18026, Training Acc : 0.919, Run Time : 1.57
INFO:root:2019-05-12 01:56:10, Epoch : 1, Step : 3423, Training Loss : 0.28449, Training Acc : 0.892, Run Time : 7.83
INFO:root:2019-05-12 01:56:11, Epoch : 1, Step : 3424, Training Loss : 0.14769, Training Acc : 0.956, Run Time : 1.20
INFO:root:2019-05-12 01:56:12, Epoch : 1, Step : 3425, Training Loss : 0.16203, Training Acc : 0.933, Run Time : 1.13
INFO:root:2019-05-12 01:56:19, Epoch : 1, Step : 3426, Training Loss : 0.16187, Training Acc : 0.950, Run Time : 6.77
INFO:root:2019-05-12 01:56:20, Epoch : 1, Step : 3427, Training Loss : 0.14187, Training Acc : 0.936, Run Time : 1.27
INFO:root:2019-05-12 01:56:21, Epoch : 1, Step : 3428, Training Loss : 0.17826, Training Acc : 0.917, Run Time : 1.15
INFO:root:2019-05-12 01:56:22, Epoch : 1, Step : 3429, Training Loss : 0.13860, Training Acc : 0.936, Run Time : 1.33
INFO:root:2019-05-12 01:56:24, Epoch : 1, Step : 3430, Training Loss : 0.14186, Training Acc : 0.942, Run Time : 1.84
INFO:root:2019-05-12 01:56:28, Epoch : 1, Step : 3431, Training Loss : 0.14546, Training Acc : 0.939, Run Time : 4.20
INFO:root:2019-05-12 01:56:30, Epoch : 1, Step : 3432, Training Loss : 0.17164, Training Acc : 0.919, Run Time : 1.33
INFO:root:2019-05-12 01:56:33, Epoch : 1, Step : 3433, Training Loss : 0.17415, Training Acc : 0.908, Run Time : 2.89
INFO:root:2019-05-12 01:56:34, Epoch : 1, Step : 3434, Training Loss : 0.14817, Training Acc : 0.931, Run Time : 1.27
INFO:root:2019-05-12 01:56:35, Epoch : 1, Step : 3435, Training Loss : 0.13138, Training Acc : 0.939, Run Time : 1.18
INFO:root:2019-05-12 01:56:37, Epoch : 1, Step : 3436, Training Loss : 0.13455, Training Acc : 0.939, Run Time : 1.96
INFO:root:2019-05-12 01:56:41, Epoch : 1, Step : 3437, Training Loss : 0.14149, Training Acc : 0.936, Run Time : 3.60
INFO:root:2019-05-12 01:56:42, Epoch : 1, Step : 3438, Training Loss : 0.13878, Training Acc : 0.933, Run Time : 1.22
INFO:root:2019-05-12 01:56:43, Epoch : 1, Step : 3439, Training Loss : 0.13104, Training Acc : 0.950, Run Time : 1.25
INFO:root:2019-05-12 01:56:50, Epoch : 1, Step : 3440, Training Loss : 0.10550, Training Acc : 0.956, Run Time : 6.57
INFO:root:2019-05-12 01:56:51, Epoch : 1, Step : 3441, Training Loss : 0.12074, Training Acc : 0.953, Run Time : 1.23
INFO:root:2019-05-12 01:57:00, Epoch : 1, Step : 3442, Training Loss : 0.13294, Training Acc : 0.944, Run Time : 8.97
INFO:root:2019-05-12 01:57:01, Epoch : 1, Step : 3443, Training Loss : 0.10868, Training Acc : 0.947, Run Time : 1.55
INFO:root:2019-05-12 01:57:08, Epoch : 1, Step : 3444, Training Loss : 0.12529, Training Acc : 0.958, Run Time : 6.10
INFO:root:2019-05-12 01:57:09, Epoch : 1, Step : 3445, Training Loss : 0.15419, Training Acc : 0.944, Run Time : 1.54
INFO:root:2019-05-12 01:57:12, Epoch : 1, Step : 3446, Training Loss : 0.36008, Training Acc : 0.872, Run Time : 3.32
INFO:root:2019-05-12 01:57:14, Epoch : 1, Step : 3447, Training Loss : 0.17087, Training Acc : 0.931, Run Time : 1.59
INFO:root:2019-05-12 01:57:16, Epoch : 1, Step : 3448, Training Loss : 0.23339, Training Acc : 0.903, Run Time : 1.92
INFO:root:2019-05-12 01:57:24, Epoch : 1, Step : 3449, Training Loss : 0.22194, Training Acc : 0.908, Run Time : 7.78
INFO:root:2019-05-12 01:57:25, Epoch : 1, Step : 3450, Training Loss : 0.19514, Training Acc : 0.906, Run Time : 1.29
INFO:root:2019-05-12 01:57:31, Epoch : 1, Step : 3451, Training Loss : 0.16531, Training Acc : 0.928, Run Time : 5.89
INFO:root:2019-05-12 01:57:32, Epoch : 1, Step : 3452, Training Loss : 0.19847, Training Acc : 0.919, Run Time : 1.41
INFO:root:2019-05-12 01:57:33, Epoch : 1, Step : 3453, Training Loss : 0.37958, Training Acc : 0.811, Run Time : 1.21
INFO:root:2019-05-12 01:57:35, Epoch : 1, Step : 3454, Training Loss : 0.44754, Training Acc : 0.817, Run Time : 1.69
INFO:root:2019-05-12 01:57:43, Epoch : 1, Step : 3455, Training Loss : 0.38408, Training Acc : 0.831, Run Time : 8.04
INFO:root:2019-05-12 01:57:45, Epoch : 1, Step : 3456, Training Loss : 0.31932, Training Acc : 0.858, Run Time : 1.52
INFO:root:2019-05-12 01:57:52, Epoch : 1, Step : 3457, Training Loss : 0.27487, Training Acc : 0.878, Run Time : 7.66
INFO:root:2019-05-12 01:57:54, Epoch : 1, Step : 3458, Training Loss : 0.36861, Training Acc : 0.822, Run Time : 1.34
INFO:root:2019-05-12 01:58:02, Epoch : 1, Step : 3459, Training Loss : 0.42533, Training Acc : 0.836, Run Time : 7.79
INFO:root:2019-05-12 01:58:03, Epoch : 1, Step : 3460, Training Loss : 0.40813, Training Acc : 0.819, Run Time : 1.68
INFO:root:2019-05-12 01:58:10, Epoch : 1, Step : 3461, Training Loss : 0.48523, Training Acc : 0.797, Run Time : 6.73
INFO:root:2019-05-12 01:58:11, Epoch : 1, Step : 3462, Training Loss : 0.50358, Training Acc : 0.778, Run Time : 1.24
INFO:root:2019-05-12 01:58:12, Epoch : 1, Step : 3463, Training Loss : 0.48517, Training Acc : 0.761, Run Time : 1.15
INFO:root:2019-05-12 01:58:14, Epoch : 1, Step : 3464, Training Loss : 0.44271, Training Acc : 0.803, Run Time : 1.19
INFO:root:2019-05-12 01:58:21, Epoch : 1, Step : 3465, Training Loss : 0.39553, Training Acc : 0.842, Run Time : 7.85
INFO:root:2019-05-12 01:58:23, Epoch : 1, Step : 3466, Training Loss : 0.51177, Training Acc : 0.797, Run Time : 1.14
INFO:root:2019-05-12 01:58:24, Epoch : 1, Step : 3467, Training Loss : 0.33960, Training Acc : 0.839, Run Time : 1.24
INFO:root:2019-05-12 01:58:31, Epoch : 1, Step : 3468, Training Loss : 0.33165, Training Acc : 0.847, Run Time : 7.65
INFO:root:2019-05-12 01:58:33, Epoch : 1, Step : 3469, Training Loss : 0.25749, Training Acc : 0.908, Run Time : 1.45
INFO:root:2019-05-12 01:58:35, Epoch : 1, Step : 3470, Training Loss : 0.36322, Training Acc : 0.856, Run Time : 2.43
INFO:root:2019-05-12 01:58:38, Epoch : 1, Step : 3471, Training Loss : 0.28220, Training Acc : 0.867, Run Time : 2.89
INFO:root:2019-05-12 01:58:40, Epoch : 1, Step : 3472, Training Loss : 0.41852, Training Acc : 0.828, Run Time : 1.42
INFO:root:2019-05-12 01:58:41, Epoch : 1, Step : 3473, Training Loss : 0.29943, Training Acc : 0.878, Run Time : 1.19
INFO:root:2019-05-12 01:58:46, Epoch : 1, Step : 3474, Training Loss : 0.40415, Training Acc : 0.844, Run Time : 4.78
INFO:root:2019-05-12 01:58:48, Epoch : 1, Step : 3475, Training Loss : 0.56895, Training Acc : 0.772, Run Time : 2.87
INFO:root:2019-05-12 01:58:52, Epoch : 1, Step : 3476, Training Loss : 0.36736, Training Acc : 0.839, Run Time : 3.96
INFO:root:2019-05-12 01:58:54, Epoch : 1, Step : 3477, Training Loss : 0.38650, Training Acc : 0.814, Run Time : 2.05
INFO:root:2019-05-12 01:58:56, Epoch : 1, Step : 3478, Training Loss : 0.36804, Training Acc : 0.847, Run Time : 1.31
INFO:root:2019-05-12 01:59:01, Epoch : 1, Step : 3479, Training Loss : 0.41712, Training Acc : 0.825, Run Time : 5.12
INFO:root:2019-05-12 01:59:02, Epoch : 1, Step : 3480, Training Loss : 0.45565, Training Acc : 0.806, Run Time : 1.54
INFO:root:2019-05-12 01:59:09, Epoch : 1, Step : 3481, Training Loss : 0.41753, Training Acc : 0.842, Run Time : 6.39
INFO:root:2019-05-12 01:59:10, Epoch : 1, Step : 3482, Training Loss : 0.45493, Training Acc : 0.825, Run Time : 1.30
INFO:root:2019-05-12 01:59:15, Epoch : 1, Step : 3483, Training Loss : 0.46111, Training Acc : 0.778, Run Time : 4.77
INFO:root:2019-05-12 01:59:17, Epoch : 1, Step : 3484, Training Loss : 0.45525, Training Acc : 0.811, Run Time : 1.60
INFO:root:2019-05-12 01:59:19, Epoch : 1, Step : 3485, Training Loss : 0.42448, Training Acc : 0.806, Run Time : 2.08
INFO:root:2019-05-12 01:59:23, Epoch : 1, Step : 3486, Training Loss : 0.44776, Training Acc : 0.778, Run Time : 4.41
INFO:root:2019-05-12 01:59:24, Epoch : 1, Step : 3487, Training Loss : 0.56700, Training Acc : 0.742, Run Time : 1.15
INFO:root:2019-05-12 01:59:29, Epoch : 1, Step : 3488, Training Loss : 0.40422, Training Acc : 0.783, Run Time : 5.02
INFO:root:2019-05-12 01:59:30, Epoch : 1, Step : 3489, Training Loss : 0.37391, Training Acc : 0.825, Run Time : 1.20
INFO:root:2019-05-12 01:59:35, Epoch : 1, Step : 3490, Training Loss : 0.51233, Training Acc : 0.775, Run Time : 5.03
INFO:root:2019-05-12 01:59:37, Epoch : 1, Step : 3491, Training Loss : 0.48916, Training Acc : 0.775, Run Time : 1.42
INFO:root:2019-05-12 01:59:41, Epoch : 1, Step : 3492, Training Loss : 0.40984, Training Acc : 0.811, Run Time : 4.04
INFO:root:2019-05-12 01:59:42, Epoch : 1, Step : 3493, Training Loss : 0.39558, Training Acc : 0.789, Run Time : 1.29
INFO:root:2019-05-12 01:59:43, Epoch : 1, Step : 3494, Training Loss : 0.35269, Training Acc : 0.839, Run Time : 1.14
INFO:root:2019-05-12 01:59:47, Epoch : 1, Step : 3495, Training Loss : 0.56849, Training Acc : 0.747, Run Time : 3.84
INFO:root:2019-05-12 01:59:49, Epoch : 1, Step : 3496, Training Loss : 0.45914, Training Acc : 0.803, Run Time : 1.41
INFO:root:2019-05-12 01:59:51, Epoch : 1, Step : 3497, Training Loss : 0.35661, Training Acc : 0.842, Run Time : 2.72
INFO:root:2019-05-12 01:59:52, Epoch : 1, Step : 3498, Training Loss : 0.43506, Training Acc : 0.800, Run Time : 1.13
INFO:root:2019-05-12 01:59:54, Epoch : 1, Step : 3499, Training Loss : 0.37974, Training Acc : 0.825, Run Time : 1.31
INFO:root:2019-05-12 01:59:58, Epoch : 1, Step : 3500, Training Loss : 0.37366, Training Acc : 0.833, Run Time : 4.64
INFO:root:2019-05-12 02:00:00, Epoch : 1, Step : 3501, Training Loss : 0.51678, Training Acc : 0.761, Run Time : 1.54
INFO:root:2019-05-12 02:00:01, Epoch : 1, Step : 3502, Training Loss : 0.31869, Training Acc : 0.894, Run Time : 1.21
INFO:root:2019-05-12 02:00:07, Epoch : 1, Step : 3503, Training Loss : 0.41300, Training Acc : 0.794, Run Time : 5.84
INFO:root:2019-05-12 02:00:08, Epoch : 1, Step : 3504, Training Loss : 0.33007, Training Acc : 0.864, Run Time : 1.20
INFO:root:2019-05-12 02:00:09, Epoch : 1, Step : 3505, Training Loss : 0.27481, Training Acc : 0.883, Run Time : 1.24
INFO:root:2019-05-12 02:00:11, Epoch : 1, Step : 3506, Training Loss : 0.25612, Training Acc : 0.917, Run Time : 1.99
INFO:root:2019-05-12 02:00:15, Epoch : 1, Step : 3507, Training Loss : 0.52301, Training Acc : 0.722, Run Time : 4.11
INFO:root:2019-05-12 02:00:17, Epoch : 1, Step : 3508, Training Loss : 0.31265, Training Acc : 0.872, Run Time : 1.25
INFO:root:2019-05-12 02:00:24, Epoch : 1, Step : 3509, Training Loss : 0.42643, Training Acc : 0.797, Run Time : 6.88
INFO:root:2019-05-12 02:00:25, Epoch : 1, Step : 3510, Training Loss : 0.38254, Training Acc : 0.825, Run Time : 1.18
INFO:root:2019-05-12 02:00:26, Epoch : 1, Step : 3511, Training Loss : 0.75698, Training Acc : 0.711, Run Time : 1.34
INFO:root:2019-05-12 02:00:33, Epoch : 1, Step : 3512, Training Loss : 0.56695, Training Acc : 0.761, Run Time : 7.10
INFO:root:2019-05-12 02:00:34, Epoch : 1, Step : 3513, Training Loss : 0.96259, Training Acc : 0.678, Run Time : 1.14
INFO:root:2019-05-12 02:00:36, Epoch : 1, Step : 3514, Training Loss : 0.42633, Training Acc : 0.850, Run Time : 1.19
INFO:root:2019-05-12 02:00:43, Epoch : 1, Step : 3515, Training Loss : 0.23142, Training Acc : 0.908, Run Time : 7.27
INFO:root:2019-05-12 02:00:44, Epoch : 1, Step : 3516, Training Loss : 0.44569, Training Acc : 0.781, Run Time : 1.20
INFO:root:2019-05-12 02:00:46, Epoch : 1, Step : 3517, Training Loss : 0.42952, Training Acc : 0.811, Run Time : 2.42
INFO:root:2019-05-12 02:00:52, Epoch : 1, Step : 3518, Training Loss : 0.40652, Training Acc : 0.817, Run Time : 5.40
INFO:root:2019-05-12 02:00:53, Epoch : 1, Step : 3519, Training Loss : 0.43310, Training Acc : 0.775, Run Time : 1.14
INFO:root:2019-05-12 02:00:54, Epoch : 1, Step : 3520, Training Loss : 0.39874, Training Acc : 0.817, Run Time : 1.39
INFO:root:2019-05-12 02:01:02, Epoch : 1, Step : 3521, Training Loss : 0.25112, Training Acc : 0.903, Run Time : 7.32
INFO:root:2019-05-12 02:01:03, Epoch : 1, Step : 3522, Training Loss : 0.35131, Training Acc : 0.831, Run Time : 1.12
INFO:root:2019-05-12 02:01:04, Epoch : 1, Step : 3523, Training Loss : 0.34436, Training Acc : 0.842, Run Time : 1.50
INFO:root:2019-05-12 02:01:10, Epoch : 1, Step : 3524, Training Loss : 0.25569, Training Acc : 0.903, Run Time : 5.97
INFO:root:2019-05-12 02:01:12, Epoch : 1, Step : 3525, Training Loss : 0.32127, Training Acc : 0.869, Run Time : 1.49
INFO:root:2019-05-12 02:01:18, Epoch : 1, Step : 3526, Training Loss : 0.24168, Training Acc : 0.928, Run Time : 6.18
INFO:root:2019-05-12 02:01:19, Epoch : 1, Step : 3527, Training Loss : 0.30743, Training Acc : 0.875, Run Time : 1.51
INFO:root:2019-05-12 02:01:24, Epoch : 1, Step : 3528, Training Loss : 0.46857, Training Acc : 0.772, Run Time : 5.01
INFO:root:2019-05-12 02:01:26, Epoch : 1, Step : 3529, Training Loss : 0.31415, Training Acc : 0.869, Run Time : 1.64
INFO:root:2019-05-12 02:01:32, Epoch : 1, Step : 3530, Training Loss : 0.25061, Training Acc : 0.892, Run Time : 5.98
INFO:root:2019-05-12 02:01:33, Epoch : 1, Step : 3531, Training Loss : 0.26369, Training Acc : 0.903, Run Time : 1.31
INFO:root:2019-05-12 02:01:40, Epoch : 1, Step : 3532, Training Loss : 0.21196, Training Acc : 0.908, Run Time : 6.23
INFO:root:2019-05-12 02:01:41, Epoch : 1, Step : 3533, Training Loss : 0.22869, Training Acc : 0.942, Run Time : 1.53
INFO:root:2019-05-12 02:01:42, Epoch : 1, Step : 3534, Training Loss : 0.15190, Training Acc : 0.947, Run Time : 1.26
INFO:root:2019-05-12 02:01:49, Epoch : 1, Step : 3535, Training Loss : 0.18286, Training Acc : 0.942, Run Time : 6.40
INFO:root:2019-05-12 02:01:50, Epoch : 1, Step : 3536, Training Loss : 0.20889, Training Acc : 0.953, Run Time : 1.56
INFO:root:2019-05-12 02:01:53, Epoch : 1, Step : 3537, Training Loss : 0.41841, Training Acc : 0.800, Run Time : 2.53
INFO:root:2019-05-12 02:01:54, Epoch : 1, Step : 3538, Training Loss : 0.26230, Training Acc : 0.942, Run Time : 1.14
INFO:root:2019-05-12 02:01:55, Epoch : 1, Step : 3539, Training Loss : 0.17574, Training Acc : 0.967, Run Time : 1.16
INFO:root:2019-05-12 02:01:57, Epoch : 1, Step : 3540, Training Loss : 0.25650, Training Acc : 0.908, Run Time : 2.04
INFO:root:2019-05-12 02:02:05, Epoch : 1, Step : 3541, Training Loss : 0.26865, Training Acc : 0.906, Run Time : 7.78
INFO:root:2019-05-12 02:02:06, Epoch : 1, Step : 3542, Training Loss : 0.30366, Training Acc : 0.864, Run Time : 1.18
INFO:root:2019-05-12 02:02:08, Epoch : 1, Step : 3543, Training Loss : 0.20568, Training Acc : 0.964, Run Time : 1.43
INFO:root:2019-05-12 02:02:14, Epoch : 1, Step : 3544, Training Loss : 0.34413, Training Acc : 0.825, Run Time : 6.22
INFO:root:2019-05-12 02:02:16, Epoch : 1, Step : 3545, Training Loss : 0.30440, Training Acc : 0.917, Run Time : 2.23
INFO:root:2019-05-12 02:02:25, Epoch : 1, Step : 3546, Training Loss : 0.40795, Training Acc : 0.808, Run Time : 8.86
INFO:root:2019-05-12 02:02:26, Epoch : 1, Step : 3547, Training Loss : 0.27548, Training Acc : 0.917, Run Time : 1.15
INFO:root:2019-05-12 02:02:27, Epoch : 1, Step : 3548, Training Loss : 0.23317, Training Acc : 0.956, Run Time : 1.21
INFO:root:2019-05-12 02:02:33, Epoch : 1, Step : 3549, Training Loss : 0.34619, Training Acc : 0.919, Run Time : 5.78
INFO:root:2019-05-12 02:02:34, Epoch : 1, Step : 3550, Training Loss : 0.28409, Training Acc : 0.939, Run Time : 1.15
INFO:root:2019-05-12 02:02:36, Epoch : 1, Step : 3551, Training Loss : 0.62569, Training Acc : 0.650, Run Time : 1.38
INFO:root:2019-05-12 02:02:41, Epoch : 1, Step : 3552, Training Loss : 0.36647, Training Acc : 0.819, Run Time : 5.69
INFO:root:2019-05-12 02:02:43, Epoch : 1, Step : 3553, Training Loss : 0.33422, Training Acc : 0.864, Run Time : 1.53
INFO:root:2019-05-12 02:02:51, Epoch : 1, Step : 3554, Training Loss : 0.24960, Training Acc : 0.919, Run Time : 8.19
INFO:root:2019-05-12 02:02:52, Epoch : 1, Step : 3555, Training Loss : 0.24550, Training Acc : 0.928, Run Time : 1.37
INFO:root:2019-05-12 02:02:54, Epoch : 1, Step : 3556, Training Loss : 0.29953, Training Acc : 0.847, Run Time : 1.28
INFO:root:2019-05-12 02:03:00, Epoch : 1, Step : 3557, Training Loss : 0.42417, Training Acc : 0.792, Run Time : 5.91
INFO:root:2019-05-12 02:03:01, Epoch : 1, Step : 3558, Training Loss : 0.27392, Training Acc : 0.850, Run Time : 1.40
INFO:root:2019-05-12 02:03:03, Epoch : 1, Step : 3559, Training Loss : 0.25073, Training Acc : 0.922, Run Time : 1.53
INFO:root:2019-05-12 02:03:12, Epoch : 1, Step : 3560, Training Loss : 0.18659, Training Acc : 0.950, Run Time : 9.81
INFO:root:2019-05-12 02:03:14, Epoch : 1, Step : 3561, Training Loss : 0.18105, Training Acc : 0.911, Run Time : 1.29
INFO:root:2019-05-12 02:03:15, Epoch : 1, Step : 3562, Training Loss : 0.14228, Training Acc : 0.967, Run Time : 1.75
INFO:root:2019-05-12 02:03:25, Epoch : 1, Step : 3563, Training Loss : 0.13804, Training Acc : 0.950, Run Time : 9.56
INFO:root:2019-05-12 02:03:28, Epoch : 1, Step : 3564, Training Loss : 0.14332, Training Acc : 0.953, Run Time : 3.02
INFO:root:2019-05-12 02:03:34, Epoch : 1, Step : 3565, Training Loss : 0.18718, Training Acc : 0.953, Run Time : 5.80
INFO:root:2019-05-12 02:03:36, Epoch : 1, Step : 3566, Training Loss : 0.25344, Training Acc : 0.939, Run Time : 1.78
INFO:root:2019-05-12 02:03:42, Epoch : 1, Step : 3567, Training Loss : 0.17054, Training Acc : 0.944, Run Time : 6.22
INFO:root:2019-05-12 02:03:43, Epoch : 1, Step : 3568, Training Loss : 0.31255, Training Acc : 0.861, Run Time : 1.14
INFO:root:2019-05-12 02:03:44, Epoch : 1, Step : 3569, Training Loss : 0.21624, Training Acc : 0.950, Run Time : 1.29
INFO:root:2019-05-12 02:03:52, Epoch : 1, Step : 3570, Training Loss : 0.21757, Training Acc : 0.933, Run Time : 8.16
INFO:root:2019-05-12 02:03:54, Epoch : 1, Step : 3571, Training Loss : 0.49638, Training Acc : 0.744, Run Time : 1.17
INFO:root:2019-05-12 02:03:55, Epoch : 1, Step : 3572, Training Loss : 0.17360, Training Acc : 0.967, Run Time : 1.31
INFO:root:2019-05-12 02:03:59, Epoch : 1, Step : 3573, Training Loss : 0.16342, Training Acc : 0.972, Run Time : 4.61
INFO:root:2019-05-12 02:04:01, Epoch : 1, Step : 3574, Training Loss : 0.43407, Training Acc : 0.775, Run Time : 1.50
INFO:root:2019-05-12 02:04:05, Epoch : 1, Step : 3575, Training Loss : 0.39579, Training Acc : 0.781, Run Time : 4.43
INFO:root:2019-05-12 02:04:07, Epoch : 1, Step : 3576, Training Loss : 0.53646, Training Acc : 0.675, Run Time : 1.22
INFO:root:2019-05-12 02:04:08, Epoch : 1, Step : 3577, Training Loss : 0.61414, Training Acc : 0.703, Run Time : 1.28
INFO:root:2019-05-12 02:04:09, Epoch : 1, Step : 3578, Training Loss : 0.56712, Training Acc : 0.653, Run Time : 1.49
INFO:root:2019-05-12 02:04:13, Epoch : 1, Step : 3579, Training Loss : 0.45262, Training Acc : 0.819, Run Time : 4.10
INFO:root:2019-05-12 02:04:15, Epoch : 1, Step : 3580, Training Loss : 0.40061, Training Acc : 0.839, Run Time : 1.23
INFO:root:2019-05-12 02:04:23, Epoch : 1, Step : 3581, Training Loss : 0.41916, Training Acc : 0.708, Run Time : 8.29
INFO:root:2019-05-12 02:04:24, Epoch : 1, Step : 3582, Training Loss : 0.59745, Training Acc : 0.647, Run Time : 1.35
INFO:root:2019-05-12 02:04:27, Epoch : 1, Step : 3583, Training Loss : 0.25385, Training Acc : 0.911, Run Time : 2.94
INFO:root:2019-05-12 02:04:30, Epoch : 1, Step : 3584, Training Loss : 0.50539, Training Acc : 0.728, Run Time : 2.30
INFO:root:2019-05-12 02:04:33, Epoch : 1, Step : 3585, Training Loss : 0.22200, Training Acc : 0.900, Run Time : 3.80
INFO:root:2019-05-12 02:04:36, Epoch : 1, Step : 3586, Training Loss : 0.24526, Training Acc : 0.911, Run Time : 2.20
INFO:root:2019-05-12 02:04:37, Epoch : 1, Step : 3587, Training Loss : 0.16810, Training Acc : 0.942, Run Time : 1.40
INFO:root:2019-05-12 02:04:38, Epoch : 1, Step : 3588, Training Loss : 0.51553, Training Acc : 0.753, Run Time : 1.13
INFO:root:2019-05-12 02:04:39, Epoch : 1, Step : 3589, Training Loss : 0.11441, Training Acc : 0.981, Run Time : 1.31
INFO:root:2019-05-12 02:04:44, Epoch : 1, Step : 3590, Training Loss : 0.23388, Training Acc : 0.931, Run Time : 4.75
INFO:root:2019-05-12 02:04:46, Epoch : 1, Step : 3591, Training Loss : 0.40722, Training Acc : 0.800, Run Time : 2.13
INFO:root:2019-05-12 02:04:51, Epoch : 1, Step : 3592, Training Loss : 0.39385, Training Acc : 0.792, Run Time : 5.15
INFO:root:2019-05-12 02:04:53, Epoch : 1, Step : 3593, Training Loss : 0.16082, Training Acc : 0.942, Run Time : 1.22
INFO:root:2019-05-12 02:04:54, Epoch : 1, Step : 3594, Training Loss : 0.25168, Training Acc : 0.900, Run Time : 1.14
INFO:root:2019-05-12 02:05:00, Epoch : 1, Step : 3595, Training Loss : 0.28590, Training Acc : 0.858, Run Time : 6.01
INFO:root:2019-05-12 02:05:01, Epoch : 1, Step : 3596, Training Loss : 0.57996, Training Acc : 0.711, Run Time : 1.38
INFO:root:2019-05-12 02:05:02, Epoch : 1, Step : 3597, Training Loss : 0.34345, Training Acc : 0.781, Run Time : 1.31
INFO:root:2019-05-12 02:05:09, Epoch : 1, Step : 3598, Training Loss : 0.42109, Training Acc : 0.728, Run Time : 6.22
INFO:root:2019-05-12 02:05:10, Epoch : 1, Step : 3599, Training Loss : 0.31604, Training Acc : 0.847, Run Time : 1.14
INFO:root:2019-05-12 02:05:18, Epoch : 1, Step : 3600, Training Loss : 0.32485, Training Acc : 0.822, Run Time : 8.38
INFO:root:2019-05-12 02:05:20, Epoch : 1, Step : 3601, Training Loss : 0.30149, Training Acc : 0.831, Run Time : 1.99
INFO:root:2019-05-12 02:05:27, Epoch : 1, Step : 3602, Training Loss : 0.29780, Training Acc : 0.886, Run Time : 7.06
INFO:root:2019-05-12 02:05:29, Epoch : 1, Step : 3603, Training Loss : 0.21385, Training Acc : 0.939, Run Time : 1.28
INFO:root:2019-05-12 02:05:36, Epoch : 1, Step : 3604, Training Loss : 0.45717, Training Acc : 0.744, Run Time : 7.05
INFO:root:2019-05-12 02:05:38, Epoch : 1, Step : 3605, Training Loss : 0.35063, Training Acc : 0.875, Run Time : 1.99
INFO:root:2019-05-12 02:05:39, Epoch : 1, Step : 3606, Training Loss : 0.27709, Training Acc : 0.903, Run Time : 1.49
INFO:root:2019-05-12 02:05:47, Epoch : 1, Step : 3607, Training Loss : 0.26732, Training Acc : 0.878, Run Time : 7.79
INFO:root:2019-05-12 02:05:49, Epoch : 1, Step : 3608, Training Loss : 0.38913, Training Acc : 0.856, Run Time : 1.66
INFO:root:2019-05-12 02:05:50, Epoch : 1, Step : 3609, Training Loss : 0.30902, Training Acc : 0.869, Run Time : 1.59
INFO:root:2019-05-12 02:05:53, Epoch : 1, Step : 3610, Training Loss : 0.17560, Training Acc : 0.953, Run Time : 2.48
INFO:root:2019-05-12 02:05:54, Epoch : 1, Step : 3611, Training Loss : 0.25703, Training Acc : 0.897, Run Time : 1.38
INFO:root:2019-05-12 02:05:57, Epoch : 1, Step : 3612, Training Loss : 0.36139, Training Acc : 0.856, Run Time : 2.94
INFO:root:2019-05-12 02:05:58, Epoch : 1, Step : 3613, Training Loss : 0.35597, Training Acc : 0.858, Run Time : 1.18
INFO:root:2019-05-12 02:05:59, Epoch : 1, Step : 3614, Training Loss : 0.26617, Training Acc : 0.900, Run Time : 1.33
INFO:root:2019-05-12 02:06:07, Epoch : 1, Step : 3615, Training Loss : 0.31315, Training Acc : 0.825, Run Time : 7.40
INFO:root:2019-05-12 02:06:08, Epoch : 1, Step : 3616, Training Loss : 0.26179, Training Acc : 0.878, Run Time : 1.34
INFO:root:2019-05-12 02:06:10, Epoch : 1, Step : 3617, Training Loss : 0.23710, Training Acc : 0.908, Run Time : 1.64
INFO:root:2019-05-12 02:06:20, Epoch : 1, Step : 3618, Training Loss : 0.28115, Training Acc : 0.856, Run Time : 9.99
INFO:root:2019-05-12 02:06:21, Epoch : 1, Step : 3619, Training Loss : 0.21630, Training Acc : 0.919, Run Time : 1.16
INFO:root:2019-05-12 02:06:23, Epoch : 1, Step : 3620, Training Loss : 0.21062, Training Acc : 0.922, Run Time : 2.03
INFO:root:2019-05-12 02:06:30, Epoch : 1, Step : 3621, Training Loss : 0.20560, Training Acc : 0.911, Run Time : 7.06
INFO:root:2019-05-12 02:06:32, Epoch : 1, Step : 3622, Training Loss : 0.21252, Training Acc : 0.908, Run Time : 1.54
INFO:root:2019-05-12 02:06:33, Epoch : 1, Step : 3623, Training Loss : 0.18588, Training Acc : 0.931, Run Time : 1.86
INFO:root:2019-05-12 02:06:35, Epoch : 1, Step : 3624, Training Loss : 0.20674, Training Acc : 0.908, Run Time : 1.42
INFO:root:2019-05-12 02:06:36, Epoch : 1, Step : 3625, Training Loss : 0.18543, Training Acc : 0.925, Run Time : 1.49
INFO:root:2019-05-12 02:06:41, Epoch : 1, Step : 3626, Training Loss : 0.21076, Training Acc : 0.933, Run Time : 4.31
INFO:root:2019-05-12 02:06:42, Epoch : 1, Step : 3627, Training Loss : 0.20803, Training Acc : 0.922, Run Time : 1.63
INFO:root:2019-05-12 02:06:49, Epoch : 1, Step : 3628, Training Loss : 0.11501, Training Acc : 0.961, Run Time : 6.74
INFO:root:2019-05-12 02:06:50, Epoch : 1, Step : 3629, Training Loss : 0.13083, Training Acc : 0.961, Run Time : 1.32
INFO:root:2019-05-12 02:06:52, Epoch : 1, Step : 3630, Training Loss : 0.15801, Training Acc : 0.964, Run Time : 1.32
INFO:root:2019-05-12 02:06:57, Epoch : 1, Step : 3631, Training Loss : 0.22638, Training Acc : 0.914, Run Time : 5.59
INFO:root:2019-05-12 02:06:59, Epoch : 1, Step : 3632, Training Loss : 0.09752, Training Acc : 0.981, Run Time : 1.33
INFO:root:2019-05-12 02:07:00, Epoch : 1, Step : 3633, Training Loss : 0.15447, Training Acc : 0.953, Run Time : 1.17
INFO:root:2019-05-12 02:07:08, Epoch : 1, Step : 3634, Training Loss : 0.10789, Training Acc : 0.969, Run Time : 8.75
INFO:root:2019-05-12 02:07:10, Epoch : 1, Step : 3635, Training Loss : 0.12921, Training Acc : 0.967, Run Time : 1.59
INFO:root:2019-05-12 02:07:16, Epoch : 1, Step : 3636, Training Loss : 0.21907, Training Acc : 0.931, Run Time : 5.50
INFO:root:2019-05-12 02:07:17, Epoch : 1, Step : 3637, Training Loss : 0.14727, Training Acc : 0.950, Run Time : 1.74
INFO:root:2019-05-12 02:07:24, Epoch : 1, Step : 3638, Training Loss : 0.15920, Training Acc : 0.967, Run Time : 6.79
INFO:root:2019-05-12 02:07:25, Epoch : 1, Step : 3639, Training Loss : 0.19067, Training Acc : 0.961, Run Time : 1.26
INFO:root:2019-05-12 02:07:33, Epoch : 1, Step : 3640, Training Loss : 0.14392, Training Acc : 0.944, Run Time : 7.26
INFO:root:2019-05-12 02:07:34, Epoch : 1, Step : 3641, Training Loss : 0.16997, Training Acc : 0.939, Run Time : 1.13
INFO:root:2019-05-12 02:07:35, Epoch : 1, Step : 3642, Training Loss : 0.18582, Training Acc : 0.947, Run Time : 1.38
INFO:root:2019-05-12 02:07:42, Epoch : 1, Step : 3643, Training Loss : 0.22095, Training Acc : 0.906, Run Time : 6.64
INFO:root:2019-05-12 02:07:43, Epoch : 1, Step : 3644, Training Loss : 0.18811, Training Acc : 0.933, Run Time : 1.28
INFO:root:2019-05-12 02:07:44, Epoch : 1, Step : 3645, Training Loss : 0.20967, Training Acc : 0.933, Run Time : 1.34
INFO:root:2019-05-12 02:07:50, Epoch : 1, Step : 3646, Training Loss : 0.18003, Training Acc : 0.931, Run Time : 5.66
INFO:root:2019-05-12 02:07:51, Epoch : 1, Step : 3647, Training Loss : 0.31745, Training Acc : 0.914, Run Time : 1.14
INFO:root:2019-05-12 02:07:52, Epoch : 1, Step : 3648, Training Loss : 0.28504, Training Acc : 0.925, Run Time : 1.17
INFO:root:2019-05-12 02:08:01, Epoch : 1, Step : 3649, Training Loss : 0.22086, Training Acc : 0.939, Run Time : 9.16
INFO:root:2019-05-12 02:08:03, Epoch : 1, Step : 3650, Training Loss : 0.57944, Training Acc : 0.719, Run Time : 1.43
INFO:root:2019-05-12 02:08:04, Epoch : 1, Step : 3651, Training Loss : 0.63749, Training Acc : 0.678, Run Time : 1.29
INFO:root:2019-05-12 02:08:13, Epoch : 1, Step : 3652, Training Loss : 0.51441, Training Acc : 0.742, Run Time : 8.42
INFO:root:2019-05-12 02:08:14, Epoch : 1, Step : 3653, Training Loss : 0.40704, Training Acc : 0.794, Run Time : 1.46
INFO:root:2019-05-12 02:08:16, Epoch : 1, Step : 3654, Training Loss : 0.37855, Training Acc : 0.911, Run Time : 1.68
INFO:root:2019-05-12 02:08:22, Epoch : 1, Step : 3655, Training Loss : 0.46881, Training Acc : 0.728, Run Time : 6.53
INFO:root:2019-05-12 02:08:24, Epoch : 1, Step : 3656, Training Loss : 0.32621, Training Acc : 0.886, Run Time : 1.40
INFO:root:2019-05-12 02:08:26, Epoch : 1, Step : 3657, Training Loss : 0.63169, Training Acc : 0.592, Run Time : 2.37
INFO:root:2019-05-12 02:08:35, Epoch : 1, Step : 3658, Training Loss : 0.26396, Training Acc : 0.894, Run Time : 8.94
INFO:root:2019-05-12 02:08:38, Epoch : 1, Step : 3659, Training Loss : 0.38516, Training Acc : 0.869, Run Time : 2.62
INFO:root:2019-05-12 02:08:44, Epoch : 1, Step : 3660, Training Loss : 0.25343, Training Acc : 0.942, Run Time : 6.84
INFO:root:2019-05-12 02:08:46, Epoch : 1, Step : 3661, Training Loss : 0.35983, Training Acc : 0.897, Run Time : 1.14
INFO:root:2019-05-12 02:08:54, Epoch : 1, Step : 3662, Training Loss : 0.27546, Training Acc : 0.925, Run Time : 7.99
INFO:root:2019-05-12 02:08:55, Epoch : 1, Step : 3663, Training Loss : 0.22665, Training Acc : 0.919, Run Time : 1.33
INFO:root:2019-05-12 02:09:02, Epoch : 1, Step : 3664, Training Loss : 0.24890, Training Acc : 0.956, Run Time : 6.98
INFO:root:2019-05-12 02:09:03, Epoch : 1, Step : 3665, Training Loss : 0.22013, Training Acc : 0.950, Run Time : 1.19
INFO:root:2019-05-12 02:09:04, Epoch : 1, Step : 3666, Training Loss : 0.30465, Training Acc : 0.914, Run Time : 1.26
INFO:root:2019-05-12 02:09:16, Epoch : 1, Step : 3667, Training Loss : 0.27455, Training Acc : 0.908, Run Time : 11.54
INFO:root:2019-05-12 02:09:17, Epoch : 1, Step : 3668, Training Loss : 0.34897, Training Acc : 0.825, Run Time : 1.13
INFO:root:2019-05-12 02:09:19, Epoch : 1, Step : 3669, Training Loss : 0.27668, Training Acc : 0.908, Run Time : 1.56
INFO:root:2019-05-12 02:09:23, Epoch : 1, Step : 3670, Training Loss : 0.33704, Training Acc : 0.925, Run Time : 4.22
INFO:root:2019-05-12 02:09:24, Epoch : 1, Step : 3671, Training Loss : 0.35706, Training Acc : 0.797, Run Time : 1.33
INFO:root:2019-05-12 02:09:25, Epoch : 1, Step : 3672, Training Loss : 0.22007, Training Acc : 0.939, Run Time : 1.19
INFO:root:2019-05-12 02:09:34, Epoch : 1, Step : 3673, Training Loss : 0.27930, Training Acc : 0.900, Run Time : 8.69
INFO:root:2019-05-12 02:09:35, Epoch : 1, Step : 3674, Training Loss : 0.60126, Training Acc : 0.669, Run Time : 1.18
INFO:root:2019-05-12 02:09:37, Epoch : 1, Step : 3675, Training Loss : 0.71981, Training Acc : 0.542, Run Time : 1.49
INFO:root:2019-05-12 02:09:43, Epoch : 1, Step : 3676, Training Loss : 0.44264, Training Acc : 0.867, Run Time : 6.61
INFO:root:2019-05-12 02:09:45, Epoch : 1, Step : 3677, Training Loss : 0.43591, Training Acc : 0.828, Run Time : 1.47
INFO:root:2019-05-12 02:09:50, Epoch : 1, Step : 3678, Training Loss : 0.86185, Training Acc : 0.539, Run Time : 4.88
INFO:root:2019-05-12 02:09:51, Epoch : 1, Step : 3679, Training Loss : 0.54162, Training Acc : 0.772, Run Time : 1.20
INFO:root:2019-05-12 02:09:52, Epoch : 1, Step : 3680, Training Loss : 0.64219, Training Acc : 0.675, Run Time : 1.44
INFO:root:2019-05-12 02:09:59, Epoch : 1, Step : 3681, Training Loss : 0.49044, Training Acc : 0.764, Run Time : 6.40
INFO:root:2019-05-12 02:10:00, Epoch : 1, Step : 3682, Training Loss : 0.71562, Training Acc : 0.642, Run Time : 1.55
INFO:root:2019-05-12 02:10:05, Epoch : 1, Step : 3683, Training Loss : 0.50255, Training Acc : 0.808, Run Time : 4.57
INFO:root:2019-05-12 02:10:07, Epoch : 1, Step : 3684, Training Loss : 0.47102, Training Acc : 0.786, Run Time : 1.79
INFO:root:2019-05-12 02:10:13, Epoch : 1, Step : 3685, Training Loss : 0.41186, Training Acc : 0.847, Run Time : 6.46
INFO:root:2019-05-12 02:10:14, Epoch : 1, Step : 3686, Training Loss : 0.33039, Training Acc : 0.867, Run Time : 1.14
INFO:root:2019-05-12 02:10:19, Epoch : 1, Step : 3687, Training Loss : 0.32658, Training Acc : 0.881, Run Time : 4.78
INFO:root:2019-05-12 02:10:20, Epoch : 1, Step : 3688, Training Loss : 0.31017, Training Acc : 0.864, Run Time : 1.27
INFO:root:2019-05-12 02:10:22, Epoch : 1, Step : 3689, Training Loss : 0.31733, Training Acc : 0.858, Run Time : 1.26
INFO:root:2019-05-12 02:10:28, Epoch : 1, Step : 3690, Training Loss : 0.22911, Training Acc : 0.914, Run Time : 6.39
INFO:root:2019-05-12 02:10:29, Epoch : 1, Step : 3691, Training Loss : 0.39637, Training Acc : 0.844, Run Time : 1.27
INFO:root:2019-05-12 02:10:30, Epoch : 1, Step : 3692, Training Loss : 0.21560, Training Acc : 0.944, Run Time : 1.13
INFO:root:2019-05-12 02:10:32, Epoch : 1, Step : 3693, Training Loss : 0.64020, Training Acc : 0.664, Run Time : 1.27
INFO:root:2019-05-12 02:10:36, Epoch : 1, Step : 3694, Training Loss : 0.27554, Training Acc : 0.886, Run Time : 4.68
INFO:root:2019-05-12 02:10:37, Epoch : 1, Step : 3695, Training Loss : 0.32178, Training Acc : 0.869, Run Time : 1.14
INFO:root:2019-05-12 02:10:41, Epoch : 1, Step : 3696, Training Loss : 0.46647, Training Acc : 0.758, Run Time : 3.60
INFO:root:2019-05-12 02:10:42, Epoch : 1, Step : 3697, Training Loss : 0.39942, Training Acc : 0.808, Run Time : 1.35
INFO:root:2019-05-12 02:10:44, Epoch : 1, Step : 3698, Training Loss : 0.47464, Training Acc : 0.744, Run Time : 1.37
INFO:root:2019-05-12 02:10:46, Epoch : 1, Step : 3699, Training Loss : 0.34830, Training Acc : 0.839, Run Time : 1.80
INFO:root:2019-05-12 02:10:54, Epoch : 1, Step : 3700, Training Loss : 0.47383, Training Acc : 0.761, Run Time : 8.02
INFO:root:2019-05-12 02:10:55, Epoch : 1, Step : 3701, Training Loss : 0.36743, Training Acc : 0.786, Run Time : 1.56
INFO:root:2019-05-12 02:10:57, Epoch : 1, Step : 3702, Training Loss : 0.44799, Training Acc : 0.792, Run Time : 1.76
INFO:root:2019-05-12 02:11:04, Epoch : 1, Step : 3703, Training Loss : 0.22351, Training Acc : 0.928, Run Time : 6.72
INFO:root:2019-05-12 02:11:05, Epoch : 1, Step : 3704, Training Loss : 0.39658, Training Acc : 0.844, Run Time : 1.41
INFO:root:2019-05-12 02:11:13, Epoch : 1, Step : 3705, Training Loss : 0.39981, Training Acc : 0.831, Run Time : 7.57
INFO:root:2019-05-12 02:11:14, Epoch : 1, Step : 3706, Training Loss : 0.39666, Training Acc : 0.828, Run Time : 1.71
INFO:root:2019-05-12 02:11:18, Epoch : 1, Step : 3707, Training Loss : 0.42212, Training Acc : 0.797, Run Time : 4.23
INFO:root:2019-05-12 02:11:23, Epoch : 1, Step : 3708, Training Loss : 0.27818, Training Acc : 0.908, Run Time : 4.62
INFO:root:2019-05-12 02:11:24, Epoch : 1, Step : 3709, Training Loss : 0.23737, Training Acc : 0.942, Run Time : 1.13
INFO:root:2019-05-12 02:11:26, Epoch : 1, Step : 3710, Training Loss : 0.21986, Training Acc : 0.925, Run Time : 1.68
INFO:root:2019-05-12 02:11:35, Epoch : 1, Step : 3711, Training Loss : 0.28946, Training Acc : 0.861, Run Time : 9.50
INFO:root:2019-05-12 02:11:37, Epoch : 1, Step : 3712, Training Loss : 0.34464, Training Acc : 0.872, Run Time : 1.19
INFO:root:2019-05-12 02:11:38, Epoch : 1, Step : 3713, Training Loss : 0.26663, Training Acc : 0.908, Run Time : 1.13
INFO:root:2019-05-12 02:11:39, Epoch : 1, Step : 3714, Training Loss : 0.43536, Training Acc : 0.769, Run Time : 1.24
INFO:root:2019-05-12 02:11:45, Epoch : 1, Step : 3715, Training Loss : 0.25010, Training Acc : 0.861, Run Time : 5.85
INFO:root:2019-05-12 02:11:47, Epoch : 1, Step : 3716, Training Loss : 0.27064, Training Acc : 0.878, Run Time : 1.77
INFO:root:2019-05-12 02:11:55, Epoch : 1, Step : 3717, Training Loss : 0.30361, Training Acc : 0.872, Run Time : 8.43
INFO:root:2019-05-12 02:11:57, Epoch : 1, Step : 3718, Training Loss : 0.33070, Training Acc : 0.878, Run Time : 1.57
INFO:root:2019-05-12 02:12:04, Epoch : 1, Step : 3719, Training Loss : 0.24644, Training Acc : 0.897, Run Time : 7.30
INFO:root:2019-05-12 02:12:10, Epoch : 1, Step : 3720, Training Loss : 0.26165, Training Acc : 0.925, Run Time : 6.13
INFO:root:2019-05-12 02:12:11, Epoch : 1, Step : 3721, Training Loss : 0.28862, Training Acc : 0.925, Run Time : 1.31
INFO:root:2019-05-12 02:12:12, Epoch : 1, Step : 3722, Training Loss : 0.14551, Training Acc : 0.969, Run Time : 1.13
INFO:root:2019-05-12 02:12:16, Epoch : 1, Step : 3723, Training Loss : 0.51233, Training Acc : 0.722, Run Time : 3.52
INFO:root:2019-05-12 02:12:19, Epoch : 1, Step : 3724, Training Loss : 0.12507, Training Acc : 0.967, Run Time : 2.61
INFO:root:2019-05-12 02:12:26, Epoch : 1, Step : 3725, Training Loss : 0.26256, Training Acc : 0.903, Run Time : 7.60
INFO:root:2019-05-12 02:12:27, Epoch : 1, Step : 3726, Training Loss : 0.11598, Training Acc : 0.969, Run Time : 1.31
INFO:root:2019-05-12 02:12:31, Epoch : 1, Step : 3727, Training Loss : 0.18137, Training Acc : 0.939, Run Time : 3.07
INFO:root:2019-05-12 02:12:34, Epoch : 1, Step : 3728, Training Loss : 0.17312, Training Acc : 0.947, Run Time : 3.07
INFO:root:2019-05-12 02:12:35, Epoch : 1, Step : 3729, Training Loss : 0.16003, Training Acc : 0.964, Run Time : 1.18
INFO:root:2019-05-12 02:12:41, Epoch : 1, Step : 3730, Training Loss : 0.24222, Training Acc : 0.908, Run Time : 6.27
INFO:root:2019-05-12 02:12:43, Epoch : 1, Step : 3731, Training Loss : 0.51209, Training Acc : 0.703, Run Time : 1.46
INFO:root:2019-05-12 02:12:44, Epoch : 1, Step : 3732, Training Loss : 0.15968, Training Acc : 0.986, Run Time : 1.18
INFO:root:2019-05-12 02:12:50, Epoch : 1, Step : 3733, Training Loss : 0.32570, Training Acc : 0.875, Run Time : 5.88
INFO:root:2019-05-12 02:12:51, Epoch : 1, Step : 3734, Training Loss : 0.47459, Training Acc : 0.817, Run Time : 1.60
INFO:root:2019-05-12 02:12:53, Epoch : 1, Step : 3735, Training Loss : 0.35313, Training Acc : 0.889, Run Time : 1.47
INFO:root:2019-05-12 02:12:58, Epoch : 1, Step : 3736, Training Loss : 0.47320, Training Acc : 0.828, Run Time : 5.49
INFO:root:2019-05-12 02:13:00, Epoch : 1, Step : 3737, Training Loss : 0.37617, Training Acc : 0.836, Run Time : 1.54
INFO:root:2019-05-12 02:13:01, Epoch : 1, Step : 3738, Training Loss : 0.21800, Training Acc : 0.919, Run Time : 1.15
INFO:root:2019-05-12 02:13:06, Epoch : 1, Step : 3739, Training Loss : 0.21607, Training Acc : 0.928, Run Time : 5.50
INFO:root:2019-05-12 02:13:08, Epoch : 1, Step : 3740, Training Loss : 0.61454, Training Acc : 0.681, Run Time : 1.82
INFO:root:2019-05-12 02:13:16, Epoch : 1, Step : 3741, Training Loss : 0.35241, Training Acc : 0.831, Run Time : 8.19
INFO:root:2019-05-12 02:13:18, Epoch : 1, Step : 3742, Training Loss : 0.39785, Training Acc : 0.797, Run Time : 1.76
INFO:root:2019-05-12 02:13:23, Epoch : 1, Step : 3743, Training Loss : 0.41187, Training Acc : 0.786, Run Time : 4.97
INFO:root:2019-05-12 02:13:25, Epoch : 1, Step : 3744, Training Loss : 0.35825, Training Acc : 0.817, Run Time : 1.78
INFO:root:2019-05-12 02:13:31, Epoch : 1, Step : 3745, Training Loss : 0.20864, Training Acc : 0.922, Run Time : 5.69
INFO:root:2019-05-12 02:13:33, Epoch : 1, Step : 3746, Training Loss : 0.34782, Training Acc : 0.889, Run Time : 2.15
INFO:root:2019-05-12 02:13:39, Epoch : 1, Step : 3747, Training Loss : 0.24801, Training Acc : 0.914, Run Time : 6.22
INFO:root:2019-05-12 02:13:41, Epoch : 1, Step : 3748, Training Loss : 0.29504, Training Acc : 0.919, Run Time : 2.44
INFO:root:2019-05-12 02:13:47, Epoch : 1, Step : 3749, Training Loss : 0.25506, Training Acc : 0.939, Run Time : 5.68
INFO:root:2019-05-12 02:13:48, Epoch : 1, Step : 3750, Training Loss : 0.16001, Training Acc : 0.961, Run Time : 1.20
INFO:root:2019-05-12 02:13:50, Epoch : 1, Step : 3751, Training Loss : 0.37038, Training Acc : 0.847, Run Time : 1.31
INFO:root:2019-05-12 02:13:56, Epoch : 1, Step : 3752, Training Loss : 0.35162, Training Acc : 0.878, Run Time : 6.64
INFO:root:2019-05-12 02:13:57, Epoch : 1, Step : 3753, Training Loss : 0.18819, Training Acc : 0.925, Run Time : 1.15
INFO:root:2019-05-12 02:13:59, Epoch : 1, Step : 3754, Training Loss : 0.18854, Training Acc : 0.942, Run Time : 1.72
INFO:root:2019-05-12 02:14:00, Epoch : 1, Step : 3755, Training Loss : 0.13889, Training Acc : 0.964, Run Time : 1.13
INFO:root:2019-05-12 02:14:05, Epoch : 1, Step : 3756, Training Loss : 0.31934, Training Acc : 0.856, Run Time : 4.39
INFO:root:2019-05-12 02:14:07, Epoch : 1, Step : 3757, Training Loss : 0.25586, Training Acc : 0.881, Run Time : 2.15
INFO:root:2019-05-12 02:14:12, Epoch : 1, Step : 3758, Training Loss : 0.39045, Training Acc : 0.753, Run Time : 4.94
INFO:root:2019-05-12 02:14:13, Epoch : 1, Step : 3759, Training Loss : 0.46966, Training Acc : 0.756, Run Time : 1.55
INFO:root:2019-05-12 02:14:21, Epoch : 1, Step : 3760, Training Loss : 0.53149, Training Acc : 0.706, Run Time : 7.29
INFO:root:2019-05-12 02:14:22, Epoch : 1, Step : 3761, Training Loss : 0.36517, Training Acc : 0.853, Run Time : 1.47
INFO:root:2019-05-12 02:14:27, Epoch : 1, Step : 3762, Training Loss : 0.15110, Training Acc : 0.958, Run Time : 4.80
INFO:root:2019-05-12 02:14:28, Epoch : 1, Step : 3763, Training Loss : 0.25011, Training Acc : 0.944, Run Time : 1.21
INFO:root:2019-05-12 02:14:29, Epoch : 1, Step : 3764, Training Loss : 0.15588, Training Acc : 0.978, Run Time : 1.13
INFO:root:2019-05-12 02:14:31, Epoch : 1, Step : 3765, Training Loss : 0.31491, Training Acc : 0.847, Run Time : 1.48
INFO:root:2019-05-12 02:14:34, Epoch : 1, Step : 3766, Training Loss : 0.69161, Training Acc : 0.619, Run Time : 3.35
INFO:root:2019-05-12 02:14:35, Epoch : 1, Step : 3767, Training Loss : 0.47432, Training Acc : 0.828, Run Time : 1.36
INFO:root:2019-05-12 02:14:40, Epoch : 1, Step : 3768, Training Loss : 0.26491, Training Acc : 0.922, Run Time : 4.35
INFO:root:2019-05-12 02:14:41, Epoch : 1, Step : 3769, Training Loss : 0.14301, Training Acc : 0.956, Run Time : 1.40
INFO:root:2019-05-12 02:14:43, Epoch : 1, Step : 3770, Training Loss : 0.14050, Training Acc : 0.975, Run Time : 1.46
INFO:root:2019-05-12 02:14:52, Epoch : 1, Step : 3771, Training Loss : 0.33529, Training Acc : 0.828, Run Time : 9.06
INFO:root:2019-05-12 02:14:53, Epoch : 1, Step : 3772, Training Loss : 0.22758, Training Acc : 0.967, Run Time : 1.53
INFO:root:2019-05-12 02:14:55, Epoch : 1, Step : 3773, Training Loss : 0.43864, Training Acc : 0.775, Run Time : 1.44
INFO:root:2019-05-12 02:15:02, Epoch : 1, Step : 3774, Training Loss : 0.34075, Training Acc : 0.847, Run Time : 7.06
INFO:root:2019-05-12 02:15:03, Epoch : 1, Step : 3775, Training Loss : 0.25755, Training Acc : 0.883, Run Time : 1.40
INFO:root:2019-05-12 02:15:11, Epoch : 1, Step : 3776, Training Loss : 0.36721, Training Acc : 0.806, Run Time : 8.36
INFO:root:2019-05-12 02:15:13, Epoch : 1, Step : 3777, Training Loss : 0.30773, Training Acc : 0.844, Run Time : 1.68
INFO:root:2019-05-12 02:15:20, Epoch : 1, Step : 3778, Training Loss : 0.22991, Training Acc : 0.914, Run Time : 6.47
INFO:root:2019-05-12 02:15:21, Epoch : 1, Step : 3779, Training Loss : 0.14601, Training Acc : 0.942, Run Time : 1.66
INFO:root:2019-05-12 02:15:26, Epoch : 1, Step : 3780, Training Loss : 0.16038, Training Acc : 0.944, Run Time : 4.34
INFO:root:2019-05-12 02:15:27, Epoch : 1, Step : 3781, Training Loss : 0.12319, Training Acc : 0.978, Run Time : 1.73
INFO:root:2019-05-12 02:15:29, Epoch : 1, Step : 3782, Training Loss : 0.17032, Training Acc : 0.972, Run Time : 1.36
INFO:root:2019-05-12 02:15:30, Epoch : 1, Step : 3783, Training Loss : 0.23483, Training Acc : 0.917, Run Time : 1.64
INFO:root:2019-05-12 02:15:34, Epoch : 1, Step : 3784, Training Loss : 0.30998, Training Acc : 0.886, Run Time : 3.61
INFO:root:2019-05-12 02:15:35, Epoch : 1, Step : 3785, Training Loss : 0.39566, Training Acc : 0.853, Run Time : 1.31
INFO:root:2019-05-12 02:15:37, Epoch : 1, Step : 3786, Training Loss : 0.28688, Training Acc : 0.858, Run Time : 1.47
INFO:root:2019-05-12 02:15:41, Epoch : 1, Step : 3787, Training Loss : 0.29900, Training Acc : 0.936, Run Time : 4.69
INFO:root:2019-05-12 02:15:42, Epoch : 1, Step : 3788, Training Loss : 0.15519, Training Acc : 0.975, Run Time : 1.13
INFO:root:2019-05-12 02:15:44, Epoch : 1, Step : 3789, Training Loss : 0.17533, Training Acc : 0.928, Run Time : 1.16
INFO:root:2019-05-12 02:15:45, Epoch : 1, Step : 3790, Training Loss : 0.15805, Training Acc : 0.978, Run Time : 1.81
INFO:root:2019-05-12 02:15:51, Epoch : 1, Step : 3791, Training Loss : 0.11721, Training Acc : 0.992, Run Time : 5.68
INFO:root:2019-05-12 02:15:53, Epoch : 1, Step : 3792, Training Loss : 0.12700, Training Acc : 0.978, Run Time : 2.18
INFO:root:2019-05-12 02:15:59, Epoch : 1, Step : 3793, Training Loss : 0.21472, Training Acc : 0.933, Run Time : 5.78
INFO:root:2019-05-12 02:16:00, Epoch : 1, Step : 3794, Training Loss : 0.29085, Training Acc : 0.858, Run Time : 1.25
INFO:root:2019-05-12 02:16:02, Epoch : 1, Step : 3795, Training Loss : 0.26472, Training Acc : 0.917, Run Time : 1.48
INFO:root:2019-05-12 02:16:07, Epoch : 1, Step : 3796, Training Loss : 0.25081, Training Acc : 0.878, Run Time : 4.87
INFO:root:2019-05-12 02:16:08, Epoch : 1, Step : 3797, Training Loss : 0.26038, Training Acc : 0.914, Run Time : 1.48
INFO:root:2019-05-12 02:16:17, Epoch : 1, Step : 3798, Training Loss : 0.31347, Training Acc : 0.828, Run Time : 8.49
INFO:root:2019-05-12 02:16:18, Epoch : 1, Step : 3799, Training Loss : 0.35326, Training Acc : 0.850, Run Time : 1.14
INFO:root:2019-05-12 02:16:19, Epoch : 1, Step : 3800, Training Loss : 1.31089, Training Acc : 0.394, Run Time : 1.54
INFO:root:2019-05-12 02:16:28, Epoch : 1, Step : 3801, Training Loss : 0.25795, Training Acc : 0.903, Run Time : 8.71
INFO:root:2019-05-12 02:16:29, Epoch : 1, Step : 3802, Training Loss : 0.51071, Training Acc : 0.775, Run Time : 1.14
INFO:root:2019-05-12 02:16:31, Epoch : 1, Step : 3803, Training Loss : 0.64354, Training Acc : 0.689, Run Time : 1.42
INFO:root:2019-05-12 02:16:37, Epoch : 1, Step : 3804, Training Loss : 0.35668, Training Acc : 0.836, Run Time : 6.83
INFO:root:2019-05-12 02:16:39, Epoch : 1, Step : 3805, Training Loss : 0.39814, Training Acc : 0.803, Run Time : 1.12
INFO:root:2019-05-12 02:16:43, Epoch : 1, Step : 3806, Training Loss : 0.48210, Training Acc : 0.761, Run Time : 4.42
INFO:root:2019-05-12 02:16:44, Epoch : 1, Step : 3807, Training Loss : 0.53449, Training Acc : 0.742, Run Time : 1.26
INFO:root:2019-05-12 02:16:46, Epoch : 1, Step : 3808, Training Loss : 0.29455, Training Acc : 0.867, Run Time : 1.73
INFO:root:2019-05-12 02:16:53, Epoch : 1, Step : 3809, Training Loss : 0.38038, Training Acc : 0.822, Run Time : 7.27
INFO:root:2019-05-12 02:16:54, Epoch : 1, Step : 3810, Training Loss : 0.36161, Training Acc : 0.833, Run Time : 1.13
INFO:root:2019-05-12 02:16:56, Epoch : 1, Step : 3811, Training Loss : 0.52248, Training Acc : 0.744, Run Time : 1.36
INFO:root:2019-05-12 02:17:01, Epoch : 1, Step : 3812, Training Loss : 0.48597, Training Acc : 0.739, Run Time : 5.72
INFO:root:2019-05-12 02:17:03, Epoch : 1, Step : 3813, Training Loss : 0.33455, Training Acc : 0.822, Run Time : 1.77
INFO:root:2019-05-12 02:17:09, Epoch : 1, Step : 3814, Training Loss : 0.29626, Training Acc : 0.836, Run Time : 5.59
INFO:root:2019-05-12 02:17:10, Epoch : 1, Step : 3815, Training Loss : 0.33559, Training Acc : 0.839, Run Time : 1.15
INFO:root:2019-05-12 02:17:11, Epoch : 1, Step : 3816, Training Loss : 0.48164, Training Acc : 0.794, Run Time : 1.26
INFO:root:2019-05-12 02:17:13, Epoch : 1, Step : 3817, Training Loss : 0.60922, Training Acc : 0.689, Run Time : 2.28
INFO:root:2019-05-12 02:17:15, Epoch : 1, Step : 3818, Training Loss : 0.29844, Training Acc : 0.847, Run Time : 1.33
INFO:root:2019-05-12 02:17:18, Epoch : 1, Step : 3819, Training Loss : 0.23827, Training Acc : 0.939, Run Time : 3.10
INFO:root:2019-05-12 02:17:19, Epoch : 1, Step : 3820, Training Loss : 0.15869, Training Acc : 0.953, Run Time : 1.16
INFO:root:2019-05-12 02:17:20, Epoch : 1, Step : 3821, Training Loss : 0.19267, Training Acc : 0.961, Run Time : 1.37
INFO:root:2019-05-12 02:17:22, Epoch : 1, Step : 3822, Training Loss : 0.49541, Training Acc : 0.672, Run Time : 1.36
INFO:root:2019-05-12 02:17:26, Epoch : 1, Step : 3823, Training Loss : 0.20798, Training Acc : 0.964, Run Time : 4.69
INFO:root:2019-05-12 02:17:29, Epoch : 1, Step : 3824, Training Loss : 0.35164, Training Acc : 0.878, Run Time : 2.21
INFO:root:2019-05-12 02:17:37, Epoch : 1, Step : 3825, Training Loss : 0.36775, Training Acc : 0.847, Run Time : 8.12
INFO:root:2019-05-12 02:17:39, Epoch : 1, Step : 3826, Training Loss : 0.30835, Training Acc : 0.892, Run Time : 1.76
INFO:root:2019-05-12 02:17:45, Epoch : 1, Step : 3827, Training Loss : 0.26358, Training Acc : 0.922, Run Time : 6.05
INFO:root:2019-05-12 02:17:47, Epoch : 1, Step : 3828, Training Loss : 0.39455, Training Acc : 0.778, Run Time : 2.09
INFO:root:2019-05-12 02:17:54, Epoch : 1, Step : 3829, Training Loss : 0.26420, Training Acc : 0.925, Run Time : 7.34
INFO:root:2019-05-12 02:17:55, Epoch : 1, Step : 3830, Training Loss : 0.47052, Training Acc : 0.736, Run Time : 1.26
INFO:root:2019-05-12 02:17:57, Epoch : 1, Step : 3831, Training Loss : 0.43206, Training Acc : 0.711, Run Time : 1.20
INFO:root:2019-05-12 02:18:05, Epoch : 1, Step : 3832, Training Loss : 0.21628, Training Acc : 0.933, Run Time : 8.62
INFO:root:2019-05-12 02:18:07, Epoch : 1, Step : 3833, Training Loss : 0.17540, Training Acc : 0.964, Run Time : 1.69
INFO:root:2019-05-12 02:18:13, Epoch : 1, Step : 3834, Training Loss : 0.31765, Training Acc : 0.917, Run Time : 6.60
INFO:root:2019-05-12 02:18:15, Epoch : 1, Step : 3835, Training Loss : 0.31568, Training Acc : 0.914, Run Time : 1.13
INFO:root:2019-05-12 02:18:16, Epoch : 1, Step : 3836, Training Loss : 0.40414, Training Acc : 0.731, Run Time : 1.62
INFO:root:2019-05-12 02:18:23, Epoch : 1, Step : 3837, Training Loss : 0.70488, Training Acc : 0.583, Run Time : 6.72
INFO:root:2019-05-12 02:18:24, Epoch : 1, Step : 3838, Training Loss : 0.72265, Training Acc : 0.517, Run Time : 1.11
INFO:root:2019-05-12 02:18:25, Epoch : 1, Step : 3839, Training Loss : 0.20071, Training Acc : 0.989, Run Time : 1.26
INFO:root:2019-05-12 02:18:34, Epoch : 1, Step : 3840, Training Loss : 0.67746, Training Acc : 0.542, Run Time : 8.93
INFO:root:2019-05-12 02:18:36, Epoch : 1, Step : 3841, Training Loss : 0.40020, Training Acc : 0.789, Run Time : 1.80
INFO:root:2019-05-12 02:18:41, Epoch : 1, Step : 3842, Training Loss : 0.43224, Training Acc : 0.694, Run Time : 4.64
INFO:root:2019-05-12 02:18:42, Epoch : 1, Step : 3843, Training Loss : 0.31027, Training Acc : 0.889, Run Time : 1.12
INFO:root:2019-05-12 02:18:51, Epoch : 1, Step : 3844, Training Loss : 0.74848, Training Acc : 0.517, Run Time : 9.54
INFO:root:2019-05-12 02:18:52, Epoch : 1, Step : 3845, Training Loss : 0.35583, Training Acc : 0.967, Run Time : 1.18
INFO:root:2019-05-12 02:18:54, Epoch : 1, Step : 3846, Training Loss : 0.33492, Training Acc : 0.906, Run Time : 1.59
INFO:root:2019-05-12 02:19:00, Epoch : 1, Step : 3847, Training Loss : 0.35987, Training Acc : 0.892, Run Time : 6.10
INFO:root:2019-05-12 02:19:01, Epoch : 1, Step : 3848, Training Loss : 0.32480, Training Acc : 0.867, Run Time : 1.20
INFO:root:2019-05-12 02:19:08, Epoch : 1, Step : 3849, Training Loss : 0.23141, Training Acc : 0.964, Run Time : 6.44
INFO:root:2019-05-12 02:19:09, Epoch : 1, Step : 3850, Training Loss : 0.45400, Training Acc : 0.628, Run Time : 1.51
INFO:root:2019-05-12 02:19:13, Epoch : 1, Step : 3851, Training Loss : 0.25224, Training Acc : 0.928, Run Time : 3.97
INFO:root:2019-05-12 02:19:14, Epoch : 1, Step : 3852, Training Loss : 0.25826, Training Acc : 0.928, Run Time : 1.13
INFO:root:2019-05-12 02:19:16, Epoch : 1, Step : 3853, Training Loss : 0.25912, Training Acc : 0.939, Run Time : 1.88
INFO:root:2019-05-12 02:19:23, Epoch : 1, Step : 3854, Training Loss : 0.30039, Training Acc : 0.919, Run Time : 6.84
INFO:root:2019-05-12 02:19:24, Epoch : 1, Step : 3855, Training Loss : 0.14628, Training Acc : 0.972, Run Time : 1.17
INFO:root:2019-05-12 02:19:30, Epoch : 1, Step : 3856, Training Loss : 0.70706, Training Acc : 0.614, Run Time : 6.05
INFO:root:2019-05-12 02:19:32, Epoch : 1, Step : 3857, Training Loss : 0.45599, Training Acc : 0.731, Run Time : 1.13
INFO:root:2019-05-12 02:19:33, Epoch : 1, Step : 3858, Training Loss : 0.16303, Training Acc : 0.944, Run Time : 1.16
INFO:root:2019-05-12 02:19:37, Epoch : 1, Step : 3859, Training Loss : 0.23623, Training Acc : 0.914, Run Time : 4.67
INFO:root:2019-05-12 02:19:39, Epoch : 1, Step : 3860, Training Loss : 0.30794, Training Acc : 0.889, Run Time : 1.38
INFO:root:2019-05-12 02:19:40, Epoch : 1, Step : 3861, Training Loss : 0.33083, Training Acc : 0.914, Run Time : 1.58
INFO:root:2019-05-12 02:19:48, Epoch : 1, Step : 3862, Training Loss : 0.31607, Training Acc : 0.839, Run Time : 7.34
INFO:root:2019-05-12 02:19:49, Epoch : 1, Step : 3863, Training Loss : 0.24180, Training Acc : 0.903, Run Time : 1.23
INFO:root:2019-05-12 02:19:50, Epoch : 1, Step : 3864, Training Loss : 0.28816, Training Acc : 0.911, Run Time : 1.17
INFO:root:2019-05-12 02:19:53, Epoch : 1, Step : 3865, Training Loss : 0.54519, Training Acc : 0.853, Run Time : 3.35
INFO:root:2019-05-12 02:19:58, Epoch : 1, Step : 3866, Training Loss : 0.25205, Training Acc : 0.917, Run Time : 4.18
INFO:root:2019-05-12 02:19:59, Epoch : 1, Step : 3867, Training Loss : 0.20421, Training Acc : 0.944, Run Time : 1.40
INFO:root:2019-05-12 02:20:00, Epoch : 1, Step : 3868, Training Loss : 0.14226, Training Acc : 0.956, Run Time : 1.45
INFO:root:2019-05-12 02:20:10, Epoch : 1, Step : 3869, Training Loss : 0.19167, Training Acc : 0.936, Run Time : 9.17
INFO:root:2019-05-12 02:20:11, Epoch : 1, Step : 3870, Training Loss : 0.16160, Training Acc : 0.958, Run Time : 1.14
INFO:root:2019-05-12 02:20:12, Epoch : 1, Step : 3871, Training Loss : 0.19956, Training Acc : 0.953, Run Time : 1.47
INFO:root:2019-05-12 02:20:17, Epoch : 1, Step : 3872, Training Loss : 0.30486, Training Acc : 0.858, Run Time : 4.90
INFO:root:2019-05-12 02:20:19, Epoch : 1, Step : 3873, Training Loss : 0.21548, Training Acc : 0.944, Run Time : 2.06
INFO:root:2019-05-12 02:20:29, Epoch : 1, Step : 3874, Training Loss : 0.12894, Training Acc : 0.992, Run Time : 10.32
INFO:root:2019-05-12 02:20:31, Epoch : 1, Step : 3875, Training Loss : 0.30048, Training Acc : 0.875, Run Time : 1.96
INFO:root:2019-05-12 02:20:40, Epoch : 1, Step : 3876, Training Loss : 0.10309, Training Acc : 0.997, Run Time : 8.25
INFO:root:2019-05-12 02:20:41, Epoch : 1, Step : 3877, Training Loss : 0.14389, Training Acc : 0.956, Run Time : 1.53
INFO:root:2019-05-12 02:20:48, Epoch : 1, Step : 3878, Training Loss : 0.28446, Training Acc : 0.922, Run Time : 7.02
INFO:root:2019-05-12 02:20:49, Epoch : 1, Step : 3879, Training Loss : 0.09688, Training Acc : 1.000, Run Time : 1.14
INFO:root:2019-05-12 02:20:51, Epoch : 1, Step : 3880, Training Loss : 0.20034, Training Acc : 0.925, Run Time : 1.36
INFO:root:2019-05-12 02:20:57, Epoch : 1, Step : 3881, Training Loss : 0.16182, Training Acc : 0.964, Run Time : 5.92
INFO:root:2019-05-12 02:20:58, Epoch : 1, Step : 3882, Training Loss : 0.13641, Training Acc : 0.983, Run Time : 1.55
INFO:root:2019-05-12 02:21:00, Epoch : 1, Step : 3883, Training Loss : 0.10127, Training Acc : 0.994, Run Time : 2.09
INFO:root:2019-05-12 02:21:05, Epoch : 1, Step : 3884, Training Loss : 0.21840, Training Acc : 0.964, Run Time : 4.76
INFO:root:2019-05-12 02:21:07, Epoch : 1, Step : 3885, Training Loss : 0.24212, Training Acc : 0.947, Run Time : 1.81
INFO:root:2019-05-12 02:21:13, Epoch : 1, Step : 3886, Training Loss : 0.39904, Training Acc : 0.903, Run Time : 6.39
INFO:root:2019-05-12 02:21:15, Epoch : 1, Step : 3887, Training Loss : 0.30211, Training Acc : 0.903, Run Time : 1.50
INFO:root:2019-05-12 02:21:25, Epoch : 1, Step : 3888, Training Loss : 0.26447, Training Acc : 0.903, Run Time : 9.85
INFO:root:2019-05-12 02:21:26, Epoch : 1, Step : 3889, Training Loss : 0.21958, Training Acc : 0.947, Run Time : 1.72
INFO:root:2019-05-12 02:21:33, Epoch : 1, Step : 3890, Training Loss : 0.26319, Training Acc : 0.897, Run Time : 6.33
INFO:root:2019-05-12 02:21:34, Epoch : 1, Step : 3891, Training Loss : 0.35114, Training Acc : 0.894, Run Time : 1.41
INFO:root:2019-05-12 02:21:41, Epoch : 1, Step : 3892, Training Loss : 0.21205, Training Acc : 0.944, Run Time : 7.25
INFO:root:2019-05-12 02:21:43, Epoch : 1, Step : 3893, Training Loss : 0.37192, Training Acc : 0.875, Run Time : 1.27
INFO:root:2019-05-12 02:21:48, Epoch : 1, Step : 3894, Training Loss : 0.44835, Training Acc : 0.756, Run Time : 5.33
INFO:root:2019-05-12 02:21:49, Epoch : 1, Step : 3895, Training Loss : 0.68566, Training Acc : 0.617, Run Time : 1.17
INFO:root:2019-05-12 02:21:56, Epoch : 1, Step : 3896, Training Loss : 0.48659, Training Acc : 0.744, Run Time : 7.06
INFO:root:2019-05-12 02:21:57, Epoch : 1, Step : 3897, Training Loss : 0.37270, Training Acc : 0.867, Run Time : 1.24
INFO:root:2019-05-12 02:22:03, Epoch : 1, Step : 3898, Training Loss : 0.24430, Training Acc : 0.958, Run Time : 5.33
INFO:root:2019-05-12 02:22:04, Epoch : 1, Step : 3899, Training Loss : 0.14888, Training Acc : 0.969, Run Time : 1.56
INFO:root:2019-05-12 02:22:09, Epoch : 1, Step : 3900, Training Loss : 1.25160, Training Acc : 0.400, Run Time : 4.99
INFO:root:2019-05-12 02:22:12, Epoch : 1, Step : 3901, Training Loss : 0.65789, Training Acc : 0.747, Run Time : 2.26
INFO:root:2019-05-12 02:22:18, Epoch : 1, Step : 3902, Training Loss : 0.84287, Training Acc : 0.772, Run Time : 6.57
INFO:root:2019-05-12 02:22:19, Epoch : 1, Step : 3903, Training Loss : 0.59486, Training Acc : 0.856, Run Time : 1.34
INFO:root:2019-05-12 02:22:21, Epoch : 1, Step : 3904, Training Loss : 0.64700, Training Acc : 0.867, Run Time : 1.41
INFO:root:2019-05-12 02:22:31, Epoch : 1, Step : 3905, Training Loss : 0.44692, Training Acc : 0.853, Run Time : 10.11
INFO:root:2019-05-12 02:22:32, Epoch : 1, Step : 3906, Training Loss : 0.61286, Training Acc : 0.842, Run Time : 1.37
INFO:root:2019-05-12 02:22:39, Epoch : 1, Step : 3907, Training Loss : 0.66422, Training Acc : 0.839, Run Time : 6.60
INFO:root:2019-05-12 02:22:40, Epoch : 1, Step : 3908, Training Loss : 0.46889, Training Acc : 0.864, Run Time : 1.40
INFO:root:2019-05-12 02:22:42, Epoch : 1, Step : 3909, Training Loss : 0.44487, Training Acc : 0.853, Run Time : 1.93
INFO:root:2019-05-12 02:22:50, Epoch : 1, Step : 3910, Training Loss : 0.37458, Training Acc : 0.869, Run Time : 8.18
INFO:root:2019-05-12 02:22:52, Epoch : 1, Step : 3911, Training Loss : 0.38262, Training Acc : 0.850, Run Time : 1.35
INFO:root:2019-05-12 02:23:01, Epoch : 1, Step : 3912, Training Loss : 0.28750, Training Acc : 0.869, Run Time : 9.26
INFO:root:2019-05-12 02:23:03, Epoch : 1, Step : 3913, Training Loss : 0.24709, Training Acc : 0.881, Run Time : 1.59
INFO:root:2019-05-12 02:23:04, Epoch : 1, Step : 3914, Training Loss : 0.23675, Training Acc : 0.883, Run Time : 1.14
INFO:root:2019-05-12 02:23:05, Epoch : 1, Step : 3915, Training Loss : 0.27386, Training Acc : 0.864, Run Time : 1.16
INFO:root:2019-05-12 02:23:10, Epoch : 1, Step : 3916, Training Loss : 0.36650, Training Acc : 0.856, Run Time : 4.92
INFO:root:2019-05-12 02:23:11, Epoch : 1, Step : 3917, Training Loss : 0.39549, Training Acc : 0.836, Run Time : 1.36
INFO:root:2019-05-12 02:23:20, Epoch : 1, Step : 3918, Training Loss : 0.56497, Training Acc : 0.775, Run Time : 8.90
INFO:root:2019-05-12 02:23:21, Epoch : 1, Step : 3919, Training Loss : 0.41878, Training Acc : 0.831, Run Time : 1.28
INFO:root:2019-05-12 02:23:31, Epoch : 1, Step : 3920, Training Loss : 0.27050, Training Acc : 0.883, Run Time : 9.49
INFO:root:2019-05-12 02:23:32, Epoch : 1, Step : 3921, Training Loss : 0.45960, Training Acc : 0.792, Run Time : 1.13
INFO:root:2019-05-12 02:23:33, Epoch : 1, Step : 3922, Training Loss : 0.39315, Training Acc : 0.842, Run Time : 1.13
INFO:root:2019-05-12 02:23:35, Epoch : 1, Step : 3923, Training Loss : 0.30549, Training Acc : 0.842, Run Time : 1.52
INFO:root:2019-05-12 02:23:46, Epoch : 1, Step : 3924, Training Loss : 0.32113, Training Acc : 0.856, Run Time : 11.55
INFO:root:2019-05-12 02:23:47, Epoch : 1, Step : 3925, Training Loss : 0.40127, Training Acc : 0.803, Run Time : 1.15
INFO:root:2019-05-12 02:23:56, Epoch : 1, Step : 3926, Training Loss : 0.44900, Training Acc : 0.797, Run Time : 8.19
INFO:root:2019-05-12 02:23:57, Epoch : 1, Step : 3927, Training Loss : 0.37899, Training Acc : 0.831, Run Time : 1.63
INFO:root:2019-05-12 02:23:59, Epoch : 1, Step : 3928, Training Loss : 0.46793, Training Acc : 0.747, Run Time : 1.78
INFO:root:2019-05-12 02:24:05, Epoch : 1, Step : 3929, Training Loss : 0.48798, Training Acc : 0.769, Run Time : 6.14
INFO:root:2019-05-12 02:24:07, Epoch : 1, Step : 3930, Training Loss : 0.64187, Training Acc : 0.697, Run Time : 2.37
INFO:root:2019-05-12 02:24:16, Epoch : 1, Step : 3931, Training Loss : 0.42400, Training Acc : 0.786, Run Time : 8.56
INFO:root:2019-05-12 02:24:17, Epoch : 1, Step : 3932, Training Loss : 0.30507, Training Acc : 0.842, Run Time : 1.15
INFO:root:2019-05-12 02:24:18, Epoch : 1, Step : 3933, Training Loss : 0.33983, Training Acc : 0.856, Run Time : 1.15
INFO:root:2019-05-12 02:24:23, Epoch : 1, Step : 3934, Training Loss : 0.30920, Training Acc : 0.856, Run Time : 4.56
INFO:root:2019-05-12 02:24:24, Epoch : 1, Step : 3935, Training Loss : 0.36102, Training Acc : 0.839, Run Time : 1.26
INFO:root:2019-05-12 02:24:28, Epoch : 1, Step : 3936, Training Loss : 0.29037, Training Acc : 0.869, Run Time : 3.72
INFO:root:2019-05-12 02:24:30, Epoch : 1, Step : 3937, Training Loss : 0.28234, Training Acc : 0.872, Run Time : 2.11
INFO:root:2019-05-12 02:24:31, Epoch : 1, Step : 3938, Training Loss : 0.29086, Training Acc : 0.850, Run Time : 1.16
INFO:root:2019-05-12 02:24:39, Epoch : 1, Step : 3939, Training Loss : 0.28708, Training Acc : 0.875, Run Time : 8.17
INFO:root:2019-05-12 02:24:41, Epoch : 1, Step : 3940, Training Loss : 0.37268, Training Acc : 0.858, Run Time : 1.35
INFO:root:2019-05-12 02:24:42, Epoch : 1, Step : 3941, Training Loss : 0.29245, Training Acc : 0.900, Run Time : 1.56
INFO:root:2019-05-12 02:24:49, Epoch : 1, Step : 3942, Training Loss : 0.27141, Training Acc : 0.867, Run Time : 6.84
INFO:root:2019-05-12 02:24:51, Epoch : 1, Step : 3943, Training Loss : 0.28245, Training Acc : 0.853, Run Time : 1.85
INFO:root:2019-05-12 02:24:58, Epoch : 1, Step : 3944, Training Loss : 0.24983, Training Acc : 0.850, Run Time : 6.60
INFO:root:2019-05-12 02:24:59, Epoch : 1, Step : 3945, Training Loss : 0.20483, Training Acc : 0.917, Run Time : 1.21
INFO:root:2019-05-12 02:25:03, Epoch : 1, Step : 3946, Training Loss : 0.23067, Training Acc : 0.914, Run Time : 4.02
INFO:root:2019-05-12 02:25:07, Epoch : 1, Step : 3947, Training Loss : 0.22784, Training Acc : 0.908, Run Time : 4.60
INFO:root:2019-05-12 02:25:09, Epoch : 1, Step : 3948, Training Loss : 0.19416, Training Acc : 0.933, Run Time : 1.44
INFO:root:2019-05-12 02:25:10, Epoch : 1, Step : 3949, Training Loss : 0.16887, Training Acc : 0.961, Run Time : 1.17
INFO:root:2019-05-12 02:25:11, Epoch : 1, Step : 3950, Training Loss : 0.18817, Training Acc : 0.922, Run Time : 1.25
INFO:root:2019-05-12 02:25:18, Epoch : 1, Step : 3951, Training Loss : 0.20261, Training Acc : 0.922, Run Time : 6.37
INFO:root:2019-05-12 02:25:20, Epoch : 1, Step : 3952, Training Loss : 0.19335, Training Acc : 0.925, Run Time : 1.97
INFO:root:2019-05-12 02:25:22, Epoch : 1, Step : 3953, Training Loss : 0.19706, Training Acc : 0.922, Run Time : 1.99
INFO:root:2019-05-12 02:25:28, Epoch : 1, Step : 3954, Training Loss : 0.26472, Training Acc : 0.886, Run Time : 6.02
INFO:root:2019-05-12 02:25:29, Epoch : 1, Step : 3955, Training Loss : 0.27555, Training Acc : 0.883, Run Time : 1.49
INFO:root:2019-05-12 02:25:30, Epoch : 1, Step : 3956, Training Loss : 0.20836, Training Acc : 0.911, Run Time : 1.28
INFO:root:2019-05-12 02:25:33, Epoch : 1, Step : 3957, Training Loss : 0.17541, Training Acc : 0.908, Run Time : 2.54
INFO:root:2019-05-12 02:25:39, Epoch : 1, Step : 3958, Training Loss : 0.20901, Training Acc : 0.917, Run Time : 5.85
INFO:root:2019-05-12 02:25:40, Epoch : 1, Step : 3959, Training Loss : 0.33085, Training Acc : 0.853, Run Time : 1.30
INFO:root:2019-05-12 02:25:42, Epoch : 1, Step : 3960, Training Loss : 0.30813, Training Acc : 0.864, Run Time : 1.80
INFO:root:2019-05-12 02:25:47, Epoch : 1, Step : 3961, Training Loss : 0.23481, Training Acc : 0.886, Run Time : 5.39
INFO:root:2019-05-12 02:25:48, Epoch : 1, Step : 3962, Training Loss : 0.18794, Training Acc : 0.914, Run Time : 1.13
INFO:root:2019-05-12 02:25:49, Epoch : 1, Step : 3963, Training Loss : 0.20292, Training Acc : 0.906, Run Time : 1.13
INFO:root:2019-05-12 02:25:51, Epoch : 1, Step : 3964, Training Loss : 0.16398, Training Acc : 0.925, Run Time : 1.88
INFO:root:2019-05-12 02:25:53, Epoch : 1, Step : 3965, Training Loss : 0.17551, Training Acc : 0.908, Run Time : 1.34
INFO:root:2019-05-12 02:25:58, Epoch : 1, Step : 3966, Training Loss : 0.15477, Training Acc : 0.922, Run Time : 4.99
INFO:root:2019-05-12 02:26:00, Epoch : 1, Step : 3967, Training Loss : 0.18111, Training Acc : 0.922, Run Time : 1.79
INFO:root:2019-05-12 02:26:06, Epoch : 1, Step : 3968, Training Loss : 0.16796, Training Acc : 0.922, Run Time : 6.07
INFO:root:2019-05-12 02:26:07, Epoch : 1, Step : 3969, Training Loss : 0.27586, Training Acc : 0.889, Run Time : 1.67
INFO:root:2019-05-12 02:26:09, Epoch : 1, Step : 3970, Training Loss : 0.24499, Training Acc : 0.906, Run Time : 1.43
INFO:root:2019-05-12 02:26:14, Epoch : 1, Step : 3971, Training Loss : 0.11542, Training Acc : 0.942, Run Time : 5.76
INFO:root:2019-05-12 02:26:16, Epoch : 1, Step : 3972, Training Loss : 0.11152, Training Acc : 0.961, Run Time : 1.13
INFO:root:2019-05-12 02:26:25, Epoch : 1, Step : 3973, Training Loss : 0.08539, Training Acc : 0.978, Run Time : 9.09
INFO:root:2019-05-12 02:26:27, Epoch : 1, Step : 3974, Training Loss : 0.12351, Training Acc : 0.944, Run Time : 2.27
INFO:root:2019-05-12 02:26:34, Epoch : 1, Step : 3975, Training Loss : 0.13179, Training Acc : 0.947, Run Time : 7.35
INFO:root:2019-05-12 02:26:36, Epoch : 1, Step : 3976, Training Loss : 0.15624, Training Acc : 0.933, Run Time : 1.27
INFO:root:2019-05-12 02:26:44, Epoch : 1, Step : 3977, Training Loss : 0.30320, Training Acc : 0.906, Run Time : 8.67
INFO:root:2019-05-12 02:26:45, Epoch : 1, Step : 3978, Training Loss : 0.39187, Training Acc : 0.908, Run Time : 1.12
INFO:root:2019-05-12 02:26:54, Epoch : 1, Step : 3979, Training Loss : 0.31084, Training Acc : 0.939, Run Time : 9.15
INFO:root:2019-05-12 02:26:57, Epoch : 1, Step : 3980, Training Loss : 0.23865, Training Acc : 0.944, Run Time : 2.61
INFO:root:2019-05-12 02:27:02, Epoch : 1, Step : 3981, Training Loss : 0.20517, Training Acc : 0.944, Run Time : 4.95
INFO:root:2019-05-12 02:27:04, Epoch : 1, Step : 3982, Training Loss : 0.19695, Training Acc : 0.939, Run Time : 1.62
INFO:root:2019-05-12 02:27:09, Epoch : 1, Step : 3983, Training Loss : 0.17053, Training Acc : 0.944, Run Time : 5.30
INFO:root:2019-05-12 02:27:11, Epoch : 1, Step : 3984, Training Loss : 0.21282, Training Acc : 0.936, Run Time : 1.58
INFO:root:2019-05-12 02:27:17, Epoch : 1, Step : 3985, Training Loss : 0.25743, Training Acc : 0.925, Run Time : 6.80
INFO:root:2019-05-12 02:27:19, Epoch : 1, Step : 3986, Training Loss : 0.24392, Training Acc : 0.928, Run Time : 1.62
INFO:root:2019-05-12 02:27:23, Epoch : 1, Step : 3987, Training Loss : 0.20087, Training Acc : 0.939, Run Time : 4.22
INFO:root:2019-05-12 02:27:24, Epoch : 1, Step : 3988, Training Loss : 0.12492, Training Acc : 0.942, Run Time : 1.12
INFO:root:2019-05-12 02:27:27, Epoch : 1, Step : 3989, Training Loss : 0.19719, Training Acc : 0.919, Run Time : 2.50
INFO:root:2019-05-12 02:27:28, Epoch : 1, Step : 3990, Training Loss : 0.14838, Training Acc : 0.950, Run Time : 1.50
INFO:root:2019-05-12 02:27:30, Epoch : 1, Step : 3991, Training Loss : 0.16598, Training Acc : 0.919, Run Time : 1.24
INFO:root:2019-05-12 02:27:34, Epoch : 1, Step : 3992, Training Loss : 0.17014, Training Acc : 0.925, Run Time : 4.74
INFO:root:2019-05-12 02:27:36, Epoch : 1, Step : 3993, Training Loss : 0.14711, Training Acc : 0.942, Run Time : 1.47
INFO:root:2019-05-12 02:27:41, Epoch : 1, Step : 3994, Training Loss : 0.18192, Training Acc : 0.925, Run Time : 5.02
INFO:root:2019-05-12 02:27:42, Epoch : 1, Step : 3995, Training Loss : 0.17726, Training Acc : 0.911, Run Time : 1.14
INFO:root:2019-05-12 02:27:43, Epoch : 1, Step : 3996, Training Loss : 0.25446, Training Acc : 0.900, Run Time : 1.37
INFO:root:2019-05-12 02:27:51, Epoch : 1, Step : 3997, Training Loss : 0.29334, Training Acc : 0.878, Run Time : 7.84
INFO:root:2019-05-12 02:27:52, Epoch : 1, Step : 3998, Training Loss : 0.22273, Training Acc : 0.900, Run Time : 1.13
INFO:root:2019-05-12 02:27:54, Epoch : 1, Step : 3999, Training Loss : 0.19625, Training Acc : 0.914, Run Time : 1.44
INFO:root:2019-05-12 02:28:00, Epoch : 1, Step : 4000, Training Loss : 0.18856, Training Acc : 0.914, Run Time : 5.79
INFO:root:2019-05-12 02:28:01, Epoch : 1, Step : 4001, Training Loss : 0.39973, Training Acc : 0.811, Run Time : 1.83
INFO:root:2019-05-12 02:28:03, Epoch : 1, Step : 4002, Training Loss : 0.40073, Training Acc : 0.817, Run Time : 1.48
INFO:root:2019-05-12 02:28:07, Epoch : 1, Step : 4003, Training Loss : 0.37928, Training Acc : 0.847, Run Time : 3.98
INFO:root:2019-05-12 02:28:08, Epoch : 1, Step : 4004, Training Loss : 0.33178, Training Acc : 0.828, Run Time : 1.49
INFO:root:2019-05-12 02:28:10, Epoch : 1, Step : 4005, Training Loss : 0.38363, Training Acc : 0.808, Run Time : 1.49
INFO:root:2019-05-12 02:28:11, Epoch : 1, Step : 4006, Training Loss : 0.38555, Training Acc : 0.811, Run Time : 1.26
INFO:root:2019-05-12 02:28:14, Epoch : 1, Step : 4007, Training Loss : 0.31105, Training Acc : 0.869, Run Time : 2.53
INFO:root:2019-05-12 02:28:15, Epoch : 1, Step : 4008, Training Loss : 0.25255, Training Acc : 0.889, Run Time : 1.52
INFO:root:2019-05-12 02:28:19, Epoch : 1, Step : 4009, Training Loss : 0.27085, Training Acc : 0.914, Run Time : 4.24
INFO:root:2019-05-12 02:28:21, Epoch : 1, Step : 4010, Training Loss : 0.27594, Training Acc : 0.906, Run Time : 1.40
INFO:root:2019-05-12 02:28:22, Epoch : 1, Step : 4011, Training Loss : 0.29877, Training Acc : 0.919, Run Time : 1.63
INFO:root:2019-05-12 02:28:28, Epoch : 1, Step : 4012, Training Loss : 0.29365, Training Acc : 0.903, Run Time : 5.18
INFO:root:2019-05-12 02:28:29, Epoch : 1, Step : 4013, Training Loss : 0.23149, Training Acc : 0.931, Run Time : 1.15
INFO:root:2019-05-12 02:28:36, Epoch : 1, Step : 4014, Training Loss : 0.21531, Training Acc : 0.919, Run Time : 7.72
INFO:root:2019-05-12 02:28:38, Epoch : 1, Step : 4015, Training Loss : 0.19322, Training Acc : 0.936, Run Time : 1.63
INFO:root:2019-05-12 02:28:44, Epoch : 1, Step : 4016, Training Loss : 0.25311, Training Acc : 0.897, Run Time : 5.64
INFO:root:2019-05-12 02:28:45, Epoch : 1, Step : 4017, Training Loss : 0.30520, Training Acc : 0.858, Run Time : 1.18
INFO:root:2019-05-12 02:28:47, Epoch : 1, Step : 4018, Training Loss : 0.35084, Training Acc : 0.869, Run Time : 2.21
INFO:root:2019-05-12 02:28:55, Epoch : 1, Step : 4019, Training Loss : 0.23593, Training Acc : 0.900, Run Time : 7.65
INFO:root:2019-05-12 02:28:57, Epoch : 1, Step : 4020, Training Loss : 0.24837, Training Acc : 0.894, Run Time : 2.23
INFO:root:2019-05-12 02:29:05, Epoch : 1, Step : 4021, Training Loss : 0.24459, Training Acc : 0.928, Run Time : 8.22
INFO:root:2019-05-12 02:29:07, Epoch : 1, Step : 4022, Training Loss : 0.31170, Training Acc : 0.889, Run Time : 1.63
INFO:root:2019-05-12 02:29:12, Epoch : 1, Step : 4023, Training Loss : 0.37961, Training Acc : 0.858, Run Time : 5.07
INFO:root:2019-05-12 02:29:13, Epoch : 1, Step : 4024, Training Loss : 0.25613, Training Acc : 0.903, Run Time : 1.16
INFO:root:2019-05-12 02:29:18, Epoch : 1, Step : 4025, Training Loss : 0.32549, Training Acc : 0.886, Run Time : 5.06
INFO:root:2019-05-12 02:29:20, Epoch : 1, Step : 4026, Training Loss : 0.23681, Training Acc : 0.892, Run Time : 1.43
INFO:root:2019-05-12 02:29:21, Epoch : 1, Step : 4027, Training Loss : 0.26194, Training Acc : 0.917, Run Time : 1.29
INFO:root:2019-05-12 02:29:27, Epoch : 1, Step : 4028, Training Loss : 0.33560, Training Acc : 0.864, Run Time : 6.21
INFO:root:2019-05-12 02:29:28, Epoch : 1, Step : 4029, Training Loss : 0.36073, Training Acc : 0.828, Run Time : 1.38
INFO:root:2019-05-12 02:29:30, Epoch : 1, Step : 4030, Training Loss : 0.31486, Training Acc : 0.875, Run Time : 1.14
INFO:root:2019-05-12 02:29:32, Epoch : 1, Step : 4031, Training Loss : 0.26210, Training Acc : 0.886, Run Time : 2.83
INFO:root:2019-05-12 02:29:34, Epoch : 1, Step : 4032, Training Loss : 0.30828, Training Acc : 0.847, Run Time : 1.27
INFO:root:2019-05-12 02:29:37, Epoch : 1, Step : 4033, Training Loss : 0.32797, Training Acc : 0.842, Run Time : 3.55
INFO:root:2019-05-12 02:29:38, Epoch : 1, Step : 4034, Training Loss : 0.42939, Training Acc : 0.761, Run Time : 1.14
INFO:root:2019-05-12 02:29:40, Epoch : 1, Step : 4035, Training Loss : 0.32277, Training Acc : 0.856, Run Time : 1.56
INFO:root:2019-05-12 02:29:43, Epoch : 1, Step : 4036, Training Loss : 0.29704, Training Acc : 0.864, Run Time : 2.69
INFO:root:2019-05-12 02:29:44, Epoch : 1, Step : 4037, Training Loss : 0.22618, Training Acc : 0.900, Run Time : 1.38
INFO:root:2019-05-12 02:29:51, Epoch : 1, Step : 4038, Training Loss : 0.40849, Training Acc : 0.769, Run Time : 7.04
INFO:root:2019-05-12 02:29:53, Epoch : 1, Step : 4039, Training Loss : 0.30940, Training Acc : 0.850, Run Time : 1.65
INFO:root:2019-05-12 02:29:59, Epoch : 1, Step : 4040, Training Loss : 0.30704, Training Acc : 0.867, Run Time : 6.14
INFO:root:2019-05-12 02:30:00, Epoch : 1, Step : 4041, Training Loss : 0.32081, Training Acc : 0.853, Run Time : 1.32
INFO:root:2019-05-12 02:30:01, Epoch : 1, Step : 4042, Training Loss : 0.32903, Training Acc : 0.847, Run Time : 1.20
INFO:root:2019-05-12 02:30:09, Epoch : 1, Step : 4043, Training Loss : 0.36252, Training Acc : 0.867, Run Time : 7.86
INFO:root:2019-05-12 02:30:11, Epoch : 1, Step : 4044, Training Loss : 0.38402, Training Acc : 0.861, Run Time : 1.84
INFO:root:2019-05-12 02:30:19, Epoch : 1, Step : 4045, Training Loss : 0.45527, Training Acc : 0.783, Run Time : 7.86
INFO:root:2019-05-12 02:30:20, Epoch : 1, Step : 4046, Training Loss : 0.49394, Training Acc : 0.742, Run Time : 1.35
INFO:root:2019-05-12 02:30:21, Epoch : 1, Step : 4047, Training Loss : 0.48236, Training Acc : 0.764, Run Time : 1.14
INFO:root:2019-05-12 02:30:30, Epoch : 1, Step : 4048, Training Loss : 0.39228, Training Acc : 0.861, Run Time : 8.43
INFO:root:2019-05-12 02:30:31, Epoch : 1, Step : 4049, Training Loss : 0.44446, Training Acc : 0.778, Run Time : 1.17
INFO:root:2019-05-12 02:30:33, Epoch : 1, Step : 4050, Training Loss : 0.31656, Training Acc : 0.878, Run Time : 2.17
INFO:root:2019-05-12 02:30:35, Epoch : 1, Step : 4051, Training Loss : 0.33771, Training Acc : 0.842, Run Time : 1.40
INFO:root:2019-05-12 02:30:40, Epoch : 1, Step : 4052, Training Loss : 0.22370, Training Acc : 0.897, Run Time : 5.27
INFO:root:2019-05-12 02:30:41, Epoch : 1, Step : 4053, Training Loss : 0.32041, Training Acc : 0.819, Run Time : 1.15
INFO:root:2019-05-12 02:30:49, Epoch : 1, Step : 4054, Training Loss : 0.36764, Training Acc : 0.808, Run Time : 8.07
INFO:root:2019-05-12 02:30:50, Epoch : 1, Step : 4055, Training Loss : 0.21650, Training Acc : 0.919, Run Time : 1.27
INFO:root:2019-05-12 02:30:52, Epoch : 1, Step : 4056, Training Loss : 0.22781, Training Acc : 0.894, Run Time : 1.56
INFO:root:2019-05-12 02:30:58, Epoch : 1, Step : 4057, Training Loss : 0.30265, Training Acc : 0.853, Run Time : 5.94
INFO:root:2019-05-12 02:31:00, Epoch : 1, Step : 4058, Training Loss : 0.23979, Training Acc : 0.889, Run Time : 2.30
INFO:root:2019-05-12 02:31:06, Epoch : 1, Step : 4059, Training Loss : 0.29263, Training Acc : 0.864, Run Time : 6.34
INFO:root:2019-05-12 02:31:08, Epoch : 1, Step : 4060, Training Loss : 0.30127, Training Acc : 0.864, Run Time : 1.46
INFO:root:2019-05-12 02:31:10, Epoch : 1, Step : 4061, Training Loss : 0.26905, Training Acc : 0.869, Run Time : 2.37
INFO:root:2019-05-12 02:31:14, Epoch : 1, Step : 4062, Training Loss : 0.47873, Training Acc : 0.814, Run Time : 3.77
INFO:root:2019-05-12 02:31:15, Epoch : 1, Step : 4063, Training Loss : 0.50688, Training Acc : 0.806, Run Time : 1.12
INFO:root:2019-05-12 02:31:21, Epoch : 1, Step : 4064, Training Loss : 0.32947, Training Acc : 0.842, Run Time : 5.52
INFO:root:2019-05-12 02:31:22, Epoch : 1, Step : 4065, Training Loss : 0.37656, Training Acc : 0.839, Run Time : 1.63
INFO:root:2019-05-12 02:31:27, Epoch : 1, Step : 4066, Training Loss : 0.36238, Training Acc : 0.833, Run Time : 4.43
INFO:root:2019-05-12 02:31:28, Epoch : 1, Step : 4067, Training Loss : 0.38338, Training Acc : 0.792, Run Time : 1.70
INFO:root:2019-05-12 02:31:30, Epoch : 1, Step : 4068, Training Loss : 0.43138, Training Acc : 0.753, Run Time : 1.15
INFO:root:2019-05-12 02:31:38, Epoch : 1, Step : 4069, Training Loss : 0.43742, Training Acc : 0.772, Run Time : 8.81
INFO:root:2019-05-12 02:31:40, Epoch : 1, Step : 4070, Training Loss : 0.40929, Training Acc : 0.786, Run Time : 1.48
INFO:root:2019-05-12 02:31:44, Epoch : 1, Step : 4071, Training Loss : 0.25988, Training Acc : 0.867, Run Time : 4.21
INFO:root:2019-05-12 02:31:45, Epoch : 1, Step : 4072, Training Loss : 0.25162, Training Acc : 0.883, Run Time : 1.21
INFO:root:2019-05-12 02:31:52, Epoch : 1, Step : 4073, Training Loss : 0.30321, Training Acc : 0.847, Run Time : 6.85
INFO:root:2019-05-12 02:31:53, Epoch : 1, Step : 4074, Training Loss : 0.23774, Training Acc : 0.903, Run Time : 1.18
INFO:root:2019-05-12 02:31:55, Epoch : 1, Step : 4075, Training Loss : 0.24717, Training Acc : 0.869, Run Time : 1.84
INFO:root:2019-05-12 02:32:02, Epoch : 1, Step : 4076, Training Loss : 0.27296, Training Acc : 0.867, Run Time : 6.83
INFO:root:2019-05-12 02:32:04, Epoch : 1, Step : 4077, Training Loss : 0.25716, Training Acc : 0.864, Run Time : 1.59
INFO:root:2019-05-12 02:32:05, Epoch : 1, Step : 4078, Training Loss : 0.25385, Training Acc : 0.850, Run Time : 1.85
INFO:root:2019-05-12 02:32:07, Epoch : 1, Step : 4079, Training Loss : 0.32076, Training Acc : 0.844, Run Time : 1.16
INFO:root:2019-05-12 02:32:11, Epoch : 1, Step : 4080, Training Loss : 0.21959, Training Acc : 0.894, Run Time : 4.36
INFO:root:2019-05-12 02:32:12, Epoch : 1, Step : 4081, Training Loss : 0.23505, Training Acc : 0.900, Run Time : 1.24
INFO:root:2019-05-12 02:32:13, Epoch : 1, Step : 4082, Training Loss : 0.19136, Training Acc : 0.928, Run Time : 1.12
INFO:root:2019-05-12 02:32:20, Epoch : 1, Step : 4083, Training Loss : 0.16591, Training Acc : 0.956, Run Time : 6.34
INFO:root:2019-05-12 02:32:21, Epoch : 1, Step : 4084, Training Loss : 0.18469, Training Acc : 0.922, Run Time : 1.16
INFO:root:2019-05-12 02:32:22, Epoch : 1, Step : 4085, Training Loss : 0.19979, Training Acc : 0.917, Run Time : 1.13
INFO:root:2019-05-12 02:32:23, Epoch : 1, Step : 4086, Training Loss : 0.22008, Training Acc : 0.908, Run Time : 1.45
INFO:root:2019-05-12 02:32:30, Epoch : 1, Step : 4087, Training Loss : 0.22712, Training Acc : 0.894, Run Time : 6.30
INFO:root:2019-05-12 02:32:31, Epoch : 1, Step : 4088, Training Loss : 0.23862, Training Acc : 0.886, Run Time : 1.24
INFO:root:2019-05-12 02:32:37, Epoch : 1, Step : 4089, Training Loss : 0.19160, Training Acc : 0.919, Run Time : 5.71
INFO:root:2019-05-12 02:32:38, Epoch : 1, Step : 4090, Training Loss : 0.20670, Training Acc : 0.919, Run Time : 1.59
INFO:root:2019-05-12 02:32:44, Epoch : 1, Step : 4091, Training Loss : 0.20385, Training Acc : 0.922, Run Time : 5.88
INFO:root:2019-05-12 02:32:45, Epoch : 1, Step : 4092, Training Loss : 0.19200, Training Acc : 0.906, Run Time : 1.20
INFO:root:2019-05-12 02:32:47, Epoch : 1, Step : 4093, Training Loss : 0.19848, Training Acc : 0.908, Run Time : 1.29
INFO:root:2019-05-12 02:32:53, Epoch : 1, Step : 4094, Training Loss : 0.23472, Training Acc : 0.878, Run Time : 5.96
INFO:root:2019-05-12 02:32:54, Epoch : 1, Step : 4095, Training Loss : 0.19221, Training Acc : 0.931, Run Time : 1.13
INFO:root:2019-05-12 02:32:55, Epoch : 1, Step : 4096, Training Loss : 0.21399, Training Acc : 0.922, Run Time : 1.22
INFO:root:2019-05-12 02:33:00, Epoch : 1, Step : 4097, Training Loss : 0.46262, Training Acc : 0.822, Run Time : 5.18
INFO:root:2019-05-12 02:33:01, Epoch : 1, Step : 4098, Training Loss : 0.29651, Training Acc : 0.867, Run Time : 1.14
INFO:root:2019-05-12 02:33:08, Epoch : 1, Step : 4099, Training Loss : 0.43100, Training Acc : 0.847, Run Time : 6.95
INFO:root:2019-05-12 02:33:09, Epoch : 1, Step : 4100, Training Loss : 0.26775, Training Acc : 0.889, Run Time : 1.23
INFO:root:2019-05-12 02:33:17, Epoch : 1, Step : 4101, Training Loss : 0.42311, Training Acc : 0.792, Run Time : 7.55
INFO:root:2019-05-12 02:33:19, Epoch : 1, Step : 4102, Training Loss : 0.38480, Training Acc : 0.847, Run Time : 1.74
INFO:root:2019-05-12 02:33:26, Epoch : 1, Step : 4103, Training Loss : 0.45621, Training Acc : 0.761, Run Time : 7.45
INFO:root:2019-05-12 02:33:28, Epoch : 1, Step : 4104, Training Loss : 0.45839, Training Acc : 0.808, Run Time : 2.15
INFO:root:2019-05-12 02:33:34, Epoch : 1, Step : 4105, Training Loss : 0.54491, Training Acc : 0.758, Run Time : 6.04
INFO:root:2019-05-12 02:33:36, Epoch : 1, Step : 4106, Training Loss : 0.57228, Training Acc : 0.778, Run Time : 1.21
INFO:root:2019-05-12 02:33:44, Epoch : 1, Step : 4107, Training Loss : 0.60236, Training Acc : 0.694, Run Time : 8.24
INFO:root:2019-05-12 02:33:45, Epoch : 1, Step : 4108, Training Loss : 0.28210, Training Acc : 0.892, Run Time : 1.23
INFO:root:2019-05-12 02:33:47, Epoch : 1, Step : 4109, Training Loss : 0.50074, Training Acc : 0.756, Run Time : 1.64
INFO:root:2019-05-12 02:33:51, Epoch : 1, Step : 4110, Training Loss : 0.36048, Training Acc : 0.836, Run Time : 4.32
INFO:root:2019-05-12 02:33:52, Epoch : 1, Step : 4111, Training Loss : 0.29233, Training Acc : 0.881, Run Time : 1.13
INFO:root:2019-05-12 02:33:53, Epoch : 1, Step : 4112, Training Loss : 0.25702, Training Acc : 0.903, Run Time : 1.18
INFO:root:2019-05-12 02:33:55, Epoch : 1, Step : 4113, Training Loss : 0.23250, Training Acc : 0.961, Run Time : 1.88
INFO:root:2019-05-12 02:34:01, Epoch : 1, Step : 4114, Training Loss : 0.13978, Training Acc : 0.964, Run Time : 5.64
INFO:root:2019-05-12 02:34:02, Epoch : 1, Step : 4115, Training Loss : 0.18305, Training Acc : 0.956, Run Time : 1.15
INFO:root:2019-05-12 02:34:03, Epoch : 1, Step : 4116, Training Loss : 0.08039, Training Acc : 0.989, Run Time : 1.28
INFO:root:2019-05-12 02:34:11, Epoch : 1, Step : 4117, Training Loss : 0.07025, Training Acc : 0.989, Run Time : 7.99
INFO:root:2019-05-12 02:34:13, Epoch : 1, Step : 4118, Training Loss : 0.09395, Training Acc : 0.986, Run Time : 1.57
INFO:root:2019-05-12 02:34:19, Epoch : 1, Step : 4119, Training Loss : 0.05286, Training Acc : 1.000, Run Time : 6.38
INFO:root:2019-05-12 02:34:20, Epoch : 1, Step : 4120, Training Loss : 0.06691, Training Acc : 0.997, Run Time : 1.20
INFO:root:2019-05-12 02:34:22, Epoch : 1, Step : 4121, Training Loss : 0.10971, Training Acc : 0.989, Run Time : 1.40
INFO:root:2019-05-12 02:34:28, Epoch : 1, Step : 4122, Training Loss : 0.13715, Training Acc : 0.986, Run Time : 5.93
INFO:root:2019-05-12 02:34:29, Epoch : 1, Step : 4123, Training Loss : 0.07403, Training Acc : 0.989, Run Time : 1.20
INFO:root:2019-05-12 02:34:30, Epoch : 1, Step : 4124, Training Loss : 0.08980, Training Acc : 0.983, Run Time : 1.36
INFO:root:2019-05-12 02:34:38, Epoch : 1, Step : 4125, Training Loss : 0.11157, Training Acc : 0.986, Run Time : 7.81
INFO:root:2019-05-12 02:34:39, Epoch : 1, Step : 4126, Training Loss : 0.09434, Training Acc : 0.989, Run Time : 1.16
INFO:root:2019-05-12 02:34:40, Epoch : 1, Step : 4127, Training Loss : 0.12563, Training Acc : 0.994, Run Time : 1.13
INFO:root:2019-05-12 02:34:42, Epoch : 1, Step : 4128, Training Loss : 0.12060, Training Acc : 0.994, Run Time : 1.24
INFO:root:2019-05-12 02:34:43, Epoch : 1, Step : 4129, Training Loss : 0.07666, Training Acc : 0.997, Run Time : 1.72
INFO:root:2019-05-12 02:34:51, Epoch : 1, Step : 4130, Training Loss : 0.03901, Training Acc : 0.994, Run Time : 7.59
INFO:root:2019-05-12 02:34:52, Epoch : 1, Step : 4131, Training Loss : 0.08912, Training Acc : 1.000, Run Time : 1.31
INFO:root:2019-05-12 02:34:53, Epoch : 1, Step : 4132, Training Loss : 0.03943, Training Acc : 1.000, Run Time : 1.20
INFO:root:2019-05-12 02:35:02, Epoch : 1, Step : 4133, Training Loss : 0.11187, Training Acc : 0.989, Run Time : 8.28
INFO:root:2019-05-12 02:35:03, Epoch : 1, Step : 4134, Training Loss : 0.04374, Training Acc : 0.992, Run Time : 1.13
INFO:root:2019-05-12 02:35:09, Epoch : 1, Step : 4135, Training Loss : 0.05894, Training Acc : 0.992, Run Time : 6.63
INFO:root:2019-05-12 02:35:11, Epoch : 1, Step : 4136, Training Loss : 0.08265, Training Acc : 0.986, Run Time : 1.27
INFO:root:2019-05-12 02:35:13, Epoch : 1, Step : 4137, Training Loss : 0.08120, Training Acc : 0.989, Run Time : 2.05
INFO:root:2019-05-12 02:35:14, Epoch : 1, Step : 4138, Training Loss : 0.08769, Training Acc : 0.986, Run Time : 1.31
INFO:root:2019-05-12 02:35:18, Epoch : 1, Step : 4139, Training Loss : 0.06528, Training Acc : 0.994, Run Time : 3.55
INFO:root:2019-05-12 02:35:19, Epoch : 1, Step : 4140, Training Loss : 0.08610, Training Acc : 0.986, Run Time : 1.20
INFO:root:2019-05-12 02:35:20, Epoch : 1, Step : 4141, Training Loss : 0.10058, Training Acc : 0.989, Run Time : 1.15
INFO:root:2019-05-12 02:35:22, Epoch : 1, Step : 4142, Training Loss : 0.13181, Training Acc : 0.964, Run Time : 1.76
INFO:root:2019-05-12 02:35:27, Epoch : 1, Step : 4143, Training Loss : 0.09099, Training Acc : 0.994, Run Time : 5.25
INFO:root:2019-05-12 02:35:28, Epoch : 1, Step : 4144, Training Loss : 0.04938, Training Acc : 0.997, Run Time : 1.16
INFO:root:2019-05-12 02:35:29, Epoch : 1, Step : 4145, Training Loss : 0.09039, Training Acc : 0.986, Run Time : 1.24
INFO:root:2019-05-12 02:35:36, Epoch : 1, Step : 4146, Training Loss : 0.05046, Training Acc : 0.997, Run Time : 6.18
INFO:root:2019-05-12 02:35:37, Epoch : 1, Step : 4147, Training Loss : 0.06965, Training Acc : 0.997, Run Time : 1.22
INFO:root:2019-05-12 02:35:38, Epoch : 1, Step : 4148, Training Loss : 0.14402, Training Acc : 0.986, Run Time : 1.13
INFO:root:2019-05-12 02:35:40, Epoch : 1, Step : 4149, Training Loss : 0.05469, Training Acc : 1.000, Run Time : 1.73
INFO:root:2019-05-12 02:35:41, Epoch : 1, Step : 4150, Training Loss : 0.04324, Training Acc : 1.000, Run Time : 1.26
INFO:root:2019-05-12 02:35:44, Epoch : 1, Step : 4151, Training Loss : 0.22604, Training Acc : 0.933, Run Time : 2.94
INFO:root:2019-05-12 02:35:46, Epoch : 1, Step : 4152, Training Loss : 0.11073, Training Acc : 0.989, Run Time : 1.74
INFO:root:2019-05-12 02:35:49, Epoch : 1, Step : 4153, Training Loss : 0.09879, Training Acc : 0.978, Run Time : 3.21
INFO:root:2019-05-12 02:35:52, Epoch : 1, Step : 4154, Training Loss : 0.07346, Training Acc : 0.989, Run Time : 3.45
INFO:root:2019-05-12 02:35:53, Epoch : 1, Step : 4155, Training Loss : 0.05231, Training Acc : 0.989, Run Time : 1.13
INFO:root:2019-05-12 02:36:00, Epoch : 1, Step : 4156, Training Loss : 0.06099, Training Acc : 0.989, Run Time : 7.11
INFO:root:2019-05-12 02:36:02, Epoch : 1, Step : 4157, Training Loss : 0.17007, Training Acc : 0.986, Run Time : 1.31
INFO:root:2019-05-12 02:36:03, Epoch : 1, Step : 4158, Training Loss : 0.12387, Training Acc : 0.975, Run Time : 1.52
INFO:root:2019-05-12 02:36:10, Epoch : 1, Step : 4159, Training Loss : 0.12925, Training Acc : 0.964, Run Time : 6.34
INFO:root:2019-05-12 02:36:11, Epoch : 1, Step : 4160, Training Loss : 0.52680, Training Acc : 0.647, Run Time : 1.57
INFO:root:2019-05-12 02:36:16, Epoch : 1, Step : 4161, Training Loss : 0.10041, Training Acc : 0.978, Run Time : 4.36
INFO:root:2019-05-12 02:36:17, Epoch : 1, Step : 4162, Training Loss : 0.24572, Training Acc : 0.992, Run Time : 1.75
INFO:root:2019-05-12 02:36:24, Epoch : 1, Step : 4163, Training Loss : 0.09247, Training Acc : 0.994, Run Time : 6.20
INFO:root:2019-05-12 02:36:25, Epoch : 1, Step : 4164, Training Loss : 0.04366, Training Acc : 1.000, Run Time : 1.49
INFO:root:2019-05-12 02:36:32, Epoch : 1, Step : 4165, Training Loss : 0.08996, Training Acc : 0.981, Run Time : 7.17
INFO:root:2019-05-12 02:36:33, Epoch : 1, Step : 4166, Training Loss : 0.05051, Training Acc : 1.000, Run Time : 1.14
INFO:root:2019-05-12 02:36:35, Epoch : 1, Step : 4167, Training Loss : 0.13943, Training Acc : 0.989, Run Time : 1.62
INFO:root:2019-05-12 02:36:41, Epoch : 1, Step : 4168, Training Loss : 0.07032, Training Acc : 0.994, Run Time : 5.76
INFO:root:2019-05-12 02:36:42, Epoch : 1, Step : 4169, Training Loss : 0.11557, Training Acc : 0.994, Run Time : 1.21
INFO:root:2019-05-12 02:36:43, Epoch : 1, Step : 4170, Training Loss : 0.04016, Training Acc : 0.997, Run Time : 1.29
INFO:root:2019-05-12 02:36:50, Epoch : 1, Step : 4171, Training Loss : 0.10543, Training Acc : 0.989, Run Time : 6.97
INFO:root:2019-05-12 02:36:52, Epoch : 1, Step : 4172, Training Loss : 0.05571, Training Acc : 0.989, Run Time : 1.62
INFO:root:2019-05-12 02:37:00, Epoch : 1, Step : 4173, Training Loss : 0.05070, Training Acc : 0.989, Run Time : 8.05
INFO:root:2019-05-12 02:37:01, Epoch : 1, Step : 4174, Training Loss : 0.09059, Training Acc : 0.978, Run Time : 1.41
INFO:root:2019-05-12 02:37:02, Epoch : 1, Step : 4175, Training Loss : 0.11038, Training Acc : 0.961, Run Time : 1.18
INFO:root:2019-05-12 02:37:14, Epoch : 1, Step : 4176, Training Loss : 0.07630, Training Acc : 0.986, Run Time : 11.18
INFO:root:2019-05-12 02:37:15, Epoch : 1, Step : 4177, Training Loss : 0.13587, Training Acc : 0.986, Run Time : 1.47
INFO:root:2019-05-12 02:37:23, Epoch : 1, Step : 4178, Training Loss : 0.06570, Training Acc : 0.989, Run Time : 7.75
INFO:root:2019-05-12 02:37:24, Epoch : 1, Step : 4179, Training Loss : 0.05774, Training Acc : 0.986, Run Time : 1.22
INFO:root:2019-05-12 02:37:26, Epoch : 1, Step : 4180, Training Loss : 0.10176, Training Acc : 0.972, Run Time : 1.51
INFO:root:2019-05-12 02:37:38, Epoch : 1, Step : 4181, Training Loss : 0.07959, Training Acc : 0.989, Run Time : 12.87
INFO:root:2019-05-12 02:37:40, Epoch : 1, Step : 4182, Training Loss : 0.13882, Training Acc : 0.958, Run Time : 1.21
INFO:root:2019-05-12 02:37:41, Epoch : 1, Step : 4183, Training Loss : 0.12181, Training Acc : 0.969, Run Time : 1.13
INFO:root:2019-05-12 02:37:42, Epoch : 1, Step : 4184, Training Loss : 0.15867, Training Acc : 0.956, Run Time : 1.60
INFO:root:2019-05-12 02:37:50, Epoch : 1, Step : 4185, Training Loss : 0.16615, Training Acc : 0.958, Run Time : 7.35
INFO:root:2019-05-12 02:37:51, Epoch : 1, Step : 4186, Training Loss : 0.16209, Training Acc : 0.950, Run Time : 1.54
INFO:root:2019-05-12 02:37:58, Epoch : 1, Step : 4187, Training Loss : 0.22134, Training Acc : 0.947, Run Time : 6.46
INFO:root:2019-05-12 02:37:59, Epoch : 1, Step : 4188, Training Loss : 0.07052, Training Acc : 0.981, Run Time : 1.50
INFO:root:2019-05-12 02:38:00, Epoch : 1, Step : 4189, Training Loss : 0.10458, Training Acc : 0.967, Run Time : 1.12
INFO:root:2019-05-12 02:38:03, Epoch : 1, Step : 4190, Training Loss : 0.11016, Training Acc : 0.964, Run Time : 2.38
INFO:root:2019-05-12 02:38:08, Epoch : 1, Step : 4191, Training Loss : 0.26051, Training Acc : 0.942, Run Time : 5.36
INFO:root:2019-05-12 02:38:09, Epoch : 1, Step : 4192, Training Loss : 0.26762, Training Acc : 0.936, Run Time : 1.18
INFO:root:2019-05-12 02:38:12, Epoch : 1, Step : 4193, Training Loss : 0.14909, Training Acc : 0.975, Run Time : 2.59
INFO:root:2019-05-12 02:38:22, Epoch : 1, Step : 4194, Training Loss : 0.13276, Training Acc : 0.981, Run Time : 10.01
INFO:root:2019-05-12 02:38:23, Epoch : 1, Step : 4195, Training Loss : 0.07484, Training Acc : 0.975, Run Time : 1.16
INFO:root:2019-05-12 02:38:33, Epoch : 1, Step : 4196, Training Loss : 0.12666, Training Acc : 0.972, Run Time : 9.97
INFO:root:2019-05-12 02:38:34, Epoch : 1, Step : 4197, Training Loss : 0.10786, Training Acc : 0.964, Run Time : 1.13
INFO:root:2019-05-12 02:38:35, Epoch : 1, Step : 4198, Training Loss : 0.21283, Training Acc : 0.919, Run Time : 1.14
INFO:root:2019-05-12 02:38:42, Epoch : 1, Step : 4199, Training Loss : 0.13221, Training Acc : 0.953, Run Time : 6.29
INFO:root:2019-05-12 02:38:43, Epoch : 1, Step : 4200, Training Loss : 0.26877, Training Acc : 0.864, Run Time : 1.26
INFO:root:2019-05-12 02:38:44, Epoch : 1, Step : 4201, Training Loss : 0.76672, Training Acc : 0.836, Run Time : 1.52
INFO:root:2019-05-12 02:38:46, Epoch : 1, Step : 4202, Training Loss : 0.62886, Training Acc : 0.858, Run Time : 2.12
INFO:root:2019-05-12 02:38:51, Epoch : 1, Step : 4203, Training Loss : 0.59792, Training Acc : 0.856, Run Time : 4.26
INFO:root:2019-05-12 02:38:52, Epoch : 1, Step : 4204, Training Loss : 0.65046, Training Acc : 0.850, Run Time : 1.58
INFO:root:2019-05-12 02:38:54, Epoch : 1, Step : 4205, Training Loss : 0.54582, Training Acc : 0.844, Run Time : 1.91
INFO:root:2019-05-12 02:38:55, Epoch : 1, Step : 4206, Training Loss : 0.35596, Training Acc : 0.825, Run Time : 1.16
INFO:root:2019-05-12 02:38:58, Epoch : 1, Step : 4207, Training Loss : 0.30407, Training Acc : 0.822, Run Time : 3.12
INFO:root:2019-05-12 02:39:00, Epoch : 1, Step : 4208, Training Loss : 0.45718, Training Acc : 0.781, Run Time : 1.33
INFO:root:2019-05-12 02:39:01, Epoch : 1, Step : 4209, Training Loss : 0.48662, Training Acc : 0.789, Run Time : 1.20
INFO:root:2019-05-12 02:39:03, Epoch : 1, Step : 4210, Training Loss : 0.25137, Training Acc : 0.894, Run Time : 1.70
INFO:root:2019-05-12 02:39:05, Epoch : 1, Step : 4211, Training Loss : 0.26036, Training Acc : 0.911, Run Time : 2.12
INFO:root:2019-05-12 02:39:07, Epoch : 1, Step : 4212, Training Loss : 0.22195, Training Acc : 0.897, Run Time : 2.11
INFO:root:2019-05-12 02:39:13, Epoch : 1, Step : 4213, Training Loss : 0.47226, Training Acc : 0.811, Run Time : 6.33
INFO:root:2019-05-12 02:39:14, Epoch : 1, Step : 4214, Training Loss : 0.98024, Training Acc : 0.703, Run Time : 1.12
INFO:root:2019-05-12 02:39:16, Epoch : 1, Step : 4215, Training Loss : 0.57267, Training Acc : 0.722, Run Time : 1.95
INFO:root:2019-05-12 02:39:22, Epoch : 1, Step : 4216, Training Loss : 0.43721, Training Acc : 0.803, Run Time : 6.07
INFO:root:2019-05-12 02:39:24, Epoch : 1, Step : 4217, Training Loss : 0.61903, Training Acc : 0.725, Run Time : 1.69
INFO:root:2019-05-12 02:39:32, Epoch : 1, Step : 4218, Training Loss : 0.48976, Training Acc : 0.744, Run Time : 7.76
INFO:root:2019-05-12 02:39:33, Epoch : 1, Step : 4219, Training Loss : 0.63780, Training Acc : 0.650, Run Time : 1.25
INFO:root:2019-05-12 02:39:37, Epoch : 1, Step : 4220, Training Loss : 0.43197, Training Acc : 0.764, Run Time : 4.25
INFO:root:2019-05-12 02:39:43, Epoch : 1, Step : 4221, Training Loss : 1.04191, Training Acc : 0.631, Run Time : 6.12
INFO:root:2019-05-12 02:39:45, Epoch : 1, Step : 4222, Training Loss : 0.59489, Training Acc : 0.689, Run Time : 1.55
INFO:root:2019-05-12 02:39:53, Epoch : 1, Step : 4223, Training Loss : 0.38597, Training Acc : 0.842, Run Time : 8.13
INFO:root:2019-05-12 02:39:54, Epoch : 1, Step : 4224, Training Loss : 0.46733, Training Acc : 0.792, Run Time : 1.18
INFO:root:2019-05-12 02:40:01, Epoch : 1, Step : 4225, Training Loss : 0.27728, Training Acc : 0.881, Run Time : 6.51
INFO:root:2019-05-12 02:40:02, Epoch : 1, Step : 4226, Training Loss : 0.39798, Training Acc : 0.806, Run Time : 1.58
INFO:root:2019-05-12 02:40:04, Epoch : 1, Step : 4227, Training Loss : 0.39538, Training Acc : 0.800, Run Time : 1.19
INFO:root:2019-05-12 02:40:05, Epoch : 1, Step : 4228, Training Loss : 0.38931, Training Acc : 0.828, Run Time : 1.88
INFO:root:2019-05-12 02:40:11, Epoch : 1, Step : 4229, Training Loss : 0.56503, Training Acc : 0.733, Run Time : 5.87
INFO:root:2019-05-12 02:40:12, Epoch : 1, Step : 4230, Training Loss : 0.47924, Training Acc : 0.786, Run Time : 1.14
INFO:root:2019-05-12 02:40:14, Epoch : 1, Step : 4231, Training Loss : 0.42168, Training Acc : 0.783, Run Time : 1.37
INFO:root:2019-05-12 02:40:20, Epoch : 1, Step : 4232, Training Loss : 0.39187, Training Acc : 0.806, Run Time : 5.91
INFO:root:2019-05-12 02:40:21, Epoch : 1, Step : 4233, Training Loss : 0.37182, Training Acc : 0.825, Run Time : 1.41
INFO:root:2019-05-12 02:40:22, Epoch : 1, Step : 4234, Training Loss : 0.38371, Training Acc : 0.811, Run Time : 1.16
INFO:root:2019-05-12 02:40:30, Epoch : 1, Step : 4235, Training Loss : 0.49165, Training Acc : 0.783, Run Time : 7.99
INFO:root:2019-05-12 02:40:32, Epoch : 1, Step : 4236, Training Loss : 0.46578, Training Acc : 0.794, Run Time : 1.60
INFO:root:2019-05-12 02:40:39, Epoch : 1, Step : 4237, Training Loss : 0.42248, Training Acc : 0.794, Run Time : 6.79
INFO:root:2019-05-12 02:40:41, Epoch : 1, Step : 4238, Training Loss : 0.40567, Training Acc : 0.833, Run Time : 2.23
INFO:root:2019-05-12 02:40:47, Epoch : 1, Step : 4239, Training Loss : 0.29819, Training Acc : 0.867, Run Time : 6.24
INFO:root:2019-05-12 02:40:48, Epoch : 1, Step : 4240, Training Loss : 0.32297, Training Acc : 0.856, Run Time : 1.27
INFO:root:2019-05-12 02:40:50, Epoch : 1, Step : 4241, Training Loss : 0.30240, Training Acc : 0.856, Run Time : 1.13
INFO:root:2019-05-12 02:40:51, Epoch : 1, Step : 4242, Training Loss : 0.29658, Training Acc : 0.864, Run Time : 1.27
INFO:root:2019-05-12 02:40:59, Epoch : 1, Step : 4243, Training Loss : 0.37833, Training Acc : 0.817, Run Time : 8.43
INFO:root:2019-05-12 02:41:01, Epoch : 1, Step : 4244, Training Loss : 0.33880, Training Acc : 0.825, Run Time : 1.27
INFO:root:2019-05-12 02:41:02, Epoch : 1, Step : 4245, Training Loss : 0.28320, Training Acc : 0.853, Run Time : 1.28
INFO:root:2019-05-12 02:41:03, Epoch : 1, Step : 4246, Training Loss : 0.27924, Training Acc : 0.867, Run Time : 1.31
INFO:root:2019-05-12 02:41:10, Epoch : 1, Step : 4247, Training Loss : 0.28815, Training Acc : 0.847, Run Time : 7.04
INFO:root:2019-05-12 02:41:12, Epoch : 1, Step : 4248, Training Loss : 0.26052, Training Acc : 0.869, Run Time : 1.42
INFO:root:2019-05-12 02:41:13, Epoch : 1, Step : 4249, Training Loss : 0.28860, Training Acc : 0.864, Run Time : 1.20
INFO:root:2019-05-12 02:41:14, Epoch : 1, Step : 4250, Training Loss : 0.27383, Training Acc : 0.883, Run Time : 1.58
INFO:root:2019-05-12 02:41:16, Epoch : 1, Step : 4251, Training Loss : 0.31153, Training Acc : 0.867, Run Time : 1.45
INFO:root:2019-05-12 02:41:22, Epoch : 1, Step : 4252, Training Loss : 0.22320, Training Acc : 0.903, Run Time : 6.04
INFO:root:2019-05-12 02:41:23, Epoch : 1, Step : 4253, Training Loss : 0.24170, Training Acc : 0.886, Run Time : 1.14
INFO:root:2019-05-12 02:41:24, Epoch : 1, Step : 4254, Training Loss : 0.22460, Training Acc : 0.906, Run Time : 1.14
INFO:root:2019-05-12 02:41:32, Epoch : 1, Step : 4255, Training Loss : 0.26646, Training Acc : 0.886, Run Time : 7.55
INFO:root:2019-05-12 02:41:33, Epoch : 1, Step : 4256, Training Loss : 0.22033, Training Acc : 0.914, Run Time : 1.17
INFO:root:2019-05-12 02:41:40, Epoch : 1, Step : 4257, Training Loss : 0.20183, Training Acc : 0.922, Run Time : 6.89
INFO:root:2019-05-12 02:41:41, Epoch : 1, Step : 4258, Training Loss : 0.19475, Training Acc : 0.936, Run Time : 1.45
INFO:root:2019-05-12 02:41:43, Epoch : 1, Step : 4259, Training Loss : 0.23130, Training Acc : 0.872, Run Time : 1.32
INFO:root:2019-05-12 02:41:49, Epoch : 1, Step : 4260, Training Loss : 0.22075, Training Acc : 0.906, Run Time : 5.98
INFO:root:2019-05-12 02:41:50, Epoch : 1, Step : 4261, Training Loss : 0.23437, Training Acc : 0.897, Run Time : 1.40
INFO:root:2019-05-12 02:41:51, Epoch : 1, Step : 4262, Training Loss : 0.51717, Training Acc : 0.828, Run Time : 1.37
INFO:root:2019-05-12 02:41:57, Epoch : 1, Step : 4263, Training Loss : 0.37618, Training Acc : 0.842, Run Time : 6.12
INFO:root:2019-05-12 02:41:59, Epoch : 1, Step : 4264, Training Loss : 0.39716, Training Acc : 0.844, Run Time : 1.20
INFO:root:2019-05-12 02:42:00, Epoch : 1, Step : 4265, Training Loss : 0.36725, Training Acc : 0.867, Run Time : 1.38
INFO:root:2019-05-12 02:42:08, Epoch : 1, Step : 4266, Training Loss : 0.21385, Training Acc : 0.914, Run Time : 8.15
INFO:root:2019-05-12 02:42:09, Epoch : 1, Step : 4267, Training Loss : 0.19319, Training Acc : 0.936, Run Time : 1.14
INFO:root:2019-05-12 02:42:12, Epoch : 1, Step : 4268, Training Loss : 0.21674, Training Acc : 0.908, Run Time : 2.88
INFO:root:2019-05-12 02:42:20, Epoch : 1, Step : 4269, Training Loss : 0.18220, Training Acc : 0.939, Run Time : 7.88
INFO:root:2019-05-12 02:42:21, Epoch : 1, Step : 4270, Training Loss : 0.23623, Training Acc : 0.931, Run Time : 1.30
INFO:root:2019-05-12 02:42:23, Epoch : 1, Step : 4271, Training Loss : 0.33957, Training Acc : 0.906, Run Time : 1.41
INFO:root:2019-05-12 02:42:31, Epoch : 1, Step : 4272, Training Loss : 0.32562, Training Acc : 0.886, Run Time : 8.72
INFO:root:2019-05-12 02:42:33, Epoch : 1, Step : 4273, Training Loss : 0.22316, Training Acc : 0.933, Run Time : 1.76
INFO:root:2019-05-12 02:42:41, Epoch : 1, Step : 4274, Training Loss : 0.50567, Training Acc : 0.825, Run Time : 8.15
INFO:root:2019-05-12 02:42:44, Epoch : 1, Step : 4275, Training Loss : 0.79426, Training Acc : 0.764, Run Time : 2.63
INFO:root:2019-05-12 02:42:48, Epoch : 1, Step : 4276, Training Loss : 0.31901, Training Acc : 0.886, Run Time : 3.95
INFO:root:2019-05-12 02:42:49, Epoch : 1, Step : 4277, Training Loss : 0.28095, Training Acc : 0.906, Run Time : 1.17
INFO:root:2019-05-12 02:42:50, Epoch : 1, Step : 4278, Training Loss : 0.25552, Training Acc : 0.908, Run Time : 1.21
INFO:root:2019-05-12 02:42:53, Epoch : 1, Step : 4279, Training Loss : 0.48464, Training Acc : 0.836, Run Time : 2.24
INFO:root:2019-05-12 02:42:57, Epoch : 1, Step : 4280, Training Loss : 0.28878, Training Acc : 0.900, Run Time : 4.44
INFO:root:2019-05-12 02:42:58, Epoch : 1, Step : 4281, Training Loss : 0.26185, Training Acc : 0.906, Run Time : 1.31
INFO:root:2019-05-12 02:43:01, Epoch : 1, Step : 4282, Training Loss : 0.22070, Training Acc : 0.906, Run Time : 2.33
INFO:root:2019-05-12 02:43:03, Epoch : 1, Step : 4283, Training Loss : 0.21761, Training Acc : 0.903, Run Time : 2.30
INFO:root:2019-05-12 02:43:04, Epoch : 1, Step : 4284, Training Loss : 0.60325, Training Acc : 0.786, Run Time : 1.35
INFO:root:2019-05-12 02:43:15, Epoch : 1, Step : 4285, Training Loss : 0.37623, Training Acc : 0.811, Run Time : 11.06
INFO:root:2019-05-12 02:43:17, Epoch : 1, Step : 4286, Training Loss : 0.39312, Training Acc : 0.814, Run Time : 1.38
INFO:root:2019-05-12 02:43:25, Epoch : 1, Step : 4287, Training Loss : 0.42384, Training Acc : 0.844, Run Time : 8.05
INFO:root:2019-05-12 02:43:27, Epoch : 1, Step : 4288, Training Loss : 0.49248, Training Acc : 0.800, Run Time : 2.16
INFO:root:2019-05-12 02:43:33, Epoch : 1, Step : 4289, Training Loss : 0.54392, Training Acc : 0.781, Run Time : 6.05
INFO:root:2019-05-12 02:43:35, Epoch : 1, Step : 4290, Training Loss : 0.48737, Training Acc : 0.797, Run Time : 1.63
INFO:root:2019-05-12 02:43:45, Epoch : 1, Step : 4291, Training Loss : 0.48490, Training Acc : 0.817, Run Time : 10.15
INFO:root:2019-05-12 02:43:47, Epoch : 1, Step : 4292, Training Loss : 0.46415, Training Acc : 0.817, Run Time : 1.90
INFO:root:2019-05-12 02:43:55, Epoch : 1, Step : 4293, Training Loss : 0.35581, Training Acc : 0.847, Run Time : 8.23
INFO:root:2019-05-12 02:43:57, Epoch : 1, Step : 4294, Training Loss : 0.77436, Training Acc : 0.731, Run Time : 1.72
INFO:root:2019-05-12 02:44:03, Epoch : 1, Step : 4295, Training Loss : 0.42921, Training Acc : 0.806, Run Time : 6.67
INFO:root:2019-05-12 02:44:05, Epoch : 1, Step : 4296, Training Loss : 0.42888, Training Acc : 0.803, Run Time : 1.18
INFO:root:2019-05-12 02:44:06, Epoch : 1, Step : 4297, Training Loss : 0.37238, Training Acc : 0.850, Run Time : 1.13
INFO:root:2019-05-12 02:44:12, Epoch : 1, Step : 4298, Training Loss : 0.22910, Training Acc : 0.908, Run Time : 6.49
INFO:root:2019-05-12 02:44:13, Epoch : 1, Step : 4299, Training Loss : 0.24071, Training Acc : 0.911, Run Time : 1.17
INFO:root:2019-05-12 02:44:15, Epoch : 1, Step : 4300, Training Loss : 0.39128, Training Acc : 0.853, Run Time : 1.45
INFO:root:2019-05-12 02:44:24, Epoch : 1, Step : 4301, Training Loss : 1.25328, Training Acc : 0.614, Run Time : 8.96
INFO:root:2019-05-12 02:44:25, Epoch : 1, Step : 4302, Training Loss : 1.62599, Training Acc : 0.525, Run Time : 1.57
INFO:root:2019-05-12 02:44:34, Epoch : 1, Step : 4303, Training Loss : 1.69053, Training Acc : 0.475, Run Time : 8.76
INFO:root:2019-05-12 02:44:35, Epoch : 1, Step : 4304, Training Loss : 0.87998, Training Acc : 0.644, Run Time : 1.23
INFO:root:2019-05-12 02:44:43, Epoch : 1, Step : 4305, Training Loss : 1.01632, Training Acc : 0.608, Run Time : 7.55
INFO:root:2019-05-12 02:44:44, Epoch : 1, Step : 4306, Training Loss : 0.56304, Training Acc : 0.769, Run Time : 1.17
INFO:root:2019-05-12 02:44:50, Epoch : 1, Step : 4307, Training Loss : 0.55041, Training Acc : 0.744, Run Time : 6.30
INFO:root:2019-05-12 02:44:51, Epoch : 1, Step : 4308, Training Loss : 0.63794, Training Acc : 0.772, Run Time : 1.14
INFO:root:2019-05-12 02:45:01, Epoch : 1, Step : 4309, Training Loss : 0.57486, Training Acc : 0.733, Run Time : 9.88
INFO:root:2019-05-12 02:45:03, Epoch : 1, Step : 4310, Training Loss : 0.64912, Training Acc : 0.678, Run Time : 2.02
INFO:root:2019-05-12 02:45:12, Epoch : 1, Step : 4311, Training Loss : 0.86901, Training Acc : 0.611, Run Time : 8.77
INFO:root:2019-05-12 02:45:14, Epoch : 1, Step : 4312, Training Loss : 0.76253, Training Acc : 0.608, Run Time : 1.44
INFO:root:2019-05-12 02:45:15, Epoch : 1, Step : 4313, Training Loss : 0.87579, Training Acc : 0.625, Run Time : 1.18
INFO:root:2019-05-12 02:45:20, Epoch : 1, Step : 4314, Training Loss : 0.56737, Training Acc : 0.683, Run Time : 5.71
INFO:root:2019-05-12 02:45:22, Epoch : 1, Step : 4315, Training Loss : 0.37142, Training Acc : 0.850, Run Time : 1.63
INFO:root:2019-05-12 02:45:24, Epoch : 1, Step : 4316, Training Loss : 0.40603, Training Acc : 0.806, Run Time : 2.20
INFO:root:2019-05-12 02:45:25, Epoch : 1, Step : 4317, Training Loss : 0.45398, Training Acc : 0.842, Run Time : 1.24
INFO:root:2019-05-12 02:45:29, Epoch : 1, Step : 4318, Training Loss : 0.45030, Training Acc : 0.800, Run Time : 3.36
INFO:root:2019-05-12 02:45:30, Epoch : 1, Step : 4319, Training Loss : 0.43758, Training Acc : 0.822, Run Time : 1.32
INFO:root:2019-05-12 02:45:37, Epoch : 1, Step : 4320, Training Loss : 0.27725, Training Acc : 0.925, Run Time : 6.72
INFO:root:2019-05-12 02:45:39, Epoch : 1, Step : 4321, Training Loss : 0.34568, Training Acc : 0.908, Run Time : 1.73
INFO:root:2019-05-12 02:45:44, Epoch : 1, Step : 4322, Training Loss : 0.25602, Training Acc : 0.925, Run Time : 4.91
INFO:root:2019-05-12 02:45:45, Epoch : 1, Step : 4323, Training Loss : 0.56909, Training Acc : 0.631, Run Time : 1.12
INFO:root:2019-05-12 02:45:47, Epoch : 1, Step : 4324, Training Loss : 0.23781, Training Acc : 0.956, Run Time : 2.14
INFO:root:2019-05-12 02:45:55, Epoch : 1, Step : 4325, Training Loss : 0.22946, Training Acc : 0.961, Run Time : 7.98
INFO:root:2019-05-12 02:45:57, Epoch : 1, Step : 4326, Training Loss : 0.20199, Training Acc : 0.978, Run Time : 2.02
INFO:root:2019-05-12 02:46:05, Epoch : 1, Step : 4327, Training Loss : 0.15184, Training Acc : 0.972, Run Time : 7.78
INFO:root:2019-05-12 02:46:07, Epoch : 1, Step : 4328, Training Loss : 0.21453, Training Acc : 0.967, Run Time : 2.09
INFO:root:2019-05-12 02:46:15, Epoch : 1, Step : 4329, Training Loss : 0.40766, Training Acc : 0.814, Run Time : 8.40
INFO:root:2019-05-12 02:46:17, Epoch : 1, Step : 4330, Training Loss : 0.47158, Training Acc : 0.694, Run Time : 1.58
INFO:root:2019-05-12 02:46:23, Epoch : 1, Step : 4331, Training Loss : 0.27525, Training Acc : 0.956, Run Time : 6.52
INFO:root:2019-05-12 02:46:25, Epoch : 1, Step : 4332, Training Loss : 0.07544, Training Acc : 0.997, Run Time : 1.69
INFO:root:2019-05-12 02:46:32, Epoch : 1, Step : 4333, Training Loss : 0.82899, Training Acc : 0.403, Run Time : 6.76
INFO:root:2019-05-12 02:46:33, Epoch : 1, Step : 4334, Training Loss : 0.31840, Training Acc : 0.869, Run Time : 1.45
INFO:root:2019-05-12 02:46:41, Epoch : 1, Step : 4335, Training Loss : 0.49801, Training Acc : 0.667, Run Time : 7.47
INFO:root:2019-05-12 02:46:42, Epoch : 1, Step : 4336, Training Loss : 0.06161, Training Acc : 1.000, Run Time : 1.21
INFO:root:2019-05-12 02:46:43, Epoch : 1, Step : 4337, Training Loss : 0.13524, Training Acc : 0.989, Run Time : 1.24
INFO:root:2019-05-12 02:46:44, Epoch : 1, Step : 4338, Training Loss : 0.18248, Training Acc : 0.994, Run Time : 1.29
INFO:root:2019-05-12 02:46:52, Epoch : 1, Step : 4339, Training Loss : 0.21778, Training Acc : 0.992, Run Time : 7.32
INFO:root:2019-05-12 02:46:53, Epoch : 1, Step : 4340, Training Loss : 0.14471, Training Acc : 0.989, Run Time : 1.38
INFO:root:2019-05-12 02:46:54, Epoch : 1, Step : 4341, Training Loss : 0.39581, Training Acc : 0.833, Run Time : 1.17
INFO:root:2019-05-12 02:46:55, Epoch : 1, Step : 4342, Training Loss : 0.38557, Training Acc : 0.817, Run Time : 1.32
INFO:root:2019-05-12 02:47:01, Epoch : 1, Step : 4343, Training Loss : 0.25201, Training Acc : 0.936, Run Time : 5.21
INFO:root:2019-05-12 02:47:02, Epoch : 1, Step : 4344, Training Loss : 0.35036, Training Acc : 0.850, Run Time : 1.12
INFO:root:2019-05-12 02:47:03, Epoch : 1, Step : 4345, Training Loss : 0.30315, Training Acc : 0.942, Run Time : 1.47
INFO:root:2019-05-12 02:47:10, Epoch : 1, Step : 4346, Training Loss : 0.16804, Training Acc : 0.981, Run Time : 6.97
INFO:root:2019-05-12 02:47:11, Epoch : 1, Step : 4347, Training Loss : 0.33021, Training Acc : 0.911, Run Time : 1.13
INFO:root:2019-05-12 02:47:20, Epoch : 1, Step : 4348, Training Loss : 0.25204, Training Acc : 0.956, Run Time : 9.02
INFO:root:2019-05-12 02:47:23, Epoch : 1, Step : 4349, Training Loss : 0.10496, Training Acc : 0.992, Run Time : 2.21
INFO:root:2019-05-12 02:47:28, Epoch : 1, Step : 4350, Training Loss : 0.18502, Training Acc : 0.961, Run Time : 5.52
INFO:root:2019-05-12 02:47:30, Epoch : 1, Step : 4351, Training Loss : 0.32004, Training Acc : 0.883, Run Time : 1.79
INFO:root:2019-05-12 02:47:40, Epoch : 1, Step : 4352, Training Loss : 0.16214, Training Acc : 0.978, Run Time : 10.22
INFO:root:2019-05-12 02:47:41, Epoch : 1, Step : 4353, Training Loss : 0.12095, Training Acc : 0.992, Run Time : 1.16
INFO:root:2019-05-12 02:47:46, Epoch : 1, Step : 4354, Training Loss : 0.45270, Training Acc : 0.844, Run Time : 4.26
INFO:root:2019-05-12 02:47:47, Epoch : 1, Step : 4355, Training Loss : 0.32665, Training Acc : 0.928, Run Time : 1.51
INFO:root:2019-05-12 02:47:48, Epoch : 1, Step : 4356, Training Loss : 0.32312, Training Acc : 0.953, Run Time : 1.20
INFO:root:2019-05-12 02:47:50, Epoch : 1, Step : 4357, Training Loss : 0.20089, Training Acc : 0.978, Run Time : 1.42
INFO:root:2019-05-12 02:47:56, Epoch : 1, Step : 4358, Training Loss : 0.19416, Training Acc : 0.986, Run Time : 6.45
INFO:root:2019-05-12 02:47:57, Epoch : 1, Step : 4359, Training Loss : 0.18518, Training Acc : 0.981, Run Time : 1.13
INFO:root:2019-05-12 02:48:01, Epoch : 1, Step : 4360, Training Loss : 0.20214, Training Acc : 0.956, Run Time : 3.82
INFO:root:2019-05-12 02:48:02, Epoch : 1, Step : 4361, Training Loss : 0.37831, Training Acc : 0.861, Run Time : 1.14
INFO:root:2019-05-12 02:48:03, Epoch : 1, Step : 4362, Training Loss : 0.99343, Training Acc : 0.506, Run Time : 1.12
INFO:root:2019-05-12 02:48:06, Epoch : 1, Step : 4363, Training Loss : 0.14252, Training Acc : 0.983, Run Time : 2.20
INFO:root:2019-05-12 02:48:08, Epoch : 1, Step : 4364, Training Loss : 0.40723, Training Acc : 0.889, Run Time : 2.75
INFO:root:2019-05-12 02:48:09, Epoch : 1, Step : 4365, Training Loss : 0.52042, Training Acc : 0.697, Run Time : 1.13
INFO:root:2019-05-12 02:48:13, Epoch : 1, Step : 4366, Training Loss : 0.37055, Training Acc : 0.825, Run Time : 3.51
INFO:root:2019-05-12 02:48:14, Epoch : 1, Step : 4367, Training Loss : 0.32276, Training Acc : 0.931, Run Time : 1.19
INFO:root:2019-05-12 02:48:16, Epoch : 1, Step : 4368, Training Loss : 0.55704, Training Acc : 0.672, Run Time : 1.55
INFO:root:2019-05-12 02:48:22, Epoch : 1, Step : 4369, Training Loss : 0.40649, Training Acc : 0.878, Run Time : 6.38
INFO:root:2019-05-12 02:48:23, Epoch : 1, Step : 4370, Training Loss : 0.44952, Training Acc : 0.847, Run Time : 1.15
INFO:root:2019-05-12 02:48:24, Epoch : 1, Step : 4371, Training Loss : 0.21889, Training Acc : 0.972, Run Time : 1.13
INFO:root:2019-05-12 02:48:26, Epoch : 1, Step : 4372, Training Loss : 0.39897, Training Acc : 0.942, Run Time : 1.34
INFO:root:2019-05-12 02:48:31, Epoch : 1, Step : 4373, Training Loss : 0.43926, Training Acc : 0.936, Run Time : 5.18
INFO:root:2019-05-12 02:48:33, Epoch : 1, Step : 4374, Training Loss : 0.60003, Training Acc : 0.533, Run Time : 2.05
INFO:root:2019-05-12 02:48:38, Epoch : 1, Step : 4375, Training Loss : 0.88985, Training Acc : 0.472, Run Time : 5.05
INFO:root:2019-05-12 02:48:39, Epoch : 1, Step : 4376, Training Loss : 0.33689, Training Acc : 0.975, Run Time : 1.22
INFO:root:2019-05-12 02:48:40, Epoch : 1, Step : 4377, Training Loss : 0.55915, Training Acc : 0.661, Run Time : 1.16
INFO:root:2019-05-12 02:48:44, Epoch : 1, Step : 4378, Training Loss : 0.38921, Training Acc : 0.833, Run Time : 4.02
INFO:root:2019-05-12 02:48:46, Epoch : 1, Step : 4379, Training Loss : 0.25221, Training Acc : 0.986, Run Time : 1.45
INFO:root:2019-05-12 02:48:48, Epoch : 1, Step : 4380, Training Loss : 0.25450, Training Acc : 0.983, Run Time : 1.89
INFO:root:2019-05-12 02:48:51, Epoch : 1, Step : 4381, Training Loss : 0.11666, Training Acc : 0.994, Run Time : 3.37
INFO:root:2019-05-12 02:48:53, Epoch : 1, Step : 4382, Training Loss : 0.11091, Training Acc : 0.978, Run Time : 1.45
INFO:root:2019-05-12 02:48:59, Epoch : 1, Step : 4383, Training Loss : 0.12660, Training Acc : 0.981, Run Time : 6.20
INFO:root:2019-05-12 02:49:00, Epoch : 1, Step : 4384, Training Loss : 0.22677, Training Acc : 0.956, Run Time : 1.73
INFO:root:2019-05-12 02:49:08, Epoch : 1, Step : 4385, Training Loss : 0.15132, Training Acc : 0.989, Run Time : 7.46
INFO:root:2019-05-12 02:49:09, Epoch : 1, Step : 4386, Training Loss : 0.20062, Training Acc : 0.969, Run Time : 1.14
INFO:root:2019-05-12 02:49:10, Epoch : 1, Step : 4387, Training Loss : 0.13840, Training Acc : 0.989, Run Time : 1.14
INFO:root:2019-05-12 02:49:12, Epoch : 1, Step : 4388, Training Loss : 0.07230, Training Acc : 1.000, Run Time : 1.43
INFO:root:2019-05-12 02:49:18, Epoch : 1, Step : 4389, Training Loss : 0.37966, Training Acc : 0.928, Run Time : 6.72
INFO:root:2019-05-12 02:49:20, Epoch : 1, Step : 4390, Training Loss : 0.71478, Training Acc : 0.550, Run Time : 1.39
INFO:root:2019-05-12 02:49:21, Epoch : 1, Step : 4391, Training Loss : 0.21413, Training Acc : 0.942, Run Time : 1.18
INFO:root:2019-05-12 02:49:29, Epoch : 1, Step : 4392, Training Loss : 0.39200, Training Acc : 0.908, Run Time : 8.14
INFO:root:2019-05-12 02:49:31, Epoch : 1, Step : 4393, Training Loss : 0.31295, Training Acc : 0.903, Run Time : 1.57
INFO:root:2019-05-12 02:49:33, Epoch : 1, Step : 4394, Training Loss : 0.34896, Training Acc : 0.975, Run Time : 2.58
INFO:root:2019-05-12 02:49:34, Epoch : 1, Step : 4395, Training Loss : 0.34248, Training Acc : 0.928, Run Time : 1.22
INFO:root:2019-05-12 02:49:36, Epoch : 1, Step : 4396, Training Loss : 0.31713, Training Acc : 0.881, Run Time : 1.96
INFO:root:2019-05-12 02:49:45, Epoch : 1, Step : 4397, Training Loss : 0.05802, Training Acc : 0.997, Run Time : 8.63
INFO:root:2019-05-12 02:49:47, Epoch : 1, Step : 4398, Training Loss : 0.21867, Training Acc : 0.989, Run Time : 1.89
INFO:root:2019-05-12 02:49:56, Epoch : 1, Step : 4399, Training Loss : 0.19154, Training Acc : 0.994, Run Time : 8.99
INFO:root:2019-05-12 02:49:57, Epoch : 1, Step : 4400, Training Loss : 0.19369, Training Acc : 0.983, Run Time : 1.58
INFO:root:2019-05-12 02:50:05, Epoch : 1, Step : 4401, Training Loss : 0.29403, Training Acc : 0.864, Run Time : 7.14
INFO:root:2019-05-12 02:50:08, Epoch : 1, Step : 4402, Training Loss : 0.93812, Training Acc : 0.717, Run Time : 2.96
INFO:root:2019-05-12 02:50:13, Epoch : 1, Step : 4403, Training Loss : 0.37773, Training Acc : 0.867, Run Time : 5.10
INFO:root:2019-05-12 02:50:14, Epoch : 1, Step : 4404, Training Loss : 0.43629, Training Acc : 0.883, Run Time : 1.19
INFO:root:2019-05-12 02:50:19, Epoch : 1, Step : 4405, Training Loss : 0.29671, Training Acc : 0.886, Run Time : 5.20
INFO:root:2019-05-12 02:50:20, Epoch : 1, Step : 4406, Training Loss : 0.21160, Training Acc : 0.914, Run Time : 1.31
INFO:root:2019-05-12 02:50:25, Epoch : 1, Step : 4407, Training Loss : 0.23476, Training Acc : 0.928, Run Time : 4.94
INFO:root:2019-05-12 02:50:27, Epoch : 1, Step : 4408, Training Loss : 0.19242, Training Acc : 0.944, Run Time : 1.60
INFO:root:2019-05-12 02:50:33, Epoch : 1, Step : 4409, Training Loss : 0.18628, Training Acc : 0.964, Run Time : 6.43
INFO:root:2019-05-12 02:50:36, Epoch : 1, Step : 4410, Training Loss : 0.23103, Training Acc : 0.908, Run Time : 2.63
INFO:root:2019-05-12 02:50:44, Epoch : 1, Step : 4411, Training Loss : 0.21736, Training Acc : 0.972, Run Time : 8.41
INFO:root:2019-05-12 02:50:46, Epoch : 1, Step : 4412, Training Loss : 0.24297, Training Acc : 0.931, Run Time : 1.93
INFO:root:2019-05-12 02:50:54, Epoch : 1, Step : 4413, Training Loss : 0.11755, Training Acc : 0.969, Run Time : 7.34
INFO:root:2019-05-12 02:50:55, Epoch : 1, Step : 4414, Training Loss : 0.10656, Training Acc : 0.967, Run Time : 1.36
INFO:root:2019-05-12 02:51:00, Epoch : 1, Step : 4415, Training Loss : 0.11309, Training Acc : 0.964, Run Time : 4.80
INFO:root:2019-05-12 02:51:01, Epoch : 1, Step : 4416, Training Loss : 0.09816, Training Acc : 0.978, Run Time : 1.32
INFO:root:2019-05-12 02:51:02, Epoch : 1, Step : 4417, Training Loss : 0.17286, Training Acc : 0.925, Run Time : 1.24
INFO:root:2019-05-12 02:51:09, Epoch : 1, Step : 4418, Training Loss : 0.18813, Training Acc : 0.944, Run Time : 6.72
INFO:root:2019-05-12 02:51:10, Epoch : 1, Step : 4419, Training Loss : 0.25577, Training Acc : 0.947, Run Time : 1.27
INFO:root:2019-05-12 02:51:11, Epoch : 1, Step : 4420, Training Loss : 0.09813, Training Acc : 0.978, Run Time : 1.14
INFO:root:2019-05-12 02:51:13, Epoch : 1, Step : 4421, Training Loss : 0.19173, Training Acc : 0.944, Run Time : 1.22
INFO:root:2019-05-12 02:51:14, Epoch : 1, Step : 4422, Training Loss : 0.08228, Training Acc : 0.992, Run Time : 1.22
INFO:root:2019-05-12 02:51:18, Epoch : 1, Step : 4423, Training Loss : 0.12896, Training Acc : 0.967, Run Time : 4.55
INFO:root:2019-05-12 02:51:20, Epoch : 1, Step : 4424, Training Loss : 0.22442, Training Acc : 0.922, Run Time : 1.32
INFO:root:2019-05-12 02:51:21, Epoch : 1, Step : 4425, Training Loss : 0.36594, Training Acc : 0.892, Run Time : 1.22
INFO:root:2019-05-12 02:51:26, Epoch : 1, Step : 4426, Training Loss : 0.20775, Training Acc : 0.942, Run Time : 5.32
INFO:root:2019-05-12 02:51:28, Epoch : 1, Step : 4427, Training Loss : 0.22055, Training Acc : 0.942, Run Time : 1.19
INFO:root:2019-05-12 02:51:29, Epoch : 1, Step : 4428, Training Loss : 0.33270, Training Acc : 0.922, Run Time : 1.65
INFO:root:2019-05-12 02:51:33, Epoch : 1, Step : 4429, Training Loss : 1.25712, Training Acc : 0.431, Run Time : 3.66
INFO:root:2019-05-12 02:51:35, Epoch : 1, Step : 4430, Training Loss : 0.36168, Training Acc : 0.828, Run Time : 1.94
INFO:root:2019-05-12 02:51:36, Epoch : 1, Step : 4431, Training Loss : 0.44678, Training Acc : 0.667, Run Time : 1.67
INFO:root:2019-05-12 02:51:43, Epoch : 1, Step : 4432, Training Loss : 0.22740, Training Acc : 0.903, Run Time : 6.79
INFO:root:2019-05-12 02:51:44, Epoch : 1, Step : 4433, Training Loss : 0.48201, Training Acc : 0.658, Run Time : 1.17
INFO:root:2019-05-12 02:51:46, Epoch : 1, Step : 4434, Training Loss : 0.31085, Training Acc : 0.844, Run Time : 1.15
INFO:root:2019-05-12 02:51:55, Epoch : 1, Step : 4435, Training Loss : 0.20482, Training Acc : 0.944, Run Time : 9.14
INFO:root:2019-05-12 02:51:57, Epoch : 1, Step : 4436, Training Loss : 0.31910, Training Acc : 0.836, Run Time : 1.97
INFO:root:2019-05-12 02:52:02, Epoch : 1, Step : 4437, Training Loss : 0.32734, Training Acc : 0.875, Run Time : 5.59
INFO:root:2019-05-12 02:52:04, Epoch : 1, Step : 4438, Training Loss : 0.30731, Training Acc : 0.917, Run Time : 1.35
INFO:root:2019-05-12 02:52:05, Epoch : 1, Step : 4439, Training Loss : 0.23121, Training Acc : 0.961, Run Time : 1.27
INFO:root:2019-05-12 02:52:15, Epoch : 1, Step : 4440, Training Loss : 0.09612, Training Acc : 0.964, Run Time : 10.52
INFO:root:2019-05-12 02:52:17, Epoch : 1, Step : 4441, Training Loss : 0.18824, Training Acc : 0.953, Run Time : 2.05
INFO:root:2019-05-12 02:52:19, Epoch : 1, Step : 4442, Training Loss : 0.21690, Training Acc : 0.953, Run Time : 1.76
INFO:root:2019-05-12 02:52:22, Epoch : 1, Step : 4443, Training Loss : 0.18418, Training Acc : 0.958, Run Time : 2.29
INFO:root:2019-05-12 02:52:23, Epoch : 1, Step : 4444, Training Loss : 0.10261, Training Acc : 0.981, Run Time : 1.68
INFO:root:2019-05-12 02:52:31, Epoch : 1, Step : 4445, Training Loss : 0.12451, Training Acc : 0.967, Run Time : 7.92
INFO:root:2019-05-12 02:52:32, Epoch : 1, Step : 4446, Training Loss : 0.29340, Training Acc : 0.892, Run Time : 1.21
INFO:root:2019-05-12 02:52:34, Epoch : 1, Step : 4447, Training Loss : 0.41795, Training Acc : 0.750, Run Time : 1.96
INFO:root:2019-05-12 02:52:42, Epoch : 1, Step : 4448, Training Loss : 0.28043, Training Acc : 0.889, Run Time : 7.79
INFO:root:2019-05-12 02:52:43, Epoch : 1, Step : 4449, Training Loss : 0.38460, Training Acc : 0.850, Run Time : 1.13
INFO:root:2019-05-12 02:52:44, Epoch : 1, Step : 4450, Training Loss : 0.14263, Training Acc : 0.956, Run Time : 1.28
INFO:root:2019-05-12 02:52:52, Epoch : 1, Step : 4451, Training Loss : 0.21340, Training Acc : 0.964, Run Time : 7.26
INFO:root:2019-05-12 02:52:53, Epoch : 1, Step : 4452, Training Loss : 0.28853, Training Acc : 0.950, Run Time : 1.17
INFO:root:2019-05-12 02:52:54, Epoch : 1, Step : 4453, Training Loss : 0.12008, Training Acc : 0.967, Run Time : 1.33
INFO:root:2019-05-12 02:52:56, Epoch : 1, Step : 4454, Training Loss : 0.19461, Training Acc : 0.975, Run Time : 1.32
INFO:root:2019-05-12 02:53:01, Epoch : 1, Step : 4455, Training Loss : 0.21364, Training Acc : 0.961, Run Time : 5.81
INFO:root:2019-05-12 02:53:03, Epoch : 1, Step : 4456, Training Loss : 0.19236, Training Acc : 0.956, Run Time : 1.13
INFO:root:2019-05-12 02:53:04, Epoch : 1, Step : 4457, Training Loss : 0.22856, Training Acc : 0.933, Run Time : 1.13
INFO:root:2019-05-12 02:53:05, Epoch : 1, Step : 4458, Training Loss : 0.30151, Training Acc : 0.472, Run Time : 1.65
wsi/bin/train.py:117: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  data_tumor = Variable(data_tumor.cuda(async=True), volatile=True)
wsi/bin/train.py:121: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  data_normal = Variable(data_normal.cuda(async=True), volatile=True)
Traceback (most recent call last):
  File "wsi/bin/train.py", line 259, in <module>
    main()
  File "wsi/bin/train.py", line 255, in main
    run(args)
  File "wsi/bin/train.py", line 224, in run
    dataloader_normal_valid)
  File "wsi/bin/train.py", line 126, in valid_epoch
    output = model(data)
  File "/scratch/sg5591/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/scratch/sg5591/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 152, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/scratch/sg5591/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 162, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/scratch/sg5591/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in parallel_apply
    raise output
  File "/scratch/sg5591/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 59, in _worker
    output = module(*input, **kwargs)
  File "/scratch/sg5591/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/scratch/sg5591/Breast-Cancer-metastisis-detection/wsi/bin/../../wsi/model/resnet.py", line 137, in forward
    x = self.bn1(x)
  File "/scratch/sg5591/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/scratch/sg5591/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 83, in forward
    exponential_average_factor, self.eps)
  File "/scratch/sg5591/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/functional.py", line 1697, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: CUDA out of memory. Tried to allocate 1.08 GiB (GPU 0; 11.17 GiB total capacity; 9.65 GiB already allocated; 685.06 MiB free; 505.48 MiB cached)
