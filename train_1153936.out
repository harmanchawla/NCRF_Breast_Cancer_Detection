INFO:root:2019-05-12 01:42:45, Epoch : 1, Step : 1, Training Loss : 0.93996, Training Acc : 0.172, Run Time : 26.57
INFO:root:2019-05-12 01:42:46, Epoch : 1, Step : 2, Training Loss : 0.84703, Training Acc : 0.211, Run Time : 0.82
INFO:root:2019-05-12 01:42:47, Epoch : 1, Step : 3, Training Loss : 0.73432, Training Acc : 0.394, Run Time : 0.67
INFO:root:2019-05-12 01:42:52, Epoch : 1, Step : 4, Training Loss : 0.60122, Training Acc : 0.722, Run Time : 4.96
INFO:root:2019-05-12 01:42:53, Epoch : 1, Step : 5, Training Loss : 0.57824, Training Acc : 0.706, Run Time : 0.67
INFO:root:2019-05-12 01:42:54, Epoch : 1, Step : 6, Training Loss : 0.58382, Training Acc : 0.700, Run Time : 1.18
INFO:root:2019-05-12 01:43:00, Epoch : 1, Step : 7, Training Loss : 0.55234, Training Acc : 0.706, Run Time : 6.64
INFO:root:2019-05-12 01:43:01, Epoch : 1, Step : 8, Training Loss : 0.74116, Training Acc : 0.689, Run Time : 0.50
INFO:root:2019-05-12 01:43:04, Epoch : 1, Step : 9, Training Loss : 0.65254, Training Acc : 0.678, Run Time : 2.68
INFO:root:2019-05-12 01:43:09, Epoch : 1, Step : 10, Training Loss : 0.68249, Training Acc : 0.689, Run Time : 5.45
INFO:root:2019-05-12 01:43:09, Epoch : 1, Step : 11, Training Loss : 0.57837, Training Acc : 0.728, Run Time : 0.43
INFO:root:2019-05-12 01:43:11, Epoch : 1, Step : 12, Training Loss : 0.77957, Training Acc : 0.661, Run Time : 1.28
INFO:root:2019-05-12 01:43:15, Epoch : 1, Step : 13, Training Loss : 0.67086, Training Acc : 0.644, Run Time : 4.18
INFO:root:2019-05-12 01:43:16, Epoch : 1, Step : 14, Training Loss : 0.74612, Training Acc : 0.622, Run Time : 0.64
INFO:root:2019-05-12 01:43:21, Epoch : 1, Step : 15, Training Loss : 0.72114, Training Acc : 0.594, Run Time : 5.21
INFO:root:2019-05-12 01:43:21, Epoch : 1, Step : 16, Training Loss : 0.68553, Training Acc : 0.611, Run Time : 0.62
INFO:root:2019-05-12 01:43:22, Epoch : 1, Step : 17, Training Loss : 0.78511, Training Acc : 0.561, Run Time : 0.66
INFO:root:2019-05-12 01:43:23, Epoch : 1, Step : 18, Training Loss : 0.61612, Training Acc : 0.639, Run Time : 0.70
INFO:root:2019-05-12 01:43:28, Epoch : 1, Step : 19, Training Loss : 0.57910, Training Acc : 0.600, Run Time : 5.01
INFO:root:2019-05-12 01:43:28, Epoch : 1, Step : 20, Training Loss : 0.63024, Training Acc : 0.633, Run Time : 0.64
INFO:root:2019-05-12 01:43:29, Epoch : 1, Step : 21, Training Loss : 0.53351, Training Acc : 0.794, Run Time : 0.56
INFO:root:2019-05-12 01:43:36, Epoch : 1, Step : 22, Training Loss : 0.58583, Training Acc : 0.722, Run Time : 7.10
INFO:root:2019-05-12 01:43:37, Epoch : 1, Step : 23, Training Loss : 0.67558, Training Acc : 0.539, Run Time : 0.62
INFO:root:2019-05-12 01:43:39, Epoch : 1, Step : 24, Training Loss : 0.64797, Training Acc : 0.606, Run Time : 2.21
INFO:root:2019-05-12 01:43:44, Epoch : 1, Step : 25, Training Loss : 0.61384, Training Acc : 0.678, Run Time : 4.82
INFO:root:2019-05-12 01:43:44, Epoch : 1, Step : 26, Training Loss : 0.66778, Training Acc : 0.617, Run Time : 0.63
INFO:root:2019-05-12 01:43:46, Epoch : 1, Step : 27, Training Loss : 0.61903, Training Acc : 0.656, Run Time : 1.77
INFO:root:2019-05-12 01:43:47, Epoch : 1, Step : 28, Training Loss : 0.59624, Training Acc : 0.700, Run Time : 0.63
INFO:root:2019-05-12 01:43:49, Epoch : 1, Step : 29, Training Loss : 0.63011, Training Acc : 0.650, Run Time : 2.50
INFO:root:2019-05-12 01:43:52, Epoch : 1, Step : 30, Training Loss : 0.54542, Training Acc : 0.706, Run Time : 2.79
INFO:root:2019-05-12 01:43:53, Epoch : 1, Step : 31, Training Loss : 0.55391, Training Acc : 0.672, Run Time : 0.61
INFO:root:2019-05-12 01:43:58, Epoch : 1, Step : 32, Training Loss : 0.51748, Training Acc : 0.822, Run Time : 5.86
INFO:root:2019-05-12 01:43:59, Epoch : 1, Step : 33, Training Loss : 0.49572, Training Acc : 0.822, Run Time : 0.56
INFO:root:2019-05-12 01:44:08, Epoch : 1, Step : 34, Training Loss : 0.54405, Training Acc : 0.728, Run Time : 8.55
INFO:root:2019-05-12 01:44:08, Epoch : 1, Step : 35, Training Loss : 0.52191, Training Acc : 0.789, Run Time : 0.47
INFO:root:2019-05-12 01:44:09, Epoch : 1, Step : 36, Training Loss : 0.43508, Training Acc : 0.878, Run Time : 1.35
INFO:root:2019-05-12 01:44:13, Epoch : 1, Step : 37, Training Loss : 0.54809, Training Acc : 0.678, Run Time : 3.71
INFO:root:2019-05-12 01:44:14, Epoch : 1, Step : 38, Training Loss : 0.48834, Training Acc : 0.778, Run Time : 0.58
INFO:root:2019-05-12 01:44:16, Epoch : 1, Step : 39, Training Loss : 0.41562, Training Acc : 0.867, Run Time : 2.63
INFO:root:2019-05-12 01:44:17, Epoch : 1, Step : 40, Training Loss : 0.44401, Training Acc : 0.833, Run Time : 0.78
INFO:root:2019-05-12 01:44:18, Epoch : 1, Step : 41, Training Loss : 0.47143, Training Acc : 0.800, Run Time : 1.03
INFO:root:2019-05-12 01:44:25, Epoch : 1, Step : 42, Training Loss : 0.43226, Training Acc : 0.833, Run Time : 6.66
INFO:root:2019-05-12 01:44:25, Epoch : 1, Step : 43, Training Loss : 0.56389, Training Acc : 0.694, Run Time : 0.68
INFO:root:2019-05-12 01:44:27, Epoch : 1, Step : 44, Training Loss : 0.50048, Training Acc : 0.794, Run Time : 1.23
INFO:root:2019-05-12 01:44:32, Epoch : 1, Step : 45, Training Loss : 0.54128, Training Acc : 0.739, Run Time : 5.44
INFO:root:2019-05-12 01:44:33, Epoch : 1, Step : 46, Training Loss : 0.41165, Training Acc : 0.911, Run Time : 0.72
INFO:root:2019-05-12 01:44:36, Epoch : 1, Step : 47, Training Loss : 0.39192, Training Acc : 0.850, Run Time : 2.89
INFO:root:2019-05-12 01:44:37, Epoch : 1, Step : 48, Training Loss : 0.43994, Training Acc : 0.789, Run Time : 1.17
INFO:root:2019-05-12 01:44:37, Epoch : 1, Step : 49, Training Loss : 0.31321, Training Acc : 0.906, Run Time : 0.52
INFO:root:2019-05-12 01:44:39, Epoch : 1, Step : 50, Training Loss : 0.46006, Training Acc : 0.789, Run Time : 1.25
INFO:root:2019-05-12 01:44:40, Epoch : 1, Step : 51, Training Loss : 0.42393, Training Acc : 0.839, Run Time : 1.07
INFO:root:2019-05-12 01:44:48, Epoch : 1, Step : 52, Training Loss : 0.32303, Training Acc : 0.900, Run Time : 7.84
INFO:root:2019-05-12 01:44:48, Epoch : 1, Step : 53, Training Loss : 0.47649, Training Acc : 0.722, Run Time : 0.56
INFO:root:2019-05-12 01:44:50, Epoch : 1, Step : 54, Training Loss : 0.27581, Training Acc : 0.956, Run Time : 2.12
INFO:root:2019-05-12 01:44:54, Epoch : 1, Step : 55, Training Loss : 0.64389, Training Acc : 0.606, Run Time : 3.88
INFO:root:2019-05-12 01:44:55, Epoch : 1, Step : 56, Training Loss : 1.36753, Training Acc : 0.278, Run Time : 0.56
INFO:root:2019-05-12 01:44:55, Epoch : 1, Step : 57, Training Loss : 1.03812, Training Acc : 0.494, Run Time : 0.67
INFO:root:2019-05-12 01:45:01, Epoch : 1, Step : 58, Training Loss : 1.00543, Training Acc : 0.439, Run Time : 5.88
INFO:root:2019-05-12 01:45:02, Epoch : 1, Step : 59, Training Loss : 0.69361, Training Acc : 0.594, Run Time : 0.57
INFO:root:2019-05-12 01:45:02, Epoch : 1, Step : 60, Training Loss : 0.44716, Training Acc : 0.789, Run Time : 0.61
INFO:root:2019-05-12 01:45:03, Epoch : 1, Step : 61, Training Loss : 0.55300, Training Acc : 0.711, Run Time : 0.59
INFO:root:2019-05-12 01:45:09, Epoch : 1, Step : 62, Training Loss : 0.50954, Training Acc : 0.789, Run Time : 5.69
INFO:root:2019-05-12 01:45:09, Epoch : 1, Step : 63, Training Loss : 0.51927, Training Acc : 0.783, Run Time : 0.61
INFO:root:2019-05-12 01:45:14, Epoch : 1, Step : 64, Training Loss : 0.54527, Training Acc : 0.794, Run Time : 4.56
INFO:root:2019-05-12 01:45:15, Epoch : 1, Step : 65, Training Loss : 0.53146, Training Acc : 0.800, Run Time : 0.89
INFO:root:2019-05-12 01:45:16, Epoch : 1, Step : 66, Training Loss : 0.56367, Training Acc : 0.750, Run Time : 0.86
INFO:root:2019-05-12 01:45:22, Epoch : 1, Step : 67, Training Loss : 0.53506, Training Acc : 0.706, Run Time : 6.63
INFO:root:2019-05-12 01:45:23, Epoch : 1, Step : 68, Training Loss : 0.54944, Training Acc : 0.711, Run Time : 0.65
INFO:root:2019-05-12 01:45:25, Epoch : 1, Step : 69, Training Loss : 0.53814, Training Acc : 0.783, Run Time : 1.70
INFO:root:2019-05-12 01:45:26, Epoch : 1, Step : 70, Training Loss : 0.63840, Training Acc : 0.633, Run Time : 1.67
INFO:root:2019-05-12 01:45:27, Epoch : 1, Step : 71, Training Loss : 0.54087, Training Acc : 0.767, Run Time : 0.73
INFO:root:2019-05-12 01:45:31, Epoch : 1, Step : 72, Training Loss : 0.40931, Training Acc : 0.789, Run Time : 4.40
INFO:root:2019-05-12 01:45:32, Epoch : 1, Step : 73, Training Loss : 0.49809, Training Acc : 0.733, Run Time : 0.67
INFO:root:2019-05-12 01:45:39, Epoch : 1, Step : 74, Training Loss : 0.51196, Training Acc : 0.756, Run Time : 6.57
INFO:root:2019-05-12 01:45:39, Epoch : 1, Step : 75, Training Loss : 0.45306, Training Acc : 0.817, Run Time : 0.54
INFO:root:2019-05-12 01:45:40, Epoch : 1, Step : 76, Training Loss : 0.56829, Training Acc : 0.683, Run Time : 1.19
INFO:root:2019-05-12 01:45:46, Epoch : 1, Step : 77, Training Loss : 0.61203, Training Acc : 0.744, Run Time : 5.34
INFO:root:2019-05-12 01:45:46, Epoch : 1, Step : 78, Training Loss : 0.56118, Training Acc : 0.672, Run Time : 0.63
INFO:root:2019-05-12 01:45:47, Epoch : 1, Step : 79, Training Loss : 0.38097, Training Acc : 0.839, Run Time : 0.56
INFO:root:2019-05-12 01:45:56, Epoch : 1, Step : 80, Training Loss : 0.40365, Training Acc : 0.856, Run Time : 9.52
INFO:root:2019-05-12 01:45:57, Epoch : 1, Step : 81, Training Loss : 0.41939, Training Acc : 0.883, Run Time : 0.50
INFO:root:2019-05-12 01:45:59, Epoch : 1, Step : 82, Training Loss : 0.56269, Training Acc : 0.733, Run Time : 1.76
INFO:root:2019-05-12 01:46:05, Epoch : 1, Step : 83, Training Loss : 0.46649, Training Acc : 0.794, Run Time : 6.14
INFO:root:2019-05-12 01:46:05, Epoch : 1, Step : 84, Training Loss : 0.62445, Training Acc : 0.739, Run Time : 0.47
INFO:root:2019-05-12 01:46:06, Epoch : 1, Step : 85, Training Loss : 0.60762, Training Acc : 0.756, Run Time : 0.60
INFO:root:2019-05-12 01:46:10, Epoch : 1, Step : 86, Training Loss : 0.54116, Training Acc : 0.683, Run Time : 4.49
INFO:root:2019-05-12 01:46:11, Epoch : 1, Step : 87, Training Loss : 0.47701, Training Acc : 0.806, Run Time : 0.60
INFO:root:2019-05-12 01:46:13, Epoch : 1, Step : 88, Training Loss : 0.46780, Training Acc : 0.750, Run Time : 1.66
INFO:root:2019-05-12 01:46:21, Epoch : 1, Step : 89, Training Loss : 0.47030, Training Acc : 0.783, Run Time : 7.95
INFO:root:2019-05-12 01:46:21, Epoch : 1, Step : 90, Training Loss : 0.46154, Training Acc : 0.761, Run Time : 0.45
INFO:root:2019-05-12 01:46:22, Epoch : 1, Step : 91, Training Loss : 0.42532, Training Acc : 0.872, Run Time : 0.69
INFO:root:2019-05-12 01:46:30, Epoch : 1, Step : 92, Training Loss : 0.41157, Training Acc : 0.844, Run Time : 7.92
INFO:root:2019-05-12 01:46:30, Epoch : 1, Step : 93, Training Loss : 0.35532, Training Acc : 0.867, Run Time : 0.67
INFO:root:2019-05-12 01:46:31, Epoch : 1, Step : 94, Training Loss : 0.42690, Training Acc : 0.833, Run Time : 1.06
INFO:root:2019-05-12 01:46:33, Epoch : 1, Step : 95, Training Loss : 0.36351, Training Acc : 0.850, Run Time : 1.62
INFO:root:2019-05-12 01:46:40, Epoch : 1, Step : 96, Training Loss : 0.42336, Training Acc : 0.850, Run Time : 6.59
INFO:root:2019-05-12 01:46:40, Epoch : 1, Step : 97, Training Loss : 0.34211, Training Acc : 0.856, Run Time : 0.70
INFO:root:2019-05-12 01:46:41, Epoch : 1, Step : 98, Training Loss : 0.28836, Training Acc : 0.900, Run Time : 0.60
INFO:root:2019-05-12 01:46:49, Epoch : 1, Step : 99, Training Loss : 0.48340, Training Acc : 0.767, Run Time : 8.05
INFO:root:2019-05-12 01:46:50, Epoch : 1, Step : 100, Training Loss : 0.50797, Training Acc : 0.717, Run Time : 0.68
INFO:root:2019-05-12 01:46:58, Epoch : 1, Step : 101, Training Loss : 0.35291, Training Acc : 0.850, Run Time : 8.07
INFO:root:2019-05-12 01:46:58, Epoch : 1, Step : 102, Training Loss : 0.37547, Training Acc : 0.794, Run Time : 0.71
INFO:root:2019-05-12 01:47:00, Epoch : 1, Step : 103, Training Loss : 0.37230, Training Acc : 0.828, Run Time : 1.42
INFO:root:2019-05-12 01:47:06, Epoch : 1, Step : 104, Training Loss : 0.55017, Training Acc : 0.722, Run Time : 6.28
INFO:root:2019-05-12 01:47:07, Epoch : 1, Step : 105, Training Loss : 0.54595, Training Acc : 0.811, Run Time : 0.78
INFO:root:2019-05-12 01:47:14, Epoch : 1, Step : 106, Training Loss : 0.28025, Training Acc : 0.872, Run Time : 6.78
INFO:root:2019-05-12 01:47:14, Epoch : 1, Step : 107, Training Loss : 0.33067, Training Acc : 0.850, Run Time : 0.50
INFO:root:2019-05-12 01:47:16, Epoch : 1, Step : 108, Training Loss : 0.42796, Training Acc : 0.794, Run Time : 1.68
INFO:root:2019-05-12 01:47:19, Epoch : 1, Step : 109, Training Loss : 0.21970, Training Acc : 0.944, Run Time : 3.06
INFO:root:2019-05-12 01:47:20, Epoch : 1, Step : 110, Training Loss : 0.43417, Training Acc : 0.761, Run Time : 0.60
INFO:root:2019-05-12 01:47:21, Epoch : 1, Step : 111, Training Loss : 0.33645, Training Acc : 0.894, Run Time : 1.02
INFO:root:2019-05-12 01:47:23, Epoch : 1, Step : 112, Training Loss : 0.43652, Training Acc : 0.739, Run Time : 2.85
INFO:root:2019-05-12 01:47:24, Epoch : 1, Step : 113, Training Loss : 0.41226, Training Acc : 0.800, Run Time : 0.67
INFO:root:2019-05-12 01:47:25, Epoch : 1, Step : 114, Training Loss : 0.42965, Training Acc : 0.806, Run Time : 0.59
INFO:root:2019-05-12 01:47:34, Epoch : 1, Step : 115, Training Loss : 0.26096, Training Acc : 0.917, Run Time : 9.28
INFO:root:2019-05-12 01:47:35, Epoch : 1, Step : 116, Training Loss : 0.34242, Training Acc : 0.856, Run Time : 0.61
INFO:root:2019-05-12 01:47:36, Epoch : 1, Step : 117, Training Loss : 0.28085, Training Acc : 0.861, Run Time : 1.35
INFO:root:2019-05-12 01:47:40, Epoch : 1, Step : 118, Training Loss : 0.30647, Training Acc : 0.883, Run Time : 4.22
INFO:root:2019-05-12 01:47:41, Epoch : 1, Step : 119, Training Loss : 0.28808, Training Acc : 0.894, Run Time : 0.74
INFO:root:2019-05-12 01:47:42, Epoch : 1, Step : 120, Training Loss : 0.40310, Training Acc : 0.783, Run Time : 1.58
INFO:root:2019-05-12 01:47:51, Epoch : 1, Step : 121, Training Loss : 0.38432, Training Acc : 0.833, Run Time : 8.43
INFO:root:2019-05-12 01:47:52, Epoch : 1, Step : 122, Training Loss : 0.39952, Training Acc : 0.800, Run Time : 0.65
INFO:root:2019-05-12 01:47:52, Epoch : 1, Step : 123, Training Loss : 0.34155, Training Acc : 0.844, Run Time : 0.72
INFO:root:2019-05-12 01:47:57, Epoch : 1, Step : 124, Training Loss : 0.31529, Training Acc : 0.867, Run Time : 4.48
INFO:root:2019-05-12 01:47:57, Epoch : 1, Step : 125, Training Loss : 0.33133, Training Acc : 0.861, Run Time : 0.67
INFO:root:2019-05-12 01:48:00, Epoch : 1, Step : 126, Training Loss : 0.65352, Training Acc : 0.672, Run Time : 2.47
INFO:root:2019-05-12 01:48:01, Epoch : 1, Step : 127, Training Loss : 0.79080, Training Acc : 0.656, Run Time : 0.73
INFO:root:2019-05-12 01:48:01, Epoch : 1, Step : 128, Training Loss : 0.52438, Training Acc : 0.728, Run Time : 0.69
INFO:root:2019-05-12 01:48:15, Epoch : 1, Step : 129, Training Loss : 0.58918, Training Acc : 0.694, Run Time : 13.44
INFO:root:2019-05-12 01:48:19, Epoch : 1, Step : 130, Training Loss : 0.50673, Training Acc : 0.733, Run Time : 4.64
INFO:root:2019-05-12 01:48:20, Epoch : 1, Step : 131, Training Loss : 0.36056, Training Acc : 0.822, Run Time : 0.39
INFO:root:2019-05-12 01:48:20, Epoch : 1, Step : 132, Training Loss : 0.35362, Training Acc : 0.867, Run Time : 0.51
INFO:root:2019-05-12 01:48:28, Epoch : 1, Step : 133, Training Loss : 0.33922, Training Acc : 0.817, Run Time : 7.77
INFO:root:2019-05-12 01:48:29, Epoch : 1, Step : 134, Training Loss : 0.49229, Training Acc : 0.767, Run Time : 0.73
INFO:root:2019-05-12 01:48:29, Epoch : 1, Step : 135, Training Loss : 0.35943, Training Acc : 0.856, Run Time : 0.67
INFO:root:2019-05-12 01:48:32, Epoch : 1, Step : 136, Training Loss : 0.40828, Training Acc : 0.828, Run Time : 3.00
INFO:root:2019-05-12 01:48:33, Epoch : 1, Step : 137, Training Loss : 0.50620, Training Acc : 0.694, Run Time : 0.61
INFO:root:2019-05-12 01:48:35, Epoch : 1, Step : 138, Training Loss : 0.52542, Training Acc : 0.750, Run Time : 1.78
INFO:root:2019-05-12 01:48:42, Epoch : 1, Step : 139, Training Loss : 0.47452, Training Acc : 0.728, Run Time : 7.54
INFO:root:2019-05-12 01:48:43, Epoch : 1, Step : 140, Training Loss : 0.58304, Training Acc : 0.728, Run Time : 0.54
INFO:root:2019-05-12 01:48:44, Epoch : 1, Step : 141, Training Loss : 0.48697, Training Acc : 0.761, Run Time : 0.62
INFO:root:2019-05-12 01:48:51, Epoch : 1, Step : 142, Training Loss : 0.44542, Training Acc : 0.800, Run Time : 7.13
INFO:root:2019-05-12 01:48:51, Epoch : 1, Step : 143, Training Loss : 0.37335, Training Acc : 0.833, Run Time : 0.72
INFO:root:2019-05-12 01:49:01, Epoch : 1, Step : 144, Training Loss : 0.28856, Training Acc : 0.894, Run Time : 9.52
INFO:root:2019-05-12 01:49:02, Epoch : 1, Step : 145, Training Loss : 0.45248, Training Acc : 0.839, Run Time : 1.15
INFO:root:2019-05-12 01:49:03, Epoch : 1, Step : 146, Training Loss : 0.29594, Training Acc : 0.906, Run Time : 0.63
INFO:root:2019-05-12 01:49:07, Epoch : 1, Step : 147, Training Loss : 0.43524, Training Acc : 0.778, Run Time : 4.37
INFO:root:2019-05-12 01:49:08, Epoch : 1, Step : 148, Training Loss : 1.28533, Training Acc : 0.467, Run Time : 0.66
INFO:root:2019-05-12 01:49:08, Epoch : 1, Step : 149, Training Loss : 0.90991, Training Acc : 0.528, Run Time : 0.80
INFO:root:2019-05-12 01:49:14, Epoch : 1, Step : 150, Training Loss : 0.29161, Training Acc : 0.906, Run Time : 5.95
INFO:root:2019-05-12 01:49:15, Epoch : 1, Step : 151, Training Loss : 0.30928, Training Acc : 0.917, Run Time : 0.61
INFO:root:2019-05-12 01:49:17, Epoch : 1, Step : 152, Training Loss : 0.35146, Training Acc : 0.844, Run Time : 2.04
INFO:root:2019-05-12 01:49:19, Epoch : 1, Step : 153, Training Loss : 0.39010, Training Acc : 0.794, Run Time : 2.03
INFO:root:2019-05-12 01:49:20, Epoch : 1, Step : 154, Training Loss : 0.43419, Training Acc : 0.783, Run Time : 0.61
INFO:root:2019-05-12 01:49:20, Epoch : 1, Step : 155, Training Loss : 0.51477, Training Acc : 0.756, Run Time : 0.60
INFO:root:2019-05-12 01:49:28, Epoch : 1, Step : 156, Training Loss : 0.55096, Training Acc : 0.733, Run Time : 7.49
INFO:root:2019-05-12 01:49:28, Epoch : 1, Step : 157, Training Loss : 0.45804, Training Acc : 0.761, Run Time : 0.65
INFO:root:2019-05-12 01:49:29, Epoch : 1, Step : 158, Training Loss : 0.37706, Training Acc : 0.794, Run Time : 0.68
INFO:root:2019-05-12 01:49:35, Epoch : 1, Step : 159, Training Loss : 0.38824, Training Acc : 0.856, Run Time : 5.52
INFO:root:2019-05-12 01:49:35, Epoch : 1, Step : 160, Training Loss : 0.40307, Training Acc : 0.822, Run Time : 0.71
INFO:root:2019-05-12 01:49:36, Epoch : 1, Step : 161, Training Loss : 0.43084, Training Acc : 0.750, Run Time : 0.60
INFO:root:2019-05-12 01:49:37, Epoch : 1, Step : 162, Training Loss : 0.31993, Training Acc : 0.906, Run Time : 0.82
INFO:root:2019-05-12 01:49:45, Epoch : 1, Step : 163, Training Loss : 0.25948, Training Acc : 0.939, Run Time : 7.92
INFO:root:2019-05-12 01:49:46, Epoch : 1, Step : 164, Training Loss : 0.35348, Training Acc : 0.856, Run Time : 0.95
INFO:root:2019-05-12 01:49:48, Epoch : 1, Step : 165, Training Loss : 0.30441, Training Acc : 0.894, Run Time : 2.37
INFO:root:2019-05-12 01:49:52, Epoch : 1, Step : 166, Training Loss : 0.82320, Training Acc : 0.517, Run Time : 3.93
INFO:root:2019-05-12 01:49:53, Epoch : 1, Step : 167, Training Loss : 0.32709, Training Acc : 0.850, Run Time : 0.68
INFO:root:2019-05-12 01:49:53, Epoch : 1, Step : 168, Training Loss : 0.27253, Training Acc : 0.906, Run Time : 0.60
INFO:root:2019-05-12 01:49:54, Epoch : 1, Step : 169, Training Loss : 0.28759, Training Acc : 0.883, Run Time : 0.63
INFO:root:2019-05-12 01:49:55, Epoch : 1, Step : 170, Training Loss : 0.28704, Training Acc : 0.883, Run Time : 0.66
INFO:root:2019-05-12 01:50:03, Epoch : 1, Step : 171, Training Loss : 0.46031, Training Acc : 0.817, Run Time : 7.97
INFO:root:2019-05-12 01:50:03, Epoch : 1, Step : 172, Training Loss : 0.38497, Training Acc : 0.822, Run Time : 0.55
INFO:root:2019-05-12 01:50:04, Epoch : 1, Step : 173, Training Loss : 0.34387, Training Acc : 0.839, Run Time : 1.01
INFO:root:2019-05-12 01:50:05, Epoch : 1, Step : 174, Training Loss : 0.39041, Training Acc : 0.856, Run Time : 0.61
INFO:root:2019-05-12 01:50:14, Epoch : 1, Step : 175, Training Loss : 0.57685, Training Acc : 0.744, Run Time : 9.75
INFO:root:2019-05-12 01:50:16, Epoch : 1, Step : 176, Training Loss : 0.27339, Training Acc : 0.911, Run Time : 1.64
INFO:root:2019-05-12 01:50:22, Epoch : 1, Step : 177, Training Loss : 0.36189, Training Acc : 0.844, Run Time : 6.11
INFO:root:2019-05-12 01:50:23, Epoch : 1, Step : 178, Training Loss : 0.30497, Training Acc : 0.900, Run Time : 0.52
INFO:root:2019-05-12 01:50:23, Epoch : 1, Step : 179, Training Loss : 0.28896, Training Acc : 0.894, Run Time : 0.67
INFO:root:2019-05-12 01:50:24, Epoch : 1, Step : 180, Training Loss : 0.31751, Training Acc : 0.889, Run Time : 0.64
INFO:root:2019-05-12 01:50:29, Epoch : 1, Step : 181, Training Loss : 0.33467, Training Acc : 0.872, Run Time : 5.01
INFO:root:2019-05-12 01:50:30, Epoch : 1, Step : 182, Training Loss : 0.38540, Training Acc : 0.811, Run Time : 0.76
INFO:root:2019-05-12 01:50:31, Epoch : 1, Step : 183, Training Loss : 0.28643, Training Acc : 0.911, Run Time : 1.20
INFO:root:2019-05-12 01:50:37, Epoch : 1, Step : 184, Training Loss : 0.35002, Training Acc : 0.844, Run Time : 5.84
INFO:root:2019-05-12 01:50:37, Epoch : 1, Step : 185, Training Loss : 0.29106, Training Acc : 0.894, Run Time : 0.57
INFO:root:2019-05-12 01:50:39, Epoch : 1, Step : 186, Training Loss : 0.30384, Training Acc : 0.906, Run Time : 1.34
INFO:root:2019-05-12 01:50:43, Epoch : 1, Step : 187, Training Loss : 0.31341, Training Acc : 0.889, Run Time : 4.59
INFO:root:2019-05-12 01:50:44, Epoch : 1, Step : 188, Training Loss : 0.45934, Training Acc : 0.800, Run Time : 0.59
INFO:root:2019-05-12 01:50:45, Epoch : 1, Step : 189, Training Loss : 0.32068, Training Acc : 0.900, Run Time : 1.00
INFO:root:2019-05-12 01:50:52, Epoch : 1, Step : 190, Training Loss : 0.33677, Training Acc : 0.839, Run Time : 7.53
INFO:root:2019-05-12 01:50:53, Epoch : 1, Step : 191, Training Loss : 0.42731, Training Acc : 0.822, Run Time : 0.55
INFO:root:2019-05-12 01:50:54, Epoch : 1, Step : 192, Training Loss : 0.33481, Training Acc : 0.872, Run Time : 0.88
INFO:root:2019-05-12 01:51:01, Epoch : 1, Step : 193, Training Loss : 0.39609, Training Acc : 0.839, Run Time : 6.79
INFO:root:2019-05-12 01:51:01, Epoch : 1, Step : 194, Training Loss : 0.48584, Training Acc : 0.756, Run Time : 0.72
INFO:root:2019-05-12 01:51:02, Epoch : 1, Step : 195, Training Loss : 0.48003, Training Acc : 0.750, Run Time : 1.01
INFO:root:2019-05-12 01:51:09, Epoch : 1, Step : 196, Training Loss : 0.37267, Training Acc : 0.783, Run Time : 6.88
INFO:root:2019-05-12 01:51:10, Epoch : 1, Step : 197, Training Loss : 0.42771, Training Acc : 0.733, Run Time : 0.65
INFO:root:2019-05-12 01:51:11, Epoch : 1, Step : 198, Training Loss : 0.40941, Training Acc : 0.783, Run Time : 1.14
INFO:root:2019-05-12 01:51:18, Epoch : 1, Step : 199, Training Loss : 0.39225, Training Acc : 0.800, Run Time : 6.49
INFO:root:2019-05-12 01:51:18, Epoch : 1, Step : 200, Training Loss : 0.44932, Training Acc : 0.750, Run Time : 0.68
INFO:root:2019-05-12 01:51:27, Epoch : 1, Step : 201, Training Loss : 0.52922, Training Acc : 0.672, Run Time : 8.90
INFO:root:2019-05-12 01:51:28, Epoch : 1, Step : 202, Training Loss : 0.54312, Training Acc : 0.672, Run Time : 0.50
INFO:root:2019-05-12 01:51:29, Epoch : 1, Step : 203, Training Loss : 0.54787, Training Acc : 0.706, Run Time : 1.10
INFO:root:2019-05-12 01:51:35, Epoch : 1, Step : 204, Training Loss : 0.47028, Training Acc : 0.789, Run Time : 6.53
INFO:root:2019-05-12 01:51:36, Epoch : 1, Step : 205, Training Loss : 0.50782, Training Acc : 0.733, Run Time : 0.59
INFO:root:2019-05-12 01:51:38, Epoch : 1, Step : 206, Training Loss : 0.44990, Training Acc : 0.761, Run Time : 2.24
INFO:root:2019-05-12 01:51:44, Epoch : 1, Step : 207, Training Loss : 0.43834, Training Acc : 0.750, Run Time : 6.15
INFO:root:2019-05-12 01:51:45, Epoch : 1, Step : 208, Training Loss : 0.36805, Training Acc : 0.811, Run Time : 0.45
INFO:root:2019-05-12 01:51:46, Epoch : 1, Step : 209, Training Loss : 0.33426, Training Acc : 0.811, Run Time : 1.39
INFO:root:2019-05-12 01:51:51, Epoch : 1, Step : 210, Training Loss : 0.52787, Training Acc : 0.789, Run Time : 4.75
INFO:root:2019-05-12 01:51:51, Epoch : 1, Step : 211, Training Loss : 0.44898, Training Acc : 0.817, Run Time : 0.53
INFO:root:2019-05-12 01:51:53, Epoch : 1, Step : 212, Training Loss : 0.43629, Training Acc : 0.806, Run Time : 1.32
INFO:root:2019-05-12 01:52:00, Epoch : 1, Step : 213, Training Loss : 0.39949, Training Acc : 0.817, Run Time : 7.36
INFO:root:2019-05-12 01:52:00, Epoch : 1, Step : 214, Training Loss : 0.38069, Training Acc : 0.844, Run Time : 0.45
INFO:root:2019-05-12 01:52:01, Epoch : 1, Step : 215, Training Loss : 0.47877, Training Acc : 0.806, Run Time : 0.62
INFO:root:2019-05-12 01:52:08, Epoch : 1, Step : 216, Training Loss : 0.40471, Training Acc : 0.811, Run Time : 6.41
INFO:root:2019-05-12 01:52:08, Epoch : 1, Step : 217, Training Loss : 0.40620, Training Acc : 0.850, Run Time : 0.54
INFO:root:2019-05-12 01:52:08, Epoch : 1, Step : 218, Training Loss : 0.32363, Training Acc : 0.833, Run Time : 0.41
INFO:root:2019-05-12 01:52:11, Epoch : 1, Step : 219, Training Loss : 0.36545, Training Acc : 0.817, Run Time : 2.25
INFO:root:2019-05-12 01:52:18, Epoch : 1, Step : 220, Training Loss : 0.35505, Training Acc : 0.811, Run Time : 7.01
INFO:root:2019-05-12 01:52:18, Epoch : 1, Step : 221, Training Loss : 0.37631, Training Acc : 0.817, Run Time : 0.64
INFO:root:2019-05-12 01:52:19, Epoch : 1, Step : 222, Training Loss : 0.39573, Training Acc : 0.800, Run Time : 0.61
INFO:root:2019-05-12 01:52:20, Epoch : 1, Step : 223, Training Loss : 0.77319, Training Acc : 0.794, Run Time : 0.69
INFO:root:2019-05-12 01:52:26, Epoch : 1, Step : 224, Training Loss : 0.43934, Training Acc : 0.772, Run Time : 6.45
INFO:root:2019-05-12 01:52:27, Epoch : 1, Step : 225, Training Loss : 0.40403, Training Acc : 0.756, Run Time : 0.65
INFO:root:2019-05-12 01:52:27, Epoch : 1, Step : 226, Training Loss : 0.44482, Training Acc : 0.783, Run Time : 0.64
INFO:root:2019-05-12 01:52:36, Epoch : 1, Step : 227, Training Loss : 0.40691, Training Acc : 0.778, Run Time : 8.64
INFO:root:2019-05-12 01:52:37, Epoch : 1, Step : 228, Training Loss : 0.42458, Training Acc : 0.800, Run Time : 0.53
INFO:root:2019-05-12 01:52:37, Epoch : 1, Step : 229, Training Loss : 0.54542, Training Acc : 0.772, Run Time : 0.62
INFO:root:2019-05-12 01:52:45, Epoch : 1, Step : 230, Training Loss : 0.58506, Training Acc : 0.689, Run Time : 8.16
INFO:root:2019-05-12 01:52:46, Epoch : 1, Step : 231, Training Loss : 0.38949, Training Acc : 0.772, Run Time : 1.00
INFO:root:2019-05-12 01:52:47, Epoch : 1, Step : 232, Training Loss : 0.43077, Training Acc : 0.744, Run Time : 0.61
INFO:root:2019-05-12 01:52:48, Epoch : 1, Step : 233, Training Loss : 0.39316, Training Acc : 0.744, Run Time : 0.91
INFO:root:2019-05-12 01:52:56, Epoch : 1, Step : 234, Training Loss : 0.46151, Training Acc : 0.728, Run Time : 8.09
INFO:root:2019-05-12 01:52:56, Epoch : 1, Step : 235, Training Loss : 0.39168, Training Acc : 0.794, Run Time : 0.47
INFO:root:2019-05-12 01:53:04, Epoch : 1, Step : 236, Training Loss : 0.42868, Training Acc : 0.750, Run Time : 7.39
INFO:root:2019-05-12 01:53:04, Epoch : 1, Step : 237, Training Loss : 0.43251, Training Acc : 0.761, Run Time : 0.61
INFO:root:2019-05-12 01:53:05, Epoch : 1, Step : 238, Training Loss : 0.58830, Training Acc : 0.756, Run Time : 1.00
INFO:root:2019-05-12 01:53:11, Epoch : 1, Step : 239, Training Loss : 0.43694, Training Acc : 0.806, Run Time : 5.93
INFO:root:2019-05-12 01:53:12, Epoch : 1, Step : 240, Training Loss : 0.47885, Training Acc : 0.756, Run Time : 0.55
INFO:root:2019-05-12 01:53:14, Epoch : 1, Step : 241, Training Loss : 0.39954, Training Acc : 0.828, Run Time : 1.77
INFO:root:2019-05-12 01:53:19, Epoch : 1, Step : 242, Training Loss : 0.44260, Training Acc : 0.767, Run Time : 5.12
INFO:root:2019-05-12 01:53:19, Epoch : 1, Step : 243, Training Loss : 0.48426, Training Acc : 0.806, Run Time : 0.56
INFO:root:2019-05-12 01:53:21, Epoch : 1, Step : 244, Training Loss : 0.43632, Training Acc : 0.750, Run Time : 1.21
INFO:root:2019-05-12 01:53:24, Epoch : 1, Step : 245, Training Loss : 0.40075, Training Acc : 0.772, Run Time : 3.79
INFO:root:2019-05-12 01:53:25, Epoch : 1, Step : 246, Training Loss : 0.40166, Training Acc : 0.800, Run Time : 0.61
INFO:root:2019-05-12 01:53:27, Epoch : 1, Step : 247, Training Loss : 0.38717, Training Acc : 0.833, Run Time : 1.66
INFO:root:2019-05-12 01:53:29, Epoch : 1, Step : 248, Training Loss : 0.33014, Training Acc : 0.872, Run Time : 2.02
INFO:root:2019-05-12 01:53:29, Epoch : 1, Step : 249, Training Loss : 0.37867, Training Acc : 0.828, Run Time : 0.60
INFO:root:2019-05-12 01:53:36, Epoch : 1, Step : 250, Training Loss : 0.32396, Training Acc : 0.878, Run Time : 6.42
INFO:root:2019-05-12 01:53:36, Epoch : 1, Step : 251, Training Loss : 0.39096, Training Acc : 0.806, Run Time : 0.69
INFO:root:2019-05-12 01:53:37, Epoch : 1, Step : 252, Training Loss : 0.37803, Training Acc : 0.817, Run Time : 0.87
INFO:root:2019-05-12 01:53:42, Epoch : 1, Step : 253, Training Loss : 0.35198, Training Acc : 0.811, Run Time : 5.00
INFO:root:2019-05-12 01:53:43, Epoch : 1, Step : 254, Training Loss : 0.32532, Training Acc : 0.867, Run Time : 0.59
INFO:root:2019-05-12 01:53:44, Epoch : 1, Step : 255, Training Loss : 0.38647, Training Acc : 0.806, Run Time : 0.70
INFO:root:2019-05-12 01:53:45, Epoch : 1, Step : 256, Training Loss : 0.37191, Training Acc : 0.833, Run Time : 1.29
INFO:root:2019-05-12 01:53:51, Epoch : 1, Step : 257, Training Loss : 0.37438, Training Acc : 0.817, Run Time : 6.33
INFO:root:2019-05-12 01:53:52, Epoch : 1, Step : 258, Training Loss : 0.34903, Training Acc : 0.828, Run Time : 0.61
INFO:root:2019-05-12 01:53:59, Epoch : 1, Step : 259, Training Loss : 0.36179, Training Acc : 0.794, Run Time : 7.72
INFO:root:2019-05-12 01:54:00, Epoch : 1, Step : 260, Training Loss : 0.30028, Training Acc : 0.850, Run Time : 0.42
INFO:root:2019-05-12 01:54:00, Epoch : 1, Step : 261, Training Loss : 0.34862, Training Acc : 0.828, Run Time : 0.55
INFO:root:2019-05-12 01:54:02, Epoch : 1, Step : 262, Training Loss : 0.33252, Training Acc : 0.833, Run Time : 1.17
INFO:root:2019-05-12 01:54:03, Epoch : 1, Step : 263, Training Loss : 0.29695, Training Acc : 0.878, Run Time : 0.92
INFO:root:2019-05-12 01:54:10, Epoch : 1, Step : 264, Training Loss : 0.31448, Training Acc : 0.856, Run Time : 7.49
INFO:root:2019-05-12 01:54:11, Epoch : 1, Step : 265, Training Loss : 0.35391, Training Acc : 0.789, Run Time : 0.56
INFO:root:2019-05-12 01:54:11, Epoch : 1, Step : 266, Training Loss : 0.35827, Training Acc : 0.850, Run Time : 0.76
INFO:root:2019-05-12 01:54:18, Epoch : 1, Step : 267, Training Loss : 0.32548, Training Acc : 0.833, Run Time : 7.07
INFO:root:2019-05-12 01:54:19, Epoch : 1, Step : 268, Training Loss : 0.31145, Training Acc : 0.856, Run Time : 0.63
INFO:root:2019-05-12 01:54:20, Epoch : 1, Step : 269, Training Loss : 0.29106, Training Acc : 0.867, Run Time : 0.86
INFO:root:2019-05-12 01:54:26, Epoch : 1, Step : 270, Training Loss : 0.31986, Training Acc : 0.833, Run Time : 6.40
INFO:root:2019-05-12 01:54:27, Epoch : 1, Step : 271, Training Loss : 0.35050, Training Acc : 0.811, Run Time : 0.40
INFO:root:2019-05-12 01:54:29, Epoch : 1, Step : 272, Training Loss : 0.35796, Training Acc : 0.794, Run Time : 2.06
INFO:root:2019-05-12 01:54:35, Epoch : 1, Step : 273, Training Loss : 0.31714, Training Acc : 0.833, Run Time : 5.94
INFO:root:2019-05-12 01:54:35, Epoch : 1, Step : 274, Training Loss : 0.35855, Training Acc : 0.800, Run Time : 0.45
INFO:root:2019-05-12 01:54:36, Epoch : 1, Step : 275, Training Loss : 0.33744, Training Acc : 0.811, Run Time : 0.65
INFO:root:2019-05-12 01:54:41, Epoch : 1, Step : 276, Training Loss : 0.32357, Training Acc : 0.828, Run Time : 5.44
INFO:root:2019-05-12 01:54:42, Epoch : 1, Step : 277, Training Loss : 0.32098, Training Acc : 0.872, Run Time : 0.73
INFO:root:2019-05-12 01:54:43, Epoch : 1, Step : 278, Training Loss : 0.37163, Training Acc : 0.778, Run Time : 0.89
INFO:root:2019-05-12 01:54:49, Epoch : 1, Step : 279, Training Loss : 0.36193, Training Acc : 0.794, Run Time : 6.66
INFO:root:2019-05-12 01:54:50, Epoch : 1, Step : 280, Training Loss : 0.40304, Training Acc : 0.761, Run Time : 0.53
INFO:root:2019-05-12 01:54:51, Epoch : 1, Step : 281, Training Loss : 0.34583, Training Acc : 0.811, Run Time : 1.41
INFO:root:2019-05-12 01:54:57, Epoch : 1, Step : 282, Training Loss : 0.83840, Training Acc : 0.650, Run Time : 5.10
INFO:root:2019-05-12 01:54:57, Epoch : 1, Step : 283, Training Loss : 0.90063, Training Acc : 0.622, Run Time : 0.56
INFO:root:2019-05-12 01:54:58, Epoch : 1, Step : 284, Training Loss : 0.35475, Training Acc : 0.794, Run Time : 0.73
INFO:root:2019-05-12 01:55:04, Epoch : 1, Step : 285, Training Loss : 0.61473, Training Acc : 0.722, Run Time : 5.78
INFO:root:2019-05-12 01:55:04, Epoch : 1, Step : 286, Training Loss : 0.46012, Training Acc : 0.789, Run Time : 0.60
INFO:root:2019-05-12 01:55:05, Epoch : 1, Step : 287, Training Loss : 0.46587, Training Acc : 0.806, Run Time : 1.28
INFO:root:2019-05-12 01:55:10, Epoch : 1, Step : 288, Training Loss : 0.39497, Training Acc : 0.822, Run Time : 4.88
INFO:root:2019-05-12 01:55:11, Epoch : 1, Step : 289, Training Loss : 0.43308, Training Acc : 0.806, Run Time : 0.79
INFO:root:2019-05-12 01:55:18, Epoch : 1, Step : 290, Training Loss : 0.47881, Training Acc : 0.711, Run Time : 6.44
INFO:root:2019-05-12 01:55:18, Epoch : 1, Step : 291, Training Loss : 0.33074, Training Acc : 0.850, Run Time : 0.68
INFO:root:2019-05-12 01:55:21, Epoch : 1, Step : 292, Training Loss : 0.37552, Training Acc : 0.822, Run Time : 2.38
INFO:root:2019-05-12 01:55:27, Epoch : 1, Step : 293, Training Loss : 0.36917, Training Acc : 0.778, Run Time : 6.10
INFO:root:2019-05-12 01:55:27, Epoch : 1, Step : 294, Training Loss : 0.43431, Training Acc : 0.756, Run Time : 0.45
INFO:root:2019-05-12 01:55:28, Epoch : 1, Step : 295, Training Loss : 0.41154, Training Acc : 0.789, Run Time : 0.56
INFO:root:2019-05-12 01:55:34, Epoch : 1, Step : 296, Training Loss : 0.33496, Training Acc : 0.883, Run Time : 6.06
INFO:root:2019-05-12 01:55:35, Epoch : 1, Step : 297, Training Loss : 0.33440, Training Acc : 0.878, Run Time : 0.77
INFO:root:2019-05-12 01:55:35, Epoch : 1, Step : 298, Training Loss : 0.35678, Training Acc : 0.856, Run Time : 0.68
INFO:root:2019-05-12 01:55:36, Epoch : 1, Step : 299, Training Loss : 0.44432, Training Acc : 0.794, Run Time : 0.79
INFO:root:2019-05-12 01:55:37, Epoch : 1, Step : 300, Training Loss : 0.32989, Training Acc : 0.850, Run Time : 0.90
INFO:root:2019-05-12 01:55:44, Epoch : 1, Step : 301, Training Loss : 0.36270, Training Acc : 0.833, Run Time : 7.53
INFO:root:2019-05-12 01:55:45, Epoch : 1, Step : 302, Training Loss : 0.37552, Training Acc : 0.839, Run Time : 0.63
INFO:root:2019-05-12 01:55:49, Epoch : 1, Step : 303, Training Loss : 0.27379, Training Acc : 0.894, Run Time : 4.03
INFO:root:2019-05-12 01:55:51, Epoch : 1, Step : 304, Training Loss : 0.33525, Training Acc : 0.811, Run Time : 1.44
INFO:root:2019-05-12 01:55:51, Epoch : 1, Step : 305, Training Loss : 0.28503, Training Acc : 0.878, Run Time : 0.58
INFO:root:2019-05-12 01:55:52, Epoch : 1, Step : 306, Training Loss : 0.27664, Training Acc : 0.889, Run Time : 1.23
INFO:root:2019-05-12 01:56:00, Epoch : 1, Step : 307, Training Loss : 0.35139, Training Acc : 0.794, Run Time : 7.54
INFO:root:2019-05-12 01:56:01, Epoch : 1, Step : 308, Training Loss : 0.29818, Training Acc : 0.878, Run Time : 0.59
INFO:root:2019-05-12 01:56:01, Epoch : 1, Step : 309, Training Loss : 0.34433, Training Acc : 0.822, Run Time : 0.57
INFO:root:2019-05-12 01:56:02, Epoch : 1, Step : 310, Training Loss : 0.25880, Training Acc : 0.906, Run Time : 0.58
INFO:root:2019-05-12 01:56:03, Epoch : 1, Step : 311, Training Loss : 0.41313, Training Acc : 0.772, Run Time : 0.86
INFO:root:2019-05-12 01:56:10, Epoch : 1, Step : 312, Training Loss : 0.28840, Training Acc : 0.872, Run Time : 7.05
INFO:root:2019-05-12 01:56:10, Epoch : 1, Step : 313, Training Loss : 0.36286, Training Acc : 0.806, Run Time : 0.63
INFO:root:2019-05-12 01:56:11, Epoch : 1, Step : 314, Training Loss : 0.32626, Training Acc : 0.856, Run Time : 0.65
INFO:root:2019-05-12 01:56:19, Epoch : 1, Step : 315, Training Loss : 0.27525, Training Acc : 0.883, Run Time : 7.76
INFO:root:2019-05-12 01:56:20, Epoch : 1, Step : 316, Training Loss : 0.26102, Training Acc : 0.911, Run Time : 0.89
INFO:root:2019-05-12 01:56:25, Epoch : 1, Step : 317, Training Loss : 0.30095, Training Acc : 0.850, Run Time : 5.91
INFO:root:2019-05-12 01:56:27, Epoch : 1, Step : 318, Training Loss : 0.27568, Training Acc : 0.872, Run Time : 1.90
INFO:root:2019-05-12 01:56:28, Epoch : 1, Step : 319, Training Loss : 0.34424, Training Acc : 0.817, Run Time : 0.56
INFO:root:2019-05-12 01:56:29, Epoch : 1, Step : 320, Training Loss : 0.26666, Training Acc : 0.900, Run Time : 1.05
INFO:root:2019-05-12 01:56:30, Epoch : 1, Step : 321, Training Loss : 0.25929, Training Acc : 0.900, Run Time : 1.50
INFO:root:2019-05-12 01:56:34, Epoch : 1, Step : 322, Training Loss : 0.23365, Training Acc : 0.894, Run Time : 3.21
INFO:root:2019-05-12 01:56:34, Epoch : 1, Step : 323, Training Loss : 0.26420, Training Acc : 0.850, Run Time : 0.69
INFO:root:2019-05-12 01:56:38, Epoch : 1, Step : 324, Training Loss : 0.26559, Training Acc : 0.883, Run Time : 4.04
INFO:root:2019-05-12 01:56:40, Epoch : 1, Step : 325, Training Loss : 0.22436, Training Acc : 0.922, Run Time : 1.39
INFO:root:2019-05-12 01:56:40, Epoch : 1, Step : 326, Training Loss : 0.26324, Training Acc : 0.900, Run Time : 0.65
INFO:root:2019-05-12 01:56:50, Epoch : 1, Step : 327, Training Loss : 0.36211, Training Acc : 0.811, Run Time : 9.20
INFO:root:2019-05-12 01:56:50, Epoch : 1, Step : 328, Training Loss : 0.21912, Training Acc : 0.928, Run Time : 0.54
INFO:root:2019-05-12 01:56:52, Epoch : 1, Step : 329, Training Loss : 0.31988, Training Acc : 0.850, Run Time : 1.52
INFO:root:2019-05-12 01:57:00, Epoch : 1, Step : 330, Training Loss : 0.22407, Training Acc : 0.917, Run Time : 7.90
INFO:root:2019-05-12 01:57:01, Epoch : 1, Step : 331, Training Loss : 0.26781, Training Acc : 0.922, Run Time : 1.08
INFO:root:2019-05-12 01:57:01, Epoch : 1, Step : 332, Training Loss : 0.26969, Training Acc : 0.861, Run Time : 0.61
INFO:root:2019-05-12 01:57:08, Epoch : 1, Step : 333, Training Loss : 0.30433, Training Acc : 0.844, Run Time : 6.40
INFO:root:2019-05-12 01:57:08, Epoch : 1, Step : 334, Training Loss : 0.28257, Training Acc : 0.872, Run Time : 0.71
INFO:root:2019-05-12 01:57:09, Epoch : 1, Step : 335, Training Loss : 0.41131, Training Acc : 0.856, Run Time : 0.76
INFO:root:2019-05-12 01:57:14, Epoch : 1, Step : 336, Training Loss : 0.22420, Training Acc : 0.939, Run Time : 5.31
INFO:root:2019-05-12 01:57:16, Epoch : 1, Step : 337, Training Loss : 0.28521, Training Acc : 0.856, Run Time : 1.28
INFO:root:2019-05-12 01:57:16, Epoch : 1, Step : 338, Training Loss : 0.29357, Training Acc : 0.889, Run Time : 0.66
INFO:root:2019-05-12 01:57:17, Epoch : 1, Step : 339, Training Loss : 0.30227, Training Acc : 0.872, Run Time : 0.82
INFO:root:2019-05-12 01:57:24, Epoch : 1, Step : 340, Training Loss : 0.36930, Training Acc : 0.861, Run Time : 6.42
INFO:root:2019-05-12 01:57:24, Epoch : 1, Step : 341, Training Loss : 0.17615, Training Acc : 0.922, Run Time : 0.58
INFO:root:2019-05-12 01:57:26, Epoch : 1, Step : 342, Training Loss : 0.33468, Training Acc : 0.856, Run Time : 1.40
INFO:root:2019-05-12 01:57:30, Epoch : 1, Step : 343, Training Loss : 0.21075, Training Acc : 0.928, Run Time : 4.76
INFO:root:2019-05-12 01:57:31, Epoch : 1, Step : 344, Training Loss : 0.19842, Training Acc : 0.944, Run Time : 0.56
INFO:root:2019-05-12 01:57:32, Epoch : 1, Step : 345, Training Loss : 0.27968, Training Acc : 0.867, Run Time : 1.45
INFO:root:2019-05-12 01:57:43, Epoch : 1, Step : 346, Training Loss : 0.24452, Training Acc : 0.889, Run Time : 10.85
INFO:root:2019-05-12 01:57:44, Epoch : 1, Step : 347, Training Loss : 0.24502, Training Acc : 0.872, Run Time : 1.06
INFO:root:2019-05-12 01:57:45, Epoch : 1, Step : 348, Training Loss : 0.23464, Training Acc : 0.900, Run Time : 0.82
INFO:root:2019-05-12 01:57:52, Epoch : 1, Step : 349, Training Loss : 0.26696, Training Acc : 0.856, Run Time : 7.25
INFO:root:2019-05-12 01:57:53, Epoch : 1, Step : 350, Training Loss : 0.29629, Training Acc : 0.861, Run Time : 1.11
INFO:root:2019-05-12 01:58:01, Epoch : 1, Step : 351, Training Loss : 0.25029, Training Acc : 0.889, Run Time : 8.02
INFO:root:2019-05-12 01:58:02, Epoch : 1, Step : 352, Training Loss : 0.22567, Training Acc : 0.906, Run Time : 0.92
INFO:root:2019-05-12 01:58:10, Epoch : 1, Step : 353, Training Loss : 0.24226, Training Acc : 0.883, Run Time : 7.56
INFO:root:2019-05-12 01:58:11, Epoch : 1, Step : 354, Training Loss : 0.23666, Training Acc : 0.861, Run Time : 0.70
INFO:root:2019-05-12 01:58:12, Epoch : 1, Step : 355, Training Loss : 0.22409, Training Acc : 0.889, Run Time : 1.61
INFO:root:2019-05-12 01:58:22, Epoch : 1, Step : 356, Training Loss : 0.25963, Training Acc : 0.900, Run Time : 9.64
INFO:root:2019-05-12 01:58:24, Epoch : 1, Step : 357, Training Loss : 0.22741, Training Acc : 0.872, Run Time : 2.30
INFO:root:2019-05-12 01:58:25, Epoch : 1, Step : 358, Training Loss : 0.22335, Training Acc : 0.906, Run Time : 0.60
INFO:root:2019-05-12 01:58:25, Epoch : 1, Step : 359, Training Loss : 0.21728, Training Acc : 0.911, Run Time : 0.62
INFO:root:2019-05-12 01:58:26, Epoch : 1, Step : 360, Training Loss : 0.25981, Training Acc : 0.883, Run Time : 0.64
INFO:root:2019-05-12 01:58:31, Epoch : 1, Step : 361, Training Loss : 0.25361, Training Acc : 0.911, Run Time : 5.42
INFO:root:2019-05-12 01:58:33, Epoch : 1, Step : 362, Training Loss : 0.20268, Training Acc : 0.939, Run Time : 1.23
INFO:root:2019-05-12 01:58:38, Epoch : 1, Step : 363, Training Loss : 0.19339, Training Acc : 0.933, Run Time : 5.33
INFO:root:2019-05-12 01:58:39, Epoch : 1, Step : 364, Training Loss : 0.26272, Training Acc : 0.922, Run Time : 0.67
INFO:root:2019-05-12 01:58:39, Epoch : 1, Step : 365, Training Loss : 0.16627, Training Acc : 0.950, Run Time : 0.63
INFO:root:2019-05-12 01:58:45, Epoch : 1, Step : 366, Training Loss : 0.18262, Training Acc : 0.933, Run Time : 6.09
INFO:root:2019-05-12 01:58:46, Epoch : 1, Step : 367, Training Loss : 0.17365, Training Acc : 0.944, Run Time : 0.62
INFO:root:2019-05-12 01:58:47, Epoch : 1, Step : 368, Training Loss : 0.23184, Training Acc : 0.900, Run Time : 0.60
INFO:root:2019-05-12 01:58:52, Epoch : 1, Step : 369, Training Loss : 0.17327, Training Acc : 0.928, Run Time : 5.75
INFO:root:2019-05-12 01:58:53, Epoch : 1, Step : 370, Training Loss : 0.22156, Training Acc : 0.889, Run Time : 0.67
INFO:root:2019-05-12 01:58:55, Epoch : 1, Step : 371, Training Loss : 0.16011, Training Acc : 0.922, Run Time : 1.59
INFO:root:2019-05-12 01:58:55, Epoch : 1, Step : 372, Training Loss : 0.19562, Training Acc : 0.911, Run Time : 0.59
INFO:root:2019-05-12 01:59:01, Epoch : 1, Step : 373, Training Loss : 0.17548, Training Acc : 0.917, Run Time : 5.73
INFO:root:2019-05-12 01:59:02, Epoch : 1, Step : 374, Training Loss : 0.14089, Training Acc : 0.950, Run Time : 0.80
INFO:root:2019-05-12 01:59:03, Epoch : 1, Step : 375, Training Loss : 0.14288, Training Acc : 0.944, Run Time : 1.26
INFO:root:2019-05-12 01:59:09, Epoch : 1, Step : 376, Training Loss : 0.26586, Training Acc : 0.883, Run Time : 5.60
INFO:root:2019-05-12 01:59:09, Epoch : 1, Step : 377, Training Loss : 0.12771, Training Acc : 0.956, Run Time : 0.55
INFO:root:2019-05-12 01:59:10, Epoch : 1, Step : 378, Training Loss : 0.17285, Training Acc : 0.944, Run Time : 1.29
INFO:root:2019-05-12 01:59:15, Epoch : 1, Step : 379, Training Loss : 0.19617, Training Acc : 0.906, Run Time : 4.19
INFO:root:2019-05-12 01:59:15, Epoch : 1, Step : 380, Training Loss : 0.17179, Training Acc : 0.939, Run Time : 0.53
INFO:root:2019-05-12 01:59:16, Epoch : 1, Step : 381, Training Loss : 0.21791, Training Acc : 0.894, Run Time : 0.93
INFO:root:2019-05-12 01:59:21, Epoch : 1, Step : 382, Training Loss : 0.13579, Training Acc : 0.961, Run Time : 5.08
INFO:root:2019-05-12 01:59:22, Epoch : 1, Step : 383, Training Loss : 0.16044, Training Acc : 0.950, Run Time : 0.66
INFO:root:2019-05-12 01:59:23, Epoch : 1, Step : 384, Training Loss : 0.16491, Training Acc : 0.922, Run Time : 0.72
INFO:root:2019-05-12 01:59:25, Epoch : 1, Step : 385, Training Loss : 0.25627, Training Acc : 0.889, Run Time : 2.74
INFO:root:2019-05-12 01:59:26, Epoch : 1, Step : 386, Training Loss : 0.16434, Training Acc : 0.928, Run Time : 0.66
INFO:root:2019-05-12 01:59:32, Epoch : 1, Step : 387, Training Loss : 0.26667, Training Acc : 0.861, Run Time : 5.51
INFO:root:2019-05-12 01:59:32, Epoch : 1, Step : 388, Training Loss : 0.47109, Training Acc : 0.817, Run Time : 0.64
INFO:root:2019-05-12 01:59:33, Epoch : 1, Step : 389, Training Loss : 0.70104, Training Acc : 0.733, Run Time : 1.14
INFO:root:2019-05-12 01:59:38, Epoch : 1, Step : 390, Training Loss : 0.26194, Training Acc : 0.906, Run Time : 4.81
INFO:root:2019-05-12 01:59:39, Epoch : 1, Step : 391, Training Loss : 0.18646, Training Acc : 0.939, Run Time : 0.64
INFO:root:2019-05-12 01:59:44, Epoch : 1, Step : 392, Training Loss : 0.32155, Training Acc : 0.861, Run Time : 4.80
INFO:root:2019-05-12 01:59:45, Epoch : 1, Step : 393, Training Loss : 0.25838, Training Acc : 0.878, Run Time : 1.01
INFO:root:2019-05-12 01:59:45, Epoch : 1, Step : 394, Training Loss : 0.29074, Training Acc : 0.917, Run Time : 0.65
INFO:root:2019-05-12 01:59:50, Epoch : 1, Step : 395, Training Loss : 0.18757, Training Acc : 0.917, Run Time : 4.87
INFO:root:2019-05-12 01:59:51, Epoch : 1, Step : 396, Training Loss : 0.19016, Training Acc : 0.911, Run Time : 0.62
INFO:root:2019-05-12 01:59:52, Epoch : 1, Step : 397, Training Loss : 0.22564, Training Acc : 0.917, Run Time : 1.30
INFO:root:2019-05-12 01:59:58, Epoch : 1, Step : 398, Training Loss : 0.22367, Training Acc : 0.906, Run Time : 6.28
INFO:root:2019-05-12 01:59:59, Epoch : 1, Step : 399, Training Loss : 0.21168, Training Acc : 0.911, Run Time : 0.61
INFO:root:2019-05-12 02:00:00, Epoch : 1, Step : 400, Training Loss : 0.29764, Training Acc : 0.850, Run Time : 1.27
INFO:root:2019-05-12 02:00:07, Epoch : 1, Step : 401, Training Loss : 0.49994, Training Acc : 0.739, Run Time : 6.77
INFO:root:2019-05-12 02:00:07, Epoch : 1, Step : 402, Training Loss : 0.36493, Training Acc : 0.811, Run Time : 0.56
INFO:root:2019-05-12 02:00:10, Epoch : 1, Step : 403, Training Loss : 0.49520, Training Acc : 0.761, Run Time : 2.15
INFO:root:2019-05-12 02:00:15, Epoch : 1, Step : 404, Training Loss : 0.50260, Training Acc : 0.772, Run Time : 5.66
INFO:root:2019-05-12 02:00:16, Epoch : 1, Step : 405, Training Loss : 0.38199, Training Acc : 0.822, Run Time : 0.49
INFO:root:2019-05-12 02:00:17, Epoch : 1, Step : 406, Training Loss : 0.42552, Training Acc : 0.844, Run Time : 1.25
INFO:root:2019-05-12 02:00:24, Epoch : 1, Step : 407, Training Loss : 0.48651, Training Acc : 0.822, Run Time : 6.59
INFO:root:2019-05-12 02:00:24, Epoch : 1, Step : 408, Training Loss : 0.43391, Training Acc : 0.856, Run Time : 0.74
INFO:root:2019-05-12 02:00:26, Epoch : 1, Step : 409, Training Loss : 0.44467, Training Acc : 0.878, Run Time : 1.48
INFO:root:2019-05-12 02:00:33, Epoch : 1, Step : 410, Training Loss : 0.42315, Training Acc : 0.783, Run Time : 7.33
INFO:root:2019-05-12 02:00:34, Epoch : 1, Step : 411, Training Loss : 0.28047, Training Acc : 0.872, Run Time : 0.50
INFO:root:2019-05-12 02:00:34, Epoch : 1, Step : 412, Training Loss : 0.43279, Training Acc : 0.772, Run Time : 0.77
INFO:root:2019-05-12 02:00:36, Epoch : 1, Step : 413, Training Loss : 0.36772, Training Acc : 0.794, Run Time : 1.82
INFO:root:2019-05-12 02:00:43, Epoch : 1, Step : 414, Training Loss : 0.30176, Training Acc : 0.839, Run Time : 6.49
INFO:root:2019-05-12 02:00:43, Epoch : 1, Step : 415, Training Loss : 0.52601, Training Acc : 0.728, Run Time : 0.50
INFO:root:2019-05-12 02:00:44, Epoch : 1, Step : 416, Training Loss : 0.32195, Training Acc : 0.822, Run Time : 0.89
INFO:root:2019-05-12 02:00:52, Epoch : 1, Step : 417, Training Loss : 0.26535, Training Acc : 0.900, Run Time : 7.64
INFO:root:2019-05-12 02:00:52, Epoch : 1, Step : 418, Training Loss : 0.32711, Training Acc : 0.833, Run Time : 0.70
INFO:root:2019-05-12 02:00:54, Epoch : 1, Step : 419, Training Loss : 0.39015, Training Acc : 0.756, Run Time : 1.29
INFO:root:2019-05-12 02:01:02, Epoch : 1, Step : 420, Training Loss : 0.31068, Training Acc : 0.856, Run Time : 7.87
INFO:root:2019-05-12 02:01:02, Epoch : 1, Step : 421, Training Loss : 0.32129, Training Acc : 0.861, Run Time : 0.48
INFO:root:2019-05-12 02:01:03, Epoch : 1, Step : 422, Training Loss : 0.26643, Training Acc : 0.894, Run Time : 0.82
INFO:root:2019-05-12 02:01:10, Epoch : 1, Step : 423, Training Loss : 0.26767, Training Acc : 0.906, Run Time : 7.26
INFO:root:2019-05-12 02:01:11, Epoch : 1, Step : 424, Training Loss : 0.20536, Training Acc : 0.967, Run Time : 0.53
INFO:root:2019-05-12 02:01:12, Epoch : 1, Step : 425, Training Loss : 0.24428, Training Acc : 0.911, Run Time : 1.02
INFO:root:2019-05-12 02:01:18, Epoch : 1, Step : 426, Training Loss : 0.19636, Training Acc : 0.967, Run Time : 6.23
INFO:root:2019-05-12 02:01:18, Epoch : 1, Step : 427, Training Loss : 0.19051, Training Acc : 0.939, Run Time : 0.44
INFO:root:2019-05-12 02:01:20, Epoch : 1, Step : 428, Training Loss : 0.24276, Training Acc : 0.928, Run Time : 1.32
INFO:root:2019-05-12 02:01:24, Epoch : 1, Step : 429, Training Loss : 0.29389, Training Acc : 0.844, Run Time : 4.53
INFO:root:2019-05-12 02:01:25, Epoch : 1, Step : 430, Training Loss : 0.26797, Training Acc : 0.883, Run Time : 0.58
INFO:root:2019-05-12 02:01:26, Epoch : 1, Step : 431, Training Loss : 0.36607, Training Acc : 0.800, Run Time : 1.27
INFO:root:2019-05-12 02:01:32, Epoch : 1, Step : 432, Training Loss : 0.47224, Training Acc : 0.767, Run Time : 5.96
INFO:root:2019-05-12 02:01:33, Epoch : 1, Step : 433, Training Loss : 0.32559, Training Acc : 0.811, Run Time : 0.51
INFO:root:2019-05-12 02:01:33, Epoch : 1, Step : 434, Training Loss : 0.26376, Training Acc : 0.861, Run Time : 0.67
INFO:root:2019-05-12 02:01:40, Epoch : 1, Step : 435, Training Loss : 0.19939, Training Acc : 0.933, Run Time : 6.36
INFO:root:2019-05-12 02:01:40, Epoch : 1, Step : 436, Training Loss : 0.21694, Training Acc : 0.900, Run Time : 0.43
INFO:root:2019-05-12 02:01:42, Epoch : 1, Step : 437, Training Loss : 0.18278, Training Acc : 0.933, Run Time : 1.63
INFO:root:2019-05-12 02:01:49, Epoch : 1, Step : 438, Training Loss : 0.17391, Training Acc : 0.950, Run Time : 6.91
INFO:root:2019-05-12 02:01:49, Epoch : 1, Step : 439, Training Loss : 0.19806, Training Acc : 0.939, Run Time : 0.47
INFO:root:2019-05-12 02:01:51, Epoch : 1, Step : 440, Training Loss : 0.19959, Training Acc : 0.933, Run Time : 1.60
INFO:root:2019-05-12 02:01:57, Epoch : 1, Step : 441, Training Loss : 0.41058, Training Acc : 0.811, Run Time : 6.51
INFO:root:2019-05-12 02:01:58, Epoch : 1, Step : 442, Training Loss : 0.62892, Training Acc : 0.722, Run Time : 0.51
INFO:root:2019-05-12 02:01:58, Epoch : 1, Step : 443, Training Loss : 0.40038, Training Acc : 0.767, Run Time : 0.75
INFO:root:2019-05-12 02:02:05, Epoch : 1, Step : 444, Training Loss : 0.32284, Training Acc : 0.839, Run Time : 6.58
INFO:root:2019-05-12 02:02:05, Epoch : 1, Step : 445, Training Loss : 0.49053, Training Acc : 0.744, Run Time : 0.46
INFO:root:2019-05-12 02:02:06, Epoch : 1, Step : 446, Training Loss : 0.47189, Training Acc : 0.806, Run Time : 0.62
INFO:root:2019-05-12 02:02:14, Epoch : 1, Step : 447, Training Loss : 0.36807, Training Acc : 0.844, Run Time : 7.78
INFO:root:2019-05-12 02:02:14, Epoch : 1, Step : 448, Training Loss : 0.29584, Training Acc : 0.883, Run Time : 0.58
INFO:root:2019-05-12 02:02:15, Epoch : 1, Step : 449, Training Loss : 0.60899, Training Acc : 0.722, Run Time : 0.69
INFO:root:2019-05-12 02:02:16, Epoch : 1, Step : 450, Training Loss : 0.29677, Training Acc : 0.856, Run Time : 0.61
INFO:root:2019-05-12 02:02:17, Epoch : 1, Step : 451, Training Loss : 0.41069, Training Acc : 0.800, Run Time : 0.91
INFO:root:2019-05-12 02:02:25, Epoch : 1, Step : 452, Training Loss : 0.35018, Training Acc : 0.833, Run Time : 8.29
INFO:root:2019-05-12 02:02:26, Epoch : 1, Step : 453, Training Loss : 0.39419, Training Acc : 0.844, Run Time : 0.62
INFO:root:2019-05-12 02:02:27, Epoch : 1, Step : 454, Training Loss : 0.26290, Training Acc : 0.900, Run Time : 1.47
INFO:root:2019-05-12 02:02:33, Epoch : 1, Step : 455, Training Loss : 0.35188, Training Acc : 0.861, Run Time : 5.92
INFO:root:2019-05-12 02:02:34, Epoch : 1, Step : 456, Training Loss : 0.34034, Training Acc : 0.817, Run Time : 0.56
INFO:root:2019-05-12 02:02:34, Epoch : 1, Step : 457, Training Loss : 0.38088, Training Acc : 0.778, Run Time : 0.61
INFO:root:2019-05-12 02:02:41, Epoch : 1, Step : 458, Training Loss : 0.29268, Training Acc : 0.867, Run Time : 7.09
INFO:root:2019-05-12 02:02:42, Epoch : 1, Step : 459, Training Loss : 0.27470, Training Acc : 0.906, Run Time : 0.64
INFO:root:2019-05-12 02:02:42, Epoch : 1, Step : 460, Training Loss : 0.26327, Training Acc : 0.883, Run Time : 0.63
INFO:root:2019-05-12 02:02:51, Epoch : 1, Step : 461, Training Loss : 0.24990, Training Acc : 0.906, Run Time : 8.64
INFO:root:2019-05-12 02:02:52, Epoch : 1, Step : 462, Training Loss : 0.32877, Training Acc : 0.817, Run Time : 0.51
INFO:root:2019-05-12 02:02:53, Epoch : 1, Step : 463, Training Loss : 0.22876, Training Acc : 0.922, Run Time : 0.89
INFO:root:2019-05-12 02:03:00, Epoch : 1, Step : 464, Training Loss : 0.26509, Training Acc : 0.894, Run Time : 7.05
INFO:root:2019-05-12 02:03:00, Epoch : 1, Step : 465, Training Loss : 0.24491, Training Acc : 0.922, Run Time : 0.85
INFO:root:2019-05-12 02:03:01, Epoch : 1, Step : 466, Training Loss : 0.16543, Training Acc : 0.950, Run Time : 0.64
INFO:root:2019-05-12 02:03:02, Epoch : 1, Step : 467, Training Loss : 0.17392, Training Acc : 0.950, Run Time : 1.28
INFO:root:2019-05-12 02:03:12, Epoch : 1, Step : 468, Training Loss : 0.14929, Training Acc : 0.950, Run Time : 9.97
INFO:root:2019-05-12 02:03:13, Epoch : 1, Step : 469, Training Loss : 0.17684, Training Acc : 0.928, Run Time : 0.79
INFO:root:2019-05-12 02:03:14, Epoch : 1, Step : 470, Training Loss : 0.23874, Training Acc : 0.906, Run Time : 0.74
INFO:root:2019-05-12 02:03:25, Epoch : 1, Step : 471, Training Loss : 0.18702, Training Acc : 0.939, Run Time : 11.27
INFO:root:2019-05-12 02:03:26, Epoch : 1, Step : 472, Training Loss : 0.18748, Training Acc : 0.928, Run Time : 0.69
INFO:root:2019-05-12 02:03:29, Epoch : 1, Step : 473, Training Loss : 0.20214, Training Acc : 0.889, Run Time : 2.72
INFO:root:2019-05-12 02:03:34, Epoch : 1, Step : 474, Training Loss : 0.15681, Training Acc : 0.956, Run Time : 5.04
INFO:root:2019-05-12 02:03:34, Epoch : 1, Step : 475, Training Loss : 0.17096, Training Acc : 0.944, Run Time : 0.52
INFO:root:2019-05-12 02:03:36, Epoch : 1, Step : 476, Training Loss : 0.08084, Training Acc : 1.000, Run Time : 1.45
INFO:root:2019-05-12 02:03:42, Epoch : 1, Step : 477, Training Loss : 0.14004, Training Acc : 0.950, Run Time : 6.14
INFO:root:2019-05-12 02:03:42, Epoch : 1, Step : 478, Training Loss : 0.14750, Training Acc : 0.944, Run Time : 0.51
INFO:root:2019-05-12 02:03:43, Epoch : 1, Step : 479, Training Loss : 0.07457, Training Acc : 0.994, Run Time : 1.14
INFO:root:2019-05-12 02:03:52, Epoch : 1, Step : 480, Training Loss : 0.11264, Training Acc : 0.978, Run Time : 8.94
INFO:root:2019-05-12 02:03:53, Epoch : 1, Step : 481, Training Loss : 0.22342, Training Acc : 0.922, Run Time : 0.63
INFO:root:2019-05-12 02:03:59, Epoch : 1, Step : 482, Training Loss : 0.09362, Training Acc : 0.961, Run Time : 6.13
INFO:root:2019-05-12 02:04:00, Epoch : 1, Step : 483, Training Loss : 0.13088, Training Acc : 0.939, Run Time : 0.64
INFO:root:2019-05-12 02:04:00, Epoch : 1, Step : 484, Training Loss : 0.15772, Training Acc : 0.928, Run Time : 0.73
INFO:root:2019-05-12 02:04:05, Epoch : 1, Step : 485, Training Loss : 0.07473, Training Acc : 0.978, Run Time : 5.02
INFO:root:2019-05-12 02:04:06, Epoch : 1, Step : 486, Training Loss : 0.07472, Training Acc : 0.972, Run Time : 0.60
INFO:root:2019-05-12 02:04:08, Epoch : 1, Step : 487, Training Loss : 0.19195, Training Acc : 0.928, Run Time : 1.66
INFO:root:2019-05-12 02:04:14, Epoch : 1, Step : 488, Training Loss : 0.08835, Training Acc : 0.961, Run Time : 6.06
INFO:root:2019-05-12 02:04:14, Epoch : 1, Step : 489, Training Loss : 0.10319, Training Acc : 0.983, Run Time : 0.48
INFO:root:2019-05-12 02:04:23, Epoch : 1, Step : 490, Training Loss : 0.16188, Training Acc : 0.950, Run Time : 8.72
INFO:root:2019-05-12 02:04:24, Epoch : 1, Step : 491, Training Loss : 0.18833, Training Acc : 0.939, Run Time : 0.72
INFO:root:2019-05-12 02:04:25, Epoch : 1, Step : 492, Training Loss : 0.08924, Training Acc : 0.989, Run Time : 1.36
INFO:root:2019-05-12 02:04:30, Epoch : 1, Step : 493, Training Loss : 0.13562, Training Acc : 0.944, Run Time : 4.47
INFO:root:2019-05-12 02:04:31, Epoch : 1, Step : 494, Training Loss : 0.08935, Training Acc : 0.967, Run Time : 1.00
INFO:root:2019-05-12 02:04:31, Epoch : 1, Step : 495, Training Loss : 0.28472, Training Acc : 0.889, Run Time : 0.64
INFO:root:2019-05-12 02:04:33, Epoch : 1, Step : 496, Training Loss : 0.07653, Training Acc : 0.978, Run Time : 1.82
INFO:root:2019-05-12 02:04:36, Epoch : 1, Step : 497, Training Loss : 0.07501, Training Acc : 0.972, Run Time : 2.66
INFO:root:2019-05-12 02:04:37, Epoch : 1, Step : 498, Training Loss : 0.12819, Training Acc : 0.967, Run Time : 1.10
INFO:root:2019-05-12 02:04:44, Epoch : 1, Step : 499, Training Loss : 0.06964, Training Acc : 0.989, Run Time : 7.40
INFO:root:2019-05-12 02:04:45, Epoch : 1, Step : 500, Training Loss : 0.23045, Training Acc : 0.939, Run Time : 0.52
INFO:root:2019-05-12 02:04:51, Epoch : 1, Step : 501, Training Loss : 0.21104, Training Acc : 0.883, Run Time : 6.78
INFO:root:2019-05-12 02:04:52, Epoch : 1, Step : 502, Training Loss : 0.50235, Training Acc : 0.800, Run Time : 0.51
INFO:root:2019-05-12 02:04:54, Epoch : 1, Step : 503, Training Loss : 0.21316, Training Acc : 0.906, Run Time : 1.69
INFO:root:2019-05-12 02:05:00, Epoch : 1, Step : 504, Training Loss : 0.10795, Training Acc : 0.972, Run Time : 5.95
INFO:root:2019-05-12 02:05:00, Epoch : 1, Step : 505, Training Loss : 0.34948, Training Acc : 0.839, Run Time : 0.53
INFO:root:2019-05-12 02:05:02, Epoch : 1, Step : 506, Training Loss : 0.45367, Training Acc : 0.767, Run Time : 1.73
INFO:root:2019-05-12 02:05:09, Epoch : 1, Step : 507, Training Loss : 0.24767, Training Acc : 0.889, Run Time : 6.68
INFO:root:2019-05-12 02:05:09, Epoch : 1, Step : 508, Training Loss : 0.20304, Training Acc : 0.900, Run Time : 0.45
INFO:root:2019-05-12 02:05:10, Epoch : 1, Step : 509, Training Loss : 0.13570, Training Acc : 0.933, Run Time : 0.69
INFO:root:2019-05-12 02:05:18, Epoch : 1, Step : 510, Training Loss : 0.19066, Training Acc : 0.928, Run Time : 8.54
INFO:root:2019-05-12 02:05:19, Epoch : 1, Step : 511, Training Loss : 0.19392, Training Acc : 0.883, Run Time : 0.59
INFO:root:2019-05-12 02:05:19, Epoch : 1, Step : 512, Training Loss : 0.10129, Training Acc : 0.961, Run Time : 0.59
INFO:root:2019-05-12 02:05:21, Epoch : 1, Step : 513, Training Loss : 0.16508, Training Acc : 0.939, Run Time : 1.52
INFO:root:2019-05-12 02:05:27, Epoch : 1, Step : 514, Training Loss : 0.20411, Training Acc : 0.928, Run Time : 6.30
INFO:root:2019-05-12 02:05:28, Epoch : 1, Step : 515, Training Loss : 0.17782, Training Acc : 0.944, Run Time : 0.74
INFO:root:2019-05-12 02:05:29, Epoch : 1, Step : 516, Training Loss : 0.27774, Training Acc : 0.856, Run Time : 1.33
INFO:root:2019-05-12 02:05:35, Epoch : 1, Step : 517, Training Loss : 0.12196, Training Acc : 0.950, Run Time : 6.16
INFO:root:2019-05-12 02:05:36, Epoch : 1, Step : 518, Training Loss : 0.19057, Training Acc : 0.928, Run Time : 0.49
INFO:root:2019-05-12 02:05:37, Epoch : 1, Step : 519, Training Loss : 0.21492, Training Acc : 0.917, Run Time : 0.75
INFO:root:2019-05-12 02:05:47, Epoch : 1, Step : 520, Training Loss : 0.54952, Training Acc : 0.794, Run Time : 9.96
INFO:root:2019-05-12 02:05:47, Epoch : 1, Step : 521, Training Loss : 0.11311, Training Acc : 0.950, Run Time : 0.69
INFO:root:2019-05-12 02:05:48, Epoch : 1, Step : 522, Training Loss : 0.07670, Training Acc : 0.956, Run Time : 0.68
INFO:root:2019-05-12 02:05:57, Epoch : 1, Step : 523, Training Loss : 0.09506, Training Acc : 0.956, Run Time : 8.69
INFO:root:2019-05-12 02:05:57, Epoch : 1, Step : 524, Training Loss : 0.13871, Training Acc : 0.950, Run Time : 0.43
INFO:root:2019-05-12 02:05:58, Epoch : 1, Step : 525, Training Loss : 0.09464, Training Acc : 0.961, Run Time : 0.65
INFO:root:2019-05-12 02:06:07, Epoch : 1, Step : 526, Training Loss : 0.09699, Training Acc : 0.967, Run Time : 9.02
INFO:root:2019-05-12 02:06:08, Epoch : 1, Step : 527, Training Loss : 0.06940, Training Acc : 0.972, Run Time : 0.80
INFO:root:2019-05-12 02:06:10, Epoch : 1, Step : 528, Training Loss : 0.12067, Training Acc : 0.950, Run Time : 2.17
INFO:root:2019-05-12 02:06:20, Epoch : 1, Step : 529, Training Loss : 0.15550, Training Acc : 0.939, Run Time : 10.00
INFO:root:2019-05-12 02:06:21, Epoch : 1, Step : 530, Training Loss : 0.99901, Training Acc : 0.622, Run Time : 0.74
INFO:root:2019-05-12 02:06:21, Epoch : 1, Step : 531, Training Loss : 0.43585, Training Acc : 0.789, Run Time : 0.61
INFO:root:2019-05-12 02:06:22, Epoch : 1, Step : 532, Training Loss : 0.12830, Training Acc : 0.972, Run Time : 0.72
INFO:root:2019-05-12 02:06:30, Epoch : 1, Step : 533, Training Loss : 0.20644, Training Acc : 0.922, Run Time : 8.18
INFO:root:2019-05-12 02:06:31, Epoch : 1, Step : 534, Training Loss : 0.19757, Training Acc : 0.917, Run Time : 0.95
INFO:root:2019-05-12 02:06:40, Epoch : 1, Step : 535, Training Loss : 0.27992, Training Acc : 0.900, Run Time : 9.34
INFO:root:2019-05-12 02:06:41, Epoch : 1, Step : 536, Training Loss : 0.14899, Training Acc : 0.961, Run Time : 0.59
INFO:root:2019-05-12 02:06:42, Epoch : 1, Step : 537, Training Loss : 0.38830, Training Acc : 0.822, Run Time : 0.67
INFO:root:2019-05-12 02:06:49, Epoch : 1, Step : 538, Training Loss : 0.26895, Training Acc : 0.922, Run Time : 7.49
INFO:root:2019-05-12 02:06:50, Epoch : 1, Step : 539, Training Loss : 0.11119, Training Acc : 0.972, Run Time : 0.79
INFO:root:2019-05-12 02:06:57, Epoch : 1, Step : 540, Training Loss : 0.23812, Training Acc : 0.900, Run Time : 7.44
INFO:root:2019-05-12 02:06:58, Epoch : 1, Step : 541, Training Loss : 0.25193, Training Acc : 0.933, Run Time : 0.74
INFO:root:2019-05-12 02:06:59, Epoch : 1, Step : 542, Training Loss : 0.43125, Training Acc : 0.833, Run Time : 1.44
INFO:root:2019-05-12 02:07:08, Epoch : 1, Step : 543, Training Loss : 0.26494, Training Acc : 0.900, Run Time : 8.82
INFO:root:2019-05-12 02:07:09, Epoch : 1, Step : 544, Training Loss : 0.29488, Training Acc : 0.867, Run Time : 0.78
INFO:root:2019-05-12 02:07:15, Epoch : 1, Step : 545, Training Loss : 0.18616, Training Acc : 0.928, Run Time : 6.39
INFO:root:2019-05-12 02:07:16, Epoch : 1, Step : 546, Training Loss : 0.18100, Training Acc : 0.939, Run Time : 0.51
INFO:root:2019-05-12 02:07:17, Epoch : 1, Step : 547, Training Loss : 0.16186, Training Acc : 0.950, Run Time : 1.32
INFO:root:2019-05-12 02:07:24, Epoch : 1, Step : 548, Training Loss : 0.67243, Training Acc : 0.778, Run Time : 6.37
INFO:root:2019-05-12 02:07:24, Epoch : 1, Step : 549, Training Loss : 0.25406, Training Acc : 0.867, Run Time : 0.70
INFO:root:2019-05-12 02:07:26, Epoch : 1, Step : 550, Training Loss : 0.58146, Training Acc : 0.767, Run Time : 1.48
INFO:root:2019-05-12 02:07:32, Epoch : 1, Step : 551, Training Loss : 0.47945, Training Acc : 0.783, Run Time : 6.65
INFO:root:2019-05-12 02:07:33, Epoch : 1, Step : 552, Training Loss : 0.41497, Training Acc : 0.822, Run Time : 0.46
INFO:root:2019-05-12 02:07:34, Epoch : 1, Step : 553, Training Loss : 0.20839, Training Acc : 0.917, Run Time : 0.61
INFO:root:2019-05-12 02:07:42, Epoch : 1, Step : 554, Training Loss : 0.25647, Training Acc : 0.878, Run Time : 8.20
INFO:root:2019-05-12 02:07:42, Epoch : 1, Step : 555, Training Loss : 0.36479, Training Acc : 0.867, Run Time : 0.45
INFO:root:2019-05-12 02:07:43, Epoch : 1, Step : 556, Training Loss : 0.28949, Training Acc : 0.872, Run Time : 0.59
INFO:root:2019-05-12 02:07:50, Epoch : 1, Step : 557, Training Loss : 0.21608, Training Acc : 0.906, Run Time : 7.12
INFO:root:2019-05-12 02:07:51, Epoch : 1, Step : 558, Training Loss : 0.28594, Training Acc : 0.828, Run Time : 0.68
INFO:root:2019-05-12 02:07:52, Epoch : 1, Step : 559, Training Loss : 0.12479, Training Acc : 0.972, Run Time : 1.54
INFO:root:2019-05-12 02:08:01, Epoch : 1, Step : 560, Training Loss : 0.28665, Training Acc : 0.839, Run Time : 9.32
INFO:root:2019-05-12 02:08:02, Epoch : 1, Step : 561, Training Loss : 0.32533, Training Acc : 0.839, Run Time : 0.50
INFO:root:2019-05-12 02:08:03, Epoch : 1, Step : 562, Training Loss : 0.29676, Training Acc : 0.917, Run Time : 0.70
INFO:root:2019-05-12 02:08:13, Epoch : 1, Step : 563, Training Loss : 0.13942, Training Acc : 0.961, Run Time : 10.07
INFO:root:2019-05-12 02:08:13, Epoch : 1, Step : 564, Training Loss : 0.19592, Training Acc : 0.956, Run Time : 0.79
INFO:root:2019-05-12 02:08:23, Epoch : 1, Step : 565, Training Loss : 0.34359, Training Acc : 0.856, Run Time : 9.08
INFO:root:2019-05-12 02:08:24, Epoch : 1, Step : 566, Training Loss : 0.22633, Training Acc : 0.911, Run Time : 1.63
INFO:root:2019-05-12 02:08:25, Epoch : 1, Step : 567, Training Loss : 0.23624, Training Acc : 0.917, Run Time : 0.56
INFO:root:2019-05-12 02:08:35, Epoch : 1, Step : 568, Training Loss : 0.15840, Training Acc : 0.961, Run Time : 10.23
INFO:root:2019-05-12 02:08:36, Epoch : 1, Step : 569, Training Loss : 0.21431, Training Acc : 0.933, Run Time : 0.51
INFO:root:2019-05-12 02:08:36, Epoch : 1, Step : 570, Training Loss : 0.20479, Training Acc : 0.922, Run Time : 0.58
INFO:root:2019-05-12 02:08:44, Epoch : 1, Step : 571, Training Loss : 0.15746, Training Acc : 0.933, Run Time : 7.94
INFO:root:2019-05-12 02:08:54, Epoch : 1, Step : 572, Training Loss : 0.16365, Training Acc : 0.956, Run Time : 9.56
INFO:root:2019-05-12 02:08:54, Epoch : 1, Step : 573, Training Loss : 0.25725, Training Acc : 0.900, Run Time : 0.55
INFO:root:2019-05-12 02:08:56, Epoch : 1, Step : 574, Training Loss : 0.20533, Training Acc : 0.917, Run Time : 1.52
INFO:root:2019-05-12 02:09:02, Epoch : 1, Step : 575, Training Loss : 0.21870, Training Acc : 0.911, Run Time : 6.14
INFO:root:2019-05-12 02:09:02, Epoch : 1, Step : 576, Training Loss : 0.14838, Training Acc : 0.961, Run Time : 0.44
INFO:root:2019-05-12 02:09:04, Epoch : 1, Step : 577, Training Loss : 0.29379, Training Acc : 0.844, Run Time : 1.68
INFO:root:2019-05-12 02:09:20, Epoch : 1, Step : 578, Training Loss : 0.20780, Training Acc : 0.928, Run Time : 16.42
INFO:root:2019-05-12 02:09:21, Epoch : 1, Step : 579, Training Loss : 0.25848, Training Acc : 0.894, Run Time : 1.06
INFO:root:2019-05-12 02:09:22, Epoch : 1, Step : 580, Training Loss : 0.22355, Training Acc : 0.889, Run Time : 0.67
INFO:root:2019-05-12 02:09:29, Epoch : 1, Step : 581, Training Loss : 0.18427, Training Acc : 0.922, Run Time : 7.41
INFO:root:2019-05-12 02:09:30, Epoch : 1, Step : 582, Training Loss : 0.20366, Training Acc : 0.906, Run Time : 0.96
INFO:root:2019-05-12 02:09:34, Epoch : 1, Step : 583, Training Loss : 0.23122, Training Acc : 0.922, Run Time : 3.66
INFO:root:2019-05-12 02:09:35, Epoch : 1, Step : 584, Training Loss : 0.27738, Training Acc : 0.878, Run Time : 0.70
INFO:root:2019-05-12 02:09:43, Epoch : 1, Step : 585, Training Loss : 0.30577, Training Acc : 0.844, Run Time : 8.53
INFO:root:2019-05-12 02:09:44, Epoch : 1, Step : 586, Training Loss : 0.22728, Training Acc : 0.894, Run Time : 0.52
INFO:root:2019-05-12 02:09:50, Epoch : 1, Step : 587, Training Loss : 0.30965, Training Acc : 0.856, Run Time : 5.74
INFO:root:2019-05-12 02:09:50, Epoch : 1, Step : 588, Training Loss : 0.30352, Training Acc : 0.872, Run Time : 0.71
INFO:root:2019-05-12 02:09:51, Epoch : 1, Step : 589, Training Loss : 0.18615, Training Acc : 0.944, Run Time : 0.64
INFO:root:2019-05-12 02:09:59, Epoch : 1, Step : 590, Training Loss : 0.24158, Training Acc : 0.889, Run Time : 7.77
INFO:root:2019-05-12 02:09:59, Epoch : 1, Step : 591, Training Loss : 0.25752, Training Acc : 0.889, Run Time : 0.76
INFO:root:2019-05-12 02:10:05, Epoch : 1, Step : 592, Training Loss : 0.30063, Training Acc : 0.878, Run Time : 5.20
INFO:root:2019-05-12 02:10:05, Epoch : 1, Step : 593, Training Loss : 0.24922, Training Acc : 0.872, Run Time : 0.46
INFO:root:2019-05-12 02:10:06, Epoch : 1, Step : 594, Training Loss : 0.20864, Training Acc : 0.939, Run Time : 0.58
INFO:root:2019-05-12 02:10:13, Epoch : 1, Step : 595, Training Loss : 0.30827, Training Acc : 0.883, Run Time : 7.44
INFO:root:2019-05-12 02:10:14, Epoch : 1, Step : 596, Training Loss : 0.40613, Training Acc : 0.800, Run Time : 0.51
INFO:root:2019-05-12 02:10:15, Epoch : 1, Step : 597, Training Loss : 0.35520, Training Acc : 0.828, Run Time : 1.43
INFO:root:2019-05-12 02:10:19, Epoch : 1, Step : 598, Training Loss : 0.33394, Training Acc : 0.839, Run Time : 3.71
INFO:root:2019-05-12 02:10:19, Epoch : 1, Step : 599, Training Loss : 0.20787, Training Acc : 0.906, Run Time : 0.52
INFO:root:2019-05-12 02:10:21, Epoch : 1, Step : 600, Training Loss : 0.27538, Training Acc : 0.878, Run Time : 1.40
INFO:root:2019-05-12 02:10:28, Epoch : 1, Step : 601, Training Loss : 0.37764, Training Acc : 0.811, Run Time : 7.10
INFO:root:2019-05-12 02:10:28, Epoch : 1, Step : 602, Training Loss : 0.52945, Training Acc : 0.794, Run Time : 0.55
INFO:root:2019-05-12 02:10:30, Epoch : 1, Step : 603, Training Loss : 0.52934, Training Acc : 0.744, Run Time : 1.51
INFO:root:2019-05-12 02:10:36, Epoch : 1, Step : 604, Training Loss : 0.33844, Training Acc : 0.828, Run Time : 6.43
INFO:root:2019-05-12 02:10:37, Epoch : 1, Step : 605, Training Loss : 0.34290, Training Acc : 0.839, Run Time : 0.44
INFO:root:2019-05-12 02:10:38, Epoch : 1, Step : 606, Training Loss : 0.36907, Training Acc : 0.833, Run Time : 1.40
INFO:root:2019-05-12 02:10:41, Epoch : 1, Step : 607, Training Loss : 0.26118, Training Acc : 0.878, Run Time : 2.48
INFO:root:2019-05-12 02:10:41, Epoch : 1, Step : 608, Training Loss : 0.30682, Training Acc : 0.861, Run Time : 0.84
INFO:root:2019-05-12 02:10:42, Epoch : 1, Step : 609, Training Loss : 0.25234, Training Acc : 0.861, Run Time : 0.59
INFO:root:2019-05-12 02:10:46, Epoch : 1, Step : 610, Training Loss : 0.27699, Training Acc : 0.850, Run Time : 4.34
INFO:root:2019-05-12 02:10:47, Epoch : 1, Step : 611, Training Loss : 0.19664, Training Acc : 0.894, Run Time : 0.71
INFO:root:2019-05-12 02:10:51, Epoch : 1, Step : 612, Training Loss : 0.22323, Training Acc : 0.917, Run Time : 3.53
INFO:root:2019-05-12 02:10:52, Epoch : 1, Step : 613, Training Loss : 0.32652, Training Acc : 0.867, Run Time : 1.71
INFO:root:2019-05-12 02:10:53, Epoch : 1, Step : 614, Training Loss : 0.39115, Training Acc : 0.800, Run Time : 0.67
INFO:root:2019-05-12 02:11:04, Epoch : 1, Step : 615, Training Loss : 0.21484, Training Acc : 0.928, Run Time : 10.51
INFO:root:2019-05-12 02:11:04, Epoch : 1, Step : 616, Training Loss : 0.24315, Training Acc : 0.917, Run Time : 0.87
INFO:root:2019-05-12 02:11:13, Epoch : 1, Step : 617, Training Loss : 0.35502, Training Acc : 0.861, Run Time : 8.17
INFO:root:2019-05-12 02:11:13, Epoch : 1, Step : 618, Training Loss : 0.43555, Training Acc : 0.872, Run Time : 0.63
INFO:root:2019-05-12 02:11:14, Epoch : 1, Step : 619, Training Loss : 0.35794, Training Acc : 0.872, Run Time : 1.18
INFO:root:2019-05-12 02:11:23, Epoch : 1, Step : 620, Training Loss : 0.52759, Training Acc : 0.761, Run Time : 8.74
INFO:root:2019-05-12 02:11:24, Epoch : 1, Step : 621, Training Loss : 0.30008, Training Acc : 0.894, Run Time : 1.14
INFO:root:2019-05-12 02:11:36, Epoch : 1, Step : 622, Training Loss : 0.42122, Training Acc : 0.817, Run Time : 11.49
INFO:root:2019-05-12 02:11:37, Epoch : 1, Step : 623, Training Loss : 0.51288, Training Acc : 0.800, Run Time : 1.20
INFO:root:2019-05-12 02:11:45, Epoch : 1, Step : 624, Training Loss : 0.31827, Training Acc : 0.872, Run Time : 7.85
INFO:root:2019-05-12 02:11:46, Epoch : 1, Step : 625, Training Loss : 0.21711, Training Acc : 0.922, Run Time : 0.95
INFO:root:2019-05-12 02:11:47, Epoch : 1, Step : 626, Training Loss : 0.31024, Training Acc : 0.878, Run Time : 1.14
INFO:root:2019-05-12 02:11:55, Epoch : 1, Step : 627, Training Loss : 0.22081, Training Acc : 0.911, Run Time : 8.12
INFO:root:2019-05-12 02:11:56, Epoch : 1, Step : 628, Training Loss : 0.40824, Training Acc : 0.828, Run Time : 1.00
INFO:root:2019-05-12 02:12:04, Epoch : 1, Step : 629, Training Loss : 0.32068, Training Acc : 0.889, Run Time : 7.95
INFO:root:2019-05-12 02:12:05, Epoch : 1, Step : 630, Training Loss : 0.19124, Training Acc : 0.950, Run Time : 0.78
INFO:root:2019-05-12 02:12:10, Epoch : 1, Step : 631, Training Loss : 0.32100, Training Acc : 0.856, Run Time : 5.20
INFO:root:2019-05-12 02:12:11, Epoch : 1, Step : 632, Training Loss : 0.19747, Training Acc : 0.906, Run Time : 0.59
INFO:root:2019-05-12 02:12:16, Epoch : 1, Step : 633, Training Loss : 0.38064, Training Acc : 0.822, Run Time : 4.96
INFO:root:2019-05-12 02:12:17, Epoch : 1, Step : 634, Training Loss : 0.20714, Training Acc : 0.944, Run Time : 1.98
INFO:root:2019-05-12 02:12:18, Epoch : 1, Step : 635, Training Loss : 0.21090, Training Acc : 0.922, Run Time : 0.64
INFO:root:2019-05-12 02:12:26, Epoch : 1, Step : 636, Training Loss : 0.16437, Training Acc : 0.939, Run Time : 8.14
INFO:root:2019-05-12 02:12:27, Epoch : 1, Step : 637, Training Loss : 0.27899, Training Acc : 0.878, Run Time : 0.54
INFO:root:2019-05-12 02:12:28, Epoch : 1, Step : 638, Training Loss : 0.25937, Training Acc : 0.911, Run Time : 1.48
INFO:root:2019-05-12 02:12:34, Epoch : 1, Step : 639, Training Loss : 0.31591, Training Acc : 0.822, Run Time : 5.26
INFO:root:2019-05-12 02:12:34, Epoch : 1, Step : 640, Training Loss : 0.20601, Training Acc : 0.911, Run Time : 0.63
INFO:root:2019-05-12 02:12:41, Epoch : 1, Step : 641, Training Loss : 0.20221, Training Acc : 0.911, Run Time : 6.88
INFO:root:2019-05-12 02:12:42, Epoch : 1, Step : 642, Training Loss : 0.38049, Training Acc : 0.839, Run Time : 0.70
INFO:root:2019-05-12 02:12:44, Epoch : 1, Step : 643, Training Loss : 0.28059, Training Acc : 0.889, Run Time : 1.98
INFO:root:2019-05-12 02:12:49, Epoch : 1, Step : 644, Training Loss : 0.27842, Training Acc : 0.889, Run Time : 5.73
INFO:root:2019-05-12 02:12:50, Epoch : 1, Step : 645, Training Loss : 0.26422, Training Acc : 0.872, Run Time : 0.56
INFO:root:2019-05-12 02:12:51, Epoch : 1, Step : 646, Training Loss : 0.22000, Training Acc : 0.928, Run Time : 1.38
INFO:root:2019-05-12 02:12:58, Epoch : 1, Step : 647, Training Loss : 0.29649, Training Acc : 0.856, Run Time : 6.63
INFO:root:2019-05-12 02:12:59, Epoch : 1, Step : 648, Training Loss : 0.16974, Training Acc : 0.922, Run Time : 0.56
INFO:root:2019-05-12 02:13:00, Epoch : 1, Step : 649, Training Loss : 0.42803, Training Acc : 0.811, Run Time : 1.59
INFO:root:2019-05-12 02:13:06, Epoch : 1, Step : 650, Training Loss : 0.49010, Training Acc : 0.728, Run Time : 5.96
INFO:root:2019-05-12 02:13:07, Epoch : 1, Step : 651, Training Loss : 0.34212, Training Acc : 0.856, Run Time : 0.48
INFO:root:2019-05-12 02:13:08, Epoch : 1, Step : 652, Training Loss : 0.21101, Training Acc : 0.922, Run Time : 1.71
INFO:root:2019-05-12 02:13:16, Epoch : 1, Step : 653, Training Loss : 0.13163, Training Acc : 0.950, Run Time : 7.92
INFO:root:2019-05-12 02:13:18, Epoch : 1, Step : 654, Training Loss : 0.23564, Training Acc : 0.906, Run Time : 1.44
INFO:root:2019-05-12 02:13:24, Epoch : 1, Step : 655, Training Loss : 0.20874, Training Acc : 0.933, Run Time : 5.99
INFO:root:2019-05-12 02:13:25, Epoch : 1, Step : 656, Training Loss : 0.25511, Training Acc : 0.889, Run Time : 0.98
INFO:root:2019-05-12 02:13:26, Epoch : 1, Step : 657, Training Loss : 0.28171, Training Acc : 0.900, Run Time : 1.22
INFO:root:2019-05-12 02:13:32, Epoch : 1, Step : 658, Training Loss : 0.29971, Training Acc : 0.861, Run Time : 6.37
INFO:root:2019-05-12 02:13:33, Epoch : 1, Step : 659, Training Loss : 0.23190, Training Acc : 0.872, Run Time : 0.63
INFO:root:2019-05-12 02:13:34, Epoch : 1, Step : 660, Training Loss : 0.23788, Training Acc : 0.883, Run Time : 1.12
INFO:root:2019-05-12 02:13:39, Epoch : 1, Step : 661, Training Loss : 0.26498, Training Acc : 0.861, Run Time : 4.88
INFO:root:2019-05-12 02:13:40, Epoch : 1, Step : 662, Training Loss : 0.20168, Training Acc : 0.917, Run Time : 0.66
INFO:root:2019-05-12 02:13:40, Epoch : 1, Step : 663, Training Loss : 0.58498, Training Acc : 0.739, Run Time : 0.58
INFO:root:2019-05-12 02:13:47, Epoch : 1, Step : 664, Training Loss : 0.71063, Training Acc : 0.744, Run Time : 6.64
INFO:root:2019-05-12 02:13:48, Epoch : 1, Step : 665, Training Loss : 0.43685, Training Acc : 0.806, Run Time : 1.04
INFO:root:2019-05-12 02:13:56, Epoch : 1, Step : 666, Training Loss : 0.43131, Training Acc : 0.794, Run Time : 8.65
INFO:root:2019-05-12 02:13:58, Epoch : 1, Step : 667, Training Loss : 0.36013, Training Acc : 0.844, Run Time : 1.98
INFO:root:2019-05-12 02:14:07, Epoch : 1, Step : 668, Training Loss : 0.25069, Training Acc : 0.911, Run Time : 8.83
INFO:root:2019-05-12 02:14:08, Epoch : 1, Step : 669, Training Loss : 0.53364, Training Acc : 0.778, Run Time : 0.89
INFO:root:2019-05-12 02:14:09, Epoch : 1, Step : 670, Training Loss : 0.47850, Training Acc : 0.783, Run Time : 0.63
INFO:root:2019-05-12 02:14:13, Epoch : 1, Step : 671, Training Loss : 0.23646, Training Acc : 0.922, Run Time : 4.57
INFO:root:2019-05-12 02:14:14, Epoch : 1, Step : 672, Training Loss : 0.29005, Training Acc : 0.872, Run Time : 0.59
INFO:root:2019-05-12 02:14:15, Epoch : 1, Step : 673, Training Loss : 0.32901, Training Acc : 0.878, Run Time : 0.63
INFO:root:2019-05-12 02:14:15, Epoch : 1, Step : 674, Training Loss : 0.39768, Training Acc : 0.861, Run Time : 0.88
INFO:root:2019-05-12 02:14:20, Epoch : 1, Step : 675, Training Loss : 0.27858, Training Acc : 0.883, Run Time : 4.77
INFO:root:2019-05-12 02:14:27, Epoch : 1, Step : 676, Training Loss : 0.26541, Training Acc : 0.917, Run Time : 6.57
INFO:root:2019-05-12 02:14:28, Epoch : 1, Step : 677, Training Loss : 0.39106, Training Acc : 0.800, Run Time : 0.75
INFO:root:2019-05-12 02:14:34, Epoch : 1, Step : 678, Training Loss : 0.36841, Training Acc : 0.833, Run Time : 6.28
INFO:root:2019-05-12 02:14:35, Epoch : 1, Step : 679, Training Loss : 0.33109, Training Acc : 0.861, Run Time : 1.11
INFO:root:2019-05-12 02:14:36, Epoch : 1, Step : 680, Training Loss : 0.30737, Training Acc : 0.883, Run Time : 0.65
INFO:root:2019-05-12 02:14:40, Epoch : 1, Step : 681, Training Loss : 0.44611, Training Acc : 0.733, Run Time : 4.16
INFO:root:2019-05-12 02:14:41, Epoch : 1, Step : 682, Training Loss : 0.34005, Training Acc : 0.889, Run Time : 0.89
INFO:root:2019-05-12 02:14:52, Epoch : 1, Step : 683, Training Loss : 0.41649, Training Acc : 0.772, Run Time : 10.95
INFO:root:2019-05-12 02:14:53, Epoch : 1, Step : 684, Training Loss : 0.58082, Training Acc : 0.700, Run Time : 1.29
INFO:root:2019-05-12 02:15:02, Epoch : 1, Step : 685, Training Loss : 0.33060, Training Acc : 0.828, Run Time : 8.73
INFO:root:2019-05-12 02:15:02, Epoch : 1, Step : 686, Training Loss : 0.32687, Training Acc : 0.833, Run Time : 0.86
INFO:root:2019-05-12 02:15:11, Epoch : 1, Step : 687, Training Loss : 0.42817, Training Acc : 0.839, Run Time : 8.92
INFO:root:2019-05-12 02:15:12, Epoch : 1, Step : 688, Training Loss : 0.41633, Training Acc : 0.839, Run Time : 0.60
INFO:root:2019-05-12 02:15:12, Epoch : 1, Step : 689, Training Loss : 0.34755, Training Acc : 0.817, Run Time : 0.47
INFO:root:2019-05-12 02:15:14, Epoch : 1, Step : 690, Training Loss : 0.27500, Training Acc : 0.867, Run Time : 1.36
INFO:root:2019-05-12 02:15:19, Epoch : 1, Step : 691, Training Loss : 0.21416, Training Acc : 0.906, Run Time : 5.64
INFO:root:2019-05-12 02:15:20, Epoch : 1, Step : 692, Training Loss : 0.29226, Training Acc : 0.856, Run Time : 0.70
INFO:root:2019-05-12 02:15:23, Epoch : 1, Step : 693, Training Loss : 0.31791, Training Acc : 0.839, Run Time : 2.72
INFO:root:2019-05-12 02:15:25, Epoch : 1, Step : 694, Training Loss : 0.23302, Training Acc : 0.900, Run Time : 2.18
INFO:root:2019-05-12 02:15:26, Epoch : 1, Step : 695, Training Loss : 0.31906, Training Acc : 0.839, Run Time : 0.61
INFO:root:2019-05-12 02:15:31, Epoch : 1, Step : 696, Training Loss : 0.27809, Training Acc : 0.878, Run Time : 5.43
INFO:root:2019-05-12 02:15:32, Epoch : 1, Step : 697, Training Loss : 0.28486, Training Acc : 0.867, Run Time : 0.97
INFO:root:2019-05-12 02:15:33, Epoch : 1, Step : 698, Training Loss : 0.34546, Training Acc : 0.850, Run Time : 0.90
INFO:root:2019-05-12 02:15:38, Epoch : 1, Step : 699, Training Loss : 0.20185, Training Acc : 0.928, Run Time : 4.76
INFO:root:2019-05-12 02:15:38, Epoch : 1, Step : 700, Training Loss : 0.28128, Training Acc : 0.867, Run Time : 0.55
INFO:root:2019-05-12 02:15:44, Epoch : 1, Step : 701, Training Loss : 0.16831, Training Acc : 0.911, Run Time : 5.43
INFO:root:2019-05-12 02:15:46, Epoch : 1, Step : 702, Training Loss : 0.18364, Training Acc : 0.917, Run Time : 2.11
INFO:root:2019-05-12 02:15:47, Epoch : 1, Step : 703, Training Loss : 0.18256, Training Acc : 0.939, Run Time : 0.70
INFO:root:2019-05-12 02:15:51, Epoch : 1, Step : 704, Training Loss : 0.27203, Training Acc : 0.894, Run Time : 4.67
INFO:root:2019-05-12 02:15:52, Epoch : 1, Step : 705, Training Loss : 0.21187, Training Acc : 0.922, Run Time : 0.91
INFO:root:2019-05-12 02:15:59, Epoch : 1, Step : 706, Training Loss : 0.21404, Training Acc : 0.917, Run Time : 7.10
INFO:root:2019-05-12 02:16:00, Epoch : 1, Step : 707, Training Loss : 0.15543, Training Acc : 0.933, Run Time : 0.83
INFO:root:2019-05-12 02:16:07, Epoch : 1, Step : 708, Training Loss : 0.24637, Training Acc : 0.917, Run Time : 6.71
INFO:root:2019-05-12 02:16:08, Epoch : 1, Step : 709, Training Loss : 0.30811, Training Acc : 0.844, Run Time : 0.95
INFO:root:2019-05-12 02:16:08, Epoch : 1, Step : 710, Training Loss : 0.24405, Training Acc : 0.928, Run Time : 0.67
INFO:root:2019-05-12 02:16:17, Epoch : 1, Step : 711, Training Loss : 0.15109, Training Acc : 0.933, Run Time : 8.22
INFO:root:2019-05-12 02:16:17, Epoch : 1, Step : 712, Training Loss : 0.29912, Training Acc : 0.878, Run Time : 0.45
INFO:root:2019-05-12 02:16:18, Epoch : 1, Step : 713, Training Loss : 0.33550, Training Acc : 0.878, Run Time : 0.68
INFO:root:2019-05-12 02:16:28, Epoch : 1, Step : 714, Training Loss : 0.32626, Training Acc : 0.894, Run Time : 10.42
INFO:root:2019-05-12 02:16:29, Epoch : 1, Step : 715, Training Loss : 0.14916, Training Acc : 0.950, Run Time : 0.75
INFO:root:2019-05-12 02:16:30, Epoch : 1, Step : 716, Training Loss : 0.75562, Training Acc : 0.611, Run Time : 1.56
INFO:root:2019-05-12 02:16:37, Epoch : 1, Step : 717, Training Loss : 0.82201, Training Acc : 0.639, Run Time : 6.96
INFO:root:2019-05-12 02:16:38, Epoch : 1, Step : 718, Training Loss : 0.23973, Training Acc : 0.883, Run Time : 0.80
INFO:root:2019-05-12 02:16:40, Epoch : 1, Step : 719, Training Loss : 0.44483, Training Acc : 0.828, Run Time : 1.39
INFO:root:2019-05-12 02:16:44, Epoch : 1, Step : 720, Training Loss : 0.15753, Training Acc : 0.956, Run Time : 4.20
INFO:root:2019-05-12 02:16:44, Epoch : 1, Step : 721, Training Loss : 0.18445, Training Acc : 0.922, Run Time : 0.58
INFO:root:2019-05-12 02:16:46, Epoch : 1, Step : 722, Training Loss : 0.09530, Training Acc : 0.983, Run Time : 1.57
INFO:root:2019-05-12 02:16:53, Epoch : 1, Step : 723, Training Loss : 0.20983, Training Acc : 0.922, Run Time : 7.27
INFO:root:2019-05-12 02:16:54, Epoch : 1, Step : 724, Training Loss : 0.14458, Training Acc : 0.972, Run Time : 0.68
INFO:root:2019-05-12 02:17:01, Epoch : 1, Step : 725, Training Loss : 0.23727, Training Acc : 0.906, Run Time : 7.40
INFO:root:2019-05-12 02:17:02, Epoch : 1, Step : 726, Training Loss : 0.21984, Training Acc : 0.944, Run Time : 0.62
INFO:root:2019-05-12 02:17:03, Epoch : 1, Step : 727, Training Loss : 0.22295, Training Acc : 0.933, Run Time : 1.44
INFO:root:2019-05-12 02:17:09, Epoch : 1, Step : 728, Training Loss : 0.34402, Training Acc : 0.872, Run Time : 5.43
INFO:root:2019-05-12 02:17:09, Epoch : 1, Step : 729, Training Loss : 0.41244, Training Acc : 0.856, Run Time : 0.69
INFO:root:2019-05-12 02:17:12, Epoch : 1, Step : 730, Training Loss : 0.56498, Training Acc : 0.817, Run Time : 2.99
INFO:root:2019-05-12 02:17:17, Epoch : 1, Step : 731, Training Loss : 0.14769, Training Acc : 0.950, Run Time : 4.98
INFO:root:2019-05-12 02:17:18, Epoch : 1, Step : 732, Training Loss : 0.19687, Training Acc : 0.928, Run Time : 0.45
INFO:root:2019-05-12 02:17:26, Epoch : 1, Step : 733, Training Loss : 0.20118, Training Acc : 0.906, Run Time : 8.62
INFO:root:2019-05-12 02:17:27, Epoch : 1, Step : 734, Training Loss : 0.28627, Training Acc : 0.872, Run Time : 0.58
INFO:root:2019-05-12 02:17:29, Epoch : 1, Step : 735, Training Loss : 0.15230, Training Acc : 0.961, Run Time : 1.62
INFO:root:2019-05-12 02:17:37, Epoch : 1, Step : 736, Training Loss : 0.09213, Training Acc : 0.983, Run Time : 8.10
INFO:root:2019-05-12 02:17:38, Epoch : 1, Step : 737, Training Loss : 0.17500, Training Acc : 0.944, Run Time : 0.75
INFO:root:2019-05-12 02:17:45, Epoch : 1, Step : 738, Training Loss : 0.18494, Training Acc : 0.928, Run Time : 7.19
INFO:root:2019-05-12 02:17:46, Epoch : 1, Step : 739, Training Loss : 0.25507, Training Acc : 0.883, Run Time : 0.79
INFO:root:2019-05-12 02:17:46, Epoch : 1, Step : 740, Training Loss : 0.13750, Training Acc : 0.950, Run Time : 0.61
INFO:root:2019-05-12 02:17:54, Epoch : 1, Step : 741, Training Loss : 0.51004, Training Acc : 0.783, Run Time : 8.07
INFO:root:2019-05-12 02:17:55, Epoch : 1, Step : 742, Training Loss : 0.96967, Training Acc : 0.667, Run Time : 0.50
INFO:root:2019-05-12 02:17:57, Epoch : 1, Step : 743, Training Loss : 0.64183, Training Acc : 0.750, Run Time : 1.82
INFO:root:2019-05-12 02:18:05, Epoch : 1, Step : 744, Training Loss : 0.19196, Training Acc : 0.922, Run Time : 8.60
INFO:root:2019-05-12 02:18:06, Epoch : 1, Step : 745, Training Loss : 0.13382, Training Acc : 0.961, Run Time : 0.48
INFO:root:2019-05-12 02:18:07, Epoch : 1, Step : 746, Training Loss : 0.15434, Training Acc : 0.939, Run Time : 1.23
INFO:root:2019-05-12 02:18:13, Epoch : 1, Step : 747, Training Loss : 0.18465, Training Acc : 0.933, Run Time : 6.57
INFO:root:2019-05-12 02:18:14, Epoch : 1, Step : 748, Training Loss : 0.13858, Training Acc : 0.961, Run Time : 0.60
INFO:root:2019-05-12 02:18:15, Epoch : 1, Step : 749, Training Loss : 0.15024, Training Acc : 0.961, Run Time : 1.38
INFO:root:2019-05-12 02:18:23, Epoch : 1, Step : 750, Training Loss : 0.11628, Training Acc : 0.956, Run Time : 7.53
INFO:root:2019-05-12 02:18:23, Epoch : 1, Step : 751, Training Loss : 0.05748, Training Acc : 1.000, Run Time : 0.53
INFO:root:2019-05-12 02:18:25, Epoch : 1, Step : 752, Training Loss : 0.08861, Training Acc : 0.983, Run Time : 1.92
INFO:root:2019-05-12 02:18:34, Epoch : 1, Step : 753, Training Loss : 0.12476, Training Acc : 0.956, Run Time : 8.82
INFO:root:2019-05-12 02:18:35, Epoch : 1, Step : 754, Training Loss : 0.09525, Training Acc : 0.978, Run Time : 0.46
INFO:root:2019-05-12 02:18:35, Epoch : 1, Step : 755, Training Loss : 0.14930, Training Acc : 0.928, Run Time : 0.64
INFO:root:2019-05-12 02:18:38, Epoch : 1, Step : 756, Training Loss : 0.13901, Training Acc : 0.956, Run Time : 3.14
INFO:root:2019-05-12 02:18:39, Epoch : 1, Step : 757, Training Loss : 0.07224, Training Acc : 0.967, Run Time : 1.04
INFO:root:2019-05-12 02:18:40, Epoch : 1, Step : 758, Training Loss : 0.10701, Training Acc : 0.972, Run Time : 0.69
INFO:root:2019-05-12 02:18:42, Epoch : 1, Step : 759, Training Loss : 0.06248, Training Acc : 1.000, Run Time : 1.37
INFO:root:2019-05-12 02:18:53, Epoch : 1, Step : 760, Training Loss : 0.11839, Training Acc : 0.956, Run Time : 10.98
INFO:root:2019-05-12 02:18:54, Epoch : 1, Step : 761, Training Loss : 0.08677, Training Acc : 0.972, Run Time : 1.69
INFO:root:2019-05-12 02:18:55, Epoch : 1, Step : 762, Training Loss : 0.18795, Training Acc : 0.900, Run Time : 0.63
INFO:root:2019-05-12 02:18:55, Epoch : 1, Step : 763, Training Loss : 0.08207, Training Acc : 0.961, Run Time : 0.67
INFO:root:2019-05-12 02:19:00, Epoch : 1, Step : 764, Training Loss : 0.08025, Training Acc : 0.972, Run Time : 4.85
INFO:root:2019-05-12 02:19:01, Epoch : 1, Step : 765, Training Loss : 0.16948, Training Acc : 0.933, Run Time : 1.13
INFO:root:2019-05-12 02:19:08, Epoch : 1, Step : 766, Training Loss : 0.04976, Training Acc : 0.989, Run Time : 6.38
INFO:root:2019-05-12 02:19:08, Epoch : 1, Step : 767, Training Loss : 0.10919, Training Acc : 0.956, Run Time : 0.61
INFO:root:2019-05-12 02:19:09, Epoch : 1, Step : 768, Training Loss : 0.07466, Training Acc : 0.978, Run Time : 0.38
INFO:root:2019-05-12 02:19:13, Epoch : 1, Step : 769, Training Loss : 0.18969, Training Acc : 0.928, Run Time : 4.11
INFO:root:2019-05-12 02:19:14, Epoch : 1, Step : 770, Training Loss : 0.17731, Training Acc : 0.939, Run Time : 0.92
INFO:root:2019-05-12 02:19:16, Epoch : 1, Step : 771, Training Loss : 0.98545, Training Acc : 0.606, Run Time : 2.06
INFO:root:2019-05-12 02:19:17, Epoch : 1, Step : 772, Training Loss : 0.19319, Training Acc : 0.922, Run Time : 1.21
INFO:root:2019-05-12 02:19:25, Epoch : 1, Step : 773, Training Loss : 0.39702, Training Acc : 0.839, Run Time : 8.09
INFO:root:2019-05-12 02:19:27, Epoch : 1, Step : 774, Training Loss : 0.23714, Training Acc : 0.900, Run Time : 1.94
INFO:root:2019-05-12 02:19:31, Epoch : 1, Step : 775, Training Loss : 0.44996, Training Acc : 0.850, Run Time : 3.99
INFO:root:2019-05-12 02:19:36, Epoch : 1, Step : 776, Training Loss : 0.17511, Training Acc : 0.944, Run Time : 4.38
INFO:root:2019-05-12 02:19:36, Epoch : 1, Step : 777, Training Loss : 0.28853, Training Acc : 0.878, Run Time : 0.46
INFO:root:2019-05-12 02:19:37, Epoch : 1, Step : 778, Training Loss : 0.18168, Training Acc : 0.922, Run Time : 1.35
INFO:root:2019-05-12 02:19:39, Epoch : 1, Step : 779, Training Loss : 0.24578, Training Acc : 0.889, Run Time : 1.56
INFO:root:2019-05-12 02:19:40, Epoch : 1, Step : 780, Training Loss : 0.22085, Training Acc : 0.900, Run Time : 0.83
INFO:root:2019-05-12 02:19:48, Epoch : 1, Step : 781, Training Loss : 0.30106, Training Acc : 0.856, Run Time : 8.03
INFO:root:2019-05-12 02:19:56, Epoch : 1, Step : 782, Training Loss : 0.38596, Training Acc : 0.861, Run Time : 8.16
INFO:root:2019-05-12 02:19:57, Epoch : 1, Step : 783, Training Loss : 0.28928, Training Acc : 0.867, Run Time : 1.08
INFO:root:2019-05-12 02:19:58, Epoch : 1, Step : 784, Training Loss : 0.42709, Training Acc : 0.761, Run Time : 0.57
INFO:root:2019-05-12 02:20:05, Epoch : 1, Step : 785, Training Loss : 0.27014, Training Acc : 0.878, Run Time : 7.58
INFO:root:2019-05-12 02:20:09, Epoch : 1, Step : 786, Training Loss : 0.28497, Training Acc : 0.856, Run Time : 3.56
INFO:root:2019-05-12 02:20:17, Epoch : 1, Step : 787, Training Loss : 0.15385, Training Acc : 0.956, Run Time : 8.44
INFO:root:2019-05-12 02:20:18, Epoch : 1, Step : 788, Training Loss : 0.30649, Training Acc : 0.872, Run Time : 0.62
INFO:root:2019-05-12 02:20:20, Epoch : 1, Step : 789, Training Loss : 0.16795, Training Acc : 0.922, Run Time : 2.23
INFO:root:2019-05-12 02:20:30, Epoch : 1, Step : 790, Training Loss : 0.32485, Training Acc : 0.861, Run Time : 9.56
INFO:root:2019-05-12 02:20:30, Epoch : 1, Step : 791, Training Loss : 0.31317, Training Acc : 0.844, Run Time : 0.47
INFO:root:2019-05-12 02:20:31, Epoch : 1, Step : 792, Training Loss : 0.31810, Training Acc : 0.872, Run Time : 1.27
INFO:root:2019-05-12 02:20:40, Epoch : 1, Step : 793, Training Loss : 0.19954, Training Acc : 0.944, Run Time : 8.47
INFO:root:2019-05-12 02:20:41, Epoch : 1, Step : 794, Training Loss : 0.24096, Training Acc : 0.900, Run Time : 0.96
INFO:root:2019-05-12 02:20:48, Epoch : 1, Step : 795, Training Loss : 0.23174, Training Acc : 0.917, Run Time : 7.53
INFO:root:2019-05-12 02:20:49, Epoch : 1, Step : 796, Training Loss : 0.25330, Training Acc : 0.889, Run Time : 1.06
INFO:root:2019-05-12 02:20:50, Epoch : 1, Step : 797, Training Loss : 0.42358, Training Acc : 0.817, Run Time : 0.57
INFO:root:2019-05-12 02:20:57, Epoch : 1, Step : 798, Training Loss : 0.29884, Training Acc : 0.856, Run Time : 6.79
INFO:root:2019-05-12 02:20:59, Epoch : 1, Step : 799, Training Loss : 0.37369, Training Acc : 0.844, Run Time : 1.95
INFO:root:2019-05-12 02:21:05, Epoch : 1, Step : 800, Training Loss : 0.47498, Training Acc : 0.750, Run Time : 6.32
INFO:root:2019-05-12 02:21:06, Epoch : 1, Step : 801, Training Loss : 0.42260, Training Acc : 0.828, Run Time : 1.00
INFO:root:2019-05-12 02:21:07, Epoch : 1, Step : 802, Training Loss : 0.41763, Training Acc : 0.800, Run Time : 0.84
INFO:root:2019-05-12 02:21:13, Epoch : 1, Step : 803, Training Loss : 0.63055, Training Acc : 0.750, Run Time : 6.50
INFO:root:2019-05-12 02:21:14, Epoch : 1, Step : 804, Training Loss : 0.44455, Training Acc : 0.778, Run Time : 0.56
INFO:root:2019-05-12 02:21:15, Epoch : 1, Step : 805, Training Loss : 0.38117, Training Acc : 0.833, Run Time : 1.27
INFO:root:2019-05-12 02:21:25, Epoch : 1, Step : 806, Training Loss : 0.40488, Training Acc : 0.789, Run Time : 9.69
INFO:root:2019-05-12 02:21:33, Epoch : 1, Step : 807, Training Loss : 0.34334, Training Acc : 0.856, Run Time : 7.92
INFO:root:2019-05-12 02:21:36, Epoch : 1, Step : 808, Training Loss : 0.43166, Training Acc : 0.844, Run Time : 3.39
INFO:root:2019-05-12 02:21:37, Epoch : 1, Step : 809, Training Loss : 0.33091, Training Acc : 0.850, Run Time : 0.55
INFO:root:2019-05-12 02:21:42, Epoch : 1, Step : 810, Training Loss : 0.31271, Training Acc : 0.839, Run Time : 5.28
INFO:root:2019-05-12 02:21:44, Epoch : 1, Step : 811, Training Loss : 0.23781, Training Acc : 0.900, Run Time : 2.26
INFO:root:2019-05-12 02:21:45, Epoch : 1, Step : 812, Training Loss : 0.30972, Training Acc : 0.839, Run Time : 0.61
INFO:root:2019-05-12 02:21:48, Epoch : 1, Step : 813, Training Loss : 0.48416, Training Acc : 0.767, Run Time : 3.53
INFO:root:2019-05-12 02:21:49, Epoch : 1, Step : 814, Training Loss : 0.26542, Training Acc : 0.861, Run Time : 0.69
INFO:root:2019-05-12 02:21:56, Epoch : 1, Step : 815, Training Loss : 0.24597, Training Acc : 0.894, Run Time : 7.14
INFO:root:2019-05-12 02:21:57, Epoch : 1, Step : 816, Training Loss : 0.34765, Training Acc : 0.867, Run Time : 0.44
INFO:root:2019-05-12 02:21:58, Epoch : 1, Step : 817, Training Loss : 0.35156, Training Acc : 0.867, Run Time : 1.49
INFO:root:2019-05-12 02:22:03, Epoch : 1, Step : 818, Training Loss : 0.58107, Training Acc : 0.756, Run Time : 4.57
INFO:root:2019-05-12 02:22:03, Epoch : 1, Step : 819, Training Loss : 0.47228, Training Acc : 0.811, Run Time : 0.56
INFO:root:2019-05-12 02:22:04, Epoch : 1, Step : 820, Training Loss : 0.40632, Training Acc : 0.856, Run Time : 1.26
INFO:root:2019-05-12 02:22:09, Epoch : 1, Step : 821, Training Loss : 0.41427, Training Acc : 0.822, Run Time : 4.72
INFO:root:2019-05-12 02:22:10, Epoch : 1, Step : 822, Training Loss : 0.49416, Training Acc : 0.767, Run Time : 0.58
INFO:root:2019-05-12 02:22:10, Epoch : 1, Step : 823, Training Loss : 0.39322, Training Acc : 0.839, Run Time : 0.62
INFO:root:2019-05-12 02:22:11, Epoch : 1, Step : 824, Training Loss : 0.50097, Training Acc : 0.778, Run Time : 0.57
INFO:root:2019-05-12 02:22:15, Epoch : 1, Step : 825, Training Loss : 0.77349, Training Acc : 0.722, Run Time : 3.93
INFO:root:2019-05-12 02:22:18, Epoch : 1, Step : 826, Training Loss : 0.87724, Training Acc : 0.672, Run Time : 3.21
INFO:root:2019-05-12 02:22:19, Epoch : 1, Step : 827, Training Loss : 0.58533, Training Acc : 0.772, Run Time : 0.86
INFO:root:2019-05-12 02:22:20, Epoch : 1, Step : 828, Training Loss : 0.63491, Training Acc : 0.744, Run Time : 1.38
INFO:root:2019-05-12 02:22:31, Epoch : 1, Step : 829, Training Loss : 0.48163, Training Acc : 0.800, Run Time : 10.61
INFO:root:2019-05-12 02:22:32, Epoch : 1, Step : 830, Training Loss : 0.24924, Training Acc : 0.917, Run Time : 0.64
INFO:root:2019-05-12 02:22:32, Epoch : 1, Step : 831, Training Loss : 0.20594, Training Acc : 0.933, Run Time : 0.67
INFO:root:2019-05-12 02:22:39, Epoch : 1, Step : 832, Training Loss : 0.20337, Training Acc : 0.933, Run Time : 6.66
INFO:root:2019-05-12 02:22:40, Epoch : 1, Step : 833, Training Loss : 0.42553, Training Acc : 0.806, Run Time : 0.93
INFO:root:2019-05-12 02:22:50, Epoch : 1, Step : 834, Training Loss : 0.15829, Training Acc : 0.950, Run Time : 10.59
INFO:root:2019-05-12 02:22:51, Epoch : 1, Step : 835, Training Loss : 0.42393, Training Acc : 0.828, Run Time : 0.80
INFO:root:2019-05-12 02:22:53, Epoch : 1, Step : 836, Training Loss : 0.33987, Training Acc : 0.867, Run Time : 1.91
INFO:root:2019-05-12 02:23:01, Epoch : 1, Step : 837, Training Loss : 0.38033, Training Acc : 0.872, Run Time : 7.63
INFO:root:2019-05-12 02:23:01, Epoch : 1, Step : 838, Training Loss : 0.23307, Training Acc : 0.939, Run Time : 0.44
INFO:root:2019-05-12 02:23:03, Epoch : 1, Step : 839, Training Loss : 0.39905, Training Acc : 0.822, Run Time : 1.83
INFO:root:2019-05-12 02:23:10, Epoch : 1, Step : 840, Training Loss : 0.27042, Training Acc : 0.900, Run Time : 6.73
INFO:root:2019-05-12 02:23:10, Epoch : 1, Step : 841, Training Loss : 0.25462, Training Acc : 0.883, Run Time : 0.43
INFO:root:2019-05-12 02:23:12, Epoch : 1, Step : 842, Training Loss : 0.25271, Training Acc : 0.894, Run Time : 1.59
INFO:root:2019-05-12 02:23:20, Epoch : 1, Step : 843, Training Loss : 0.18775, Training Acc : 0.922, Run Time : 8.24
INFO:root:2019-05-12 02:23:21, Epoch : 1, Step : 844, Training Loss : 0.24262, Training Acc : 0.883, Run Time : 0.45
INFO:root:2019-05-12 02:23:21, Epoch : 1, Step : 845, Training Loss : 0.13150, Training Acc : 0.983, Run Time : 0.41
INFO:root:2019-05-12 02:23:22, Epoch : 1, Step : 846, Training Loss : 0.21140, Training Acc : 0.922, Run Time : 1.58
INFO:root:2019-05-12 02:23:31, Epoch : 1, Step : 847, Training Loss : 0.14447, Training Acc : 0.972, Run Time : 8.30
INFO:root:2019-05-12 02:23:32, Epoch : 1, Step : 848, Training Loss : 0.18613, Training Acc : 0.939, Run Time : 0.89
INFO:root:2019-05-12 02:23:32, Epoch : 1, Step : 849, Training Loss : 0.26679, Training Acc : 0.900, Run Time : 0.76
INFO:root:2019-05-12 02:23:33, Epoch : 1, Step : 850, Training Loss : 0.21545, Training Acc : 0.894, Run Time : 0.80
INFO:root:2019-05-12 02:23:35, Epoch : 1, Step : 851, Training Loss : 0.10897, Training Acc : 0.967, Run Time : 1.44
INFO:root:2019-05-12 02:23:46, Epoch : 1, Step : 852, Training Loss : 0.20466, Training Acc : 0.911, Run Time : 11.62
INFO:root:2019-05-12 02:23:47, Epoch : 1, Step : 853, Training Loss : 0.16645, Training Acc : 0.944, Run Time : 1.01
INFO:root:2019-05-12 02:23:56, Epoch : 1, Step : 854, Training Loss : 0.34224, Training Acc : 0.833, Run Time : 8.26
INFO:root:2019-05-12 02:23:56, Epoch : 1, Step : 855, Training Loss : 0.29003, Training Acc : 0.850, Run Time : 0.47
INFO:root:2019-05-12 02:23:57, Epoch : 1, Step : 856, Training Loss : 0.29448, Training Acc : 0.894, Run Time : 0.64
INFO:root:2019-05-12 02:24:05, Epoch : 1, Step : 857, Training Loss : 0.32248, Training Acc : 0.844, Run Time : 8.59
INFO:root:2019-05-12 02:24:06, Epoch : 1, Step : 858, Training Loss : 0.40582, Training Acc : 0.794, Run Time : 0.56
INFO:root:2019-05-12 02:24:07, Epoch : 1, Step : 859, Training Loss : 0.46984, Training Acc : 0.794, Run Time : 1.67
INFO:root:2019-05-12 02:24:17, Epoch : 1, Step : 860, Training Loss : 0.27388, Training Acc : 0.906, Run Time : 9.17
INFO:root:2019-05-12 02:24:22, Epoch : 1, Step : 861, Training Loss : 0.14952, Training Acc : 0.961, Run Time : 5.12
INFO:root:2019-05-12 02:24:22, Epoch : 1, Step : 862, Training Loss : 0.17659, Training Acc : 0.939, Run Time : 0.53
INFO:root:2019-05-12 02:24:23, Epoch : 1, Step : 863, Training Loss : 0.15169, Training Acc : 0.939, Run Time : 0.56
INFO:root:2019-05-12 02:24:23, Epoch : 1, Step : 864, Training Loss : 0.38323, Training Acc : 0.822, Run Time : 0.60
INFO:root:2019-05-12 02:24:30, Epoch : 1, Step : 865, Training Loss : 0.31128, Training Acc : 0.861, Run Time : 6.57
INFO:root:2019-05-12 02:24:31, Epoch : 1, Step : 866, Training Loss : 0.17439, Training Acc : 0.939, Run Time : 0.57
INFO:root:2019-05-12 02:24:31, Epoch : 1, Step : 867, Training Loss : 0.15702, Training Acc : 0.928, Run Time : 0.80
INFO:root:2019-05-12 02:24:39, Epoch : 1, Step : 868, Training Loss : 0.19426, Training Acc : 0.939, Run Time : 7.94
INFO:root:2019-05-12 02:24:40, Epoch : 1, Step : 869, Training Loss : 0.19940, Training Acc : 0.933, Run Time : 0.95
INFO:root:2019-05-12 02:24:41, Epoch : 1, Step : 870, Training Loss : 0.27261, Training Acc : 0.900, Run Time : 0.57
INFO:root:2019-05-12 02:24:42, Epoch : 1, Step : 871, Training Loss : 0.18325, Training Acc : 0.939, Run Time : 1.40
INFO:root:2019-05-12 02:24:49, Epoch : 1, Step : 872, Training Loss : 0.12319, Training Acc : 0.956, Run Time : 6.83
INFO:root:2019-05-12 02:24:50, Epoch : 1, Step : 873, Training Loss : 0.20125, Training Acc : 0.922, Run Time : 0.68
INFO:root:2019-05-12 02:24:51, Epoch : 1, Step : 874, Training Loss : 0.36451, Training Acc : 0.822, Run Time : 1.16
INFO:root:2019-05-12 02:24:57, Epoch : 1, Step : 875, Training Loss : 0.17362, Training Acc : 0.922, Run Time : 6.30
INFO:root:2019-05-12 02:24:58, Epoch : 1, Step : 876, Training Loss : 0.35614, Training Acc : 0.872, Run Time : 0.84
INFO:root:2019-05-12 02:24:59, Epoch : 1, Step : 877, Training Loss : 0.22970, Training Acc : 0.883, Run Time : 0.74
INFO:root:2019-05-12 02:25:00, Epoch : 1, Step : 878, Training Loss : 0.18064, Training Acc : 0.900, Run Time : 0.70
INFO:root:2019-05-12 02:25:07, Epoch : 1, Step : 879, Training Loss : 0.14662, Training Acc : 0.939, Run Time : 7.85
INFO:root:2019-05-12 02:25:09, Epoch : 1, Step : 880, Training Loss : 0.16000, Training Acc : 0.928, Run Time : 1.17
INFO:root:2019-05-12 02:25:16, Epoch : 1, Step : 881, Training Loss : 0.12048, Training Acc : 0.950, Run Time : 7.01
INFO:root:2019-05-12 02:25:17, Epoch : 1, Step : 882, Training Loss : 0.16288, Training Acc : 0.950, Run Time : 1.59
INFO:root:2019-05-12 02:25:18, Epoch : 1, Step : 883, Training Loss : 0.10073, Training Acc : 0.983, Run Time : 0.57
INFO:root:2019-05-12 02:25:26, Epoch : 1, Step : 884, Training Loss : 0.20072, Training Acc : 0.928, Run Time : 8.76
INFO:root:2019-05-12 02:25:27, Epoch : 1, Step : 885, Training Loss : 0.13770, Training Acc : 0.961, Run Time : 0.66
INFO:root:2019-05-12 02:25:28, Epoch : 1, Step : 886, Training Loss : 0.26258, Training Acc : 0.906, Run Time : 0.59
INFO:root:2019-05-12 02:25:35, Epoch : 1, Step : 887, Training Loss : 0.09284, Training Acc : 0.983, Run Time : 7.30
INFO:root:2019-05-12 02:25:39, Epoch : 1, Step : 888, Training Loss : 0.12330, Training Acc : 0.950, Run Time : 3.55
INFO:root:2019-05-12 02:25:47, Epoch : 1, Step : 889, Training Loss : 0.18497, Training Acc : 0.928, Run Time : 8.43
INFO:root:2019-05-12 02:25:48, Epoch : 1, Step : 890, Training Loss : 0.14188, Training Acc : 0.939, Run Time : 0.90
INFO:root:2019-05-12 02:25:48, Epoch : 1, Step : 891, Training Loss : 0.14413, Training Acc : 0.939, Run Time : 0.59
INFO:root:2019-05-12 02:25:58, Epoch : 1, Step : 892, Training Loss : 0.11614, Training Acc : 0.956, Run Time : 9.28
INFO:root:2019-05-12 02:25:58, Epoch : 1, Step : 893, Training Loss : 0.11249, Training Acc : 0.978, Run Time : 0.58
INFO:root:2019-05-12 02:25:59, Epoch : 1, Step : 894, Training Loss : 0.14690, Training Acc : 0.967, Run Time : 0.61
INFO:root:2019-05-12 02:26:00, Epoch : 1, Step : 895, Training Loss : 0.15967, Training Acc : 0.950, Run Time : 0.72
INFO:root:2019-05-12 02:26:00, Epoch : 1, Step : 896, Training Loss : 0.23950, Training Acc : 0.906, Run Time : 0.57
INFO:root:2019-05-12 02:26:06, Epoch : 1, Step : 897, Training Loss : 0.12844, Training Acc : 0.961, Run Time : 6.21
INFO:root:2019-05-12 02:26:07, Epoch : 1, Step : 898, Training Loss : 0.12684, Training Acc : 0.972, Run Time : 0.86
INFO:root:2019-05-12 02:26:15, Epoch : 1, Step : 899, Training Loss : 0.22404, Training Acc : 0.872, Run Time : 7.29
INFO:root:2019-05-12 02:26:16, Epoch : 1, Step : 900, Training Loss : 0.13922, Training Acc : 0.944, Run Time : 1.03
INFO:root:2019-05-12 02:26:17, Epoch : 1, Step : 901, Training Loss : 0.14932, Training Acc : 0.939, Run Time : 1.08
INFO:root:2019-05-12 02:26:17, Epoch : 1, Step : 902, Training Loss : 0.14240, Training Acc : 0.950, Run Time : 0.61
INFO:root:2019-05-12 02:26:25, Epoch : 1, Step : 903, Training Loss : 0.11569, Training Acc : 0.972, Run Time : 7.33
INFO:root:2019-05-12 02:26:25, Epoch : 1, Step : 904, Training Loss : 0.14025, Training Acc : 0.933, Run Time : 0.64
INFO:root:2019-05-12 02:26:27, Epoch : 1, Step : 905, Training Loss : 0.14687, Training Acc : 0.956, Run Time : 1.47
INFO:root:2019-05-12 02:26:34, Epoch : 1, Step : 906, Training Loss : 0.14017, Training Acc : 0.944, Run Time : 7.40
INFO:root:2019-05-12 02:26:35, Epoch : 1, Step : 907, Training Loss : 0.18018, Training Acc : 0.939, Run Time : 0.97
INFO:root:2019-05-12 02:26:44, Epoch : 1, Step : 908, Training Loss : 0.13141, Training Acc : 0.944, Run Time : 9.08
INFO:root:2019-05-12 02:26:45, Epoch : 1, Step : 909, Training Loss : 0.13815, Training Acc : 0.922, Run Time : 0.43
INFO:root:2019-05-12 02:26:46, Epoch : 1, Step : 910, Training Loss : 0.14298, Training Acc : 0.950, Run Time : 1.64
INFO:root:2019-05-12 02:26:54, Epoch : 1, Step : 911, Training Loss : 0.10531, Training Acc : 0.956, Run Time : 8.04
INFO:root:2019-05-12 02:26:55, Epoch : 1, Step : 912, Training Loss : 0.14764, Training Acc : 0.939, Run Time : 0.45
INFO:root:2019-05-12 02:26:55, Epoch : 1, Step : 913, Training Loss : 0.13388, Training Acc : 0.944, Run Time : 0.59
INFO:root:2019-05-12 02:27:01, Epoch : 1, Step : 914, Training Loss : 0.10682, Training Acc : 0.967, Run Time : 5.56
INFO:root:2019-05-12 02:27:02, Epoch : 1, Step : 915, Training Loss : 0.10855, Training Acc : 0.961, Run Time : 0.96
INFO:root:2019-05-12 02:27:02, Epoch : 1, Step : 916, Training Loss : 0.15305, Training Acc : 0.928, Run Time : 0.56
INFO:root:2019-05-12 02:27:04, Epoch : 1, Step : 917, Training Loss : 0.18056, Training Acc : 0.928, Run Time : 1.06
INFO:root:2019-05-12 02:27:09, Epoch : 1, Step : 918, Training Loss : 0.19080, Training Acc : 0.900, Run Time : 5.50
INFO:root:2019-05-12 02:27:10, Epoch : 1, Step : 919, Training Loss : 0.11502, Training Acc : 0.944, Run Time : 0.55
INFO:root:2019-05-12 02:27:11, Epoch : 1, Step : 920, Training Loss : 0.15798, Training Acc : 0.917, Run Time : 1.01
INFO:root:2019-05-12 02:27:17, Epoch : 1, Step : 921, Training Loss : 0.07660, Training Acc : 0.983, Run Time : 6.61
INFO:root:2019-05-12 02:27:18, Epoch : 1, Step : 922, Training Loss : 0.13548, Training Acc : 0.933, Run Time : 0.55
INFO:root:2019-05-12 02:27:19, Epoch : 1, Step : 923, Training Loss : 0.18317, Training Acc : 0.917, Run Time : 1.23
INFO:root:2019-05-12 02:27:23, Epoch : 1, Step : 924, Training Loss : 0.12428, Training Acc : 0.961, Run Time : 4.03
INFO:root:2019-05-12 02:27:24, Epoch : 1, Step : 925, Training Loss : 0.14724, Training Acc : 0.922, Run Time : 0.59
INFO:root:2019-05-12 02:27:28, Epoch : 1, Step : 926, Training Loss : 0.15342, Training Acc : 0.939, Run Time : 4.28
INFO:root:2019-05-12 02:27:29, Epoch : 1, Step : 927, Training Loss : 0.19449, Training Acc : 0.911, Run Time : 0.81
INFO:root:2019-05-12 02:27:31, Epoch : 1, Step : 928, Training Loss : 0.10583, Training Acc : 0.950, Run Time : 2.00
INFO:root:2019-05-12 02:27:34, Epoch : 1, Step : 929, Training Loss : 0.06903, Training Acc : 0.972, Run Time : 3.04
INFO:root:2019-05-12 02:27:34, Epoch : 1, Step : 930, Training Loss : 0.06998, Training Acc : 0.978, Run Time : 0.76
INFO:root:2019-05-12 02:27:41, Epoch : 1, Step : 931, Training Loss : 0.09168, Training Acc : 0.967, Run Time : 6.09
INFO:root:2019-05-12 02:27:41, Epoch : 1, Step : 932, Training Loss : 0.08578, Training Acc : 0.972, Run Time : 0.83
INFO:root:2019-05-12 02:27:51, Epoch : 1, Step : 933, Training Loss : 0.12192, Training Acc : 0.939, Run Time : 9.94
INFO:root:2019-05-12 02:27:52, Epoch : 1, Step : 934, Training Loss : 0.16711, Training Acc : 0.939, Run Time : 0.80
INFO:root:2019-05-12 02:27:59, Epoch : 1, Step : 935, Training Loss : 0.13065, Training Acc : 0.944, Run Time : 6.64
INFO:root:2019-05-12 02:28:00, Epoch : 1, Step : 936, Training Loss : 0.14626, Training Acc : 0.928, Run Time : 0.79
INFO:root:2019-05-12 02:28:00, Epoch : 1, Step : 937, Training Loss : 0.34752, Training Acc : 0.883, Run Time : 0.57
INFO:root:2019-05-12 02:28:01, Epoch : 1, Step : 938, Training Loss : 0.12111, Training Acc : 0.944, Run Time : 0.88
INFO:root:2019-05-12 02:28:08, Epoch : 1, Step : 939, Training Loss : 0.08338, Training Acc : 0.972, Run Time : 7.13
INFO:root:2019-05-12 02:28:09, Epoch : 1, Step : 940, Training Loss : 0.10369, Training Acc : 0.956, Run Time : 0.72
INFO:root:2019-05-12 02:28:10, Epoch : 1, Step : 941, Training Loss : 0.10946, Training Acc : 0.956, Run Time : 0.67
INFO:root:2019-05-12 02:28:11, Epoch : 1, Step : 942, Training Loss : 0.13411, Training Acc : 0.944, Run Time : 1.25
INFO:root:2019-05-12 02:28:15, Epoch : 1, Step : 943, Training Loss : 0.29310, Training Acc : 0.894, Run Time : 4.63
INFO:root:2019-05-12 02:28:16, Epoch : 1, Step : 944, Training Loss : 0.63382, Training Acc : 0.789, Run Time : 0.69
INFO:root:2019-05-12 02:28:17, Epoch : 1, Step : 945, Training Loss : 0.07548, Training Acc : 0.978, Run Time : 0.63
INFO:root:2019-05-12 02:28:28, Epoch : 1, Step : 946, Training Loss : 0.09586, Training Acc : 0.978, Run Time : 10.90
INFO:root:2019-05-12 02:28:28, Epoch : 1, Step : 947, Training Loss : 0.17491, Training Acc : 0.928, Run Time : 0.83
INFO:root:2019-05-12 02:28:29, Epoch : 1, Step : 948, Training Loss : 0.09660, Training Acc : 0.967, Run Time : 0.58
INFO:root:2019-05-12 02:28:30, Epoch : 1, Step : 949, Training Loss : 0.13113, Training Acc : 0.967, Run Time : 1.17
INFO:root:2019-05-12 02:28:36, Epoch : 1, Step : 950, Training Loss : 0.16104, Training Acc : 0.944, Run Time : 6.00
INFO:root:2019-05-12 02:28:37, Epoch : 1, Step : 951, Training Loss : 0.23749, Training Acc : 0.906, Run Time : 0.56
INFO:root:2019-05-12 02:28:38, Epoch : 1, Step : 952, Training Loss : 0.20348, Training Acc : 0.928, Run Time : 1.50
INFO:root:2019-05-12 02:28:44, Epoch : 1, Step : 953, Training Loss : 0.34699, Training Acc : 0.850, Run Time : 5.35
INFO:root:2019-05-12 02:28:44, Epoch : 1, Step : 954, Training Loss : 0.24145, Training Acc : 0.911, Run Time : 0.72
INFO:root:2019-05-12 02:28:45, Epoch : 1, Step : 955, Training Loss : 0.18566, Training Acc : 0.928, Run Time : 0.78
INFO:root:2019-05-12 02:28:46, Epoch : 1, Step : 956, Training Loss : 0.16417, Training Acc : 0.939, Run Time : 0.66
INFO:root:2019-05-12 02:28:55, Epoch : 1, Step : 957, Training Loss : 0.21517, Training Acc : 0.911, Run Time : 9.14
INFO:root:2019-05-12 02:28:55, Epoch : 1, Step : 958, Training Loss : 0.13578, Training Acc : 0.961, Run Time : 0.52
INFO:root:2019-05-12 02:28:57, Epoch : 1, Step : 959, Training Loss : 0.42721, Training Acc : 0.856, Run Time : 1.54
INFO:root:2019-05-12 02:29:05, Epoch : 1, Step : 960, Training Loss : 0.27011, Training Acc : 0.900, Run Time : 8.01
INFO:root:2019-05-12 02:29:05, Epoch : 1, Step : 961, Training Loss : 0.32893, Training Acc : 0.856, Run Time : 0.44
INFO:root:2019-05-12 02:29:07, Epoch : 1, Step : 962, Training Loss : 0.19988, Training Acc : 0.917, Run Time : 1.51
INFO:root:2019-05-12 02:29:12, Epoch : 1, Step : 963, Training Loss : 0.38906, Training Acc : 0.844, Run Time : 5.40
INFO:root:2019-05-12 02:29:14, Epoch : 1, Step : 964, Training Loss : 0.27744, Training Acc : 0.900, Run Time : 1.42
INFO:root:2019-05-12 02:29:18, Epoch : 1, Step : 965, Training Loss : 0.25611, Training Acc : 0.894, Run Time : 4.30
INFO:root:2019-05-12 02:29:19, Epoch : 1, Step : 966, Training Loss : 0.36708, Training Acc : 0.828, Run Time : 0.48
INFO:root:2019-05-12 02:29:20, Epoch : 1, Step : 967, Training Loss : 0.44149, Training Acc : 0.828, Run Time : 1.48
INFO:root:2019-05-12 02:29:27, Epoch : 1, Step : 968, Training Loss : 0.24014, Training Acc : 0.894, Run Time : 6.90
INFO:root:2019-05-12 02:29:27, Epoch : 1, Step : 969, Training Loss : 0.34091, Training Acc : 0.867, Run Time : 0.46
INFO:root:2019-05-12 02:29:29, Epoch : 1, Step : 970, Training Loss : 0.32075, Training Acc : 0.850, Run Time : 1.68
INFO:root:2019-05-12 02:29:35, Epoch : 1, Step : 971, Training Loss : 0.12080, Training Acc : 0.961, Run Time : 5.90
INFO:root:2019-05-12 02:29:35, Epoch : 1, Step : 972, Training Loss : 0.27720, Training Acc : 0.844, Run Time : 0.43
INFO:root:2019-05-12 02:29:36, Epoch : 1, Step : 973, Training Loss : 0.25544, Training Acc : 0.906, Run Time : 0.60
INFO:root:2019-05-12 02:29:43, Epoch : 1, Step : 974, Training Loss : 0.28790, Training Acc : 0.878, Run Time : 6.58
INFO:root:2019-05-12 02:29:43, Epoch : 1, Step : 975, Training Loss : 0.24201, Training Acc : 0.906, Run Time : 0.55
INFO:root:2019-05-12 02:29:44, Epoch : 1, Step : 976, Training Loss : 0.24037, Training Acc : 0.878, Run Time : 0.62
INFO:root:2019-05-12 02:29:51, Epoch : 1, Step : 977, Training Loss : 0.19741, Training Acc : 0.917, Run Time : 7.39
INFO:root:2019-05-12 02:29:52, Epoch : 1, Step : 978, Training Loss : 0.10291, Training Acc : 0.961, Run Time : 0.59
INFO:root:2019-05-12 02:29:53, Epoch : 1, Step : 979, Training Loss : 0.17250, Training Acc : 0.922, Run Time : 0.82
INFO:root:2019-05-12 02:29:59, Epoch : 1, Step : 980, Training Loss : 0.15863, Training Acc : 0.933, Run Time : 6.32
INFO:root:2019-05-12 02:29:59, Epoch : 1, Step : 981, Training Loss : 0.21649, Training Acc : 0.911, Run Time : 0.52
INFO:root:2019-05-12 02:30:00, Epoch : 1, Step : 982, Training Loss : 0.12497, Training Acc : 0.956, Run Time : 0.63
INFO:root:2019-05-12 02:30:01, Epoch : 1, Step : 983, Training Loss : 0.18591, Training Acc : 0.933, Run Time : 0.76
INFO:root:2019-05-12 02:30:09, Epoch : 1, Step : 984, Training Loss : 0.22173, Training Acc : 0.906, Run Time : 8.42
INFO:root:2019-05-12 02:30:10, Epoch : 1, Step : 985, Training Loss : 0.18926, Training Acc : 0.922, Run Time : 0.86
INFO:root:2019-05-12 02:30:11, Epoch : 1, Step : 986, Training Loss : 0.17119, Training Acc : 0.928, Run Time : 0.66
INFO:root:2019-05-12 02:30:11, Epoch : 1, Step : 987, Training Loss : 0.26271, Training Acc : 0.889, Run Time : 0.64
INFO:root:2019-05-12 02:30:19, Epoch : 1, Step : 988, Training Loss : 0.18826, Training Acc : 0.906, Run Time : 7.65
INFO:root:2019-05-12 02:30:20, Epoch : 1, Step : 989, Training Loss : 0.16839, Training Acc : 0.922, Run Time : 0.79
INFO:root:2019-05-12 02:30:22, Epoch : 1, Step : 990, Training Loss : 0.17410, Training Acc : 0.928, Run Time : 2.27
INFO:root:2019-05-12 02:30:30, Epoch : 1, Step : 991, Training Loss : 0.24517, Training Acc : 0.889, Run Time : 7.68
INFO:root:2019-05-12 02:30:30, Epoch : 1, Step : 992, Training Loss : 0.32201, Training Acc : 0.861, Run Time : 0.76
INFO:root:2019-05-12 02:30:40, Epoch : 1, Step : 993, Training Loss : 0.25585, Training Acc : 0.900, Run Time : 9.44
INFO:root:2019-05-12 02:30:41, Epoch : 1, Step : 994, Training Loss : 0.49057, Training Acc : 0.828, Run Time : 1.29
INFO:root:2019-05-12 02:30:46, Epoch : 1, Step : 995, Training Loss : 0.30146, Training Acc : 0.894, Run Time : 4.96
INFO:root:2019-05-12 02:30:47, Epoch : 1, Step : 996, Training Loss : 0.27503, Training Acc : 0.906, Run Time : 0.56
INFO:root:2019-05-12 02:30:48, Epoch : 1, Step : 997, Training Loss : 0.30640, Training Acc : 0.872, Run Time : 1.49
INFO:root:2019-05-12 02:30:58, Epoch : 1, Step : 998, Training Loss : 0.33439, Training Acc : 0.844, Run Time : 9.57
INFO:root:2019-05-12 02:30:59, Epoch : 1, Step : 999, Training Loss : 0.15493, Training Acc : 0.928, Run Time : 0.76
INFO:root:2019-05-12 02:31:07, Epoch : 1, Step : 1000, Training Loss : 0.44826, Training Acc : 0.856, Run Time : 7.96
INFO:root:2019-05-12 02:31:08, Epoch : 1, Step : 1001, Training Loss : 1.28522, Training Acc : 0.689, Run Time : 0.97
INFO:root:2019-05-12 02:31:14, Epoch : 1, Step : 1002, Training Loss : 1.32440, Training Acc : 0.689, Run Time : 6.56
INFO:root:2019-05-12 02:31:15, Epoch : 1, Step : 1003, Training Loss : 1.32727, Training Acc : 0.711, Run Time : 0.77
INFO:root:2019-05-12 02:31:16, Epoch : 1, Step : 1004, Training Loss : 1.35255, Training Acc : 0.694, Run Time : 1.31
INFO:root:2019-05-12 02:31:22, Epoch : 1, Step : 1005, Training Loss : 1.11546, Training Acc : 0.756, Run Time : 5.42
INFO:root:2019-05-12 02:31:22, Epoch : 1, Step : 1006, Training Loss : 1.12527, Training Acc : 0.706, Run Time : 0.56
INFO:root:2019-05-12 02:31:24, Epoch : 1, Step : 1007, Training Loss : 0.90285, Training Acc : 0.706, Run Time : 1.38
INFO:root:2019-05-12 02:31:28, Epoch : 1, Step : 1008, Training Loss : 0.68351, Training Acc : 0.739, Run Time : 4.86
INFO:root:2019-05-12 02:31:29, Epoch : 1, Step : 1009, Training Loss : 0.67651, Training Acc : 0.772, Run Time : 0.60
INFO:root:2019-05-12 02:31:30, Epoch : 1, Step : 1010, Training Loss : 0.70306, Training Acc : 0.739, Run Time : 0.57
INFO:root:2019-05-12 02:31:30, Epoch : 1, Step : 1011, Training Loss : 1.05965, Training Acc : 0.739, Run Time : 0.61
INFO:root:2019-05-12 02:31:31, Epoch : 1, Step : 1012, Training Loss : 0.81330, Training Acc : 0.667, Run Time : 0.68
INFO:root:2019-05-12 02:31:38, Epoch : 1, Step : 1013, Training Loss : 0.54058, Training Acc : 0.706, Run Time : 7.55
INFO:root:2019-05-12 02:31:39, Epoch : 1, Step : 1014, Training Loss : 0.58531, Training Acc : 0.733, Run Time : 0.75
INFO:root:2019-05-12 02:31:44, Epoch : 1, Step : 1015, Training Loss : 0.45939, Training Acc : 0.750, Run Time : 4.76
INFO:root:2019-05-12 02:31:46, Epoch : 1, Step : 1016, Training Loss : 0.45736, Training Acc : 0.761, Run Time : 1.93
INFO:root:2019-05-12 02:31:48, Epoch : 1, Step : 1017, Training Loss : 0.52085, Training Acc : 0.733, Run Time : 1.87
INFO:root:2019-05-12 02:31:48, Epoch : 1, Step : 1018, Training Loss : 0.36669, Training Acc : 0.867, Run Time : 0.60
INFO:root:2019-05-12 02:31:53, Epoch : 1, Step : 1019, Training Loss : 0.40449, Training Acc : 0.828, Run Time : 4.41
INFO:root:2019-05-12 02:31:54, Epoch : 1, Step : 1020, Training Loss : 0.47321, Training Acc : 0.806, Run Time : 1.00
INFO:root:2019-05-12 02:32:02, Epoch : 1, Step : 1021, Training Loss : 0.45262, Training Acc : 0.806, Run Time : 8.42
INFO:root:2019-05-12 02:32:03, Epoch : 1, Step : 1022, Training Loss : 0.45470, Training Acc : 0.806, Run Time : 0.57
INFO:root:2019-05-12 02:32:04, Epoch : 1, Step : 1023, Training Loss : 0.31210, Training Acc : 0.861, Run Time : 0.90
INFO:root:2019-05-12 02:32:11, Epoch : 1, Step : 1024, Training Loss : 0.36906, Training Acc : 0.833, Run Time : 7.50
INFO:root:2019-05-12 02:32:12, Epoch : 1, Step : 1025, Training Loss : 0.37830, Training Acc : 0.828, Run Time : 0.75
INFO:root:2019-05-12 02:32:17, Epoch : 1, Step : 1026, Training Loss : 0.33811, Training Acc : 0.878, Run Time : 5.56
INFO:root:2019-05-12 02:32:19, Epoch : 1, Step : 1027, Training Loss : 0.35974, Training Acc : 0.878, Run Time : 1.51
INFO:root:2019-05-12 02:32:19, Epoch : 1, Step : 1028, Training Loss : 0.28382, Training Acc : 0.933, Run Time : 0.57
INFO:root:2019-05-12 02:32:21, Epoch : 1, Step : 1029, Training Loss : 0.34024, Training Acc : 0.833, Run Time : 1.06
INFO:root:2019-05-12 02:32:22, Epoch : 1, Step : 1030, Training Loss : 0.37771, Training Acc : 0.789, Run Time : 1.05
INFO:root:2019-05-12 02:32:30, Epoch : 1, Step : 1031, Training Loss : 0.34399, Training Acc : 0.850, Run Time : 7.96
INFO:root:2019-05-12 02:32:30, Epoch : 1, Step : 1032, Training Loss : 0.36983, Training Acc : 0.833, Run Time : 0.77
INFO:root:2019-05-12 02:32:37, Epoch : 1, Step : 1033, Training Loss : 0.30156, Training Acc : 0.844, Run Time : 6.22
INFO:root:2019-05-12 02:32:37, Epoch : 1, Step : 1034, Training Loss : 0.45702, Training Acc : 0.783, Run Time : 0.57
INFO:root:2019-05-12 02:32:38, Epoch : 1, Step : 1035, Training Loss : 0.32138, Training Acc : 0.867, Run Time : 1.15
INFO:root:2019-05-12 02:32:44, Epoch : 1, Step : 1036, Training Loss : 0.44784, Training Acc : 0.733, Run Time : 6.07
INFO:root:2019-05-12 02:32:45, Epoch : 1, Step : 1037, Training Loss : 0.40893, Training Acc : 0.800, Run Time : 0.52
INFO:root:2019-05-12 02:32:53, Epoch : 1, Step : 1038, Training Loss : 0.39801, Training Acc : 0.833, Run Time : 7.83
INFO:root:2019-05-12 02:32:54, Epoch : 1, Step : 1039, Training Loss : 0.33414, Training Acc : 0.861, Run Time : 0.92
INFO:root:2019-05-12 02:33:00, Epoch : 1, Step : 1040, Training Loss : 0.32074, Training Acc : 0.850, Run Time : 6.52
INFO:root:2019-05-12 02:33:01, Epoch : 1, Step : 1041, Training Loss : 0.45638, Training Acc : 0.756, Run Time : 0.55
INFO:root:2019-05-12 02:33:01, Epoch : 1, Step : 1042, Training Loss : 0.28679, Training Acc : 0.872, Run Time : 0.60
INFO:root:2019-05-12 02:33:04, Epoch : 1, Step : 1043, Training Loss : 0.27235, Training Acc : 0.889, Run Time : 2.41
INFO:root:2019-05-12 02:33:05, Epoch : 1, Step : 1044, Training Loss : 0.35201, Training Acc : 0.828, Run Time : 1.81
INFO:root:2019-05-12 02:33:06, Epoch : 1, Step : 1045, Training Loss : 0.28418, Training Acc : 0.906, Run Time : 0.61
INFO:root:2019-05-12 02:33:07, Epoch : 1, Step : 1046, Training Loss : 0.43592, Training Acc : 0.778, Run Time : 1.02
INFO:root:2019-05-12 02:33:17, Epoch : 1, Step : 1047, Training Loss : 0.49893, Training Acc : 0.711, Run Time : 9.87
INFO:root:2019-05-12 02:33:18, Epoch : 1, Step : 1048, Training Loss : 1.28087, Training Acc : 0.422, Run Time : 0.76
INFO:root:2019-05-12 02:33:26, Epoch : 1, Step : 1049, Training Loss : 0.53224, Training Acc : 0.772, Run Time : 8.41
INFO:root:2019-05-12 02:33:27, Epoch : 1, Step : 1050, Training Loss : 0.44408, Training Acc : 0.800, Run Time : 0.63
INFO:root:2019-05-12 02:33:28, Epoch : 1, Step : 1051, Training Loss : 0.40660, Training Acc : 0.811, Run Time : 0.77
INFO:root:2019-05-12 02:33:34, Epoch : 1, Step : 1052, Training Loss : 0.39464, Training Acc : 0.806, Run Time : 6.91
INFO:root:2019-05-12 02:33:36, Epoch : 1, Step : 1053, Training Loss : 0.58838, Training Acc : 0.728, Run Time : 1.07
INFO:root:2019-05-12 02:33:37, Epoch : 1, Step : 1054, Training Loss : 0.51144, Training Acc : 0.728, Run Time : 1.50
INFO:root:2019-05-12 02:33:44, Epoch : 1, Step : 1055, Training Loss : 0.46572, Training Acc : 0.750, Run Time : 6.67
INFO:root:2019-05-12 02:33:44, Epoch : 1, Step : 1056, Training Loss : 0.72773, Training Acc : 0.678, Run Time : 0.41
INFO:root:2019-05-12 02:33:45, Epoch : 1, Step : 1057, Training Loss : 0.59503, Training Acc : 0.678, Run Time : 0.83
INFO:root:2019-05-12 02:33:51, Epoch : 1, Step : 1058, Training Loss : 0.48275, Training Acc : 0.800, Run Time : 6.23
INFO:root:2019-05-12 02:33:52, Epoch : 1, Step : 1059, Training Loss : 0.65026, Training Acc : 0.656, Run Time : 0.60
INFO:root:2019-05-12 02:33:52, Epoch : 1, Step : 1060, Training Loss : 0.52412, Training Acc : 0.789, Run Time : 0.69
INFO:root:2019-05-12 02:33:53, Epoch : 1, Step : 1061, Training Loss : 0.51924, Training Acc : 0.806, Run Time : 0.57
INFO:root:2019-05-12 02:34:01, Epoch : 1, Step : 1062, Training Loss : 0.48752, Training Acc : 0.756, Run Time : 7.89
INFO:root:2019-05-12 02:34:02, Epoch : 1, Step : 1063, Training Loss : 0.46490, Training Acc : 0.789, Run Time : 0.72
INFO:root:2019-05-12 02:34:03, Epoch : 1, Step : 1064, Training Loss : 0.38058, Training Acc : 0.850, Run Time : 1.64
INFO:root:2019-05-12 02:34:11, Epoch : 1, Step : 1065, Training Loss : 0.38423, Training Acc : 0.850, Run Time : 7.88
INFO:root:2019-05-12 02:34:12, Epoch : 1, Step : 1066, Training Loss : 0.46106, Training Acc : 0.767, Run Time : 0.56
INFO:root:2019-05-12 02:34:13, Epoch : 1, Step : 1067, Training Loss : 0.35982, Training Acc : 0.889, Run Time : 1.37
INFO:root:2019-05-12 02:34:19, Epoch : 1, Step : 1068, Training Loss : 0.39512, Training Acc : 0.861, Run Time : 6.10
INFO:root:2019-05-12 02:34:20, Epoch : 1, Step : 1069, Training Loss : 0.37086, Training Acc : 0.872, Run Time : 0.49
INFO:root:2019-05-12 02:34:21, Epoch : 1, Step : 1070, Training Loss : 0.38991, Training Acc : 0.861, Run Time : 0.85
INFO:root:2019-05-12 02:34:28, Epoch : 1, Step : 1071, Training Loss : 0.31450, Training Acc : 0.911, Run Time : 7.26
INFO:root:2019-05-12 02:34:28, Epoch : 1, Step : 1072, Training Loss : 0.32435, Training Acc : 0.872, Run Time : 0.57
INFO:root:2019-05-12 02:34:30, Epoch : 1, Step : 1073, Training Loss : 0.29626, Training Acc : 0.894, Run Time : 1.93
INFO:root:2019-05-12 02:34:38, Epoch : 1, Step : 1074, Training Loss : 0.30078, Training Acc : 0.900, Run Time : 7.80
INFO:root:2019-05-12 02:34:39, Epoch : 1, Step : 1075, Training Loss : 0.29487, Training Acc : 0.883, Run Time : 0.48
INFO:root:2019-05-12 02:34:40, Epoch : 1, Step : 1076, Training Loss : 0.34963, Training Acc : 0.867, Run Time : 1.68
INFO:root:2019-05-12 02:34:51, Epoch : 1, Step : 1077, Training Loss : 0.29437, Training Acc : 0.883, Run Time : 10.73
INFO:root:2019-05-12 02:34:52, Epoch : 1, Step : 1078, Training Loss : 0.35031, Training Acc : 0.806, Run Time : 0.78
INFO:root:2019-05-12 02:34:57, Epoch : 1, Step : 1079, Training Loss : 0.34867, Training Acc : 0.844, Run Time : 5.38
INFO:root:2019-05-12 02:35:00, Epoch : 1, Step : 1080, Training Loss : 0.33976, Training Acc : 0.889, Run Time : 3.35
INFO:root:2019-05-12 02:35:01, Epoch : 1, Step : 1081, Training Loss : 0.33184, Training Acc : 0.828, Run Time : 0.40
INFO:root:2019-05-12 02:35:03, Epoch : 1, Step : 1082, Training Loss : 0.31200, Training Acc : 0.844, Run Time : 2.54
INFO:root:2019-05-12 02:35:09, Epoch : 1, Step : 1083, Training Loss : 0.28310, Training Acc : 0.878, Run Time : 5.69
INFO:root:2019-05-12 02:35:10, Epoch : 1, Step : 1084, Training Loss : 0.26980, Training Acc : 0.861, Run Time : 0.52
INFO:root:2019-05-12 02:35:10, Epoch : 1, Step : 1085, Training Loss : 0.30126, Training Acc : 0.861, Run Time : 0.63
INFO:root:2019-05-12 02:35:11, Epoch : 1, Step : 1086, Training Loss : 0.37839, Training Acc : 0.794, Run Time : 1.13
INFO:root:2019-05-12 02:35:18, Epoch : 1, Step : 1087, Training Loss : 0.30946, Training Acc : 0.867, Run Time : 6.21
INFO:root:2019-05-12 02:35:18, Epoch : 1, Step : 1088, Training Loss : 0.30959, Training Acc : 0.844, Run Time : 0.57
INFO:root:2019-05-12 02:35:20, Epoch : 1, Step : 1089, Training Loss : 0.33852, Training Acc : 0.850, Run Time : 2.03
INFO:root:2019-05-12 02:35:27, Epoch : 1, Step : 1090, Training Loss : 0.28007, Training Acc : 0.878, Run Time : 6.65
INFO:root:2019-05-12 02:35:27, Epoch : 1, Step : 1091, Training Loss : 0.29348, Training Acc : 0.856, Run Time : 0.58
INFO:root:2019-05-12 02:35:29, Epoch : 1, Step : 1092, Training Loss : 0.36360, Training Acc : 0.844, Run Time : 1.60
INFO:root:2019-05-12 02:35:36, Epoch : 1, Step : 1093, Training Loss : 0.32626, Training Acc : 0.867, Run Time : 6.54
INFO:root:2019-05-12 02:35:36, Epoch : 1, Step : 1094, Training Loss : 0.28111, Training Acc : 0.900, Run Time : 0.50
INFO:root:2019-05-12 02:35:38, Epoch : 1, Step : 1095, Training Loss : 0.27287, Training Acc : 0.878, Run Time : 2.01
INFO:root:2019-05-12 02:35:45, Epoch : 1, Step : 1096, Training Loss : 0.23447, Training Acc : 0.894, Run Time : 7.13
INFO:root:2019-05-12 02:35:46, Epoch : 1, Step : 1097, Training Loss : 0.28194, Training Acc : 0.867, Run Time : 0.41
INFO:root:2019-05-12 02:35:47, Epoch : 1, Step : 1098, Training Loss : 0.22774, Training Acc : 0.922, Run Time : 1.38
INFO:root:2019-05-12 02:35:52, Epoch : 1, Step : 1099, Training Loss : 0.55335, Training Acc : 0.761, Run Time : 5.38
INFO:root:2019-05-12 02:35:53, Epoch : 1, Step : 1100, Training Loss : 0.55186, Training Acc : 0.761, Run Time : 0.44
INFO:root:2019-05-12 02:35:54, Epoch : 1, Step : 1101, Training Loss : 0.42291, Training Acc : 0.794, Run Time : 1.02
INFO:root:2019-05-12 02:36:00, Epoch : 1, Step : 1102, Training Loss : 0.34458, Training Acc : 0.844, Run Time : 6.65
INFO:root:2019-05-12 02:36:01, Epoch : 1, Step : 1103, Training Loss : 0.34400, Training Acc : 0.828, Run Time : 0.52
INFO:root:2019-05-12 02:36:02, Epoch : 1, Step : 1104, Training Loss : 0.24079, Training Acc : 0.872, Run Time : 0.67
INFO:root:2019-05-12 02:36:10, Epoch : 1, Step : 1105, Training Loss : 0.29901, Training Acc : 0.867, Run Time : 8.17
INFO:root:2019-05-12 02:36:10, Epoch : 1, Step : 1106, Training Loss : 0.34359, Training Acc : 0.839, Run Time : 0.47
INFO:root:2019-05-12 02:36:11, Epoch : 1, Step : 1107, Training Loss : 0.33856, Training Acc : 0.828, Run Time : 0.91
INFO:root:2019-05-12 02:36:15, Epoch : 1, Step : 1108, Training Loss : 0.41105, Training Acc : 0.800, Run Time : 4.04
INFO:root:2019-05-12 02:36:17, Epoch : 1, Step : 1109, Training Loss : 0.42167, Training Acc : 0.767, Run Time : 1.66
INFO:root:2019-05-12 02:36:24, Epoch : 1, Step : 1110, Training Loss : 0.50431, Training Acc : 0.744, Run Time : 6.66
INFO:root:2019-05-12 02:36:24, Epoch : 1, Step : 1111, Training Loss : 0.42023, Training Acc : 0.817, Run Time : 0.50
INFO:root:2019-05-12 02:36:25, Epoch : 1, Step : 1112, Training Loss : 0.37719, Training Acc : 0.817, Run Time : 0.93
INFO:root:2019-05-12 02:36:32, Epoch : 1, Step : 1113, Training Loss : 0.43108, Training Acc : 0.811, Run Time : 7.15
INFO:root:2019-05-12 02:36:33, Epoch : 1, Step : 1114, Training Loss : 0.46446, Training Acc : 0.739, Run Time : 0.59
INFO:root:2019-05-12 02:36:33, Epoch : 1, Step : 1115, Training Loss : 0.36554, Training Acc : 0.850, Run Time : 0.64
INFO:root:2019-05-12 02:36:41, Epoch : 1, Step : 1116, Training Loss : 0.48026, Training Acc : 0.794, Run Time : 7.37
INFO:root:2019-05-12 02:36:42, Epoch : 1, Step : 1117, Training Loss : 0.41218, Training Acc : 0.767, Run Time : 1.12
INFO:root:2019-05-12 02:36:46, Epoch : 1, Step : 1118, Training Loss : 0.40567, Training Acc : 0.817, Run Time : 3.94
INFO:root:2019-05-12 02:36:46, Epoch : 1, Step : 1119, Training Loss : 0.45565, Training Acc : 0.767, Run Time : 0.49
INFO:root:2019-05-12 02:36:47, Epoch : 1, Step : 1120, Training Loss : 0.29147, Training Acc : 0.861, Run Time : 1.01
INFO:root:2019-05-12 02:36:50, Epoch : 1, Step : 1121, Training Loss : 0.33602, Training Acc : 0.844, Run Time : 2.83
INFO:root:2019-05-12 02:36:51, Epoch : 1, Step : 1122, Training Loss : 0.39373, Training Acc : 0.789, Run Time : 0.98
INFO:root:2019-05-12 02:36:52, Epoch : 1, Step : 1123, Training Loss : 0.36374, Training Acc : 0.822, Run Time : 0.57
INFO:root:2019-05-12 02:37:00, Epoch : 1, Step : 1124, Training Loss : 0.22606, Training Acc : 0.933, Run Time : 8.12
INFO:root:2019-05-12 02:37:00, Epoch : 1, Step : 1125, Training Loss : 0.32256, Training Acc : 0.839, Run Time : 0.58
INFO:root:2019-05-12 02:37:02, Epoch : 1, Step : 1126, Training Loss : 0.32387, Training Acc : 0.844, Run Time : 1.38
INFO:root:2019-05-12 02:37:14, Epoch : 1, Step : 1127, Training Loss : 0.29209, Training Acc : 0.883, Run Time : 11.81
INFO:root:2019-05-12 02:37:15, Epoch : 1, Step : 1128, Training Loss : 0.31490, Training Acc : 0.850, Run Time : 1.24
INFO:root:2019-05-12 02:37:16, Epoch : 1, Step : 1129, Training Loss : 0.26101, Training Acc : 0.928, Run Time : 0.92
INFO:root:2019-05-12 02:37:23, Epoch : 1, Step : 1130, Training Loss : 0.20671, Training Acc : 0.950, Run Time : 7.04
INFO:root:2019-05-12 02:37:23, Epoch : 1, Step : 1131, Training Loss : 0.28328, Training Acc : 0.878, Run Time : 0.50
INFO:root:2019-05-12 02:37:24, Epoch : 1, Step : 1132, Training Loss : 0.28497, Training Acc : 0.878, Run Time : 0.65
INFO:root:2019-05-12 02:37:38, Epoch : 1, Step : 1133, Training Loss : 0.21011, Training Acc : 0.933, Run Time : 14.52
INFO:root:2019-05-12 02:37:39, Epoch : 1, Step : 1134, Training Loss : 0.21510, Training Acc : 0.939, Run Time : 0.64
INFO:root:2019-05-12 02:37:40, Epoch : 1, Step : 1135, Training Loss : 0.20606, Training Acc : 0.944, Run Time : 0.89
INFO:root:2019-05-12 02:37:50, Epoch : 1, Step : 1136, Training Loss : 0.19167, Training Acc : 0.939, Run Time : 9.69
INFO:root:2019-05-12 02:37:50, Epoch : 1, Step : 1137, Training Loss : 0.28576, Training Acc : 0.861, Run Time : 0.66
INFO:root:2019-05-12 02:37:53, Epoch : 1, Step : 1138, Training Loss : 0.21925, Training Acc : 0.917, Run Time : 2.58
INFO:root:2019-05-12 02:37:56, Epoch : 1, Step : 1139, Training Loss : 0.18257, Training Acc : 0.933, Run Time : 2.76
INFO:root:2019-05-12 02:37:56, Epoch : 1, Step : 1140, Training Loss : 0.26530, Training Acc : 0.889, Run Time : 0.56
INFO:root:2019-05-12 02:37:59, Epoch : 1, Step : 1141, Training Loss : 0.26373, Training Acc : 0.883, Run Time : 2.57
INFO:root:2019-05-12 02:38:08, Epoch : 1, Step : 1142, Training Loss : 0.25549, Training Acc : 0.867, Run Time : 9.35
INFO:root:2019-05-12 02:38:09, Epoch : 1, Step : 1143, Training Loss : 0.23040, Training Acc : 0.883, Run Time : 0.48
INFO:root:2019-05-12 02:38:09, Epoch : 1, Step : 1144, Training Loss : 0.21015, Training Acc : 0.917, Run Time : 0.75
INFO:root:2019-05-12 02:38:10, Epoch : 1, Step : 1145, Training Loss : 0.21178, Training Acc : 0.917, Run Time : 0.60
INFO:root:2019-05-12 02:38:12, Epoch : 1, Step : 1146, Training Loss : 0.22727, Training Acc : 0.917, Run Time : 1.82
INFO:root:2019-05-12 02:38:22, Epoch : 1, Step : 1147, Training Loss : 0.14754, Training Acc : 0.961, Run Time : 10.02
INFO:root:2019-05-12 02:38:22, Epoch : 1, Step : 1148, Training Loss : 0.18164, Training Acc : 0.939, Run Time : 0.45
INFO:root:2019-05-12 02:38:25, Epoch : 1, Step : 1149, Training Loss : 0.18361, Training Acc : 0.961, Run Time : 2.18
INFO:root:2019-05-12 02:38:33, Epoch : 1, Step : 1150, Training Loss : 0.16877, Training Acc : 0.939, Run Time : 8.37
INFO:root:2019-05-12 02:38:33, Epoch : 1, Step : 1151, Training Loss : 0.16790, Training Acc : 0.961, Run Time : 0.51
INFO:root:2019-05-12 02:38:34, Epoch : 1, Step : 1152, Training Loss : 0.19206, Training Acc : 0.906, Run Time : 0.62
INFO:root:2019-05-12 02:38:35, Epoch : 1, Step : 1153, Training Loss : 0.51731, Training Acc : 0.794, Run Time : 0.96
INFO:root:2019-05-12 02:38:42, Epoch : 1, Step : 1154, Training Loss : 0.83990, Training Acc : 0.728, Run Time : 6.59
INFO:root:2019-05-12 02:38:43, Epoch : 1, Step : 1155, Training Loss : 0.26753, Training Acc : 0.878, Run Time : 1.47
INFO:root:2019-05-12 02:38:51, Epoch : 1, Step : 1156, Training Loss : 0.28730, Training Acc : 0.861, Run Time : 7.63
INFO:root:2019-05-12 02:38:51, Epoch : 1, Step : 1157, Training Loss : 0.28951, Training Acc : 0.883, Run Time : 0.71
INFO:root:2019-05-12 02:38:52, Epoch : 1, Step : 1158, Training Loss : 0.28998, Training Acc : 0.894, Run Time : 1.09
INFO:root:2019-05-12 02:38:58, Epoch : 1, Step : 1159, Training Loss : 0.46035, Training Acc : 0.800, Run Time : 5.90
INFO:root:2019-05-12 02:38:59, Epoch : 1, Step : 1160, Training Loss : 0.55098, Training Acc : 0.728, Run Time : 0.50
INFO:root:2019-05-12 02:39:00, Epoch : 1, Step : 1161, Training Loss : 0.53139, Training Acc : 0.750, Run Time : 1.20
INFO:root:2019-05-12 02:39:07, Epoch : 1, Step : 1162, Training Loss : 0.48057, Training Acc : 0.761, Run Time : 7.33
INFO:root:2019-05-12 02:39:08, Epoch : 1, Step : 1163, Training Loss : 0.54134, Training Acc : 0.733, Run Time : 0.51
INFO:root:2019-05-12 02:39:09, Epoch : 1, Step : 1164, Training Loss : 0.43672, Training Acc : 0.789, Run Time : 0.85
INFO:root:2019-05-12 02:39:13, Epoch : 1, Step : 1165, Training Loss : 0.40737, Training Acc : 0.806, Run Time : 4.34
INFO:root:2019-05-12 02:39:14, Epoch : 1, Step : 1166, Training Loss : 0.39730, Training Acc : 0.839, Run Time : 0.93
INFO:root:2019-05-12 02:39:23, Epoch : 1, Step : 1167, Training Loss : 0.32792, Training Acc : 0.872, Run Time : 8.59
INFO:root:2019-05-12 02:39:24, Epoch : 1, Step : 1168, Training Loss : 0.26433, Training Acc : 0.900, Run Time : 1.53
INFO:root:2019-05-12 02:39:25, Epoch : 1, Step : 1169, Training Loss : 0.29860, Training Acc : 0.872, Run Time : 0.61
INFO:root:2019-05-12 02:39:25, Epoch : 1, Step : 1170, Training Loss : 0.29084, Training Acc : 0.867, Run Time : 0.63
INFO:root:2019-05-12 02:39:32, Epoch : 1, Step : 1171, Training Loss : 0.42378, Training Acc : 0.789, Run Time : 6.55
INFO:root:2019-05-12 02:39:33, Epoch : 1, Step : 1172, Training Loss : 0.42011, Training Acc : 0.794, Run Time : 0.93
INFO:root:2019-05-12 02:39:34, Epoch : 1, Step : 1173, Training Loss : 0.39267, Training Acc : 0.806, Run Time : 1.04
INFO:root:2019-05-12 02:39:49, Epoch : 1, Step : 1174, Training Loss : 0.60121, Training Acc : 0.739, Run Time : 14.72
INFO:root:2019-05-12 02:39:53, Epoch : 1, Step : 1175, Training Loss : 0.44862, Training Acc : 0.800, Run Time : 4.03
INFO:root:2019-05-12 02:40:01, Epoch : 1, Step : 1176, Training Loss : 0.43475, Training Acc : 0.817, Run Time : 8.19
INFO:root:2019-05-12 02:40:01, Epoch : 1, Step : 1177, Training Loss : 0.33101, Training Acc : 0.861, Run Time : 0.63
INFO:root:2019-05-12 02:40:03, Epoch : 1, Step : 1178, Training Loss : 0.28103, Training Acc : 0.883, Run Time : 1.85
INFO:root:2019-05-12 02:40:11, Epoch : 1, Step : 1179, Training Loss : 0.34614, Training Acc : 0.828, Run Time : 7.99
INFO:root:2019-05-12 02:40:12, Epoch : 1, Step : 1180, Training Loss : 0.35785, Training Acc : 0.844, Run Time : 0.85
INFO:root:2019-05-12 02:40:20, Epoch : 1, Step : 1181, Training Loss : 0.29493, Training Acc : 0.894, Run Time : 7.57
INFO:root:2019-05-12 02:40:21, Epoch : 1, Step : 1182, Training Loss : 0.35169, Training Acc : 0.817, Run Time : 1.06
INFO:root:2019-05-12 02:40:22, Epoch : 1, Step : 1183, Training Loss : 0.44481, Training Acc : 0.783, Run Time : 1.38
INFO:root:2019-05-12 02:40:30, Epoch : 1, Step : 1184, Training Loss : 0.38294, Training Acc : 0.844, Run Time : 8.07
INFO:root:2019-05-12 02:40:31, Epoch : 1, Step : 1185, Training Loss : 0.35758, Training Acc : 0.856, Run Time : 0.45
INFO:root:2019-05-12 02:40:31, Epoch : 1, Step : 1186, Training Loss : 0.36137, Training Acc : 0.856, Run Time : 0.59
INFO:root:2019-05-12 02:40:39, Epoch : 1, Step : 1187, Training Loss : 0.35855, Training Acc : 0.839, Run Time : 8.08
INFO:root:2019-05-12 02:40:40, Epoch : 1, Step : 1188, Training Loss : 0.32056, Training Acc : 0.822, Run Time : 0.56
INFO:root:2019-05-12 02:40:41, Epoch : 1, Step : 1189, Training Loss : 0.31112, Training Acc : 0.872, Run Time : 0.89
INFO:root:2019-05-12 02:40:47, Epoch : 1, Step : 1190, Training Loss : 0.33846, Training Acc : 0.850, Run Time : 6.30
INFO:root:2019-05-12 02:40:48, Epoch : 1, Step : 1191, Training Loss : 0.29904, Training Acc : 0.861, Run Time : 0.75
INFO:root:2019-05-12 02:40:50, Epoch : 1, Step : 1192, Training Loss : 0.30105, Training Acc : 0.872, Run Time : 2.20
INFO:root:2019-05-12 02:40:59, Epoch : 1, Step : 1193, Training Loss : 0.29257, Training Acc : 0.872, Run Time : 9.32
INFO:root:2019-05-12 02:41:00, Epoch : 1, Step : 1194, Training Loss : 0.23257, Training Acc : 0.900, Run Time : 0.81
INFO:root:2019-05-12 02:41:10, Epoch : 1, Step : 1195, Training Loss : 0.35931, Training Acc : 0.867, Run Time : 9.99
INFO:root:2019-05-12 02:41:11, Epoch : 1, Step : 1196, Training Loss : 0.35413, Training Acc : 0.856, Run Time : 0.56
INFO:root:2019-05-12 02:41:13, Epoch : 1, Step : 1197, Training Loss : 0.29184, Training Acc : 0.861, Run Time : 1.75
INFO:root:2019-05-12 02:41:22, Epoch : 1, Step : 1198, Training Loss : 0.39143, Training Acc : 0.844, Run Time : 9.39
INFO:root:2019-05-12 02:41:23, Epoch : 1, Step : 1199, Training Loss : 0.33097, Training Acc : 0.839, Run Time : 0.76
INFO:root:2019-05-12 02:41:25, Epoch : 1, Step : 1200, Training Loss : 0.27005, Training Acc : 0.883, Run Time : 2.60
INFO:root:2019-05-12 02:41:26, Epoch : 1, Step : 1201, Training Loss : 0.45680, Training Acc : 0.844, Run Time : 1.18
INFO:root:2019-05-12 02:41:27, Epoch : 1, Step : 1202, Training Loss : 0.36140, Training Acc : 0.878, Run Time : 0.63
INFO:root:2019-05-12 02:41:28, Epoch : 1, Step : 1203, Training Loss : 0.33151, Training Acc : 0.878, Run Time : 0.65
INFO:root:2019-05-12 02:41:32, Epoch : 1, Step : 1204, Training Loss : 0.36515, Training Acc : 0.856, Run Time : 4.01
INFO:root:2019-05-12 02:41:32, Epoch : 1, Step : 1205, Training Loss : 0.38131, Training Acc : 0.772, Run Time : 0.64
INFO:root:2019-05-12 02:41:40, Epoch : 1, Step : 1206, Training Loss : 0.37770, Training Acc : 0.750, Run Time : 7.44
INFO:root:2019-05-12 02:41:40, Epoch : 1, Step : 1207, Training Loss : 0.42767, Training Acc : 0.767, Run Time : 0.54
INFO:root:2019-05-12 02:41:42, Epoch : 1, Step : 1208, Training Loss : 0.62869, Training Acc : 0.717, Run Time : 1.50
INFO:root:2019-05-12 02:41:49, Epoch : 1, Step : 1209, Training Loss : 0.40274, Training Acc : 0.800, Run Time : 6.70
INFO:root:2019-05-12 02:41:49, Epoch : 1, Step : 1210, Training Loss : 0.31018, Training Acc : 0.861, Run Time : 0.68
INFO:root:2019-05-12 02:41:50, Epoch : 1, Step : 1211, Training Loss : 0.33453, Training Acc : 0.822, Run Time : 0.64
INFO:root:2019-05-12 02:41:57, Epoch : 1, Step : 1212, Training Loss : 0.30860, Training Acc : 0.861, Run Time : 7.57
INFO:root:2019-05-12 02:41:58, Epoch : 1, Step : 1213, Training Loss : 0.20624, Training Acc : 0.983, Run Time : 0.64
INFO:root:2019-05-12 02:41:59, Epoch : 1, Step : 1214, Training Loss : 0.22050, Training Acc : 0.928, Run Time : 1.30
INFO:root:2019-05-12 02:42:08, Epoch : 1, Step : 1215, Training Loss : 0.28162, Training Acc : 0.894, Run Time : 8.76
INFO:root:2019-05-12 02:42:09, Epoch : 1, Step : 1216, Training Loss : 0.26906, Training Acc : 0.889, Run Time : 0.64
INFO:root:2019-05-12 02:42:09, Epoch : 1, Step : 1217, Training Loss : 0.17804, Training Acc : 0.967, Run Time : 0.62
INFO:root:2019-05-12 02:42:10, Epoch : 1, Step : 1218, Training Loss : 0.16265, Training Acc : 0.972, Run Time : 0.61
INFO:root:2019-05-12 02:42:20, Epoch : 1, Step : 1219, Training Loss : 0.23429, Training Acc : 0.928, Run Time : 10.09
INFO:root:2019-05-12 02:42:21, Epoch : 1, Step : 1220, Training Loss : 0.17064, Training Acc : 0.944, Run Time : 0.50
INFO:root:2019-05-12 02:42:31, Epoch : 1, Step : 1221, Training Loss : 0.22390, Training Acc : 0.950, Run Time : 10.88
INFO:root:2019-05-12 02:42:32, Epoch : 1, Step : 1222, Training Loss : 0.16595, Training Acc : 0.944, Run Time : 0.66
INFO:root:2019-05-12 02:42:33, Epoch : 1, Step : 1223, Training Loss : 0.14040, Training Acc : 0.961, Run Time : 0.61
INFO:root:2019-05-12 02:42:35, Epoch : 1, Step : 1224, Training Loss : 0.22175, Training Acc : 0.917, Run Time : 2.61
INFO:root:2019-05-12 02:42:36, Epoch : 1, Step : 1225, Training Loss : 0.20552, Training Acc : 0.939, Run Time : 0.87
INFO:root:2019-05-12 02:42:38, Epoch : 1, Step : 1226, Training Loss : 0.13586, Training Acc : 0.972, Run Time : 1.50
INFO:root:2019-05-12 02:42:47, Epoch : 1, Step : 1227, Training Loss : 0.21567, Training Acc : 0.906, Run Time : 9.28
INFO:root:2019-05-12 02:42:47, Epoch : 1, Step : 1228, Training Loss : 0.18243, Training Acc : 0.939, Run Time : 0.48
INFO:root:2019-05-12 02:42:49, Epoch : 1, Step : 1229, Training Loss : 0.18191, Training Acc : 0.922, Run Time : 1.38
INFO:root:2019-05-12 02:42:57, Epoch : 1, Step : 1230, Training Loss : 0.15567, Training Acc : 0.939, Run Time : 7.72
INFO:root:2019-05-12 02:42:57, Epoch : 1, Step : 1231, Training Loss : 0.17504, Training Acc : 0.933, Run Time : 0.51
INFO:root:2019-05-12 02:42:58, Epoch : 1, Step : 1232, Training Loss : 0.22404, Training Acc : 0.933, Run Time : 0.94
INFO:root:2019-05-12 02:43:06, Epoch : 1, Step : 1233, Training Loss : 0.22795, Training Acc : 0.906, Run Time : 7.53
INFO:root:2019-05-12 02:43:06, Epoch : 1, Step : 1234, Training Loss : 0.25167, Training Acc : 0.867, Run Time : 0.43
INFO:root:2019-05-12 02:43:07, Epoch : 1, Step : 1235, Training Loss : 0.20957, Training Acc : 0.922, Run Time : 0.80
INFO:root:2019-05-12 02:43:15, Epoch : 1, Step : 1236, Training Loss : 0.17451, Training Acc : 0.922, Run Time : 8.18
INFO:root:2019-05-12 02:43:15, Epoch : 1, Step : 1237, Training Loss : 0.20444, Training Acc : 0.922, Run Time : 0.41
INFO:root:2019-05-12 02:43:16, Epoch : 1, Step : 1238, Training Loss : 0.22153, Training Acc : 0.917, Run Time : 0.70
INFO:root:2019-05-12 02:43:25, Epoch : 1, Step : 1239, Training Loss : 0.34108, Training Acc : 0.811, Run Time : 8.73
INFO:root:2019-05-12 02:43:25, Epoch : 1, Step : 1240, Training Loss : 0.22204, Training Acc : 0.917, Run Time : 0.43
INFO:root:2019-05-12 02:43:26, Epoch : 1, Step : 1241, Training Loss : 0.23090, Training Acc : 0.917, Run Time : 0.41
INFO:root:2019-05-12 02:43:33, Epoch : 1, Step : 1242, Training Loss : 0.20949, Training Acc : 0.917, Run Time : 7.46
INFO:root:2019-05-12 02:43:34, Epoch : 1, Step : 1243, Training Loss : 0.17891, Training Acc : 0.956, Run Time : 0.60
INFO:root:2019-05-12 02:43:35, Epoch : 1, Step : 1244, Training Loss : 0.21039, Training Acc : 0.917, Run Time : 1.32
INFO:root:2019-05-12 02:43:45, Epoch : 1, Step : 1245, Training Loss : 0.15403, Training Acc : 0.944, Run Time : 9.64
INFO:root:2019-05-12 02:43:45, Epoch : 1, Step : 1246, Training Loss : 0.33229, Training Acc : 0.867, Run Time : 0.48
INFO:root:2019-05-12 02:43:47, Epoch : 1, Step : 1247, Training Loss : 0.14060, Training Acc : 0.956, Run Time : 1.55
INFO:root:2019-05-12 02:43:55, Epoch : 1, Step : 1248, Training Loss : 0.19550, Training Acc : 0.922, Run Time : 8.29
INFO:root:2019-05-12 02:43:55, Epoch : 1, Step : 1249, Training Loss : 0.23657, Training Acc : 0.894, Run Time : 0.49
INFO:root:2019-05-12 02:43:57, Epoch : 1, Step : 1250, Training Loss : 0.25984, Training Acc : 0.872, Run Time : 1.39
INFO:root:2019-05-12 02:44:03, Epoch : 1, Step : 1251, Training Loss : 0.22676, Training Acc : 0.917, Run Time : 6.37
INFO:root:2019-05-12 02:44:04, Epoch : 1, Step : 1252, Training Loss : 0.29840, Training Acc : 0.894, Run Time : 0.71
INFO:root:2019-05-12 02:44:05, Epoch : 1, Step : 1253, Training Loss : 0.30042, Training Acc : 0.850, Run Time : 1.25
INFO:root:2019-05-12 02:44:12, Epoch : 1, Step : 1254, Training Loss : 0.20355, Training Acc : 0.900, Run Time : 6.89
INFO:root:2019-05-12 02:44:13, Epoch : 1, Step : 1255, Training Loss : 0.35346, Training Acc : 0.856, Run Time : 0.49
INFO:root:2019-05-12 02:44:14, Epoch : 1, Step : 1256, Training Loss : 0.28847, Training Acc : 0.850, Run Time : 1.51
INFO:root:2019-05-12 02:44:24, Epoch : 1, Step : 1257, Training Loss : 0.23404, Training Acc : 0.906, Run Time : 9.62
INFO:root:2019-05-12 02:44:25, Epoch : 1, Step : 1258, Training Loss : 0.48320, Training Acc : 0.756, Run Time : 0.99
INFO:root:2019-05-12 02:44:34, Epoch : 1, Step : 1259, Training Loss : 0.21551, Training Acc : 0.911, Run Time : 9.43
INFO:root:2019-05-12 02:44:35, Epoch : 1, Step : 1260, Training Loss : 0.29716, Training Acc : 0.883, Run Time : 0.94
INFO:root:2019-05-12 02:44:43, Epoch : 1, Step : 1261, Training Loss : 0.42632, Training Acc : 0.822, Run Time : 7.76
INFO:root:2019-05-12 02:44:43, Epoch : 1, Step : 1262, Training Loss : 0.54063, Training Acc : 0.772, Run Time : 0.53
INFO:root:2019-05-12 02:44:49, Epoch : 1, Step : 1263, Training Loss : 0.74839, Training Acc : 0.756, Run Time : 5.42
INFO:root:2019-05-12 02:44:50, Epoch : 1, Step : 1264, Training Loss : 0.34826, Training Acc : 0.822, Run Time : 1.58
INFO:root:2019-05-12 02:44:51, Epoch : 1, Step : 1265, Training Loss : 0.25035, Training Acc : 0.906, Run Time : 0.57
INFO:root:2019-05-12 02:44:52, Epoch : 1, Step : 1266, Training Loss : 0.60530, Training Acc : 0.756, Run Time : 0.64
INFO:root:2019-05-12 02:45:01, Epoch : 1, Step : 1267, Training Loss : 0.26441, Training Acc : 0.894, Run Time : 9.76
INFO:root:2019-05-12 02:45:02, Epoch : 1, Step : 1268, Training Loss : 0.43760, Training Acc : 0.811, Run Time : 0.68
INFO:root:2019-05-12 02:45:03, Epoch : 1, Step : 1269, Training Loss : 0.66051, Training Acc : 0.728, Run Time : 0.57
INFO:root:2019-05-12 02:45:03, Epoch : 1, Step : 1270, Training Loss : 0.45165, Training Acc : 0.761, Run Time : 0.60
INFO:root:2019-05-12 02:45:12, Epoch : 1, Step : 1271, Training Loss : 0.36296, Training Acc : 0.828, Run Time : 9.03
INFO:root:2019-05-12 02:45:13, Epoch : 1, Step : 1272, Training Loss : 0.28832, Training Acc : 0.861, Run Time : 0.85
INFO:root:2019-05-12 02:45:20, Epoch : 1, Step : 1273, Training Loss : 0.29165, Training Acc : 0.839, Run Time : 7.37
INFO:root:2019-05-12 02:45:21, Epoch : 1, Step : 1274, Training Loss : 0.37631, Training Acc : 0.817, Run Time : 0.82
INFO:root:2019-05-12 02:45:29, Epoch : 1, Step : 1275, Training Loss : 0.27357, Training Acc : 0.850, Run Time : 7.60
INFO:root:2019-05-12 02:45:29, Epoch : 1, Step : 1276, Training Loss : 0.35340, Training Acc : 0.844, Run Time : 0.63
INFO:root:2019-05-12 02:45:31, Epoch : 1, Step : 1277, Training Loss : 0.24855, Training Acc : 0.894, Run Time : 1.24
INFO:root:2019-05-12 02:45:37, Epoch : 1, Step : 1278, Training Loss : 0.35092, Training Acc : 0.839, Run Time : 5.90
INFO:root:2019-05-12 02:45:37, Epoch : 1, Step : 1279, Training Loss : 0.35294, Training Acc : 0.844, Run Time : 0.84
INFO:root:2019-05-12 02:45:38, Epoch : 1, Step : 1280, Training Loss : 0.33231, Training Acc : 0.839, Run Time : 1.01
INFO:root:2019-05-12 02:45:39, Epoch : 1, Step : 1281, Training Loss : 0.30581, Training Acc : 0.883, Run Time : 0.71
INFO:root:2019-05-12 02:45:43, Epoch : 1, Step : 1282, Training Loss : 0.46023, Training Acc : 0.772, Run Time : 4.21
INFO:root:2019-05-12 02:45:44, Epoch : 1, Step : 1283, Training Loss : 0.20831, Training Acc : 0.911, Run Time : 0.66
INFO:root:2019-05-12 02:45:45, Epoch : 1, Step : 1284, Training Loss : 0.29222, Training Acc : 0.861, Run Time : 0.65
INFO:root:2019-05-12 02:45:55, Epoch : 1, Step : 1285, Training Loss : 0.34257, Training Acc : 0.839, Run Time : 10.17
INFO:root:2019-05-12 02:45:55, Epoch : 1, Step : 1286, Training Loss : 0.30826, Training Acc : 0.872, Run Time : 0.55
INFO:root:2019-05-12 02:45:57, Epoch : 1, Step : 1287, Training Loss : 0.32358, Training Acc : 0.850, Run Time : 1.38
INFO:root:2019-05-12 02:46:05, Epoch : 1, Step : 1288, Training Loss : 0.35465, Training Acc : 0.861, Run Time : 7.75
INFO:root:2019-05-12 02:46:05, Epoch : 1, Step : 1289, Training Loss : 0.38443, Training Acc : 0.828, Run Time : 0.48
INFO:root:2019-05-12 02:46:07, Epoch : 1, Step : 1290, Training Loss : 0.37063, Training Acc : 0.850, Run Time : 1.63
INFO:root:2019-05-12 02:46:15, Epoch : 1, Step : 1291, Training Loss : 0.26913, Training Acc : 0.872, Run Time : 8.40
INFO:root:2019-05-12 02:46:16, Epoch : 1, Step : 1292, Training Loss : 0.28709, Training Acc : 0.906, Run Time : 0.81
INFO:root:2019-05-12 02:46:17, Epoch : 1, Step : 1293, Training Loss : 0.20035, Training Acc : 0.944, Run Time : 0.93
INFO:root:2019-05-12 02:46:23, Epoch : 1, Step : 1294, Training Loss : 0.27231, Training Acc : 0.894, Run Time : 6.42
INFO:root:2019-05-12 02:46:24, Epoch : 1, Step : 1295, Training Loss : 0.27683, Training Acc : 0.878, Run Time : 0.51
INFO:root:2019-05-12 02:46:25, Epoch : 1, Step : 1296, Training Loss : 0.34740, Training Acc : 0.850, Run Time : 1.11
INFO:root:2019-05-12 02:46:31, Epoch : 1, Step : 1297, Training Loss : 0.29026, Training Acc : 0.917, Run Time : 6.45
INFO:root:2019-05-12 02:46:32, Epoch : 1, Step : 1298, Training Loss : 0.34679, Training Acc : 0.883, Run Time : 0.41
INFO:root:2019-05-12 02:46:33, Epoch : 1, Step : 1299, Training Loss : 0.31383, Training Acc : 0.861, Run Time : 1.40
INFO:root:2019-05-12 02:46:40, Epoch : 1, Step : 1300, Training Loss : 0.30758, Training Acc : 0.850, Run Time : 7.40
INFO:root:2019-05-12 02:46:41, Epoch : 1, Step : 1301, Training Loss : 0.32096, Training Acc : 0.872, Run Time : 0.96
INFO:root:2019-05-12 02:46:52, Epoch : 1, Step : 1302, Training Loss : 0.24174, Training Acc : 0.889, Run Time : 10.12
INFO:root:2019-05-12 02:46:52, Epoch : 1, Step : 1303, Training Loss : 0.22144, Training Acc : 0.906, Run Time : 0.60
INFO:root:2019-05-12 02:46:53, Epoch : 1, Step : 1304, Training Loss : 0.25224, Training Acc : 0.900, Run Time : 0.58
INFO:root:2019-05-12 02:47:01, Epoch : 1, Step : 1305, Training Loss : 0.27903, Training Acc : 0.867, Run Time : 7.99
INFO:root:2019-05-12 02:47:01, Epoch : 1, Step : 1306, Training Loss : 0.24589, Training Acc : 0.906, Run Time : 0.67
INFO:root:2019-05-12 02:47:11, Epoch : 1, Step : 1307, Training Loss : 0.35651, Training Acc : 0.856, Run Time : 9.17
INFO:root:2019-05-12 02:47:11, Epoch : 1, Step : 1308, Training Loss : 0.23330, Training Acc : 0.922, Run Time : 0.66
INFO:root:2019-05-12 02:47:12, Epoch : 1, Step : 1309, Training Loss : 0.32544, Training Acc : 0.889, Run Time : 1.00
INFO:root:2019-05-12 02:47:20, Epoch : 1, Step : 1310, Training Loss : 0.33994, Training Acc : 0.822, Run Time : 7.90
INFO:root:2019-05-12 02:47:21, Epoch : 1, Step : 1311, Training Loss : 0.28098, Training Acc : 0.900, Run Time : 0.57
INFO:root:2019-05-12 02:47:21, Epoch : 1, Step : 1312, Training Loss : 0.39982, Training Acc : 0.800, Run Time : 0.62
INFO:root:2019-05-12 02:47:28, Epoch : 1, Step : 1313, Training Loss : 0.36543, Training Acc : 0.817, Run Time : 6.88
INFO:root:2019-05-12 02:47:29, Epoch : 1, Step : 1314, Training Loss : 0.64089, Training Acc : 0.717, Run Time : 0.70
INFO:root:2019-05-12 02:47:30, Epoch : 1, Step : 1315, Training Loss : 0.56851, Training Acc : 0.728, Run Time : 0.62
INFO:root:2019-05-12 02:47:31, Epoch : 1, Step : 1316, Training Loss : 0.76389, Training Acc : 0.689, Run Time : 1.04
INFO:root:2019-05-12 02:47:31, Epoch : 1, Step : 1317, Training Loss : 0.34720, Training Acc : 0.794, Run Time : 0.64
INFO:root:2019-05-12 02:47:40, Epoch : 1, Step : 1318, Training Loss : 0.34119, Training Acc : 0.800, Run Time : 8.94
INFO:root:2019-05-12 02:47:41, Epoch : 1, Step : 1319, Training Loss : 0.37582, Training Acc : 0.794, Run Time : 0.66
INFO:root:2019-05-12 02:47:42, Epoch : 1, Step : 1320, Training Loss : 0.47935, Training Acc : 0.739, Run Time : 1.40
INFO:root:2019-05-12 02:47:47, Epoch : 1, Step : 1321, Training Loss : 0.32995, Training Acc : 0.844, Run Time : 4.64
INFO:root:2019-05-12 02:47:47, Epoch : 1, Step : 1322, Training Loss : 0.32241, Training Acc : 0.856, Run Time : 0.55
INFO:root:2019-05-12 02:47:49, Epoch : 1, Step : 1323, Training Loss : 0.29661, Training Acc : 0.883, Run Time : 1.54
INFO:root:2019-05-12 02:47:56, Epoch : 1, Step : 1324, Training Loss : 0.31687, Training Acc : 0.839, Run Time : 7.16
INFO:root:2019-05-12 02:47:57, Epoch : 1, Step : 1325, Training Loss : 0.36738, Training Acc : 0.828, Run Time : 0.56
INFO:root:2019-05-12 02:47:57, Epoch : 1, Step : 1326, Training Loss : 0.32311, Training Acc : 0.872, Run Time : 0.64
INFO:root:2019-05-12 02:47:59, Epoch : 1, Step : 1327, Training Loss : 0.41706, Training Acc : 0.828, Run Time : 1.51
INFO:root:2019-05-12 02:48:06, Epoch : 1, Step : 1328, Training Loss : 0.50387, Training Acc : 0.761, Run Time : 6.70
INFO:root:2019-05-12 02:48:06, Epoch : 1, Step : 1329, Training Loss : 0.43300, Training Acc : 0.806, Run Time : 0.86
INFO:root:2019-05-12 02:48:13, Epoch : 1, Step : 1330, Training Loss : 0.38756, Training Acc : 0.839, Run Time : 6.54
INFO:root:2019-05-12 02:48:14, Epoch : 1, Step : 1331, Training Loss : 0.38972, Training Acc : 0.806, Run Time : 1.18
INFO:root:2019-05-12 02:48:15, Epoch : 1, Step : 1332, Training Loss : 0.54413, Training Acc : 0.728, Run Time : 0.61
INFO:root:2019-05-12 02:48:22, Epoch : 1, Step : 1333, Training Loss : 0.43958, Training Acc : 0.767, Run Time : 7.33
INFO:root:2019-05-12 02:48:22, Epoch : 1, Step : 1334, Training Loss : 0.42116, Training Acc : 0.772, Run Time : 0.45
INFO:root:2019-05-12 02:48:24, Epoch : 1, Step : 1335, Training Loss : 0.44103, Training Acc : 0.822, Run Time : 1.40
INFO:root:2019-05-12 02:48:31, Epoch : 1, Step : 1336, Training Loss : 0.39084, Training Acc : 0.811, Run Time : 6.93
INFO:root:2019-05-12 02:48:31, Epoch : 1, Step : 1337, Training Loss : 0.36168, Training Acc : 0.828, Run Time : 0.56
INFO:root:2019-05-12 02:48:37, Epoch : 1, Step : 1338, Training Loss : 0.39255, Training Acc : 0.794, Run Time : 5.86
INFO:root:2019-05-12 02:48:38, Epoch : 1, Step : 1339, Training Loss : 0.42770, Training Acc : 0.761, Run Time : 0.84
INFO:root:2019-05-12 02:48:39, Epoch : 1, Step : 1340, Training Loss : 0.44040, Training Acc : 0.756, Run Time : 0.55
INFO:root:2019-05-12 02:48:44, Epoch : 1, Step : 1341, Training Loss : 0.50652, Training Acc : 0.756, Run Time : 5.67
INFO:root:2019-05-12 02:48:45, Epoch : 1, Step : 1342, Training Loss : 0.39322, Training Acc : 0.822, Run Time : 0.64
INFO:root:2019-05-12 02:48:46, Epoch : 1, Step : 1343, Training Loss : 0.43333, Training Acc : 0.789, Run Time : 0.86
INFO:root:2019-05-12 02:48:51, Epoch : 1, Step : 1344, Training Loss : 0.37720, Training Acc : 0.806, Run Time : 5.43
INFO:root:2019-05-12 02:48:52, Epoch : 1, Step : 1345, Training Loss : 0.26896, Training Acc : 0.883, Run Time : 0.59
INFO:root:2019-05-12 02:48:59, Epoch : 1, Step : 1346, Training Loss : 0.28971, Training Acc : 0.872, Run Time : 6.90
INFO:root:2019-05-12 02:48:59, Epoch : 1, Step : 1347, Training Loss : 0.29323, Training Acc : 0.872, Run Time : 0.67
INFO:root:2019-05-12 02:49:00, Epoch : 1, Step : 1348, Training Loss : 0.29230, Training Acc : 0.883, Run Time : 1.04
INFO:root:2019-05-12 02:49:08, Epoch : 1, Step : 1349, Training Loss : 0.26738, Training Acc : 0.878, Run Time : 7.42
INFO:root:2019-05-12 02:49:08, Epoch : 1, Step : 1350, Training Loss : 0.24699, Training Acc : 0.900, Run Time : 0.51
INFO:root:2019-05-12 02:49:10, Epoch : 1, Step : 1351, Training Loss : 0.27539, Training Acc : 0.906, Run Time : 1.60
INFO:root:2019-05-12 02:49:18, Epoch : 1, Step : 1352, Training Loss : 0.39107, Training Acc : 0.856, Run Time : 8.28
INFO:root:2019-05-12 02:49:19, Epoch : 1, Step : 1353, Training Loss : 0.39883, Training Acc : 0.839, Run Time : 0.45
INFO:root:2019-05-12 02:49:20, Epoch : 1, Step : 1354, Training Loss : 0.39934, Training Acc : 0.822, Run Time : 1.79
INFO:root:2019-05-12 02:49:29, Epoch : 1, Step : 1355, Training Loss : 0.32799, Training Acc : 0.861, Run Time : 8.27
INFO:root:2019-05-12 02:49:29, Epoch : 1, Step : 1356, Training Loss : 0.32984, Training Acc : 0.844, Run Time : 0.44
INFO:root:2019-05-12 02:49:31, Epoch : 1, Step : 1357, Training Loss : 0.35239, Training Acc : 0.833, Run Time : 1.71
INFO:root:2019-05-12 02:49:38, Epoch : 1, Step : 1358, Training Loss : 0.33147, Training Acc : 0.839, Run Time : 7.33
INFO:root:2019-05-12 02:49:39, Epoch : 1, Step : 1359, Training Loss : 0.32033, Training Acc : 0.861, Run Time : 0.57
INFO:root:2019-05-12 02:49:40, Epoch : 1, Step : 1360, Training Loss : 0.34427, Training Acc : 0.850, Run Time : 0.80
INFO:root:2019-05-12 02:49:45, Epoch : 1, Step : 1361, Training Loss : 0.33703, Training Acc : 0.839, Run Time : 5.42
INFO:root:2019-05-12 02:49:46, Epoch : 1, Step : 1362, Training Loss : 0.32326, Training Acc : 0.828, Run Time : 0.55
INFO:root:2019-05-12 02:49:47, Epoch : 1, Step : 1363, Training Loss : 0.35626, Training Acc : 0.817, Run Time : 0.99
INFO:root:2019-05-12 02:49:56, Epoch : 1, Step : 1364, Training Loss : 0.34231, Training Acc : 0.822, Run Time : 9.27
INFO:root:2019-05-12 02:49:56, Epoch : 1, Step : 1365, Training Loss : 0.30400, Training Acc : 0.856, Run Time : 0.43
INFO:root:2019-05-12 02:49:57, Epoch : 1, Step : 1366, Training Loss : 0.39148, Training Acc : 0.800, Run Time : 0.68
INFO:root:2019-05-12 02:50:05, Epoch : 1, Step : 1367, Training Loss : 0.35765, Training Acc : 0.833, Run Time : 7.97
INFO:root:2019-05-12 02:50:13, Epoch : 1, Step : 1368, Training Loss : 0.34575, Training Acc : 0.828, Run Time : 7.73
INFO:root:2019-05-12 02:50:15, Epoch : 1, Step : 1369, Training Loss : 0.27173, Training Acc : 0.867, Run Time : 2.59
INFO:root:2019-05-12 02:50:19, Epoch : 1, Step : 1370, Training Loss : 0.39144, Training Acc : 0.800, Run Time : 3.86
INFO:root:2019-05-12 02:50:20, Epoch : 1, Step : 1371, Training Loss : 0.38443, Training Acc : 0.817, Run Time : 0.95
INFO:root:2019-05-12 02:50:25, Epoch : 1, Step : 1372, Training Loss : 0.31445, Training Acc : 0.872, Run Time : 5.23
INFO:root:2019-05-12 02:50:26, Epoch : 1, Step : 1373, Training Loss : 0.35563, Training Acc : 0.844, Run Time : 0.77
INFO:root:2019-05-12 02:50:33, Epoch : 1, Step : 1374, Training Loss : 0.33632, Training Acc : 0.850, Run Time : 7.34
INFO:root:2019-05-12 02:50:34, Epoch : 1, Step : 1375, Training Loss : 0.38812, Training Acc : 0.806, Run Time : 0.70
INFO:root:2019-05-12 02:50:35, Epoch : 1, Step : 1376, Training Loss : 0.31899, Training Acc : 0.850, Run Time : 0.81
INFO:root:2019-05-12 02:50:36, Epoch : 1, Step : 1377, Training Loss : 0.27968, Training Acc : 0.900, Run Time : 1.19
INFO:root:2019-05-12 02:50:44, Epoch : 1, Step : 1378, Training Loss : 0.34234, Training Acc : 0.811, Run Time : 8.28
INFO:root:2019-05-12 02:50:45, Epoch : 1, Step : 1379, Training Loss : 0.32854, Training Acc : 0.822, Run Time : 0.57
INFO:root:2019-05-12 02:50:46, Epoch : 1, Step : 1380, Training Loss : 0.25047, Training Acc : 0.894, Run Time : 1.02
INFO:root:2019-05-12 02:50:54, Epoch : 1, Step : 1381, Training Loss : 0.30387, Training Acc : 0.872, Run Time : 7.69
INFO:root:2019-05-12 02:50:54, Epoch : 1, Step : 1382, Training Loss : 0.32676, Training Acc : 0.844, Run Time : 0.55
INFO:root:2019-05-12 02:50:56, Epoch : 1, Step : 1383, Training Loss : 0.24754, Training Acc : 0.878, Run Time : 1.62
INFO:root:2019-05-12 02:51:00, Epoch : 1, Step : 1384, Training Loss : 0.19958, Training Acc : 0.917, Run Time : 3.93
INFO:root:2019-05-12 02:51:00, Epoch : 1, Step : 1385, Training Loss : 0.29222, Training Acc : 0.867, Run Time : 0.58
INFO:root:2019-05-12 02:51:02, Epoch : 1, Step : 1386, Training Loss : 0.22102, Training Acc : 0.889, Run Time : 1.55
INFO:root:2019-05-12 02:51:09, Epoch : 1, Step : 1387, Training Loss : 0.27296, Training Acc : 0.900, Run Time : 7.18
INFO:root:2019-05-12 02:51:10, Epoch : 1, Step : 1388, Training Loss : 0.26239, Training Acc : 0.900, Run Time : 0.47
INFO:root:2019-05-12 02:51:10, Epoch : 1, Step : 1389, Training Loss : 0.23270, Training Acc : 0.906, Run Time : 0.81
INFO:root:2019-05-12 02:51:12, Epoch : 1, Step : 1390, Training Loss : 0.24782, Training Acc : 0.911, Run Time : 1.38
INFO:root:2019-05-12 02:51:19, Epoch : 1, Step : 1391, Training Loss : 0.20351, Training Acc : 0.906, Run Time : 7.23
INFO:root:2019-05-12 02:51:19, Epoch : 1, Step : 1392, Training Loss : 0.19984, Training Acc : 0.917, Run Time : 0.44
INFO:root:2019-05-12 02:51:20, Epoch : 1, Step : 1393, Training Loss : 0.25819, Training Acc : 0.867, Run Time : 0.49
INFO:root:2019-05-12 02:51:26, Epoch : 1, Step : 1394, Training Loss : 0.24178, Training Acc : 0.889, Run Time : 6.35
INFO:root:2019-05-12 02:51:27, Epoch : 1, Step : 1395, Training Loss : 0.23445, Training Acc : 0.889, Run Time : 0.57
INFO:root:2019-05-12 02:51:28, Epoch : 1, Step : 1396, Training Loss : 0.24319, Training Acc : 0.883, Run Time : 1.20
INFO:root:2019-05-12 02:51:35, Epoch : 1, Step : 1397, Training Loss : 0.19784, Training Acc : 0.922, Run Time : 6.48
INFO:root:2019-05-12 02:51:35, Epoch : 1, Step : 1398, Training Loss : 0.22489, Training Acc : 0.894, Run Time : 0.52
INFO:root:2019-05-12 02:51:36, Epoch : 1, Step : 1399, Training Loss : 0.22641, Training Acc : 0.922, Run Time : 0.64
INFO:root:2019-05-12 02:51:43, Epoch : 1, Step : 1400, Training Loss : 0.19435, Training Acc : 0.933, Run Time : 7.58
INFO:root:2019-05-12 02:51:45, Epoch : 1, Step : 1401, Training Loss : 0.28284, Training Acc : 0.867, Run Time : 1.25
INFO:root:2019-05-12 02:51:55, Epoch : 1, Step : 1402, Training Loss : 0.33000, Training Acc : 0.844, Run Time : 10.22
INFO:root:2019-05-12 02:51:56, Epoch : 1, Step : 1403, Training Loss : 0.29445, Training Acc : 0.856, Run Time : 1.04
INFO:root:2019-05-12 02:52:02, Epoch : 1, Step : 1404, Training Loss : 0.22744, Training Acc : 0.894, Run Time : 6.63
INFO:root:2019-05-12 02:52:03, Epoch : 1, Step : 1405, Training Loss : 0.24183, Training Acc : 0.872, Run Time : 0.88
INFO:root:2019-05-12 02:52:04, Epoch : 1, Step : 1406, Training Loss : 0.31158, Training Acc : 0.883, Run Time : 0.40
INFO:root:2019-05-12 02:52:15, Epoch : 1, Step : 1407, Training Loss : 0.33540, Training Acc : 0.833, Run Time : 11.75
INFO:root:2019-05-12 02:52:16, Epoch : 1, Step : 1408, Training Loss : 0.26209, Training Acc : 0.883, Run Time : 0.71
INFO:root:2019-05-12 02:52:18, Epoch : 1, Step : 1409, Training Loss : 0.36307, Training Acc : 0.839, Run Time : 1.89
INFO:root:2019-05-12 02:52:24, Epoch : 1, Step : 1410, Training Loss : 0.19695, Training Acc : 0.906, Run Time : 5.87
INFO:root:2019-05-12 02:52:25, Epoch : 1, Step : 1411, Training Loss : 0.19373, Training Acc : 0.889, Run Time : 0.97
INFO:root:2019-05-12 02:52:31, Epoch : 1, Step : 1412, Training Loss : 0.20935, Training Acc : 0.883, Run Time : 6.31
INFO:root:2019-05-12 02:52:32, Epoch : 1, Step : 1413, Training Loss : 0.30102, Training Acc : 0.861, Run Time : 0.55
INFO:root:2019-05-12 02:52:32, Epoch : 1, Step : 1414, Training Loss : 0.31228, Training Acc : 0.811, Run Time : 0.62
INFO:root:2019-05-12 02:52:33, Epoch : 1, Step : 1415, Training Loss : 0.30066, Training Acc : 0.861, Run Time : 0.58
INFO:root:2019-05-12 02:52:34, Epoch : 1, Step : 1416, Training Loss : 0.20142, Training Acc : 0.917, Run Time : 1.06
INFO:root:2019-05-12 02:52:42, Epoch : 1, Step : 1417, Training Loss : 0.32573, Training Acc : 0.844, Run Time : 8.21
INFO:root:2019-05-12 02:52:43, Epoch : 1, Step : 1418, Training Loss : 0.28458, Training Acc : 0.856, Run Time : 1.05
INFO:root:2019-05-12 02:52:44, Epoch : 1, Step : 1419, Training Loss : 0.28670, Training Acc : 0.833, Run Time : 0.61
INFO:root:2019-05-12 02:52:52, Epoch : 1, Step : 1420, Training Loss : 0.20946, Training Acc : 0.906, Run Time : 8.08
INFO:root:2019-05-12 02:52:53, Epoch : 1, Step : 1421, Training Loss : 0.22436, Training Acc : 0.894, Run Time : 1.10
INFO:root:2019-05-12 02:53:01, Epoch : 1, Step : 1422, Training Loss : 0.30993, Training Acc : 0.811, Run Time : 8.35
INFO:root:2019-05-12 02:53:02, Epoch : 1, Step : 1423, Training Loss : 0.24493, Training Acc : 0.878, Run Time : 0.88
INFO:root:2019-05-12 02:53:03, Epoch : 1, Step : 1424, Training Loss : 0.37036, Training Acc : 0.822, Run Time : 1.20
INFO:root:2019-05-12 02:53:14, Epoch : 1, Step : 1425, Training Loss : 0.23347, Training Acc : 0.906, Run Time : 10.89
INFO:root:2019-05-12 02:53:15, Epoch : 1, Step : 1426, Training Loss : 0.30540, Training Acc : 0.883, Run Time : 0.57
INFO:root:2019-05-12 02:53:25, Epoch : 1, Step : 1427, Training Loss : 0.24184, Training Acc : 0.900, Run Time : 10.29
INFO:root:2019-05-12 02:53:26, Epoch : 1, Step : 1428, Training Loss : 0.30236, Training Acc : 0.850, Run Time : 0.54
INFO:root:2019-05-12 02:53:27, Epoch : 1, Step : 1429, Training Loss : 0.44386, Training Acc : 0.806, Run Time : 0.86
INFO:root:2019-05-12 02:53:33, Epoch : 1, Step : 1430, Training Loss : 0.32120, Training Acc : 0.878, Run Time : 6.29
INFO:root:2019-05-12 02:53:34, Epoch : 1, Step : 1431, Training Loss : 0.29239, Training Acc : 0.861, Run Time : 1.29
INFO:root:2019-05-12 02:53:51, Epoch : 1, Step : 1432, Training Loss : 0.27794, Training Acc : 0.867, Run Time : 16.34
INFO:root:2019-05-12 02:53:52, Epoch : 1, Step : 1433, Training Loss : 0.30892, Training Acc : 0.856, Run Time : 1.75
INFO:root:2019-05-12 02:53:53, Epoch : 1, Step : 1434, Training Loss : 0.29307, Training Acc : 0.872, Run Time : 0.59
INFO:root:2019-05-12 02:53:59, Epoch : 1, Step : 1435, Training Loss : 0.22749, Training Acc : 0.906, Run Time : 6.51
INFO:root:2019-05-12 02:54:00, Epoch : 1, Step : 1436, Training Loss : 0.31267, Training Acc : 0.872, Run Time : 0.59
INFO:root:2019-05-12 02:54:01, Epoch : 1, Step : 1437, Training Loss : 0.28779, Training Acc : 0.872, Run Time : 0.64
INFO:root:2019-05-12 02:54:10, Epoch : 1, Step : 1438, Training Loss : 0.21979, Training Acc : 0.900, Run Time : 9.39
INFO:root:2019-05-12 02:54:15, Epoch : 1, Step : 1439, Training Loss : 0.26860, Training Acc : 0.850, Run Time : 5.36
INFO:root:2019-05-12 02:54:16, Epoch : 1, Step : 1440, Training Loss : 0.43601, Training Acc : 0.800, Run Time : 0.67
INFO:root:2019-05-12 02:54:27, Epoch : 1, Step : 1441, Training Loss : 0.25841, Training Acc : 0.889, Run Time : 11.01
INFO:root:2019-05-12 02:54:28, Epoch : 1, Step : 1442, Training Loss : 0.32033, Training Acc : 0.883, Run Time : 0.76
INFO:root:2019-05-12 02:54:28, Epoch : 1, Step : 1443, Training Loss : 0.21079, Training Acc : 0.917, Run Time : 0.64
INFO:root:2019-05-12 02:54:42, Epoch : 1, Step : 1444, Training Loss : 0.29242, Training Acc : 0.894, Run Time : 13.29
INFO:root:2019-05-12 02:54:42, Epoch : 1, Step : 1445, Training Loss : 0.26244, Training Acc : 0.889, Run Time : 0.62
INFO:root:2019-05-12 02:54:44, Epoch : 1, Step : 1446, Training Loss : 0.20139, Training Acc : 0.917, Run Time : 1.27
INFO:root:2019-05-12 02:54:57, Epoch : 1, Step : 1447, Training Loss : 0.27754, Training Acc : 0.867, Run Time : 13.09
INFO:root:2019-05-12 02:54:57, Epoch : 1, Step : 1448, Training Loss : 0.19945, Training Acc : 0.928, Run Time : 0.76
INFO:root:2019-05-12 02:54:59, Epoch : 1, Step : 1449, Training Loss : 0.23206, Training Acc : 0.928, Run Time : 1.90
INFO:root:2019-05-12 02:55:11, Epoch : 1, Step : 1450, Training Loss : 0.21129, Training Acc : 0.933, Run Time : 11.41
INFO:root:2019-05-12 02:55:12, Epoch : 1, Step : 1451, Training Loss : 0.25136, Training Acc : 0.883, Run Time : 0.81
INFO:root:2019-05-12 02:55:30, Epoch : 1, Step : 1452, Training Loss : 0.19912, Training Acc : 0.917, Run Time : 17.97
INFO:root:2019-05-12 02:55:38, Epoch : 1, Step : 1453, Training Loss : 0.20412, Training Acc : 0.933, Run Time : 8.03
INFO:root:2019-05-12 02:55:39, Epoch : 1, Step : 1454, Training Loss : 0.19149, Training Acc : 0.922, Run Time : 1.55
INFO:root:2019-05-12 02:55:40, Epoch : 1, Step : 1455, Training Loss : 0.31282, Training Acc : 0.878, Run Time : 0.60
INFO:root:2019-05-12 02:55:41, Epoch : 1, Step : 1456, Training Loss : 0.21624, Training Acc : 0.889, Run Time : 1.57
INFO:root:2019-05-12 02:55:50, Epoch : 1, Step : 1457, Training Loss : 0.17863, Training Acc : 0.950, Run Time : 8.80
INFO:root:2019-05-12 02:55:51, Epoch : 1, Step : 1458, Training Loss : 0.17103, Training Acc : 0.939, Run Time : 0.81
INFO:root:2019-05-12 02:55:52, Epoch : 1, Step : 1459, Training Loss : 0.24295, Training Acc : 0.894, Run Time : 1.15
INFO:root:2019-05-12 02:56:03, Epoch : 1, Step : 1460, Training Loss : 0.17086, Training Acc : 0.933, Run Time : 11.16
INFO:root:2019-05-12 02:56:04, Epoch : 1, Step : 1461, Training Loss : 0.29639, Training Acc : 0.878, Run Time : 0.54
INFO:root:2019-05-12 02:56:04, Epoch : 1, Step : 1462, Training Loss : 0.33233, Training Acc : 0.839, Run Time : 0.54
INFO:root:2019-05-12 02:56:17, Epoch : 1, Step : 1463, Training Loss : 0.29429, Training Acc : 0.850, Run Time : 12.25
INFO:root:2019-05-12 02:56:18, Epoch : 1, Step : 1464, Training Loss : 0.28505, Training Acc : 0.878, Run Time : 1.30
INFO:root:2019-05-12 02:56:31, Epoch : 1, Step : 1465, Training Loss : 0.28347, Training Acc : 0.861, Run Time : 13.09
INFO:root:2019-05-12 02:56:31, Epoch : 1, Step : 1466, Training Loss : 0.21905, Training Acc : 0.922, Run Time : 0.49
INFO:root:2019-05-12 02:56:33, Epoch : 1, Step : 1467, Training Loss : 0.29549, Training Acc : 0.883, Run Time : 1.30
INFO:root:2019-05-12 02:56:49, Epoch : 1, Step : 1468, Training Loss : 0.19778, Training Acc : 0.922, Run Time : 15.80
INFO:root:2019-05-12 02:56:50, Epoch : 1, Step : 1469, Training Loss : 0.34528, Training Acc : 0.900, Run Time : 1.44
INFO:root:2019-05-12 02:56:51, Epoch : 1, Step : 1470, Training Loss : 0.29277, Training Acc : 0.922, Run Time : 0.62
INFO:root:2019-05-12 02:57:03, Epoch : 1, Step : 1471, Training Loss : 0.19976, Training Acc : 0.928, Run Time : 12.89
INFO:root:2019-05-12 02:57:15, Epoch : 1, Step : 1472, Training Loss : 0.31823, Training Acc : 0.861, Run Time : 11.59
INFO:root:2019-05-12 02:57:16, Epoch : 1, Step : 1473, Training Loss : 0.39536, Training Acc : 0.833, Run Time : 1.27
INFO:root:2019-05-12 02:57:18, Epoch : 1, Step : 1474, Training Loss : 0.21453, Training Acc : 0.917, Run Time : 1.62
INFO:root:2019-05-12 02:57:32, Epoch : 1, Step : 1475, Training Loss : 0.14840, Training Acc : 0.956, Run Time : 13.54
INFO:root:2019-05-12 02:57:33, Epoch : 1, Step : 1476, Training Loss : 0.20414, Training Acc : 0.911, Run Time : 1.28
INFO:root:2019-05-12 02:57:33, Epoch : 1, Step : 1477, Training Loss : 0.18603, Training Acc : 0.961, Run Time : 0.61
INFO:root:2019-05-12 02:57:36, Epoch : 1, Step : 1478, Training Loss : 0.17455, Training Acc : 0.944, Run Time : 2.22
INFO:root:2019-05-12 02:57:45, Epoch : 1, Step : 1479, Training Loss : 0.22942, Training Acc : 0.861, Run Time : 9.17
INFO:root:2019-05-12 02:57:45, Epoch : 1, Step : 1480, Training Loss : 0.27391, Training Acc : 0.889, Run Time : 0.56
INFO:root:2019-05-12 02:57:47, Epoch : 1, Step : 1481, Training Loss : 0.22845, Training Acc : 0.906, Run Time : 2.11
INFO:root:2019-05-12 02:58:02, Epoch : 1, Step : 1482, Training Loss : 0.23964, Training Acc : 0.917, Run Time : 14.73
INFO:root:2019-05-12 02:58:04, Epoch : 1, Step : 1483, Training Loss : 0.25509, Training Acc : 0.900, Run Time : 1.59
INFO:root:2019-05-12 02:58:23, Epoch : 1, Step : 1484, Training Loss : 0.25262, Training Acc : 0.889, Run Time : 19.63
INFO:root:2019-05-12 02:58:25, Epoch : 1, Step : 1485, Training Loss : 0.27026, Training Acc : 0.878, Run Time : 1.32
INFO:root:2019-05-12 02:58:26, Epoch : 1, Step : 1486, Training Loss : 0.19125, Training Acc : 0.928, Run Time : 1.30
INFO:root:2019-05-12 02:58:37, Epoch : 1, Step : 1487, Training Loss : 0.20100, Training Acc : 0.883, Run Time : 11.29
INFO:root:2019-05-12 02:58:38, Epoch : 1, Step : 1488, Training Loss : 0.23920, Training Acc : 0.900, Run Time : 0.75
INFO:root:2019-05-12 02:58:40, Epoch : 1, Step : 1489, Training Loss : 0.17308, Training Acc : 0.928, Run Time : 1.72
INFO:root:2019-05-12 02:58:52, Epoch : 1, Step : 1490, Training Loss : 0.22932, Training Acc : 0.878, Run Time : 11.87
INFO:root:2019-05-12 02:58:53, Epoch : 1, Step : 1491, Training Loss : 0.19699, Training Acc : 0.883, Run Time : 0.94
INFO:root:2019-05-12 02:59:05, Epoch : 1, Step : 1492, Training Loss : 0.19580, Training Acc : 0.911, Run Time : 12.89
INFO:root:2019-05-12 02:59:06, Epoch : 1, Step : 1493, Training Loss : 0.22132, Training Acc : 0.872, Run Time : 0.69
INFO:root:2019-05-12 02:59:07, Epoch : 1, Step : 1494, Training Loss : 0.21732, Training Acc : 0.900, Run Time : 0.62
INFO:root:2019-05-12 02:59:07, Epoch : 1, Step : 1495, Training Loss : 0.23004, Training Acc : 0.911, Run Time : 0.57
INFO:root:2019-05-12 02:59:10, Epoch : 1, Step : 1496, Training Loss : 0.26416, Training Acc : 0.889, Run Time : 2.90
INFO:root:2019-05-12 02:59:25, Epoch : 1, Step : 1497, Training Loss : 0.24660, Training Acc : 0.872, Run Time : 15.01
INFO:root:2019-05-12 02:59:27, Epoch : 1, Step : 1498, Training Loss : 0.37859, Training Acc : 0.794, Run Time : 1.43
INFO:root:2019-05-12 02:59:27, Epoch : 1, Step : 1499, Training Loss : 0.18144, Training Acc : 0.894, Run Time : 0.56
INFO:root:2019-05-12 02:59:39, Epoch : 1, Step : 1500, Training Loss : 0.16542, Training Acc : 0.917, Run Time : 12.04
INFO:root:2019-05-12 02:59:41, Epoch : 1, Step : 1501, Training Loss : 0.26419, Training Acc : 0.861, Run Time : 1.86
INFO:root:2019-05-12 02:59:53, Epoch : 1, Step : 1502, Training Loss : 0.28682, Training Acc : 0.878, Run Time : 12.14
INFO:root:2019-05-12 02:59:55, Epoch : 1, Step : 1503, Training Loss : 0.19472, Training Acc : 0.917, Run Time : 1.46
INFO:root:2019-05-12 03:00:15, Epoch : 1, Step : 1504, Training Loss : 0.33215, Training Acc : 0.883, Run Time : 19.93
INFO:root:2019-05-12 03:00:17, Epoch : 1, Step : 1505, Training Loss : 0.32704, Training Acc : 0.872, Run Time : 2.01
INFO:root:2019-05-12 03:00:18, Epoch : 1, Step : 1506, Training Loss : 0.25478, Training Acc : 0.917, Run Time : 1.55
INFO:root:2019-05-12 03:00:31, Epoch : 1, Step : 1507, Training Loss : 0.19687, Training Acc : 0.917, Run Time : 12.38
INFO:root:2019-05-12 03:00:32, Epoch : 1, Step : 1508, Training Loss : 0.25002, Training Acc : 0.872, Run Time : 1.10
INFO:root:2019-05-12 03:00:44, Epoch : 1, Step : 1509, Training Loss : 0.17058, Training Acc : 0.928, Run Time : 12.57
INFO:root:2019-05-12 03:00:45, Epoch : 1, Step : 1510, Training Loss : 0.19190, Training Acc : 0.917, Run Time : 0.77
INFO:root:2019-05-12 03:00:56, Epoch : 1, Step : 1511, Training Loss : 0.24059, Training Acc : 0.889, Run Time : 11.34
INFO:root:2019-05-12 03:01:01, Epoch : 1, Step : 1512, Training Loss : 0.19676, Training Acc : 0.917, Run Time : 4.07
INFO:root:2019-05-12 03:01:11, Epoch : 1, Step : 1513, Training Loss : 0.21694, Training Acc : 0.900, Run Time : 10.70
INFO:root:2019-05-12 03:01:13, Epoch : 1, Step : 1514, Training Loss : 0.19611, Training Acc : 0.917, Run Time : 1.66
INFO:root:2019-05-12 03:01:24, Epoch : 1, Step : 1515, Training Loss : 0.21033, Training Acc : 0.906, Run Time : 10.79
INFO:root:2019-05-12 03:01:25, Epoch : 1, Step : 1516, Training Loss : 0.17839, Training Acc : 0.917, Run Time : 0.99
INFO:root:2019-05-12 03:01:37, Epoch : 1, Step : 1517, Training Loss : 0.22775, Training Acc : 0.894, Run Time : 11.89
INFO:root:2019-05-12 03:01:37, Epoch : 1, Step : 1518, Training Loss : 0.21495, Training Acc : 0.872, Run Time : 0.60
INFO:root:2019-05-12 03:01:39, Epoch : 1, Step : 1519, Training Loss : 0.24350, Training Acc : 0.894, Run Time : 1.92
INFO:root:2019-05-12 03:01:50, Epoch : 1, Step : 1520, Training Loss : 0.21337, Training Acc : 0.911, Run Time : 10.64
INFO:root:2019-05-12 03:01:50, Epoch : 1, Step : 1521, Training Loss : 0.21989, Training Acc : 0.911, Run Time : 0.46
INFO:root:2019-05-12 03:01:51, Epoch : 1, Step : 1522, Training Loss : 0.23399, Training Acc : 0.883, Run Time : 0.68
INFO:root:2019-05-12 03:02:02, Epoch : 1, Step : 1523, Training Loss : 0.31860, Training Acc : 0.844, Run Time : 10.71
INFO:root:2019-05-12 03:02:03, Epoch : 1, Step : 1524, Training Loss : 0.22571, Training Acc : 0.894, Run Time : 1.13
INFO:root:2019-05-12 03:02:14, Epoch : 1, Step : 1525, Training Loss : 0.29070, Training Acc : 0.850, Run Time : 11.23
INFO:root:2019-05-12 03:02:14, Epoch : 1, Step : 1526, Training Loss : 0.22696, Training Acc : 0.878, Run Time : 0.56
INFO:root:2019-05-12 03:02:15, Epoch : 1, Step : 1527, Training Loss : 0.19855, Training Acc : 0.900, Run Time : 0.74
INFO:root:2019-05-12 03:02:16, Epoch : 1, Step : 1528, Training Loss : 0.19186, Training Acc : 0.928, Run Time : 0.62
INFO:root:2019-05-12 03:02:28, Epoch : 1, Step : 1529, Training Loss : 0.19454, Training Acc : 0.911, Run Time : 12.57
INFO:root:2019-05-12 03:02:30, Epoch : 1, Step : 1530, Training Loss : 0.16063, Training Acc : 0.961, Run Time : 2.07
INFO:root:2019-05-12 03:02:40, Epoch : 1, Step : 1531, Training Loss : 0.15763, Training Acc : 0.956, Run Time : 9.31
INFO:root:2019-05-12 03:02:40, Epoch : 1, Step : 1532, Training Loss : 0.13872, Training Acc : 0.950, Run Time : 0.50
INFO:root:2019-05-12 03:02:41, Epoch : 1, Step : 1533, Training Loss : 0.24968, Training Acc : 0.917, Run Time : 0.49
INFO:root:2019-05-12 03:02:53, Epoch : 1, Step : 1534, Training Loss : 0.16505, Training Acc : 0.944, Run Time : 11.98
INFO:root:2019-05-12 03:02:56, Epoch : 1, Step : 1535, Training Loss : 0.25964, Training Acc : 0.900, Run Time : 3.59
INFO:root:2019-05-12 03:02:57, Epoch : 1, Step : 1536, Training Loss : 0.21592, Training Acc : 0.889, Run Time : 0.44
INFO:root:2019-05-12 03:03:07, Epoch : 1, Step : 1537, Training Loss : 0.16767, Training Acc : 0.922, Run Time : 10.64
INFO:root:2019-05-12 03:03:08, Epoch : 1, Step : 1538, Training Loss : 0.31430, Training Acc : 0.861, Run Time : 0.79
INFO:root:2019-05-12 03:03:21, Epoch : 1, Step : 1539, Training Loss : 0.28366, Training Acc : 0.867, Run Time : 12.96
INFO:root:2019-05-12 03:03:22, Epoch : 1, Step : 1540, Training Loss : 0.29827, Training Acc : 0.850, Run Time : 0.86
INFO:root:2019-05-12 03:03:35, Epoch : 1, Step : 1541, Training Loss : 0.34094, Training Acc : 0.861, Run Time : 13.00
INFO:root:2019-05-12 03:03:36, Epoch : 1, Step : 1542, Training Loss : 0.21116, Training Acc : 0.872, Run Time : 1.13
INFO:root:2019-05-12 03:03:50, Epoch : 1, Step : 1543, Training Loss : 0.27894, Training Acc : 0.861, Run Time : 14.08
INFO:root:2019-05-12 03:03:51, Epoch : 1, Step : 1544, Training Loss : 0.33724, Training Acc : 0.867, Run Time : 1.24
INFO:root:2019-05-12 03:03:52, Epoch : 1, Step : 1545, Training Loss : 0.26707, Training Acc : 0.889, Run Time : 0.76
INFO:root:2019-05-12 03:03:53, Epoch : 1, Step : 1546, Training Loss : 0.22065, Training Acc : 0.894, Run Time : 0.60
INFO:root:2019-05-12 03:03:55, Epoch : 1, Step : 1547, Training Loss : 0.20427, Training Acc : 0.922, Run Time : 1.83
INFO:root:2019-05-12 03:04:06, Epoch : 1, Step : 1548, Training Loss : 0.26464, Training Acc : 0.889, Run Time : 11.82
INFO:root:2019-05-12 03:04:07, Epoch : 1, Step : 1549, Training Loss : 0.29761, Training Acc : 0.850, Run Time : 0.65
INFO:root:2019-05-12 03:04:08, Epoch : 1, Step : 1550, Training Loss : 0.27297, Training Acc : 0.861, Run Time : 0.59
INFO:root:2019-05-12 03:04:26, Epoch : 1, Step : 1551, Training Loss : 0.31718, Training Acc : 0.856, Run Time : 18.35
INFO:root:2019-05-12 03:04:27, Epoch : 1, Step : 1552, Training Loss : 0.17405, Training Acc : 0.922, Run Time : 1.40
INFO:root:2019-05-12 03:04:37, Epoch : 1, Step : 1553, Training Loss : 0.21520, Training Acc : 0.900, Run Time : 9.75
INFO:root:2019-05-12 03:04:38, Epoch : 1, Step : 1554, Training Loss : 0.27789, Training Acc : 0.867, Run Time : 0.79
INFO:root:2019-05-12 03:04:50, Epoch : 1, Step : 1555, Training Loss : 0.25280, Training Acc : 0.889, Run Time : 11.75
INFO:root:2019-05-12 03:04:58, Epoch : 1, Step : 1556, Training Loss : 0.21699, Training Acc : 0.894, Run Time : 8.15
INFO:root:2019-05-12 03:05:00, Epoch : 1, Step : 1557, Training Loss : 0.26751, Training Acc : 0.889, Run Time : 2.44
INFO:root:2019-05-12 03:05:08, Epoch : 1, Step : 1558, Training Loss : 0.27636, Training Acc : 0.900, Run Time : 7.65
INFO:root:2019-05-12 03:05:21, Epoch : 1, Step : 1559, Training Loss : 0.20095, Training Acc : 0.894, Run Time : 12.80
INFO:root:2019-05-12 03:05:30, Epoch : 1, Step : 1560, Training Loss : 0.18011, Training Acc : 0.911, Run Time : 9.18
INFO:root:2019-05-12 03:05:38, Epoch : 1, Step : 1561, Training Loss : 0.15507, Training Acc : 0.950, Run Time : 8.23
INFO:root:2019-05-12 03:05:42, Epoch : 1, Step : 1562, Training Loss : 0.19772, Training Acc : 0.933, Run Time : 4.14
INFO:root:2019-05-12 03:05:43, Epoch : 1, Step : 1563, Training Loss : 0.17701, Training Acc : 0.911, Run Time : 0.47
INFO:root:2019-05-12 03:05:55, Epoch : 1, Step : 1564, Training Loss : 0.29004, Training Acc : 0.867, Run Time : 12.29
INFO:root:2019-05-12 03:05:56, Epoch : 1, Step : 1565, Training Loss : 0.17987, Training Acc : 0.933, Run Time : 0.84
INFO:root:2019-05-12 03:05:57, Epoch : 1, Step : 1566, Training Loss : 0.15969, Training Acc : 0.933, Run Time : 1.53
INFO:root:2019-05-12 03:06:08, Epoch : 1, Step : 1567, Training Loss : 0.20319, Training Acc : 0.894, Run Time : 10.89
INFO:root:2019-05-12 03:06:09, Epoch : 1, Step : 1568, Training Loss : 0.27187, Training Acc : 0.861, Run Time : 0.44
INFO:root:2019-05-12 03:06:10, Epoch : 1, Step : 1569, Training Loss : 0.23928, Training Acc : 0.844, Run Time : 1.21
INFO:root:2019-05-12 03:06:23, Epoch : 1, Step : 1570, Training Loss : 0.17731, Training Acc : 0.933, Run Time : 12.72
INFO:root:2019-05-12 03:06:26, Epoch : 1, Step : 1571, Training Loss : 0.22172, Training Acc : 0.906, Run Time : 3.45
INFO:root:2019-05-12 03:06:44, Epoch : 1, Step : 1572, Training Loss : 0.27136, Training Acc : 0.867, Run Time : 17.50
INFO:root:2019-05-12 03:06:48, Epoch : 1, Step : 1573, Training Loss : 0.19856, Training Acc : 0.917, Run Time : 4.77
INFO:root:2019-05-12 03:06:49, Epoch : 1, Step : 1574, Training Loss : 0.18210, Training Acc : 0.933, Run Time : 0.50
INFO:root:2019-05-12 03:06:51, Epoch : 1, Step : 1575, Training Loss : 0.19643, Training Acc : 0.906, Run Time : 1.63
INFO:root:2019-05-12 03:07:05, Epoch : 1, Step : 1576, Training Loss : 0.16785, Training Acc : 0.933, Run Time : 14.88
INFO:root:2019-05-12 03:07:06, Epoch : 1, Step : 1577, Training Loss : 0.23575, Training Acc : 0.883, Run Time : 0.69
INFO:root:2019-05-12 03:07:08, Epoch : 1, Step : 1578, Training Loss : 0.21097, Training Acc : 0.900, Run Time : 1.62
INFO:root:2019-05-12 03:07:21, Epoch : 1, Step : 1579, Training Loss : 0.29133, Training Acc : 0.883, Run Time : 12.77
INFO:root:2019-05-12 03:07:21, Epoch : 1, Step : 1580, Training Loss : 0.25661, Training Acc : 0.889, Run Time : 0.72
INFO:root:2019-05-12 03:07:35, Epoch : 1, Step : 1581, Training Loss : 0.21825, Training Acc : 0.889, Run Time : 14.03
INFO:root:2019-05-12 03:07:38, Epoch : 1, Step : 1582, Training Loss : 0.27882, Training Acc : 0.883, Run Time : 2.88
INFO:root:2019-05-12 03:07:50, Epoch : 1, Step : 1583, Training Loss : 0.18602, Training Acc : 0.917, Run Time : 11.96
INFO:root:2019-05-12 03:07:51, Epoch : 1, Step : 1584, Training Loss : 0.22148, Training Acc : 0.917, Run Time : 0.89
INFO:root:2019-05-12 03:07:52, Epoch : 1, Step : 1585, Training Loss : 0.23316, Training Acc : 0.911, Run Time : 1.17
INFO:root:2019-05-12 03:08:07, Epoch : 1, Step : 1586, Training Loss : 0.22078, Training Acc : 0.911, Run Time : 14.45
INFO:root:2019-05-12 03:08:20, Epoch : 1, Step : 1587, Training Loss : 0.24246, Training Acc : 0.922, Run Time : 13.81
INFO:root:2019-05-12 03:08:22, Epoch : 1, Step : 1588, Training Loss : 0.31851, Training Acc : 0.900, Run Time : 1.21
INFO:root:2019-05-12 03:08:32, Epoch : 1, Step : 1589, Training Loss : 0.30090, Training Acc : 0.917, Run Time : 10.70
INFO:root:2019-05-12 03:08:34, Epoch : 1, Step : 1590, Training Loss : 0.25392, Training Acc : 0.911, Run Time : 1.55
INFO:root:2019-05-12 03:08:48, Epoch : 1, Step : 1591, Training Loss : 0.23819, Training Acc : 0.906, Run Time : 14.08
INFO:root:2019-05-12 03:08:49, Epoch : 1, Step : 1592, Training Loss : 0.30258, Training Acc : 0.906, Run Time : 0.80
INFO:root:2019-05-12 03:08:51, Epoch : 1, Step : 1593, Training Loss : 0.16805, Training Acc : 0.928, Run Time : 1.69
INFO:root:2019-05-12 03:09:00, Epoch : 1, Step : 1594, Training Loss : 0.17729, Training Acc : 0.928, Run Time : 9.97
INFO:root:2019-05-12 03:09:01, Epoch : 1, Step : 1595, Training Loss : 0.16925, Training Acc : 0.928, Run Time : 0.48
INFO:root:2019-05-12 03:09:03, Epoch : 1, Step : 1596, Training Loss : 0.20549, Training Acc : 0.928, Run Time : 1.73
INFO:root:2019-05-12 03:09:21, Epoch : 1, Step : 1597, Training Loss : 0.37341, Training Acc : 0.878, Run Time : 18.01
INFO:root:2019-05-12 03:09:38, Epoch : 1, Step : 1598, Training Loss : 0.27243, Training Acc : 0.894, Run Time : 17.55
INFO:root:2019-05-12 03:09:40, Epoch : 1, Step : 1599, Training Loss : 0.33791, Training Acc : 0.883, Run Time : 1.28
INFO:root:2019-05-12 03:09:40, Epoch : 1, Step : 1600, Training Loss : 0.28555, Training Acc : 0.900, Run Time : 0.59
INFO:root:2019-05-12 03:09:53, Epoch : 1, Step : 1601, Training Loss : 0.94943, Training Acc : 0.656, Run Time : 12.50
INFO:root:2019-05-12 03:09:53, Epoch : 1, Step : 1602, Training Loss : 1.10062, Training Acc : 0.633, Run Time : 0.78
INFO:root:2019-05-12 03:10:05, Epoch : 1, Step : 1603, Training Loss : 1.14370, Training Acc : 0.606, Run Time : 11.79
INFO:root:2019-05-12 03:10:06, Epoch : 1, Step : 1604, Training Loss : 0.92434, Training Acc : 0.644, Run Time : 0.70
INFO:root:2019-05-12 03:10:07, Epoch : 1, Step : 1605, Training Loss : 0.94740, Training Acc : 0.700, Run Time : 0.87
INFO:root:2019-05-12 03:10:07, Epoch : 1, Step : 1606, Training Loss : 0.90806, Training Acc : 0.600, Run Time : 0.61
INFO:root:2019-05-12 03:10:21, Epoch : 1, Step : 1607, Training Loss : 0.55251, Training Acc : 0.694, Run Time : 13.88
INFO:root:2019-05-12 03:10:22, Epoch : 1, Step : 1608, Training Loss : 0.56352, Training Acc : 0.750, Run Time : 0.74
INFO:root:2019-05-12 03:10:34, Epoch : 1, Step : 1609, Training Loss : 0.57188, Training Acc : 0.750, Run Time : 12.13
INFO:root:2019-05-12 03:10:35, Epoch : 1, Step : 1610, Training Loss : 0.47448, Training Acc : 0.828, Run Time : 0.45
INFO:root:2019-05-12 03:10:36, Epoch : 1, Step : 1611, Training Loss : 0.21062, Training Acc : 0.956, Run Time : 1.56
INFO:root:2019-05-12 03:10:49, Epoch : 1, Step : 1612, Training Loss : 0.31091, Training Acc : 0.889, Run Time : 12.73
INFO:root:2019-05-12 03:11:00, Epoch : 1, Step : 1613, Training Loss : 0.58134, Training Acc : 0.722, Run Time : 11.08
INFO:root:2019-05-12 03:11:01, Epoch : 1, Step : 1614, Training Loss : 0.50596, Training Acc : 0.828, Run Time : 1.12
INFO:root:2019-05-12 03:11:02, Epoch : 1, Step : 1615, Training Loss : 0.53418, Training Acc : 0.750, Run Time : 1.39
INFO:root:2019-05-12 03:11:12, Epoch : 1, Step : 1616, Training Loss : 0.51785, Training Acc : 0.783, Run Time : 9.92
INFO:root:2019-05-12 03:11:13, Epoch : 1, Step : 1617, Training Loss : 0.26998, Training Acc : 0.928, Run Time : 0.45
INFO:root:2019-05-12 03:11:14, Epoch : 1, Step : 1618, Training Loss : 0.36125, Training Acc : 0.883, Run Time : 1.57
INFO:root:2019-05-12 03:11:25, Epoch : 1, Step : 1619, Training Loss : 0.40612, Training Acc : 0.806, Run Time : 10.58
INFO:root:2019-05-12 03:11:26, Epoch : 1, Step : 1620, Training Loss : 0.45263, Training Acc : 0.872, Run Time : 0.72
INFO:root:2019-05-12 03:11:28, Epoch : 1, Step : 1621, Training Loss : 0.72996, Training Acc : 0.739, Run Time : 2.52
INFO:root:2019-05-12 03:11:39, Epoch : 1, Step : 1622, Training Loss : 0.38008, Training Acc : 0.833, Run Time : 10.40
INFO:root:2019-05-12 03:11:41, Epoch : 1, Step : 1623, Training Loss : 0.32192, Training Acc : 0.817, Run Time : 2.26
INFO:root:2019-05-12 03:11:57, Epoch : 1, Step : 1624, Training Loss : 0.35322, Training Acc : 0.850, Run Time : 16.15
INFO:root:2019-05-12 03:12:10, Epoch : 1, Step : 1625, Training Loss : 0.44464, Training Acc : 0.833, Run Time : 12.89
INFO:root:2019-05-12 03:12:12, Epoch : 1, Step : 1626, Training Loss : 0.60603, Training Acc : 0.722, Run Time : 1.97
INFO:root:2019-05-12 03:12:13, Epoch : 1, Step : 1627, Training Loss : 0.67240, Training Acc : 0.761, Run Time : 0.62
INFO:root:2019-05-12 03:12:31, Epoch : 1, Step : 1628, Training Loss : 0.37624, Training Acc : 0.844, Run Time : 18.43
INFO:root:2019-05-12 03:12:32, Epoch : 1, Step : 1629, Training Loss : 0.25257, Training Acc : 0.906, Run Time : 1.31
INFO:root:2019-05-12 03:12:33, Epoch : 1, Step : 1630, Training Loss : 0.20308, Training Acc : 0.900, Run Time : 0.65
INFO:root:2019-05-12 03:12:44, Epoch : 1, Step : 1631, Training Loss : 0.42006, Training Acc : 0.750, Run Time : 11.27
INFO:root:2019-05-12 03:12:45, Epoch : 1, Step : 1632, Training Loss : 0.35891, Training Acc : 0.844, Run Time : 0.89
INFO:root:2019-05-12 03:12:56, Epoch : 1, Step : 1633, Training Loss : 0.17737, Training Acc : 0.961, Run Time : 10.98
INFO:root:2019-05-12 03:12:57, Epoch : 1, Step : 1634, Training Loss : 0.27266, Training Acc : 0.956, Run Time : 0.77
INFO:root:2019-05-12 03:13:08, Epoch : 1, Step : 1635, Training Loss : 0.31972, Training Acc : 0.861, Run Time : 10.94
INFO:root:2019-05-12 03:13:09, Epoch : 1, Step : 1636, Training Loss : 0.23786, Training Acc : 0.917, Run Time : 1.28
INFO:root:2019-05-12 03:13:10, Epoch : 1, Step : 1637, Training Loss : 0.24729, Training Acc : 0.922, Run Time : 0.62
INFO:root:2019-05-12 03:13:22, Epoch : 1, Step : 1638, Training Loss : 0.15909, Training Acc : 0.944, Run Time : 12.64
INFO:root:2019-05-12 03:13:23, Epoch : 1, Step : 1639, Training Loss : 0.22916, Training Acc : 0.911, Run Time : 0.51
INFO:root:2019-05-12 03:13:24, Epoch : 1, Step : 1640, Training Loss : 0.14668, Training Acc : 0.933, Run Time : 1.33
INFO:root:2019-05-12 03:13:36, Epoch : 1, Step : 1641, Training Loss : 0.11320, Training Acc : 0.994, Run Time : 11.74
INFO:root:2019-05-12 03:13:37, Epoch : 1, Step : 1642, Training Loss : 0.10256, Training Acc : 0.978, Run Time : 0.63
INFO:root:2019-05-12 03:13:41, Epoch : 1, Step : 1643, Training Loss : 0.27275, Training Acc : 0.900, Run Time : 4.77
INFO:root:2019-05-12 03:13:47, Epoch : 1, Step : 1644, Training Loss : 0.17534, Training Acc : 0.939, Run Time : 5.32
INFO:root:2019-05-12 03:13:48, Epoch : 1, Step : 1645, Training Loss : 0.24253, Training Acc : 0.917, Run Time : 0.94
INFO:root:2019-05-12 03:13:54, Epoch : 1, Step : 1646, Training Loss : 0.23310, Training Acc : 0.844, Run Time : 6.21
INFO:root:2019-05-12 03:13:54, Epoch : 1, Step : 1647, Training Loss : 0.27502, Training Acc : 0.917, Run Time : 0.47
INFO:root:2019-05-12 03:14:07, Epoch : 1, Step : 1648, Training Loss : 0.28447, Training Acc : 0.883, Run Time : 13.26
INFO:root:2019-05-12 03:14:11, Epoch : 1, Step : 1649, Training Loss : 0.31799, Training Acc : 0.872, Run Time : 3.26
INFO:root:2019-05-12 03:14:20, Epoch : 1, Step : 1650, Training Loss : 0.27064, Training Acc : 0.883, Run Time : 9.02
INFO:root:2019-05-12 03:14:21, Epoch : 1, Step : 1651, Training Loss : 0.22389, Training Acc : 0.911, Run Time : 1.28
INFO:root:2019-05-12 03:14:31, Epoch : 1, Step : 1652, Training Loss : 0.17348, Training Acc : 0.950, Run Time : 10.29
INFO:root:2019-05-12 03:14:33, Epoch : 1, Step : 1653, Training Loss : 0.14280, Training Acc : 0.972, Run Time : 1.70
INFO:root:2019-05-12 03:14:47, Epoch : 1, Step : 1654, Training Loss : 0.15527, Training Acc : 0.950, Run Time : 14.23
INFO:root:2019-05-12 03:14:48, Epoch : 1, Step : 1655, Training Loss : 0.21008, Training Acc : 0.933, Run Time : 0.81
INFO:root:2019-05-12 03:14:51, Epoch : 1, Step : 1656, Training Loss : 0.15021, Training Acc : 0.939, Run Time : 2.69
INFO:root:2019-05-12 03:15:02, Epoch : 1, Step : 1657, Training Loss : 0.11598, Training Acc : 0.972, Run Time : 10.98
INFO:root:2019-05-12 03:15:03, Epoch : 1, Step : 1658, Training Loss : 0.16505, Training Acc : 0.950, Run Time : 1.03
INFO:root:2019-05-12 03:15:14, Epoch : 1, Step : 1659, Training Loss : 0.09709, Training Acc : 0.983, Run Time : 10.79
INFO:root:2019-05-12 03:15:17, Epoch : 1, Step : 1660, Training Loss : 0.34999, Training Acc : 0.822, Run Time : 3.83
INFO:root:2019-05-12 03:15:18, Epoch : 1, Step : 1661, Training Loss : 0.18428, Training Acc : 0.944, Run Time : 0.69
INFO:root:2019-05-12 03:15:32, Epoch : 1, Step : 1662, Training Loss : 0.18394, Training Acc : 0.922, Run Time : 13.62
INFO:root:2019-05-12 03:15:53, Epoch : 1, Step : 1663, Training Loss : 0.06910, Training Acc : 0.989, Run Time : 21.83
INFO:root:2019-05-12 03:16:01, Epoch : 1, Step : 1664, Training Loss : 0.63509, Training Acc : 0.694, Run Time : 7.50
INFO:root:2019-05-12 03:16:11, Epoch : 1, Step : 1665, Training Loss : 0.78893, Training Acc : 0.694, Run Time : 10.36
INFO:root:2019-05-12 03:16:13, Epoch : 1, Step : 1666, Training Loss : 0.21884, Training Acc : 0.922, Run Time : 1.15
INFO:root:2019-05-12 03:16:13, Epoch : 1, Step : 1667, Training Loss : 0.14583, Training Acc : 0.956, Run Time : 0.61
INFO:root:2019-05-12 03:16:14, Epoch : 1, Step : 1668, Training Loss : 0.13366, Training Acc : 0.961, Run Time : 0.64
INFO:root:2019-05-12 03:16:27, Epoch : 1, Step : 1669, Training Loss : 0.14993, Training Acc : 0.956, Run Time : 12.90
INFO:root:2019-05-12 03:16:28, Epoch : 1, Step : 1670, Training Loss : 0.11108, Training Acc : 0.978, Run Time : 0.97
INFO:root:2019-05-12 03:16:39, Epoch : 1, Step : 1671, Training Loss : 0.44155, Training Acc : 0.911, Run Time : 11.35
INFO:root:2019-05-12 03:16:40, Epoch : 1, Step : 1672, Training Loss : 0.34096, Training Acc : 0.839, Run Time : 0.64
INFO:root:2019-05-12 03:16:40, Epoch : 1, Step : 1673, Training Loss : 0.19995, Training Acc : 0.944, Run Time : 0.62
INFO:root:2019-05-12 03:16:41, Epoch : 1, Step : 1674, Training Loss : 0.32740, Training Acc : 0.933, Run Time : 0.75
INFO:root:2019-05-12 03:16:52, Epoch : 1, Step : 1675, Training Loss : 0.21067, Training Acc : 0.950, Run Time : 11.30
INFO:root:2019-05-12 03:16:53, Epoch : 1, Step : 1676, Training Loss : 0.18111, Training Acc : 0.950, Run Time : 0.69
INFO:root:2019-05-12 03:16:54, Epoch : 1, Step : 1677, Training Loss : 0.13900, Training Acc : 0.950, Run Time : 1.19
INFO:root:2019-05-12 03:17:07, Epoch : 1, Step : 1678, Training Loss : 0.15419, Training Acc : 0.939, Run Time : 12.52
INFO:root:2019-05-12 03:17:07, Epoch : 1, Step : 1679, Training Loss : 0.12012, Training Acc : 0.978, Run Time : 0.77
INFO:root:2019-05-12 03:17:10, Epoch : 1, Step : 1680, Training Loss : 0.06758, Training Acc : 0.978, Run Time : 2.31
INFO:root:2019-05-12 03:17:22, Epoch : 1, Step : 1681, Training Loss : 0.15548, Training Acc : 0.956, Run Time : 12.65
INFO:root:2019-05-12 03:17:23, Epoch : 1, Step : 1682, Training Loss : 0.11328, Training Acc : 0.950, Run Time : 0.80
INFO:root:2019-05-12 03:17:34, Epoch : 1, Step : 1683, Training Loss : 0.07245, Training Acc : 0.994, Run Time : 10.86
INFO:root:2019-05-12 03:17:36, Epoch : 1, Step : 1684, Training Loss : 0.10493, Training Acc : 0.967, Run Time : 1.65
INFO:root:2019-05-12 03:17:36, Epoch : 1, Step : 1685, Training Loss : 0.12231, Training Acc : 0.961, Run Time : 0.74
INFO:root:2019-05-12 03:17:48, Epoch : 1, Step : 1686, Training Loss : 0.13013, Training Acc : 0.933, Run Time : 11.39
INFO:root:2019-05-12 03:17:48, Epoch : 1, Step : 1687, Training Loss : 0.11651, Training Acc : 0.956, Run Time : 0.63
INFO:root:2019-05-12 03:18:02, Epoch : 1, Step : 1688, Training Loss : 0.15651, Training Acc : 0.950, Run Time : 13.09
INFO:root:2019-05-12 03:18:03, Epoch : 1, Step : 1689, Training Loss : 0.11161, Training Acc : 0.978, Run Time : 1.47
INFO:root:2019-05-12 03:18:04, Epoch : 1, Step : 1690, Training Loss : 0.15676, Training Acc : 0.944, Run Time : 1.00
INFO:root:2019-05-12 03:18:13, Epoch : 1, Step : 1691, Training Loss : 0.14219, Training Acc : 0.956, Run Time : 8.75
INFO:root:2019-05-12 03:18:13, Epoch : 1, Step : 1692, Training Loss : 0.13136, Training Acc : 0.956, Run Time : 0.64
INFO:root:2019-05-12 03:18:31, Epoch : 1, Step : 1693, Training Loss : 0.25760, Training Acc : 0.917, Run Time : 17.26
INFO:root:2019-05-12 03:18:32, Epoch : 1, Step : 1694, Training Loss : 0.12055, Training Acc : 0.978, Run Time : 1.64
INFO:root:2019-05-12 03:18:33, Epoch : 1, Step : 1695, Training Loss : 0.21992, Training Acc : 0.917, Run Time : 0.60
INFO:root:2019-05-12 03:18:34, Epoch : 1, Step : 1696, Training Loss : 0.11008, Training Acc : 0.956, Run Time : 1.22
INFO:root:2019-05-12 03:18:44, Epoch : 1, Step : 1697, Training Loss : 0.50326, Training Acc : 0.750, Run Time : 9.82
INFO:root:2019-05-12 03:18:45, Epoch : 1, Step : 1698, Training Loss : 0.26799, Training Acc : 0.900, Run Time : 0.86
INFO:root:2019-05-12 03:19:01, Epoch : 1, Step : 1699, Training Loss : 0.20914, Training Acc : 0.917, Run Time : 15.73
INFO:root:2019-05-12 03:19:02, Epoch : 1, Step : 1700, Training Loss : 0.25396, Training Acc : 0.894, Run Time : 1.25
INFO:root:2019-05-12 03:19:14, Epoch : 1, Step : 1701, Training Loss : 0.37769, Training Acc : 0.872, Run Time : 11.81
INFO:root:2019-05-12 03:19:26, Epoch : 1, Step : 1702, Training Loss : 0.27009, Training Acc : 0.900, Run Time : 12.77
INFO:root:2019-05-12 03:19:37, Epoch : 1, Step : 1703, Training Loss : 0.33231, Training Acc : 0.878, Run Time : 10.70
INFO:root:2019-05-12 03:19:38, Epoch : 1, Step : 1704, Training Loss : 0.22596, Training Acc : 0.917, Run Time : 1.41
INFO:root:2019-05-12 03:19:40, Epoch : 1, Step : 1705, Training Loss : 0.39554, Training Acc : 0.817, Run Time : 1.74
INFO:root:2019-05-12 03:19:50, Epoch : 1, Step : 1706, Training Loss : 0.10526, Training Acc : 0.956, Run Time : 10.16
INFO:root:2019-05-12 03:19:51, Epoch : 1, Step : 1707, Training Loss : 0.12553, Training Acc : 0.956, Run Time : 1.02
INFO:root:2019-05-12 03:20:02, Epoch : 1, Step : 1708, Training Loss : 0.21139, Training Acc : 0.911, Run Time : 10.83
INFO:root:2019-05-12 03:20:03, Epoch : 1, Step : 1709, Training Loss : 0.27532, Training Acc : 0.883, Run Time : 0.89
INFO:root:2019-05-12 03:20:15, Epoch : 1, Step : 1710, Training Loss : 0.19228, Training Acc : 0.906, Run Time : 11.73
INFO:root:2019-05-12 03:20:16, Epoch : 1, Step : 1711, Training Loss : 0.22241, Training Acc : 0.900, Run Time : 1.28
INFO:root:2019-05-12 03:20:17, Epoch : 1, Step : 1712, Training Loss : 0.44157, Training Acc : 0.817, Run Time : 0.60
INFO:root:2019-05-12 03:20:29, Epoch : 1, Step : 1713, Training Loss : 0.62109, Training Acc : 0.783, Run Time : 11.78
INFO:root:2019-05-12 03:20:30, Epoch : 1, Step : 1714, Training Loss : 0.42913, Training Acc : 0.794, Run Time : 1.05
INFO:root:2019-05-12 03:20:31, Epoch : 1, Step : 1715, Training Loss : 0.14202, Training Acc : 0.956, Run Time : 1.44
INFO:root:2019-05-12 03:20:41, Epoch : 1, Step : 1716, Training Loss : 0.33330, Training Acc : 0.900, Run Time : 9.69
INFO:root:2019-05-12 03:20:42, Epoch : 1, Step : 1717, Training Loss : 0.24828, Training Acc : 0.889, Run Time : 0.81
INFO:root:2019-05-12 03:20:43, Epoch : 1, Step : 1718, Training Loss : 0.16063, Training Acc : 0.933, Run Time : 1.18
INFO:root:2019-05-12 03:20:54, Epoch : 1, Step : 1719, Training Loss : 0.19143, Training Acc : 0.933, Run Time : 11.72
INFO:root:2019-05-12 03:20:55, Epoch : 1, Step : 1720, Training Loss : 0.15392, Training Acc : 0.944, Run Time : 0.43
INFO:root:2019-05-12 03:20:57, Epoch : 1, Step : 1721, Training Loss : 0.14328, Training Acc : 0.939, Run Time : 1.71
INFO:root:2019-05-12 03:21:18, Epoch : 1, Step : 1722, Training Loss : 0.11784, Training Acc : 0.972, Run Time : 21.49
INFO:root:2019-05-12 03:21:23, Epoch : 1, Step : 1723, Training Loss : 0.22962, Training Acc : 0.922, Run Time : 5.36
INFO:root:2019-05-12 03:21:24, Epoch : 1, Step : 1724, Training Loss : 0.15403, Training Acc : 0.944, Run Time : 0.60
INFO:root:2019-05-12 03:21:34, Epoch : 1, Step : 1725, Training Loss : 0.13262, Training Acc : 0.956, Run Time : 10.42
INFO:root:2019-05-12 03:21:35, Epoch : 1, Step : 1726, Training Loss : 0.13880, Training Acc : 0.939, Run Time : 0.65
INFO:root:2019-05-12 03:21:36, Epoch : 1, Step : 1727, Training Loss : 0.16442, Training Acc : 0.917, Run Time : 1.32
INFO:root:2019-05-12 03:21:50, Epoch : 1, Step : 1728, Training Loss : 0.37126, Training Acc : 0.822, Run Time : 13.73
INFO:root:2019-05-12 03:21:51, Epoch : 1, Step : 1729, Training Loss : 0.13830, Training Acc : 0.967, Run Time : 1.09
INFO:root:2019-05-12 03:21:54, Epoch : 1, Step : 1730, Training Loss : 0.06398, Training Acc : 0.994, Run Time : 2.43
INFO:root:2019-05-12 03:22:03, Epoch : 1, Step : 1731, Training Loss : 0.13798, Training Acc : 0.989, Run Time : 9.52
INFO:root:2019-05-12 03:22:04, Epoch : 1, Step : 1732, Training Loss : 0.16442, Training Acc : 0.944, Run Time : 0.63
INFO:root:2019-05-12 03:22:05, Epoch : 1, Step : 1733, Training Loss : 0.11180, Training Acc : 0.978, Run Time : 1.04
INFO:root:2019-05-12 03:22:14, Epoch : 1, Step : 1734, Training Loss : 0.14727, Training Acc : 0.917, Run Time : 9.35
INFO:root:2019-05-12 03:22:15, Epoch : 1, Step : 1735, Training Loss : 0.30473, Training Acc : 0.867, Run Time : 1.25
INFO:root:2019-05-12 03:22:16, Epoch : 1, Step : 1736, Training Loss : 0.20291, Training Acc : 0.917, Run Time : 0.58
INFO:root:2019-05-12 03:22:17, Epoch : 1, Step : 1737, Training Loss : 0.08376, Training Acc : 0.967, Run Time : 0.77
INFO:root:2019-05-12 03:22:25, Epoch : 1, Step : 1738, Training Loss : 0.16667, Training Acc : 0.956, Run Time : 7.93
INFO:root:2019-05-12 03:22:29, Epoch : 1, Step : 1739, Training Loss : 0.15524, Training Acc : 0.933, Run Time : 4.05
INFO:root:2019-05-12 03:22:29, Epoch : 1, Step : 1740, Training Loss : 0.15401, Training Acc : 0.933, Run Time : 0.52
INFO:root:2019-05-12 03:22:31, Epoch : 1, Step : 1741, Training Loss : 0.19994, Training Acc : 0.922, Run Time : 1.84
INFO:root:2019-05-12 03:22:51, Epoch : 1, Step : 1742, Training Loss : 0.04597, Training Acc : 0.994, Run Time : 19.40
INFO:root:2019-05-12 03:22:57, Epoch : 1, Step : 1743, Training Loss : 0.09481, Training Acc : 0.972, Run Time : 6.25
INFO:root:2019-05-12 03:22:57, Epoch : 1, Step : 1744, Training Loss : 0.17394, Training Acc : 0.939, Run Time : 0.51
INFO:root:2019-05-12 03:23:00, Epoch : 1, Step : 1745, Training Loss : 0.06796, Training Acc : 0.983, Run Time : 2.37
INFO:root:2019-05-12 03:23:10, Epoch : 1, Step : 1746, Training Loss : 0.19874, Training Acc : 0.900, Run Time : 10.04
INFO:root:2019-05-12 03:23:10, Epoch : 1, Step : 1747, Training Loss : 0.17206, Training Acc : 0.922, Run Time : 0.43
INFO:root:2019-05-12 03:23:11, Epoch : 1, Step : 1748, Training Loss : 0.36209, Training Acc : 0.800, Run Time : 0.64
INFO:root:2019-05-12 03:23:21, Epoch : 1, Step : 1749, Training Loss : 0.13017, Training Acc : 0.939, Run Time : 10.56
INFO:root:2019-05-12 03:23:22, Epoch : 1, Step : 1750, Training Loss : 0.88117, Training Acc : 0.711, Run Time : 0.80
INFO:root:2019-05-12 03:23:24, Epoch : 1, Step : 1751, Training Loss : 0.24805, Training Acc : 0.911, Run Time : 1.36
INFO:root:2019-05-12 03:23:35, Epoch : 1, Step : 1752, Training Loss : 0.14198, Training Acc : 0.961, Run Time : 11.13
INFO:root:2019-05-12 03:23:35, Epoch : 1, Step : 1753, Training Loss : 0.14802, Training Acc : 0.939, Run Time : 0.65
INFO:root:2019-05-12 03:23:46, Epoch : 1, Step : 1754, Training Loss : 0.17511, Training Acc : 0.933, Run Time : 10.97
INFO:root:2019-05-12 03:23:47, Epoch : 1, Step : 1755, Training Loss : 0.15387, Training Acc : 0.944, Run Time : 1.18
INFO:root:2019-05-12 03:23:48, Epoch : 1, Step : 1756, Training Loss : 0.12713, Training Acc : 0.944, Run Time : 0.73
INFO:root:2019-05-12 03:23:58, Epoch : 1, Step : 1757, Training Loss : 0.13315, Training Acc : 0.961, Run Time : 9.68
INFO:root:2019-05-12 03:23:58, Epoch : 1, Step : 1758, Training Loss : 0.27031, Training Acc : 0.850, Run Time : 0.57
INFO:root:2019-05-12 03:24:00, Epoch : 1, Step : 1759, Training Loss : 0.28798, Training Acc : 0.828, Run Time : 1.32
INFO:root:2019-05-12 03:24:12, Epoch : 1, Step : 1760, Training Loss : 0.20330, Training Acc : 0.906, Run Time : 12.54
INFO:root:2019-05-12 03:24:13, Epoch : 1, Step : 1761, Training Loss : 0.20137, Training Acc : 0.911, Run Time : 0.55
INFO:root:2019-05-12 03:24:14, Epoch : 1, Step : 1762, Training Loss : 0.09253, Training Acc : 0.972, Run Time : 0.81
INFO:root:2019-05-12 03:24:28, Epoch : 1, Step : 1763, Training Loss : 0.15252, Training Acc : 0.939, Run Time : 14.57
INFO:root:2019-05-12 03:24:39, Epoch : 1, Step : 1764, Training Loss : 0.27995, Training Acc : 0.878, Run Time : 10.86
INFO:root:2019-05-12 03:24:40, Epoch : 1, Step : 1765, Training Loss : 0.21349, Training Acc : 0.922, Run Time : 1.28
INFO:root:2019-05-12 03:24:50, Epoch : 1, Step : 1766, Training Loss : 0.04862, Training Acc : 1.000, Run Time : 9.69
INFO:root:2019-05-12 03:24:51, Epoch : 1, Step : 1767, Training Loss : 0.15779, Training Acc : 0.933, Run Time : 0.69
INFO:root:2019-05-12 03:24:51, Epoch : 1, Step : 1768, Training Loss : 0.11866, Training Acc : 0.944, Run Time : 0.68
INFO:root:2019-05-12 03:25:03, Epoch : 1, Step : 1769, Training Loss : 0.25214, Training Acc : 0.878, Run Time : 11.78
INFO:root:2019-05-12 03:25:12, Epoch : 1, Step : 1770, Training Loss : 0.23325, Training Acc : 0.883, Run Time : 9.27
INFO:root:2019-05-12 03:25:14, Epoch : 1, Step : 1771, Training Loss : 0.17845, Training Acc : 0.911, Run Time : 1.29
INFO:root:2019-05-12 03:25:22, Epoch : 1, Step : 1772, Training Loss : 0.11288, Training Acc : 0.961, Run Time : 8.02
INFO:root:2019-05-12 03:25:25, Epoch : 1, Step : 1773, Training Loss : 0.25500, Training Acc : 0.900, Run Time : 2.95
INFO:root:2019-05-12 03:25:26, Epoch : 1, Step : 1774, Training Loss : 0.20906, Training Acc : 0.922, Run Time : 1.03
INFO:root:2019-05-12 03:25:37, Epoch : 1, Step : 1775, Training Loss : 0.09639, Training Acc : 0.983, Run Time : 10.82
INFO:root:2019-05-12 03:25:37, Epoch : 1, Step : 1776, Training Loss : 0.46534, Training Acc : 0.761, Run Time : 0.85
INFO:root:2019-05-12 03:25:48, Epoch : 1, Step : 1777, Training Loss : 0.09954, Training Acc : 0.983, Run Time : 10.46
INFO:root:2019-05-12 03:25:49, Epoch : 1, Step : 1778, Training Loss : 0.21667, Training Acc : 0.911, Run Time : 0.84
INFO:root:2019-05-12 03:26:02, Epoch : 1, Step : 1779, Training Loss : 0.15663, Training Acc : 0.961, Run Time : 13.14
INFO:root:2019-05-12 03:26:03, Epoch : 1, Step : 1780, Training Loss : 0.17150, Training Acc : 0.950, Run Time : 1.16
INFO:root:2019-05-12 03:26:04, Epoch : 1, Step : 1781, Training Loss : 0.32338, Training Acc : 0.883, Run Time : 0.60
INFO:root:2019-05-12 03:26:14, Epoch : 1, Step : 1782, Training Loss : 0.23900, Training Acc : 0.883, Run Time : 10.84
INFO:root:2019-05-12 03:26:16, Epoch : 1, Step : 1783, Training Loss : 0.24491, Training Acc : 0.878, Run Time : 1.43
INFO:root:2019-05-12 03:26:16, Epoch : 1, Step : 1784, Training Loss : 0.11301, Training Acc : 0.939, Run Time : 0.59
INFO:root:2019-05-12 03:26:28, Epoch : 1, Step : 1785, Training Loss : 0.17253, Training Acc : 0.961, Run Time : 11.50
INFO:root:2019-05-12 03:26:29, Epoch : 1, Step : 1786, Training Loss : 0.25149, Training Acc : 0.900, Run Time : 0.66
INFO:root:2019-05-12 03:26:31, Epoch : 1, Step : 1787, Training Loss : 0.15191, Training Acc : 0.944, Run Time : 1.91
INFO:root:2019-05-12 03:26:45, Epoch : 1, Step : 1788, Training Loss : 0.21585, Training Acc : 0.911, Run Time : 14.37
INFO:root:2019-05-12 03:26:47, Epoch : 1, Step : 1789, Training Loss : 0.16336, Training Acc : 0.944, Run Time : 1.81
INFO:root:2019-05-12 03:26:58, Epoch : 1, Step : 1790, Training Loss : 0.08210, Training Acc : 0.994, Run Time : 11.57
INFO:root:2019-05-12 03:27:00, Epoch : 1, Step : 1791, Training Loss : 0.10169, Training Acc : 0.983, Run Time : 1.51
INFO:root:2019-05-12 03:27:07, Epoch : 1, Step : 1792, Training Loss : 0.09717, Training Acc : 0.989, Run Time : 7.54
INFO:root:2019-05-12 03:27:08, Epoch : 1, Step : 1793, Training Loss : 0.19181, Training Acc : 0.933, Run Time : 1.13
INFO:root:2019-05-12 03:27:21, Epoch : 1, Step : 1794, Training Loss : 0.07565, Training Acc : 0.989, Run Time : 12.66
INFO:root:2019-05-12 03:27:22, Epoch : 1, Step : 1795, Training Loss : 0.19894, Training Acc : 0.944, Run Time : 0.71
INFO:root:2019-05-12 03:27:22, Epoch : 1, Step : 1796, Training Loss : 0.14946, Training Acc : 0.944, Run Time : 0.59
INFO:root:2019-05-12 03:27:33, Epoch : 1, Step : 1797, Training Loss : 0.45645, Training Acc : 0.783, Run Time : 10.24
INFO:root:2019-05-12 03:27:33, Epoch : 1, Step : 1798, Training Loss : 0.23905, Training Acc : 0.906, Run Time : 0.77
INFO:root:2019-05-12 03:27:45, Epoch : 1, Step : 1799, Training Loss : 0.18491, Training Acc : 0.911, Run Time : 11.50
INFO:root:2019-05-12 03:27:46, Epoch : 1, Step : 1800, Training Loss : 0.49883, Training Acc : 0.850, Run Time : 0.61
INFO:root:2019-05-12 03:28:00, Epoch : 1, Step : 1801, Training Loss : 0.89633, Training Acc : 0.656, Run Time : 14.23
INFO:root:2019-05-12 03:28:02, Epoch : 1, Step : 1802, Training Loss : 0.80170, Training Acc : 0.672, Run Time : 1.81
INFO:root:2019-05-12 03:28:02, Epoch : 1, Step : 1803, Training Loss : 0.71621, Training Acc : 0.761, Run Time : 0.59
INFO:root:2019-05-12 03:28:03, Epoch : 1, Step : 1804, Training Loss : 0.66503, Training Acc : 0.700, Run Time : 0.77
INFO:root:2019-05-12 03:28:04, Epoch : 1, Step : 1805, Training Loss : 0.74425, Training Acc : 0.706, Run Time : 0.70
INFO:root:2019-05-12 03:28:17, Epoch : 1, Step : 1806, Training Loss : 0.68013, Training Acc : 0.733, Run Time : 13.02
INFO:root:2019-05-12 03:28:17, Epoch : 1, Step : 1807, Training Loss : 0.46716, Training Acc : 0.783, Run Time : 0.64
INFO:root:2019-05-12 03:28:19, Epoch : 1, Step : 1808, Training Loss : 0.33060, Training Acc : 0.856, Run Time : 1.66
INFO:root:2019-05-12 03:28:29, Epoch : 1, Step : 1809, Training Loss : 0.31839, Training Acc : 0.867, Run Time : 10.37
INFO:root:2019-05-12 03:28:30, Epoch : 1, Step : 1810, Training Loss : 0.18450, Training Acc : 0.911, Run Time : 0.45
INFO:root:2019-05-12 03:28:32, Epoch : 1, Step : 1811, Training Loss : 0.16341, Training Acc : 0.917, Run Time : 1.91
INFO:root:2019-05-12 03:28:43, Epoch : 1, Step : 1812, Training Loss : 0.23579, Training Acc : 0.906, Run Time : 11.49
INFO:root:2019-05-12 03:28:44, Epoch : 1, Step : 1813, Training Loss : 0.21479, Training Acc : 0.922, Run Time : 0.47
INFO:root:2019-05-12 03:28:45, Epoch : 1, Step : 1814, Training Loss : 0.17813, Training Acc : 0.928, Run Time : 1.77
INFO:root:2019-05-12 03:28:58, Epoch : 1, Step : 1815, Training Loss : 0.22338, Training Acc : 0.911, Run Time : 12.56
INFO:root:2019-05-12 03:28:59, Epoch : 1, Step : 1816, Training Loss : 0.22541, Training Acc : 0.928, Run Time : 0.53
INFO:root:2019-05-12 03:29:00, Epoch : 1, Step : 1817, Training Loss : 0.20339, Training Acc : 0.917, Run Time : 1.75
INFO:root:2019-05-12 03:29:11, Epoch : 1, Step : 1818, Training Loss : 0.22043, Training Acc : 0.917, Run Time : 10.27
INFO:root:2019-05-12 03:29:11, Epoch : 1, Step : 1819, Training Loss : 0.22322, Training Acc : 0.922, Run Time : 0.79
INFO:root:2019-05-12 03:29:13, Epoch : 1, Step : 1820, Training Loss : 0.27000, Training Acc : 0.917, Run Time : 1.28
INFO:root:2019-05-12 03:29:29, Epoch : 1, Step : 1821, Training Loss : 0.23904, Training Acc : 0.917, Run Time : 16.25
INFO:root:2019-05-12 03:29:46, Epoch : 1, Step : 1822, Training Loss : 0.24780, Training Acc : 0.917, Run Time : 16.68
INFO:root:2019-05-12 03:29:51, Epoch : 1, Step : 1823, Training Loss : 0.22199, Training Acc : 0.922, Run Time : 5.00
INFO:root:2019-05-12 03:29:51, Epoch : 1, Step : 1824, Training Loss : 0.18096, Training Acc : 0.928, Run Time : 0.61
INFO:root:2019-05-12 03:29:53, Epoch : 1, Step : 1825, Training Loss : 0.14136, Training Acc : 0.939, Run Time : 1.86
INFO:root:2019-05-12 03:30:06, Epoch : 1, Step : 1826, Training Loss : 0.20561, Training Acc : 0.917, Run Time : 13.05
INFO:root:2019-05-12 03:30:07, Epoch : 1, Step : 1827, Training Loss : 0.15384, Training Acc : 0.933, Run Time : 0.83
INFO:root:2019-05-12 03:30:28, Epoch : 1, Step : 1828, Training Loss : 0.17717, Training Acc : 0.928, Run Time : 20.67
INFO:root:2019-05-12 03:30:30, Epoch : 1, Step : 1829, Training Loss : 0.15723, Training Acc : 0.939, Run Time : 2.20
INFO:root:2019-05-12 03:30:31, Epoch : 1, Step : 1830, Training Loss : 0.09391, Training Acc : 0.939, Run Time : 1.56
INFO:root:2019-05-12 03:30:43, Epoch : 1, Step : 1831, Training Loss : 0.10065, Training Acc : 0.944, Run Time : 12.05
INFO:root:2019-05-12 03:30:44, Epoch : 1, Step : 1832, Training Loss : 0.09128, Training Acc : 0.950, Run Time : 0.66
INFO:root:2019-05-12 03:30:45, Epoch : 1, Step : 1833, Training Loss : 0.07985, Training Acc : 0.967, Run Time : 0.56
INFO:root:2019-05-12 03:30:59, Epoch : 1, Step : 1834, Training Loss : 0.08150, Training Acc : 0.967, Run Time : 14.42
INFO:root:2019-05-12 03:30:59, Epoch : 1, Step : 1835, Training Loss : 0.10369, Training Acc : 0.939, Run Time : 0.44
INFO:root:2019-05-12 03:31:00, Epoch : 1, Step : 1836, Training Loss : 0.11202, Training Acc : 0.944, Run Time : 0.63
INFO:root:2019-05-12 03:31:02, Epoch : 1, Step : 1837, Training Loss : 0.12935, Training Acc : 0.939, Run Time : 1.64
INFO:root:2019-05-12 03:31:12, Epoch : 1, Step : 1838, Training Loss : 0.12008, Training Acc : 0.939, Run Time : 10.08
INFO:root:2019-05-12 03:31:13, Epoch : 1, Step : 1839, Training Loss : 0.20935, Training Acc : 0.894, Run Time : 1.23
INFO:root:2019-05-12 03:31:26, Epoch : 1, Step : 1840, Training Loss : 0.31059, Training Acc : 0.872, Run Time : 13.28
INFO:root:2019-05-12 03:31:27, Epoch : 1, Step : 1841, Training Loss : 0.41802, Training Acc : 0.822, Run Time : 0.50
INFO:root:2019-05-12 03:31:28, Epoch : 1, Step : 1842, Training Loss : 0.33759, Training Acc : 0.878, Run Time : 0.82
INFO:root:2019-05-12 03:31:47, Epoch : 1, Step : 1843, Training Loss : 0.19581, Training Acc : 0.917, Run Time : 19.15
INFO:root:2019-05-12 03:31:51, Epoch : 1, Step : 1844, Training Loss : 0.23161, Training Acc : 0.883, Run Time : 3.97
INFO:root:2019-05-12 03:32:03, Epoch : 1, Step : 1845, Training Loss : 0.16647, Training Acc : 0.911, Run Time : 12.70
INFO:root:2019-05-12 03:32:05, Epoch : 1, Step : 1846, Training Loss : 0.31048, Training Acc : 0.844, Run Time : 1.39
INFO:root:2019-05-12 03:32:16, Epoch : 1, Step : 1847, Training Loss : 0.19044, Training Acc : 0.911, Run Time : 11.07
INFO:root:2019-05-12 03:32:17, Epoch : 1, Step : 1848, Training Loss : 0.20333, Training Acc : 0.889, Run Time : 1.41
INFO:root:2019-05-12 03:32:28, Epoch : 1, Step : 1849, Training Loss : 0.16304, Training Acc : 0.928, Run Time : 11.16
INFO:root:2019-05-12 03:32:29, Epoch : 1, Step : 1850, Training Loss : 0.18156, Training Acc : 0.900, Run Time : 0.56
INFO:root:2019-05-12 03:32:31, Epoch : 1, Step : 1851, Training Loss : 0.11382, Training Acc : 0.967, Run Time : 1.66
INFO:root:2019-05-12 03:32:42, Epoch : 1, Step : 1852, Training Loss : 0.22985, Training Acc : 0.889, Run Time : 11.27
INFO:root:2019-05-12 03:32:56, Epoch : 1, Step : 1853, Training Loss : 0.11161, Training Acc : 0.967, Run Time : 14.35
INFO:root:2019-05-12 03:32:57, Epoch : 1, Step : 1854, Training Loss : 0.17429, Training Acc : 0.917, Run Time : 1.02
INFO:root:2019-05-12 03:32:59, Epoch : 1, Step : 1855, Training Loss : 0.22655, Training Acc : 0.911, Run Time : 1.63
INFO:root:2019-05-12 03:33:11, Epoch : 1, Step : 1856, Training Loss : 0.16874, Training Acc : 0.911, Run Time : 12.33
INFO:root:2019-05-12 03:33:12, Epoch : 1, Step : 1857, Training Loss : 0.19751, Training Acc : 0.922, Run Time : 0.49
INFO:root:2019-05-12 03:33:12, Epoch : 1, Step : 1858, Training Loss : 0.25722, Training Acc : 0.872, Run Time : 0.67
INFO:root:2019-05-12 03:33:13, Epoch : 1, Step : 1859, Training Loss : 0.18469, Training Acc : 0.900, Run Time : 0.61
INFO:root:2019-05-12 03:33:26, Epoch : 1, Step : 1860, Training Loss : 0.21383, Training Acc : 0.889, Run Time : 13.16
INFO:root:2019-05-12 03:33:27, Epoch : 1, Step : 1861, Training Loss : 0.42392, Training Acc : 0.828, Run Time : 0.64
INFO:root:2019-05-12 03:33:28, Epoch : 1, Step : 1862, Training Loss : 0.45034, Training Acc : 0.789, Run Time : 0.65
INFO:root:2019-05-12 03:33:39, Epoch : 1, Step : 1863, Training Loss : 0.47656, Training Acc : 0.822, Run Time : 11.71
INFO:root:2019-05-12 03:33:40, Epoch : 1, Step : 1864, Training Loss : 0.54620, Training Acc : 0.789, Run Time : 0.93
INFO:root:2019-05-12 03:33:42, Epoch : 1, Step : 1865, Training Loss : 0.57901, Training Acc : 0.789, Run Time : 1.57
INFO:root:2019-05-12 03:33:53, Epoch : 1, Step : 1866, Training Loss : 0.42566, Training Acc : 0.822, Run Time : 10.98
INFO:root:2019-05-12 03:33:53, Epoch : 1, Step : 1867, Training Loss : 0.62539, Training Acc : 0.767, Run Time : 0.51
INFO:root:2019-05-12 03:33:54, Epoch : 1, Step : 1868, Training Loss : 0.38453, Training Acc : 0.844, Run Time : 1.01
INFO:root:2019-05-12 03:34:05, Epoch : 1, Step : 1869, Training Loss : 0.33890, Training Acc : 0.833, Run Time : 10.74
INFO:root:2019-05-12 03:34:05, Epoch : 1, Step : 1870, Training Loss : 0.26852, Training Acc : 0.878, Run Time : 0.47
INFO:root:2019-05-12 03:34:06, Epoch : 1, Step : 1871, Training Loss : 0.23018, Training Acc : 0.883, Run Time : 0.60
INFO:root:2019-05-12 03:34:18, Epoch : 1, Step : 1872, Training Loss : 0.44127, Training Acc : 0.828, Run Time : 11.65
INFO:root:2019-05-12 03:34:18, Epoch : 1, Step : 1873, Training Loss : 0.46791, Training Acc : 0.828, Run Time : 0.68
INFO:root:2019-05-12 03:34:19, Epoch : 1, Step : 1874, Training Loss : 0.40351, Training Acc : 0.822, Run Time : 0.75
INFO:root:2019-05-12 03:34:36, Epoch : 1, Step : 1875, Training Loss : 0.41608, Training Acc : 0.856, Run Time : 16.99
INFO:root:2019-05-12 03:34:38, Epoch : 1, Step : 1876, Training Loss : 0.43763, Training Acc : 0.850, Run Time : 1.71
INFO:root:2019-05-12 03:34:38, Epoch : 1, Step : 1877, Training Loss : 0.28659, Training Acc : 0.889, Run Time : 0.61
INFO:root:2019-05-12 03:34:40, Epoch : 1, Step : 1878, Training Loss : 0.30514, Training Acc : 0.867, Run Time : 1.35
INFO:root:2019-05-12 03:34:53, Epoch : 1, Step : 1879, Training Loss : 0.25990, Training Acc : 0.900, Run Time : 13.66
INFO:root:2019-05-12 03:34:56, Epoch : 1, Step : 1880, Training Loss : 0.37598, Training Acc : 0.856, Run Time : 2.55
INFO:root:2019-05-12 03:35:06, Epoch : 1, Step : 1881, Training Loss : 0.37400, Training Acc : 0.867, Run Time : 9.79
INFO:root:2019-05-12 03:35:08, Epoch : 1, Step : 1882, Training Loss : 0.28495, Training Acc : 0.872, Run Time : 1.80
INFO:root:2019-05-12 03:35:19, Epoch : 1, Step : 1883, Training Loss : 0.21472, Training Acc : 0.917, Run Time : 11.04
INFO:root:2019-05-12 03:35:19, Epoch : 1, Step : 1884, Training Loss : 0.27953, Training Acc : 0.883, Run Time : 0.81
INFO:root:2019-05-12 03:35:20, Epoch : 1, Step : 1885, Training Loss : 0.24445, Training Acc : 0.872, Run Time : 0.53
INFO:root:2019-05-12 03:35:32, Epoch : 1, Step : 1886, Training Loss : 0.24154, Training Acc : 0.900, Run Time : 12.10
INFO:root:2019-05-12 03:35:33, Epoch : 1, Step : 1887, Training Loss : 0.20009, Training Acc : 0.900, Run Time : 0.86
INFO:root:2019-05-12 03:35:42, Epoch : 1, Step : 1888, Training Loss : 0.22410, Training Acc : 0.861, Run Time : 9.21
INFO:root:2019-05-12 03:35:43, Epoch : 1, Step : 1889, Training Loss : 0.18411, Training Acc : 0.933, Run Time : 0.81
INFO:root:2019-05-12 03:36:00, Epoch : 1, Step : 1890, Training Loss : 0.29965, Training Acc : 0.822, Run Time : 16.91
INFO:root:2019-05-12 03:36:06, Epoch : 1, Step : 1891, Training Loss : 0.19270, Training Acc : 0.911, Run Time : 5.91
INFO:root:2019-05-12 03:36:07, Epoch : 1, Step : 1892, Training Loss : 0.17999, Training Acc : 0.944, Run Time : 1.36
INFO:root:2019-05-12 03:36:18, Epoch : 1, Step : 1893, Training Loss : 0.22548, Training Acc : 0.911, Run Time : 11.19
INFO:root:2019-05-12 03:36:19, Epoch : 1, Step : 1894, Training Loss : 0.20754, Training Acc : 0.900, Run Time : 0.44
INFO:root:2019-05-12 03:36:30, Epoch : 1, Step : 1895, Training Loss : 0.22720, Training Acc : 0.872, Run Time : 11.36
INFO:root:2019-05-12 03:36:31, Epoch : 1, Step : 1896, Training Loss : 0.19199, Training Acc : 0.933, Run Time : 0.51
INFO:root:2019-05-12 03:36:31, Epoch : 1, Step : 1897, Training Loss : 0.19118, Training Acc : 0.872, Run Time : 0.75
INFO:root:2019-05-12 03:36:32, Epoch : 1, Step : 1898, Training Loss : 0.18482, Training Acc : 0.922, Run Time : 0.64
INFO:root:2019-05-12 03:36:43, Epoch : 1, Step : 1899, Training Loss : 0.18736, Training Acc : 0.922, Run Time : 11.35
INFO:root:2019-05-12 03:36:44, Epoch : 1, Step : 1900, Training Loss : 0.19065, Training Acc : 0.917, Run Time : 0.71
INFO:root:2019-05-12 03:36:53, Epoch : 1, Step : 1901, Training Loss : 0.15000, Training Acc : 0.944, Run Time : 9.11
INFO:root:2019-05-12 03:36:54, Epoch : 1, Step : 1902, Training Loss : 0.21672, Training Acc : 0.917, Run Time : 0.66
INFO:root:2019-05-12 03:36:55, Epoch : 1, Step : 1903, Training Loss : 0.18300, Training Acc : 0.906, Run Time : 0.99
INFO:root:2019-05-12 03:37:07, Epoch : 1, Step : 1904, Training Loss : 0.15323, Training Acc : 0.950, Run Time : 12.04
INFO:root:2019-05-12 03:37:08, Epoch : 1, Step : 1905, Training Loss : 0.16906, Training Acc : 0.939, Run Time : 0.73
INFO:root:2019-05-12 03:37:22, Epoch : 1, Step : 1906, Training Loss : 0.16990, Training Acc : 0.933, Run Time : 14.02
INFO:root:2019-05-12 03:37:24, Epoch : 1, Step : 1907, Training Loss : 0.22922, Training Acc : 0.883, Run Time : 2.54
INFO:root:2019-05-12 03:37:32, Epoch : 1, Step : 1908, Training Loss : 0.43115, Training Acc : 0.800, Run Time : 7.97
INFO:root:2019-05-12 03:37:34, Epoch : 1, Step : 1909, Training Loss : 0.19413, Training Acc : 0.917, Run Time : 1.76
INFO:root:2019-05-12 03:37:49, Epoch : 1, Step : 1910, Training Loss : 0.40850, Training Acc : 0.839, Run Time : 14.77
INFO:root:2019-05-12 03:37:50, Epoch : 1, Step : 1911, Training Loss : 0.26961, Training Acc : 0.883, Run Time : 1.47
INFO:root:2019-05-12 03:38:00, Epoch : 1, Step : 1912, Training Loss : 0.38283, Training Acc : 0.800, Run Time : 10.15
INFO:root:2019-05-12 03:38:01, Epoch : 1, Step : 1913, Training Loss : 0.23336, Training Acc : 0.889, Run Time : 0.43
INFO:root:2019-05-12 03:38:02, Epoch : 1, Step : 1914, Training Loss : 0.19452, Training Acc : 0.900, Run Time : 1.70
INFO:root:2019-05-12 03:38:14, Epoch : 1, Step : 1915, Training Loss : 0.22063, Training Acc : 0.900, Run Time : 11.65
INFO:root:2019-05-12 03:38:15, Epoch : 1, Step : 1916, Training Loss : 0.17427, Training Acc : 0.933, Run Time : 0.81
INFO:root:2019-05-12 03:38:30, Epoch : 1, Step : 1917, Training Loss : 0.18054, Training Acc : 0.928, Run Time : 14.85
INFO:root:2019-05-12 03:38:32, Epoch : 1, Step : 1918, Training Loss : 0.19036, Training Acc : 0.933, Run Time : 2.13
INFO:root:2019-05-12 03:38:41, Epoch : 1, Step : 1919, Training Loss : 0.20782, Training Acc : 0.906, Run Time : 9.22
INFO:root:2019-05-12 03:38:42, Epoch : 1, Step : 1920, Training Loss : 0.24447, Training Acc : 0.933, Run Time : 1.02
INFO:root:2019-05-12 03:38:44, Epoch : 1, Step : 1921, Training Loss : 0.15999, Training Acc : 0.939, Run Time : 1.61
INFO:root:2019-05-12 03:38:56, Epoch : 1, Step : 1922, Training Loss : 0.19568, Training Acc : 0.922, Run Time : 11.89
INFO:root:2019-05-12 03:38:56, Epoch : 1, Step : 1923, Training Loss : 0.18100, Training Acc : 0.933, Run Time : 0.44
INFO:root:2019-05-12 03:38:59, Epoch : 1, Step : 1924, Training Loss : 0.16361, Training Acc : 0.939, Run Time : 2.86
INFO:root:2019-05-12 03:39:08, Epoch : 1, Step : 1925, Training Loss : 0.19764, Training Acc : 0.928, Run Time : 9.43
INFO:root:2019-05-12 03:39:09, Epoch : 1, Step : 1926, Training Loss : 0.16470, Training Acc : 0.928, Run Time : 0.48
INFO:root:2019-05-12 03:39:11, Epoch : 1, Step : 1927, Training Loss : 0.14638, Training Acc : 0.911, Run Time : 1.76
INFO:root:2019-05-12 03:39:43, Epoch : 1, Step : 1928, Training Loss : 0.19366, Training Acc : 0.928, Run Time : 32.62
INFO:root:2019-05-12 03:39:45, Epoch : 1, Step : 1929, Training Loss : 0.15792, Training Acc : 0.944, Run Time : 2.02
INFO:root:2019-05-12 03:39:47, Epoch : 1, Step : 1930, Training Loss : 0.16136, Training Acc : 0.922, Run Time : 1.50
INFO:root:2019-05-12 03:39:56, Epoch : 1, Step : 1931, Training Loss : 0.21780, Training Acc : 0.906, Run Time : 8.85
INFO:root:2019-05-12 03:39:56, Epoch : 1, Step : 1932, Training Loss : 0.16398, Training Acc : 0.906, Run Time : 0.58
INFO:root:2019-05-12 03:39:58, Epoch : 1, Step : 1933, Training Loss : 0.16269, Training Acc : 0.933, Run Time : 2.23
INFO:root:2019-05-12 03:40:08, Epoch : 1, Step : 1934, Training Loss : 0.12405, Training Acc : 0.978, Run Time : 9.83
INFO:root:2019-05-12 03:40:09, Epoch : 1, Step : 1935, Training Loss : 0.17767, Training Acc : 0.911, Run Time : 0.69
INFO:root:2019-05-12 03:40:11, Epoch : 1, Step : 1936, Training Loss : 0.15635, Training Acc : 0.939, Run Time : 2.07
INFO:root:2019-05-12 03:40:21, Epoch : 1, Step : 1937, Training Loss : 0.15545, Training Acc : 0.917, Run Time : 10.29
INFO:root:2019-05-12 03:40:22, Epoch : 1, Step : 1938, Training Loss : 0.16476, Training Acc : 0.917, Run Time : 0.51
INFO:root:2019-05-12 03:40:23, Epoch : 1, Step : 1939, Training Loss : 0.16483, Training Acc : 0.928, Run Time : 1.74
INFO:root:2019-05-12 03:40:35, Epoch : 1, Step : 1940, Training Loss : 0.14213, Training Acc : 0.939, Run Time : 11.51
INFO:root:2019-05-12 03:40:36, Epoch : 1, Step : 1941, Training Loss : 0.22356, Training Acc : 0.861, Run Time : 0.55
INFO:root:2019-05-12 03:40:38, Epoch : 1, Step : 1942, Training Loss : 0.11711, Training Acc : 0.961, Run Time : 2.21
INFO:root:2019-05-12 03:40:48, Epoch : 1, Step : 1943, Training Loss : 0.10488, Training Acc : 0.967, Run Time : 9.78
INFO:root:2019-05-12 03:40:51, Epoch : 1, Step : 1944, Training Loss : 0.20504, Training Acc : 0.944, Run Time : 3.06
INFO:root:2019-05-12 03:41:01, Epoch : 1, Step : 1945, Training Loss : 0.11796, Training Acc : 0.950, Run Time : 10.00
INFO:root:2019-05-12 03:41:01, Epoch : 1, Step : 1946, Training Loss : 0.13454, Training Acc : 0.956, Run Time : 0.46
INFO:root:2019-05-12 03:41:02, Epoch : 1, Step : 1947, Training Loss : 0.15056, Training Acc : 0.939, Run Time : 0.52
INFO:root:2019-05-12 03:41:16, Epoch : 1, Step : 1948, Training Loss : 0.16463, Training Acc : 0.933, Run Time : 14.94
INFO:root:2019-05-12 03:41:27, Epoch : 1, Step : 1949, Training Loss : 0.21373, Training Acc : 0.894, Run Time : 10.49
INFO:root:2019-05-12 03:41:29, Epoch : 1, Step : 1950, Training Loss : 0.10712, Training Acc : 0.972, Run Time : 2.09
INFO:root:2019-05-12 03:41:40, Epoch : 1, Step : 1951, Training Loss : 0.15132, Training Acc : 0.933, Run Time : 10.92
INFO:root:2019-05-12 03:41:40, Epoch : 1, Step : 1952, Training Loss : 0.12257, Training Acc : 0.978, Run Time : 0.47
INFO:root:2019-05-12 03:41:41, Epoch : 1, Step : 1953, Training Loss : 0.13482, Training Acc : 0.961, Run Time : 0.62
INFO:root:2019-05-12 03:41:55, Epoch : 1, Step : 1954, Training Loss : 0.12963, Training Acc : 0.944, Run Time : 13.66
INFO:root:2019-05-12 03:41:55, Epoch : 1, Step : 1955, Training Loss : 0.13065, Training Acc : 0.950, Run Time : 0.68
INFO:root:2019-05-12 03:41:56, Epoch : 1, Step : 1956, Training Loss : 0.13160, Training Acc : 0.950, Run Time : 0.61
INFO:root:2019-05-12 03:42:08, Epoch : 1, Step : 1957, Training Loss : 0.11405, Training Acc : 0.967, Run Time : 11.79
INFO:root:2019-05-12 03:42:09, Epoch : 1, Step : 1958, Training Loss : 0.22863, Training Acc : 0.894, Run Time : 0.85
INFO:root:2019-05-12 03:42:09, Epoch : 1, Step : 1959, Training Loss : 0.19790, Training Acc : 0.917, Run Time : 0.49
INFO:root:2019-05-12 03:42:10, Epoch : 1, Step : 1960, Training Loss : 0.12627, Training Acc : 0.944, Run Time : 0.62
INFO:root:2019-05-12 03:42:10, Epoch : 1, Step : 1961, Training Loss : 0.12477, Training Acc : 0.944, Run Time : 0.62
INFO:root:2019-05-12 03:42:25, Epoch : 1, Step : 1962, Training Loss : 0.11134, Training Acc : 0.956, Run Time : 14.62
INFO:root:2019-05-12 03:42:26, Epoch : 1, Step : 1963, Training Loss : 0.14079, Training Acc : 0.944, Run Time : 0.53
INFO:root:2019-05-12 03:42:26, Epoch : 1, Step : 1964, Training Loss : 0.12304, Training Acc : 0.961, Run Time : 0.63
INFO:root:2019-05-12 03:42:39, Epoch : 1, Step : 1965, Training Loss : 0.13293, Training Acc : 0.944, Run Time : 12.57
INFO:root:2019-05-12 03:42:40, Epoch : 1, Step : 1966, Training Loss : 0.20623, Training Acc : 0.906, Run Time : 0.88
INFO:root:2019-05-12 03:42:40, Epoch : 1, Step : 1967, Training Loss : 0.16369, Training Acc : 0.944, Run Time : 0.51
INFO:root:2019-05-12 03:42:52, Epoch : 1, Step : 1968, Training Loss : 0.15871, Training Acc : 0.917, Run Time : 11.44
INFO:root:2019-05-12 03:42:55, Epoch : 1, Step : 1969, Training Loss : 0.12349, Training Acc : 0.956, Run Time : 3.20
INFO:root:2019-05-12 03:42:55, Epoch : 1, Step : 1970, Training Loss : 0.14713, Training Acc : 0.933, Run Time : 0.53
INFO:root:2019-05-12 03:43:01, Epoch : 1, Step : 1971, Training Loss : 0.08953, Training Acc : 0.972, Run Time : 6.03
INFO:root:2019-05-12 03:43:02, Epoch : 1, Step : 1972, Training Loss : 0.21123, Training Acc : 0.933, Run Time : 0.87
INFO:root:2019-05-12 03:43:03, Epoch : 1, Step : 1973, Training Loss : 0.13988, Training Acc : 0.939, Run Time : 0.62
INFO:root:2019-05-12 03:43:15, Epoch : 1, Step : 1974, Training Loss : 0.13722, Training Acc : 0.939, Run Time : 11.92
INFO:root:2019-05-12 03:43:15, Epoch : 1, Step : 1975, Training Loss : 0.10468, Training Acc : 0.956, Run Time : 0.45
INFO:root:2019-05-12 03:43:17, Epoch : 1, Step : 1976, Training Loss : 0.18389, Training Acc : 0.917, Run Time : 1.62
INFO:root:2019-05-12 03:43:29, Epoch : 1, Step : 1977, Training Loss : 0.14454, Training Acc : 0.939, Run Time : 12.00
INFO:root:2019-05-12 03:43:30, Epoch : 1, Step : 1978, Training Loss : 0.12218, Training Acc : 0.944, Run Time : 1.07
INFO:root:2019-05-12 03:43:32, Epoch : 1, Step : 1979, Training Loss : 0.16472, Training Acc : 0.928, Run Time : 2.05
INFO:root:2019-05-12 03:43:44, Epoch : 1, Step : 1980, Training Loss : 0.13066, Training Acc : 0.961, Run Time : 11.68
INFO:root:2019-05-12 03:43:45, Epoch : 1, Step : 1981, Training Loss : 0.12626, Training Acc : 0.950, Run Time : 1.48
INFO:root:2019-05-12 03:43:46, Epoch : 1, Step : 1982, Training Loss : 0.11116, Training Acc : 0.978, Run Time : 0.86
INFO:root:2019-05-12 03:43:56, Epoch : 1, Step : 1983, Training Loss : 0.08078, Training Acc : 0.989, Run Time : 9.63
INFO:root:2019-05-12 03:43:56, Epoch : 1, Step : 1984, Training Loss : 0.11121, Training Acc : 0.950, Run Time : 0.82
INFO:root:2019-05-12 03:43:58, Epoch : 1, Step : 1985, Training Loss : 0.06946, Training Acc : 0.983, Run Time : 1.38
INFO:root:2019-05-12 03:44:09, Epoch : 1, Step : 1986, Training Loss : 0.06414, Training Acc : 1.000, Run Time : 10.79
INFO:root:2019-05-12 03:44:10, Epoch : 1, Step : 1987, Training Loss : 0.07633, Training Acc : 0.978, Run Time : 0.96
INFO:root:2019-05-12 03:44:10, Epoch : 1, Step : 1988, Training Loss : 0.10426, Training Acc : 0.961, Run Time : 0.59
INFO:root:2019-05-12 03:44:11, Epoch : 1, Step : 1989, Training Loss : 0.06685, Training Acc : 0.994, Run Time : 1.17
INFO:root:2019-05-12 03:44:24, Epoch : 1, Step : 1990, Training Loss : 0.06493, Training Acc : 0.978, Run Time : 12.56
INFO:root:2019-05-12 03:44:24, Epoch : 1, Step : 1991, Training Loss : 0.08395, Training Acc : 0.967, Run Time : 0.62
INFO:root:2019-05-12 03:44:30, Epoch : 1, Step : 1992, Training Loss : 0.07284, Training Acc : 0.994, Run Time : 5.55
INFO:root:2019-05-12 03:44:38, Epoch : 1, Step : 1993, Training Loss : 0.07713, Training Acc : 0.983, Run Time : 7.82
INFO:root:2019-05-12 03:44:55, Epoch : 1, Step : 1994, Training Loss : 0.07498, Training Acc : 0.978, Run Time : 17.34
INFO:root:2019-05-12 03:44:57, Epoch : 1, Step : 1995, Training Loss : 0.09768, Training Acc : 0.972, Run Time : 1.70
INFO:root:2019-05-12 03:45:07, Epoch : 1, Step : 1996, Training Loss : 0.05372, Training Acc : 0.983, Run Time : 10.59
INFO:root:2019-05-12 03:45:08, Epoch : 1, Step : 1997, Training Loss : 0.09199, Training Acc : 0.961, Run Time : 0.69
INFO:root:2019-05-12 03:45:09, Epoch : 1, Step : 1998, Training Loss : 0.07732, Training Acc : 0.961, Run Time : 0.43
INFO:root:2019-05-12 03:45:10, Epoch : 1, Step : 1999, Training Loss : 0.05892, Training Acc : 0.978, Run Time : 1.52
INFO:root:2019-05-12 03:45:20, Epoch : 1, Step : 2000, Training Loss : 0.11761, Training Acc : 0.944, Run Time : 10.03
INFO:root:2019-05-12 03:45:22, Epoch : 1, Step : 2001, Training Loss : 1.00900, Training Acc : 0.672, Run Time : 2.31
INFO:root:2019-05-12 03:45:34, Epoch : 1, Step : 2002, Training Loss : 1.33123, Training Acc : 0.600, Run Time : 11.31
INFO:root:2019-05-12 03:45:35, Epoch : 1, Step : 2003, Training Loss : 1.37764, Training Acc : 0.611, Run Time : 1.51
INFO:root:2019-05-12 03:45:47, Epoch : 1, Step : 2004, Training Loss : 1.37396, Training Acc : 0.594, Run Time : 11.50
INFO:root:2019-05-12 03:45:47, Epoch : 1, Step : 2005, Training Loss : 0.95361, Training Acc : 0.672, Run Time : 0.66
INFO:root:2019-05-12 03:45:49, Epoch : 1, Step : 2006, Training Loss : 0.95746, Training Acc : 0.683, Run Time : 1.95
INFO:root:2019-05-12 03:46:02, Epoch : 1, Step : 2007, Training Loss : 0.68571, Training Acc : 0.733, Run Time : 12.87
INFO:root:2019-05-12 03:46:03, Epoch : 1, Step : 2008, Training Loss : 0.53325, Training Acc : 0.772, Run Time : 1.01
INFO:root:2019-05-12 03:46:04, Epoch : 1, Step : 2009, Training Loss : 0.49647, Training Acc : 0.767, Run Time : 0.58
INFO:root:2019-05-12 03:46:06, Epoch : 1, Step : 2010, Training Loss : 0.33131, Training Acc : 0.844, Run Time : 1.84
INFO:root:2019-05-12 03:46:16, Epoch : 1, Step : 2011, Training Loss : 0.29543, Training Acc : 0.833, Run Time : 10.34
INFO:root:2019-05-12 03:46:17, Epoch : 1, Step : 2012, Training Loss : 0.23276, Training Acc : 0.911, Run Time : 0.57
INFO:root:2019-05-12 03:46:33, Epoch : 1, Step : 2013, Training Loss : 0.11923, Training Acc : 0.961, Run Time : 15.90
INFO:root:2019-05-12 03:46:40, Epoch : 1, Step : 2014, Training Loss : 0.15791, Training Acc : 0.933, Run Time : 7.97
INFO:root:2019-05-12 03:46:54, Epoch : 1, Step : 2015, Training Loss : 0.27928, Training Acc : 0.850, Run Time : 13.88
INFO:root:2019-05-12 03:46:56, Epoch : 1, Step : 2016, Training Loss : 0.13353, Training Acc : 0.956, Run Time : 1.84
INFO:root:2019-05-12 03:46:57, Epoch : 1, Step : 2017, Training Loss : 0.16133, Training Acc : 0.944, Run Time : 0.53
INFO:root:2019-05-12 03:46:58, Epoch : 1, Step : 2018, Training Loss : 0.26514, Training Acc : 0.922, Run Time : 1.72
INFO:root:2019-05-12 03:47:10, Epoch : 1, Step : 2019, Training Loss : 0.16212, Training Acc : 0.972, Run Time : 11.96
INFO:root:2019-05-12 03:47:12, Epoch : 1, Step : 2020, Training Loss : 0.53595, Training Acc : 0.789, Run Time : 1.43
INFO:root:2019-05-12 03:47:16, Epoch : 1, Step : 2021, Training Loss : 0.69484, Training Acc : 0.706, Run Time : 3.70
INFO:root:2019-05-12 03:47:18, Epoch : 1, Step : 2022, Training Loss : 0.77077, Training Acc : 0.733, Run Time : 2.27
INFO:root:2019-05-12 03:47:19, Epoch : 1, Step : 2023, Training Loss : 0.53604, Training Acc : 0.778, Run Time : 0.79
INFO:root:2019-05-12 03:47:29, Epoch : 1, Step : 2024, Training Loss : 0.43834, Training Acc : 0.806, Run Time : 10.57
INFO:root:2019-05-12 03:47:30, Epoch : 1, Step : 2025, Training Loss : 0.64556, Training Acc : 0.722, Run Time : 1.04
INFO:root:2019-05-12 03:47:42, Epoch : 1, Step : 2026, Training Loss : 0.50084, Training Acc : 0.778, Run Time : 11.80
INFO:root:2019-05-12 03:47:43, Epoch : 1, Step : 2027, Training Loss : 0.29691, Training Acc : 0.889, Run Time : 0.69
INFO:root:2019-05-12 03:47:43, Epoch : 1, Step : 2028, Training Loss : 0.36650, Training Acc : 0.844, Run Time : 0.61
INFO:root:2019-05-12 03:47:44, Epoch : 1, Step : 2029, Training Loss : 0.29828, Training Acc : 0.906, Run Time : 0.67
INFO:root:2019-05-12 03:47:45, Epoch : 1, Step : 2030, Training Loss : 0.37668, Training Acc : 0.856, Run Time : 0.57
INFO:root:2019-05-12 03:48:00, Epoch : 1, Step : 2031, Training Loss : 0.28962, Training Acc : 0.889, Run Time : 15.52
INFO:root:2019-05-12 03:48:01, Epoch : 1, Step : 2032, Training Loss : 0.23994, Training Acc : 0.883, Run Time : 0.54
INFO:root:2019-05-12 03:48:03, Epoch : 1, Step : 2033, Training Loss : 0.22151, Training Acc : 0.900, Run Time : 2.04
INFO:root:2019-05-12 03:48:14, Epoch : 1, Step : 2034, Training Loss : 0.34735, Training Acc : 0.861, Run Time : 11.26
INFO:root:2019-05-12 03:48:15, Epoch : 1, Step : 2035, Training Loss : 0.30780, Training Acc : 0.856, Run Time : 0.69
INFO:root:2019-05-12 03:48:15, Epoch : 1, Step : 2036, Training Loss : 0.23826, Training Acc : 0.922, Run Time : 0.55
INFO:root:2019-05-12 03:48:31, Epoch : 1, Step : 2037, Training Loss : 0.24116, Training Acc : 0.917, Run Time : 15.94
INFO:root:2019-05-12 03:48:32, Epoch : 1, Step : 2038, Training Loss : 0.39844, Training Acc : 0.811, Run Time : 1.34
INFO:root:2019-05-12 03:48:43, Epoch : 1, Step : 2039, Training Loss : 0.56878, Training Acc : 0.800, Run Time : 11.07
INFO:root:2019-05-12 03:48:44, Epoch : 1, Step : 2040, Training Loss : 0.20838, Training Acc : 0.894, Run Time : 0.82
INFO:root:2019-05-12 03:48:45, Epoch : 1, Step : 2041, Training Loss : 0.44168, Training Acc : 0.856, Run Time : 0.62
INFO:root:2019-05-12 03:48:46, Epoch : 1, Step : 2042, Training Loss : 0.21017, Training Acc : 0.911, Run Time : 0.65
INFO:root:2019-05-12 03:48:46, Epoch : 1, Step : 2043, Training Loss : 0.32313, Training Acc : 0.889, Run Time : 0.62
INFO:root:2019-05-12 03:49:03, Epoch : 1, Step : 2044, Training Loss : 0.38343, Training Acc : 0.800, Run Time : 16.43
INFO:root:2019-05-12 03:49:03, Epoch : 1, Step : 2045, Training Loss : 0.30621, Training Acc : 0.872, Run Time : 0.53
INFO:root:2019-05-12 03:49:04, Epoch : 1, Step : 2046, Training Loss : 0.41547, Training Acc : 0.844, Run Time : 0.65
INFO:root:2019-05-12 03:49:19, Epoch : 1, Step : 2047, Training Loss : 0.41395, Training Acc : 0.811, Run Time : 15.02
INFO:root:2019-05-12 03:49:20, Epoch : 1, Step : 2048, Training Loss : 0.28161, Training Acc : 0.894, Run Time : 1.04
INFO:root:2019-05-12 03:49:21, Epoch : 1, Step : 2049, Training Loss : 0.43205, Training Acc : 0.833, Run Time : 1.57
INFO:root:2019-05-12 03:49:33, Epoch : 1, Step : 2050, Training Loss : 0.63772, Training Acc : 0.750, Run Time : 12.01
INFO:root:2019-05-12 03:49:34, Epoch : 1, Step : 2051, Training Loss : 0.30865, Training Acc : 0.883, Run Time : 1.05
INFO:root:2019-05-12 03:49:52, Epoch : 1, Step : 2052, Training Loss : 0.42032, Training Acc : 0.850, Run Time : 17.85
INFO:root:2019-05-12 03:50:00, Epoch : 1, Step : 2053, Training Loss : 0.23782, Training Acc : 0.867, Run Time : 8.02
INFO:root:2019-05-12 03:50:01, Epoch : 1, Step : 2054, Training Loss : 0.48323, Training Acc : 0.844, Run Time : 0.65
INFO:root:2019-05-12 03:50:02, Epoch : 1, Step : 2055, Training Loss : 0.49648, Training Acc : 0.861, Run Time : 1.16
INFO:root:2019-05-12 03:50:23, Epoch : 1, Step : 2056, Training Loss : 0.33299, Training Acc : 0.900, Run Time : 20.78
INFO:root:2019-05-12 03:50:24, Epoch : 1, Step : 2057, Training Loss : 0.30558, Training Acc : 0.856, Run Time : 1.44
INFO:root:2019-05-12 03:50:25, Epoch : 1, Step : 2058, Training Loss : 0.46301, Training Acc : 0.828, Run Time : 0.63
INFO:root:2019-05-12 03:50:37, Epoch : 1, Step : 2059, Training Loss : 0.30492, Training Acc : 0.883, Run Time : 12.32
INFO:root:2019-05-12 03:50:38, Epoch : 1, Step : 2060, Training Loss : 0.27329, Training Acc : 0.917, Run Time : 0.84
INFO:root:2019-05-12 03:50:43, Epoch : 1, Step : 2061, Training Loss : 0.22663, Training Acc : 0.933, Run Time : 4.50
INFO:root:2019-05-12 03:50:47, Epoch : 1, Step : 2062, Training Loss : 0.24038, Training Acc : 0.906, Run Time : 4.06
INFO:root:2019-05-12 03:50:47, Epoch : 1, Step : 2063, Training Loss : 0.44055, Training Acc : 0.833, Run Time : 0.59
INFO:root:2019-05-12 03:51:01, Epoch : 1, Step : 2064, Training Loss : 0.24703, Training Acc : 0.906, Run Time : 13.18
INFO:root:2019-05-12 03:51:01, Epoch : 1, Step : 2065, Training Loss : 0.31137, Training Acc : 0.894, Run Time : 0.42
INFO:root:2019-05-12 03:51:03, Epoch : 1, Step : 2066, Training Loss : 0.29105, Training Acc : 0.894, Run Time : 1.60
INFO:root:2019-05-12 03:51:13, Epoch : 1, Step : 2067, Training Loss : 0.57136, Training Acc : 0.822, Run Time : 10.07
INFO:root:2019-05-12 03:51:13, Epoch : 1, Step : 2068, Training Loss : 0.28364, Training Acc : 0.922, Run Time : 0.50
INFO:root:2019-05-12 03:51:14, Epoch : 1, Step : 2069, Training Loss : 0.58295, Training Acc : 0.822, Run Time : 0.62
INFO:root:2019-05-12 03:51:25, Epoch : 1, Step : 2070, Training Loss : 0.28915, Training Acc : 0.889, Run Time : 11.34
INFO:root:2019-05-12 03:51:26, Epoch : 1, Step : 2071, Training Loss : 0.48398, Training Acc : 0.878, Run Time : 1.39
INFO:root:2019-05-12 03:51:27, Epoch : 1, Step : 2072, Training Loss : 0.29326, Training Acc : 0.900, Run Time : 0.75
INFO:root:2019-05-12 03:51:40, Epoch : 1, Step : 2073, Training Loss : 0.36562, Training Acc : 0.878, Run Time : 13.16
INFO:root:2019-05-12 03:51:41, Epoch : 1, Step : 2074, Training Loss : 0.30838, Training Acc : 0.889, Run Time : 0.90
INFO:root:2019-05-12 03:51:44, Epoch : 1, Step : 2075, Training Loss : 0.38103, Training Acc : 0.900, Run Time : 2.94
INFO:root:2019-05-12 03:51:55, Epoch : 1, Step : 2076, Training Loss : 0.68193, Training Acc : 0.694, Run Time : 10.31
INFO:root:2019-05-12 03:51:55, Epoch : 1, Step : 2077, Training Loss : 1.24369, Training Acc : 0.567, Run Time : 0.55
INFO:root:2019-05-12 03:51:56, Epoch : 1, Step : 2078, Training Loss : 0.73820, Training Acc : 0.717, Run Time : 0.80
INFO:root:2019-05-12 03:52:08, Epoch : 1, Step : 2079, Training Loss : 0.71737, Training Acc : 0.739, Run Time : 12.48
INFO:root:2019-05-12 03:52:10, Epoch : 1, Step : 2080, Training Loss : 0.54485, Training Acc : 0.767, Run Time : 1.39
INFO:root:2019-05-12 03:52:25, Epoch : 1, Step : 2081, Training Loss : 0.54683, Training Acc : 0.767, Run Time : 15.72
INFO:root:2019-05-12 03:52:37, Epoch : 1, Step : 2082, Training Loss : 0.54925, Training Acc : 0.806, Run Time : 11.33
INFO:root:2019-05-12 03:52:38, Epoch : 1, Step : 2083, Training Loss : 0.43349, Training Acc : 0.861, Run Time : 1.20
INFO:root:2019-05-12 03:52:39, Epoch : 1, Step : 2084, Training Loss : 0.50303, Training Acc : 0.783, Run Time : 0.89
INFO:root:2019-05-12 03:53:01, Epoch : 1, Step : 2085, Training Loss : 0.32377, Training Acc : 0.878, Run Time : 21.80
INFO:root:2019-05-12 03:53:05, Epoch : 1, Step : 2086, Training Loss : 0.46763, Training Acc : 0.794, Run Time : 4.06
INFO:root:2019-05-12 03:53:05, Epoch : 1, Step : 2087, Training Loss : 0.38191, Training Acc : 0.817, Run Time : 0.51
INFO:root:2019-05-12 03:53:16, Epoch : 1, Step : 2088, Training Loss : 0.48451, Training Acc : 0.817, Run Time : 11.12
INFO:root:2019-05-12 03:53:17, Epoch : 1, Step : 2089, Training Loss : 0.29989, Training Acc : 0.844, Run Time : 0.64
INFO:root:2019-05-12 03:53:18, Epoch : 1, Step : 2090, Training Loss : 0.43783, Training Acc : 0.806, Run Time : 0.58
INFO:root:2019-05-12 03:53:29, Epoch : 1, Step : 2091, Training Loss : 0.49725, Training Acc : 0.811, Run Time : 11.33
INFO:root:2019-05-12 03:53:30, Epoch : 1, Step : 2092, Training Loss : 0.38867, Training Acc : 0.828, Run Time : 0.91
INFO:root:2019-05-12 03:53:43, Epoch : 1, Step : 2093, Training Loss : 0.52852, Training Acc : 0.772, Run Time : 12.80
INFO:root:2019-05-12 03:53:43, Epoch : 1, Step : 2094, Training Loss : 0.35821, Training Acc : 0.789, Run Time : 0.70
INFO:root:2019-05-12 03:53:54, Epoch : 1, Step : 2095, Training Loss : 0.50061, Training Acc : 0.750, Run Time : 10.33
INFO:root:2019-05-12 03:53:55, Epoch : 1, Step : 2096, Training Loss : 0.38843, Training Acc : 0.817, Run Time : 1.47
INFO:root:2019-05-12 03:53:56, Epoch : 1, Step : 2097, Training Loss : 0.23974, Training Acc : 0.917, Run Time : 0.46
INFO:root:2019-05-12 03:54:06, Epoch : 1, Step : 2098, Training Loss : 0.46020, Training Acc : 0.794, Run Time : 10.71
INFO:root:2019-05-12 03:54:07, Epoch : 1, Step : 2099, Training Loss : 0.33524, Training Acc : 0.833, Run Time : 0.81
INFO:root:2019-05-12 03:54:08, Epoch : 1, Step : 2100, Training Loss : 0.31159, Training Acc : 0.839, Run Time : 0.59
INFO:root:2019-05-12 03:54:22, Epoch : 1, Step : 2101, Training Loss : 0.45621, Training Acc : 0.789, Run Time : 14.43
INFO:root:2019-05-12 03:54:23, Epoch : 1, Step : 2102, Training Loss : 0.34819, Training Acc : 0.850, Run Time : 0.85
INFO:root:2019-05-12 03:54:24, Epoch : 1, Step : 2103, Training Loss : 0.32653, Training Acc : 0.872, Run Time : 0.63
INFO:root:2019-05-12 03:54:25, Epoch : 1, Step : 2104, Training Loss : 0.33332, Training Acc : 0.839, Run Time : 1.08
INFO:root:2019-05-12 03:54:35, Epoch : 1, Step : 2105, Training Loss : 0.28797, Training Acc : 0.889, Run Time : 10.47
INFO:root:2019-05-12 03:54:36, Epoch : 1, Step : 2106, Training Loss : 0.35110, Training Acc : 0.828, Run Time : 0.50
INFO:root:2019-05-12 03:54:46, Epoch : 1, Step : 2107, Training Loss : 0.39335, Training Acc : 0.822, Run Time : 10.35
INFO:root:2019-05-12 03:54:51, Epoch : 1, Step : 2108, Training Loss : 0.36150, Training Acc : 0.822, Run Time : 4.77
INFO:root:2019-05-12 03:54:51, Epoch : 1, Step : 2109, Training Loss : 0.37227, Training Acc : 0.811, Run Time : 0.45
INFO:root:2019-05-12 03:55:02, Epoch : 1, Step : 2110, Training Loss : 0.46736, Training Acc : 0.811, Run Time : 10.33
INFO:root:2019-05-12 03:55:02, Epoch : 1, Step : 2111, Training Loss : 0.57663, Training Acc : 0.778, Run Time : 0.49
INFO:root:2019-05-12 03:55:03, Epoch : 1, Step : 2112, Training Loss : 0.33349, Training Acc : 0.850, Run Time : 0.60
INFO:root:2019-05-12 03:55:03, Epoch : 1, Step : 2113, Training Loss : 0.22205, Training Acc : 0.933, Run Time : 0.60
INFO:root:2019-05-12 03:55:14, Epoch : 1, Step : 2114, Training Loss : 0.29603, Training Acc : 0.839, Run Time : 10.81
INFO:root:2019-05-12 03:55:15, Epoch : 1, Step : 2115, Training Loss : 0.53166, Training Acc : 0.772, Run Time : 1.06
INFO:root:2019-05-12 03:55:16, Epoch : 1, Step : 2116, Training Loss : 0.27515, Training Acc : 0.906, Run Time : 0.61
INFO:root:2019-05-12 03:55:18, Epoch : 1, Step : 2117, Training Loss : 0.37576, Training Acc : 0.856, Run Time : 1.98
INFO:root:2019-05-12 03:55:28, Epoch : 1, Step : 2118, Training Loss : 0.36721, Training Acc : 0.789, Run Time : 10.69
INFO:root:2019-05-12 03:55:29, Epoch : 1, Step : 2119, Training Loss : 0.33945, Training Acc : 0.817, Run Time : 1.01
INFO:root:2019-05-12 03:55:39, Epoch : 1, Step : 2120, Training Loss : 0.42529, Training Acc : 0.828, Run Time : 9.10
INFO:root:2019-05-12 03:55:50, Epoch : 1, Step : 2121, Training Loss : 0.43847, Training Acc : 0.811, Run Time : 11.07
INFO:root:2019-05-12 03:55:50, Epoch : 1, Step : 2122, Training Loss : 0.38203, Training Acc : 0.844, Run Time : 0.68
INFO:root:2019-05-12 03:56:01, Epoch : 1, Step : 2123, Training Loss : 0.50085, Training Acc : 0.744, Run Time : 10.68
INFO:root:2019-05-12 03:56:02, Epoch : 1, Step : 2124, Training Loss : 0.58322, Training Acc : 0.722, Run Time : 0.70
INFO:root:2019-05-12 03:56:02, Epoch : 1, Step : 2125, Training Loss : 0.31346, Training Acc : 0.844, Run Time : 0.62
INFO:root:2019-05-12 03:56:12, Epoch : 1, Step : 2126, Training Loss : 0.38451, Training Acc : 0.828, Run Time : 9.47
INFO:root:2019-05-12 03:56:13, Epoch : 1, Step : 2127, Training Loss : 0.43992, Training Acc : 0.828, Run Time : 0.94
INFO:root:2019-05-12 03:56:24, Epoch : 1, Step : 2128, Training Loss : 0.34146, Training Acc : 0.839, Run Time : 11.58
INFO:root:2019-05-12 03:56:25, Epoch : 1, Step : 2129, Training Loss : 0.50297, Training Acc : 0.711, Run Time : 0.55
INFO:root:2019-05-12 03:56:26, Epoch : 1, Step : 2130, Training Loss : 0.23692, Training Acc : 0.933, Run Time : 0.97
INFO:root:2019-05-12 03:56:36, Epoch : 1, Step : 2131, Training Loss : 0.30750, Training Acc : 0.867, Run Time : 10.65
INFO:root:2019-05-12 03:56:37, Epoch : 1, Step : 2132, Training Loss : 0.17701, Training Acc : 0.967, Run Time : 0.75
INFO:root:2019-05-12 03:56:39, Epoch : 1, Step : 2133, Training Loss : 0.30227, Training Acc : 0.878, Run Time : 1.63
INFO:root:2019-05-12 03:56:49, Epoch : 1, Step : 2134, Training Loss : 0.23906, Training Acc : 0.933, Run Time : 10.04
INFO:root:2019-05-12 03:56:49, Epoch : 1, Step : 2135, Training Loss : 0.26010, Training Acc : 0.900, Run Time : 0.58
INFO:root:2019-05-12 03:57:02, Epoch : 1, Step : 2136, Training Loss : 0.23769, Training Acc : 0.944, Run Time : 12.50
INFO:root:2019-05-12 03:57:03, Epoch : 1, Step : 2137, Training Loss : 0.32091, Training Acc : 0.856, Run Time : 1.18
INFO:root:2019-05-12 03:57:04, Epoch : 1, Step : 2138, Training Loss : 0.27641, Training Acc : 0.911, Run Time : 1.23
INFO:root:2019-05-12 03:57:16, Epoch : 1, Step : 2139, Training Loss : 0.21667, Training Acc : 0.950, Run Time : 11.96
INFO:root:2019-05-12 03:57:17, Epoch : 1, Step : 2140, Training Loss : 0.30451, Training Acc : 0.856, Run Time : 0.63
INFO:root:2019-05-12 03:57:18, Epoch : 1, Step : 2141, Training Loss : 0.23399, Training Acc : 0.922, Run Time : 0.79
INFO:root:2019-05-12 03:57:29, Epoch : 1, Step : 2142, Training Loss : 0.42897, Training Acc : 0.811, Run Time : 10.81
INFO:root:2019-05-12 03:57:29, Epoch : 1, Step : 2143, Training Loss : 0.28677, Training Acc : 0.883, Run Time : 0.88
INFO:root:2019-05-12 03:57:31, Epoch : 1, Step : 2144, Training Loss : 0.28556, Training Acc : 0.833, Run Time : 1.81
INFO:root:2019-05-12 03:57:40, Epoch : 1, Step : 2145, Training Loss : 0.23640, Training Acc : 0.911, Run Time : 9.10
INFO:root:2019-05-12 03:57:41, Epoch : 1, Step : 2146, Training Loss : 0.13473, Training Acc : 0.978, Run Time : 0.53
INFO:root:2019-05-12 03:57:42, Epoch : 1, Step : 2147, Training Loss : 0.26451, Training Acc : 0.906, Run Time : 0.98
INFO:root:2019-05-12 03:57:53, Epoch : 1, Step : 2148, Training Loss : 0.15523, Training Acc : 0.950, Run Time : 11.62
INFO:root:2019-05-12 03:57:54, Epoch : 1, Step : 2149, Training Loss : 0.18531, Training Acc : 0.939, Run Time : 0.82
INFO:root:2019-05-12 03:57:56, Epoch : 1, Step : 2150, Training Loss : 0.12880, Training Acc : 0.950, Run Time : 1.40
INFO:root:2019-05-12 03:58:04, Epoch : 1, Step : 2151, Training Loss : 0.25155, Training Acc : 0.900, Run Time : 8.11
INFO:root:2019-05-12 03:58:05, Epoch : 1, Step : 2152, Training Loss : 0.28551, Training Acc : 0.883, Run Time : 0.92
INFO:root:2019-05-12 03:58:16, Epoch : 1, Step : 2153, Training Loss : 0.16438, Training Acc : 0.950, Run Time : 11.08
INFO:root:2019-05-12 03:58:16, Epoch : 1, Step : 2154, Training Loss : 0.16956, Training Acc : 0.950, Run Time : 0.42
INFO:root:2019-05-12 03:58:18, Epoch : 1, Step : 2155, Training Loss : 0.27535, Training Acc : 0.856, Run Time : 1.68
INFO:root:2019-05-12 03:58:29, Epoch : 1, Step : 2156, Training Loss : 0.17005, Training Acc : 0.944, Run Time : 11.46
INFO:root:2019-05-12 03:58:30, Epoch : 1, Step : 2157, Training Loss : 0.26913, Training Acc : 0.861, Run Time : 1.01
INFO:root:2019-05-12 03:58:42, Epoch : 1, Step : 2158, Training Loss : 0.20277, Training Acc : 0.911, Run Time : 11.57
INFO:root:2019-05-12 03:58:43, Epoch : 1, Step : 2159, Training Loss : 0.24733, Training Acc : 0.906, Run Time : 0.89
INFO:root:2019-05-12 03:58:55, Epoch : 1, Step : 2160, Training Loss : 0.19588, Training Acc : 0.917, Run Time : 12.30
INFO:root:2019-05-12 03:58:56, Epoch : 1, Step : 2161, Training Loss : 0.38442, Training Acc : 0.811, Run Time : 0.48
INFO:root:2019-05-12 03:58:56, Epoch : 1, Step : 2162, Training Loss : 0.24976, Training Acc : 0.889, Run Time : 0.65
INFO:root:2019-05-12 03:59:22, Epoch : 1, Step : 2163, Training Loss : 0.11809, Training Acc : 0.972, Run Time : 25.50
INFO:root:2019-05-12 03:59:31, Epoch : 1, Step : 2164, Training Loss : 0.22735, Training Acc : 0.928, Run Time : 9.53
INFO:root:2019-05-12 03:59:32, Epoch : 1, Step : 2165, Training Loss : 0.33842, Training Acc : 0.867, Run Time : 0.84
INFO:root:2019-05-12 03:59:34, Epoch : 1, Step : 2166, Training Loss : 0.15895, Training Acc : 0.944, Run Time : 1.66
INFO:root:2019-05-12 03:59:44, Epoch : 1, Step : 2167, Training Loss : 0.42148, Training Acc : 0.867, Run Time : 10.23
INFO:root:2019-05-12 03:59:44, Epoch : 1, Step : 2168, Training Loss : 0.11481, Training Acc : 0.989, Run Time : 0.48
INFO:root:2019-05-12 03:59:45, Epoch : 1, Step : 2169, Training Loss : 0.09376, Training Acc : 0.983, Run Time : 0.84
INFO:root:2019-05-12 03:59:57, Epoch : 1, Step : 2170, Training Loss : 0.07689, Training Acc : 0.978, Run Time : 11.63
INFO:root:2019-05-12 03:59:58, Epoch : 1, Step : 2171, Training Loss : 0.30154, Training Acc : 0.900, Run Time : 0.83
INFO:root:2019-05-12 03:59:59, Epoch : 1, Step : 2172, Training Loss : 0.20409, Training Acc : 0.900, Run Time : 0.87
INFO:root:2019-05-12 04:00:10, Epoch : 1, Step : 2173, Training Loss : 0.30365, Training Acc : 0.872, Run Time : 11.31
INFO:root:2019-05-12 04:00:10, Epoch : 1, Step : 2174, Training Loss : 0.13905, Training Acc : 0.961, Run Time : 0.54
INFO:root:2019-05-12 04:00:12, Epoch : 1, Step : 2175, Training Loss : 0.14831, Training Acc : 0.939, Run Time : 1.19
INFO:root:2019-05-12 04:00:22, Epoch : 1, Step : 2176, Training Loss : 0.20672, Training Acc : 0.922, Run Time : 10.69
INFO:root:2019-05-12 04:00:23, Epoch : 1, Step : 2177, Training Loss : 0.09582, Training Acc : 0.989, Run Time : 0.68
INFO:root:2019-05-12 04:00:31, Epoch : 1, Step : 2178, Training Loss : 0.07074, Training Acc : 0.989, Run Time : 7.86
INFO:root:2019-05-12 04:00:35, Epoch : 1, Step : 2179, Training Loss : 0.14651, Training Acc : 0.956, Run Time : 4.08
INFO:root:2019-05-12 04:00:36, Epoch : 1, Step : 2180, Training Loss : 0.23437, Training Acc : 0.906, Run Time : 0.70
INFO:root:2019-05-12 04:00:43, Epoch : 1, Step : 2181, Training Loss : 0.31148, Training Acc : 0.867, Run Time : 7.39
INFO:root:2019-05-12 04:00:46, Epoch : 1, Step : 2182, Training Loss : 0.32643, Training Acc : 0.833, Run Time : 2.87
INFO:root:2019-05-12 04:00:47, Epoch : 1, Step : 2183, Training Loss : 0.10958, Training Acc : 0.983, Run Time : 0.67
INFO:root:2019-05-12 04:00:48, Epoch : 1, Step : 2184, Training Loss : 0.14785, Training Acc : 0.917, Run Time : 1.73
INFO:root:2019-05-12 04:00:58, Epoch : 1, Step : 2185, Training Loss : 0.17051, Training Acc : 0.928, Run Time : 9.63
INFO:root:2019-05-12 04:00:59, Epoch : 1, Step : 2186, Training Loss : 0.20167, Training Acc : 0.922, Run Time : 0.85
INFO:root:2019-05-12 04:00:59, Epoch : 1, Step : 2187, Training Loss : 0.05102, Training Acc : 0.994, Run Time : 0.63
INFO:root:2019-05-12 04:01:00, Epoch : 1, Step : 2188, Training Loss : 0.16555, Training Acc : 0.928, Run Time : 0.63
INFO:root:2019-05-12 04:01:11, Epoch : 1, Step : 2189, Training Loss : 0.16778, Training Acc : 0.933, Run Time : 10.87
INFO:root:2019-05-12 04:01:12, Epoch : 1, Step : 2190, Training Loss : 0.08416, Training Acc : 0.989, Run Time : 0.69
INFO:root:2019-05-12 04:01:13, Epoch : 1, Step : 2191, Training Loss : 0.07271, Training Acc : 1.000, Run Time : 1.64
INFO:root:2019-05-12 04:01:26, Epoch : 1, Step : 2192, Training Loss : 0.16257, Training Acc : 0.950, Run Time : 12.57
INFO:root:2019-05-12 04:01:27, Epoch : 1, Step : 2193, Training Loss : 0.04034, Training Acc : 1.000, Run Time : 0.89
INFO:root:2019-05-12 04:01:37, Epoch : 1, Step : 2194, Training Loss : 0.05473, Training Acc : 0.989, Run Time : 10.69
INFO:root:2019-05-12 04:01:40, Epoch : 1, Step : 2195, Training Loss : 0.19870, Training Acc : 0.889, Run Time : 2.17
INFO:root:2019-05-12 04:01:48, Epoch : 1, Step : 2196, Training Loss : 0.07243, Training Acc : 0.989, Run Time : 8.60
INFO:root:2019-05-12 04:01:49, Epoch : 1, Step : 2197, Training Loss : 0.11872, Training Acc : 0.961, Run Time : 0.45
INFO:root:2019-05-12 04:01:49, Epoch : 1, Step : 2198, Training Loss : 0.15033, Training Acc : 0.928, Run Time : 0.43
INFO:root:2019-05-12 04:02:03, Epoch : 1, Step : 2199, Training Loss : 0.15580, Training Acc : 0.933, Run Time : 14.39
INFO:root:2019-05-12 04:02:04, Epoch : 1, Step : 2200, Training Loss : 0.35505, Training Acc : 0.844, Run Time : 0.50
INFO:root:2019-05-12 04:02:05, Epoch : 1, Step : 2201, Training Loss : 1.29992, Training Acc : 0.533, Run Time : 0.91
INFO:root:2019-05-12 04:02:15, Epoch : 1, Step : 2202, Training Loss : 1.13687, Training Acc : 0.533, Run Time : 10.01
INFO:root:2019-05-12 04:02:16, Epoch : 1, Step : 2203, Training Loss : 0.89114, Training Acc : 0.606, Run Time : 1.30
INFO:root:2019-05-12 04:02:17, Epoch : 1, Step : 2204, Training Loss : 0.86450, Training Acc : 0.683, Run Time : 1.19
INFO:root:2019-05-12 04:02:28, Epoch : 1, Step : 2205, Training Loss : 0.81514, Training Acc : 0.722, Run Time : 10.86
INFO:root:2019-05-12 04:02:29, Epoch : 1, Step : 2206, Training Loss : 0.59240, Training Acc : 0.717, Run Time : 0.84
INFO:root:2019-05-12 04:02:40, Epoch : 1, Step : 2207, Training Loss : 0.72870, Training Acc : 0.683, Run Time : 11.20
INFO:root:2019-05-12 04:02:41, Epoch : 1, Step : 2208, Training Loss : 0.65762, Training Acc : 0.722, Run Time : 0.60
INFO:root:2019-05-12 04:02:54, Epoch : 1, Step : 2209, Training Loss : 0.48951, Training Acc : 0.767, Run Time : 13.01
INFO:root:2019-05-12 04:03:01, Epoch : 1, Step : 2210, Training Loss : 0.56432, Training Acc : 0.772, Run Time : 7.24
INFO:root:2019-05-12 04:03:05, Epoch : 1, Step : 2211, Training Loss : 0.41676, Training Acc : 0.806, Run Time : 3.37
INFO:root:2019-05-12 04:03:05, Epoch : 1, Step : 2212, Training Loss : 0.54664, Training Acc : 0.794, Run Time : 0.66
INFO:root:2019-05-12 04:03:15, Epoch : 1, Step : 2213, Training Loss : 0.53252, Training Acc : 0.783, Run Time : 9.68
INFO:root:2019-05-12 04:03:16, Epoch : 1, Step : 2214, Training Loss : 0.64722, Training Acc : 0.761, Run Time : 0.67
INFO:root:2019-05-12 04:03:17, Epoch : 1, Step : 2215, Training Loss : 0.64669, Training Acc : 0.739, Run Time : 1.61
INFO:root:2019-05-12 04:03:28, Epoch : 1, Step : 2216, Training Loss : 0.53251, Training Acc : 0.778, Run Time : 10.36
INFO:root:2019-05-12 04:03:28, Epoch : 1, Step : 2217, Training Loss : 0.50881, Training Acc : 0.744, Run Time : 0.64
INFO:root:2019-05-12 04:03:30, Epoch : 1, Step : 2218, Training Loss : 0.60967, Training Acc : 0.711, Run Time : 1.61
INFO:root:2019-05-12 04:03:41, Epoch : 1, Step : 2219, Training Loss : 0.50721, Training Acc : 0.739, Run Time : 11.24
INFO:root:2019-05-12 04:03:42, Epoch : 1, Step : 2220, Training Loss : 0.51558, Training Acc : 0.706, Run Time : 0.53
INFO:root:2019-05-12 04:03:44, Epoch : 1, Step : 2221, Training Loss : 0.43755, Training Acc : 0.767, Run Time : 2.53
INFO:root:2019-05-12 04:04:11, Epoch : 1, Step : 2222, Training Loss : 0.48858, Training Acc : 0.722, Run Time : 26.98
INFO:root:2019-05-12 04:04:17, Epoch : 1, Step : 2223, Training Loss : 0.38316, Training Acc : 0.794, Run Time : 5.54
INFO:root:2019-05-12 04:04:17, Epoch : 1, Step : 2224, Training Loss : 0.41313, Training Acc : 0.772, Run Time : 0.59
INFO:root:2019-05-12 04:04:29, Epoch : 1, Step : 2225, Training Loss : 0.34558, Training Acc : 0.817, Run Time : 11.59
INFO:root:2019-05-12 04:04:29, Epoch : 1, Step : 2226, Training Loss : 0.38509, Training Acc : 0.828, Run Time : 0.48
INFO:root:2019-05-12 04:04:30, Epoch : 1, Step : 2227, Training Loss : 0.37627, Training Acc : 0.806, Run Time : 1.09
INFO:root:2019-05-12 04:04:44, Epoch : 1, Step : 2228, Training Loss : 0.37523, Training Acc : 0.800, Run Time : 13.22
INFO:root:2019-05-12 04:04:44, Epoch : 1, Step : 2229, Training Loss : 0.30978, Training Acc : 0.850, Run Time : 0.77
INFO:root:2019-05-12 04:04:45, Epoch : 1, Step : 2230, Training Loss : 0.28390, Training Acc : 0.878, Run Time : 0.72
INFO:root:2019-05-12 04:04:55, Epoch : 1, Step : 2231, Training Loss : 0.49923, Training Acc : 0.772, Run Time : 10.33
INFO:root:2019-05-12 04:04:56, Epoch : 1, Step : 2232, Training Loss : 0.48070, Training Acc : 0.739, Run Time : 0.56
INFO:root:2019-05-12 04:04:56, Epoch : 1, Step : 2233, Training Loss : 0.47182, Training Acc : 0.772, Run Time : 0.57
INFO:root:2019-05-12 04:05:05, Epoch : 1, Step : 2234, Training Loss : 0.44816, Training Acc : 0.761, Run Time : 8.16
INFO:root:2019-05-12 04:05:06, Epoch : 1, Step : 2235, Training Loss : 0.56854, Training Acc : 0.722, Run Time : 0.90
INFO:root:2019-05-12 04:05:06, Epoch : 1, Step : 2236, Training Loss : 0.40976, Training Acc : 0.811, Run Time : 0.53
INFO:root:2019-05-12 04:05:15, Epoch : 1, Step : 2237, Training Loss : 0.31716, Training Acc : 0.850, Run Time : 9.36
INFO:root:2019-05-12 04:05:16, Epoch : 1, Step : 2238, Training Loss : 0.29369, Training Acc : 0.867, Run Time : 0.88
INFO:root:2019-05-12 04:05:28, Epoch : 1, Step : 2239, Training Loss : 1.00728, Training Acc : 0.578, Run Time : 11.85
INFO:root:2019-05-12 04:05:29, Epoch : 1, Step : 2240, Training Loss : 1.06596, Training Acc : 0.589, Run Time : 0.57
INFO:root:2019-05-12 04:05:30, Epoch : 1, Step : 2241, Training Loss : 0.56736, Training Acc : 0.706, Run Time : 1.65
INFO:root:2019-05-12 04:05:41, Epoch : 1, Step : 2242, Training Loss : 0.40588, Training Acc : 0.817, Run Time : 10.59
INFO:root:2019-05-12 04:05:42, Epoch : 1, Step : 2243, Training Loss : 0.41030, Training Acc : 0.794, Run Time : 0.94
INFO:root:2019-05-12 04:05:44, Epoch : 1, Step : 2244, Training Loss : 0.34931, Training Acc : 0.844, Run Time : 1.85
INFO:root:2019-05-12 04:05:54, Epoch : 1, Step : 2245, Training Loss : 0.37009, Training Acc : 0.844, Run Time : 10.06
INFO:root:2019-05-12 04:05:55, Epoch : 1, Step : 2246, Training Loss : 0.43040, Training Acc : 0.783, Run Time : 1.03
INFO:root:2019-05-12 04:06:05, Epoch : 1, Step : 2247, Training Loss : 0.46284, Training Acc : 0.767, Run Time : 10.33
INFO:root:2019-05-12 04:06:06, Epoch : 1, Step : 2248, Training Loss : 0.47308, Training Acc : 0.794, Run Time : 0.52
INFO:root:2019-05-12 04:06:07, Epoch : 1, Step : 2249, Training Loss : 0.35988, Training Acc : 0.872, Run Time : 0.84
INFO:root:2019-05-12 04:06:17, Epoch : 1, Step : 2250, Training Loss : 0.37150, Training Acc : 0.828, Run Time : 10.56
INFO:root:2019-05-12 04:06:18, Epoch : 1, Step : 2251, Training Loss : 0.37206, Training Acc : 0.800, Run Time : 1.06
INFO:root:2019-05-12 04:06:30, Epoch : 1, Step : 2252, Training Loss : 0.33172, Training Acc : 0.878, Run Time : 12.25
INFO:root:2019-05-12 04:06:31, Epoch : 1, Step : 2253, Training Loss : 0.29740, Training Acc : 0.872, Run Time : 0.57
INFO:root:2019-05-12 04:06:32, Epoch : 1, Step : 2254, Training Loss : 0.35387, Training Acc : 0.811, Run Time : 1.46
INFO:root:2019-05-12 04:06:44, Epoch : 1, Step : 2255, Training Loss : 0.39153, Training Acc : 0.822, Run Time : 11.48
INFO:root:2019-05-12 04:06:45, Epoch : 1, Step : 2256, Training Loss : 0.46189, Training Acc : 0.717, Run Time : 0.58
INFO:root:2019-05-12 04:06:54, Epoch : 1, Step : 2257, Training Loss : 0.34861, Training Acc : 0.889, Run Time : 9.69
INFO:root:2019-05-12 04:06:56, Epoch : 1, Step : 2258, Training Loss : 0.38336, Training Acc : 0.794, Run Time : 2.24
INFO:root:2019-05-12 04:06:57, Epoch : 1, Step : 2259, Training Loss : 0.37733, Training Acc : 0.850, Run Time : 0.76
INFO:root:2019-05-12 04:06:58, Epoch : 1, Step : 2260, Training Loss : 0.30737, Training Acc : 0.889, Run Time : 0.64
INFO:root:2019-05-12 04:07:09, Epoch : 1, Step : 2261, Training Loss : 0.38717, Training Acc : 0.839, Run Time : 10.72
INFO:root:2019-05-12 04:07:10, Epoch : 1, Step : 2262, Training Loss : 0.47431, Training Acc : 0.717, Run Time : 0.97
INFO:root:2019-05-12 04:07:19, Epoch : 1, Step : 2263, Training Loss : 0.33579, Training Acc : 0.856, Run Time : 9.87
INFO:root:2019-05-12 04:07:20, Epoch : 1, Step : 2264, Training Loss : 0.32698, Training Acc : 0.822, Run Time : 0.68
INFO:root:2019-05-12 04:07:21, Epoch : 1, Step : 2265, Training Loss : 0.39491, Training Acc : 0.806, Run Time : 0.49
INFO:root:2019-05-12 04:07:33, Epoch : 1, Step : 2266, Training Loss : 0.38655, Training Acc : 0.833, Run Time : 12.10
INFO:root:2019-05-12 04:07:33, Epoch : 1, Step : 2267, Training Loss : 0.37396, Training Acc : 0.817, Run Time : 0.74
INFO:root:2019-05-12 04:07:35, Epoch : 1, Step : 2268, Training Loss : 0.39963, Training Acc : 0.822, Run Time : 1.52
INFO:root:2019-05-12 04:07:46, Epoch : 1, Step : 2269, Training Loss : 0.38657, Training Acc : 0.817, Run Time : 11.29
INFO:root:2019-05-12 04:07:48, Epoch : 1, Step : 2270, Training Loss : 0.32247, Training Acc : 0.861, Run Time : 1.47
INFO:root:2019-05-12 04:07:59, Epoch : 1, Step : 2271, Training Loss : 0.33832, Training Acc : 0.861, Run Time : 11.77
INFO:root:2019-05-12 04:08:00, Epoch : 1, Step : 2272, Training Loss : 0.27225, Training Acc : 0.922, Run Time : 0.53
INFO:root:2019-05-12 04:08:02, Epoch : 1, Step : 2273, Training Loss : 0.44430, Training Acc : 0.767, Run Time : 1.62
INFO:root:2019-05-12 04:08:11, Epoch : 1, Step : 2274, Training Loss : 0.32013, Training Acc : 0.883, Run Time : 9.81
INFO:root:2019-05-12 04:08:12, Epoch : 1, Step : 2275, Training Loss : 0.36157, Training Acc : 0.833, Run Time : 0.86
INFO:root:2019-05-12 04:08:27, Epoch : 1, Step : 2276, Training Loss : 0.36286, Training Acc : 0.861, Run Time : 14.74
INFO:root:2019-05-12 04:08:31, Epoch : 1, Step : 2277, Training Loss : 0.37255, Training Acc : 0.822, Run Time : 4.20
INFO:root:2019-05-12 04:08:32, Epoch : 1, Step : 2278, Training Loss : 0.32586, Training Acc : 0.844, Run Time : 0.93
INFO:root:2019-05-12 04:08:42, Epoch : 1, Step : 2279, Training Loss : 0.27730, Training Acc : 0.933, Run Time : 9.42
INFO:root:2019-05-12 04:08:42, Epoch : 1, Step : 2280, Training Loss : 0.28543, Training Acc : 0.883, Run Time : 0.63
INFO:root:2019-05-12 04:08:46, Epoch : 1, Step : 2281, Training Loss : 0.26205, Training Acc : 0.883, Run Time : 4.11
INFO:root:2019-05-12 04:08:51, Epoch : 1, Step : 2282, Training Loss : 0.28833, Training Acc : 0.867, Run Time : 4.87
INFO:root:2019-05-12 04:08:52, Epoch : 1, Step : 2283, Training Loss : 0.31690, Training Acc : 0.867, Run Time : 0.78
INFO:root:2019-05-12 04:08:53, Epoch : 1, Step : 2284, Training Loss : 0.27005, Training Acc : 0.861, Run Time : 0.86
INFO:root:2019-05-12 04:09:07, Epoch : 1, Step : 2285, Training Loss : 0.25710, Training Acc : 0.906, Run Time : 14.25
INFO:root:2019-05-12 04:09:09, Epoch : 1, Step : 2286, Training Loss : 0.26601, Training Acc : 0.900, Run Time : 1.44
INFO:root:2019-05-12 04:09:10, Epoch : 1, Step : 2287, Training Loss : 0.23912, Training Acc : 0.900, Run Time : 1.37
INFO:root:2019-05-12 04:09:19, Epoch : 1, Step : 2288, Training Loss : 0.24379, Training Acc : 0.872, Run Time : 9.39
INFO:root:2019-05-12 04:09:20, Epoch : 1, Step : 2289, Training Loss : 0.32009, Training Acc : 0.844, Run Time : 1.07
INFO:root:2019-05-12 04:09:32, Epoch : 1, Step : 2290, Training Loss : 0.27581, Training Acc : 0.883, Run Time : 11.69
INFO:root:2019-05-12 04:09:33, Epoch : 1, Step : 2291, Training Loss : 0.29038, Training Acc : 0.872, Run Time : 0.66
INFO:root:2019-05-12 04:09:35, Epoch : 1, Step : 2292, Training Loss : 0.24365, Training Acc : 0.894, Run Time : 1.83
INFO:root:2019-05-12 04:09:45, Epoch : 1, Step : 2293, Training Loss : 0.29393, Training Acc : 0.850, Run Time : 10.74
INFO:root:2019-05-12 04:09:46, Epoch : 1, Step : 2294, Training Loss : 0.37347, Training Acc : 0.817, Run Time : 0.47
INFO:root:2019-05-12 04:09:54, Epoch : 1, Step : 2295, Training Loss : 0.46811, Training Acc : 0.767, Run Time : 7.79
INFO:root:2019-05-12 04:09:58, Epoch : 1, Step : 2296, Training Loss : 0.42021, Training Acc : 0.789, Run Time : 4.04
INFO:root:2019-05-12 04:09:58, Epoch : 1, Step : 2297, Training Loss : 0.30396, Training Acc : 0.844, Run Time : 0.49
INFO:root:2019-05-12 04:10:06, Epoch : 1, Step : 2298, Training Loss : 0.40058, Training Acc : 0.794, Run Time : 7.52
INFO:root:2019-05-12 04:10:21, Epoch : 1, Step : 2299, Training Loss : 0.23547, Training Acc : 0.894, Run Time : 15.67
INFO:root:2019-05-12 04:10:23, Epoch : 1, Step : 2300, Training Loss : 0.35519, Training Acc : 0.806, Run Time : 1.35
INFO:root:2019-05-12 04:10:36, Epoch : 1, Step : 2301, Training Loss : 0.56945, Training Acc : 0.750, Run Time : 12.90
INFO:root:2019-05-12 04:10:39, Epoch : 1, Step : 2302, Training Loss : 0.43365, Training Acc : 0.800, Run Time : 3.29
INFO:root:2019-05-12 04:10:46, Epoch : 1, Step : 2303, Training Loss : 0.36070, Training Acc : 0.844, Run Time : 7.26
INFO:root:2019-05-12 04:10:47, Epoch : 1, Step : 2304, Training Loss : 0.47427, Training Acc : 0.817, Run Time : 0.96
INFO:root:2019-05-12 04:10:58, Epoch : 1, Step : 2305, Training Loss : 0.38005, Training Acc : 0.811, Run Time : 10.83
INFO:root:2019-05-12 04:10:59, Epoch : 1, Step : 2306, Training Loss : 0.28623, Training Acc : 0.872, Run Time : 0.96
INFO:root:2019-05-12 04:11:12, Epoch : 1, Step : 2307, Training Loss : 0.30565, Training Acc : 0.900, Run Time : 13.32
INFO:root:2019-05-12 04:11:13, Epoch : 1, Step : 2308, Training Loss : 0.31835, Training Acc : 0.833, Run Time : 0.89
INFO:root:2019-05-12 04:11:23, Epoch : 1, Step : 2309, Training Loss : 0.33853, Training Acc : 0.856, Run Time : 10.21
INFO:root:2019-05-12 04:11:24, Epoch : 1, Step : 2310, Training Loss : 0.33722, Training Acc : 0.850, Run Time : 1.08
INFO:root:2019-05-12 04:11:35, Epoch : 1, Step : 2311, Training Loss : 0.41249, Training Acc : 0.822, Run Time : 10.56
INFO:root:2019-05-12 04:11:36, Epoch : 1, Step : 2312, Training Loss : 0.35379, Training Acc : 0.861, Run Time : 1.26
INFO:root:2019-05-12 04:11:44, Epoch : 1, Step : 2313, Training Loss : 0.33906, Training Acc : 0.844, Run Time : 8.23
INFO:root:2019-05-12 04:11:55, Epoch : 1, Step : 2314, Training Loss : 0.38849, Training Acc : 0.822, Run Time : 11.00
INFO:root:2019-05-12 04:11:57, Epoch : 1, Step : 2315, Training Loss : 0.25707, Training Acc : 0.889, Run Time : 1.42
INFO:root:2019-05-12 04:12:10, Epoch : 1, Step : 2316, Training Loss : 0.30687, Training Acc : 0.878, Run Time : 13.00
INFO:root:2019-05-12 04:12:11, Epoch : 1, Step : 2317, Training Loss : 0.27858, Training Acc : 0.878, Run Time : 1.10
INFO:root:2019-05-12 04:12:12, Epoch : 1, Step : 2318, Training Loss : 0.59177, Training Acc : 0.722, Run Time : 0.95
INFO:root:2019-05-12 04:12:23, Epoch : 1, Step : 2319, Training Loss : 0.64629, Training Acc : 0.667, Run Time : 10.80
INFO:root:2019-05-12 04:12:23, Epoch : 1, Step : 2320, Training Loss : 0.46011, Training Acc : 0.761, Run Time : 0.55
INFO:root:2019-05-12 04:12:25, Epoch : 1, Step : 2321, Training Loss : 0.36661, Training Acc : 0.861, Run Time : 1.75
INFO:root:2019-05-12 04:12:36, Epoch : 1, Step : 2322, Training Loss : 0.59585, Training Acc : 0.717, Run Time : 11.14
INFO:root:2019-05-12 04:12:37, Epoch : 1, Step : 2323, Training Loss : 0.37418, Training Acc : 0.811, Run Time : 0.53
INFO:root:2019-05-12 04:12:38, Epoch : 1, Step : 2324, Training Loss : 0.32751, Training Acc : 0.844, Run Time : 1.63
INFO:root:2019-05-12 04:12:49, Epoch : 1, Step : 2325, Training Loss : 0.28345, Training Acc : 0.900, Run Time : 11.14
INFO:root:2019-05-12 04:12:50, Epoch : 1, Step : 2326, Training Loss : 0.39082, Training Acc : 0.794, Run Time : 0.81
INFO:root:2019-05-12 04:12:51, Epoch : 1, Step : 2327, Training Loss : 0.31055, Training Acc : 0.861, Run Time : 0.62
INFO:root:2019-05-12 04:13:00, Epoch : 1, Step : 2328, Training Loss : 0.29864, Training Acc : 0.856, Run Time : 9.14
INFO:root:2019-05-12 04:13:01, Epoch : 1, Step : 2329, Training Loss : 0.33873, Training Acc : 0.844, Run Time : 1.17
INFO:root:2019-05-12 04:13:02, Epoch : 1, Step : 2330, Training Loss : 0.30569, Training Acc : 0.867, Run Time : 0.44
INFO:root:2019-05-12 04:13:12, Epoch : 1, Step : 2331, Training Loss : 0.26257, Training Acc : 0.861, Run Time : 10.23
INFO:root:2019-05-12 04:13:12, Epoch : 1, Step : 2332, Training Loss : 0.34321, Training Acc : 0.833, Run Time : 0.61
INFO:root:2019-05-12 04:13:14, Epoch : 1, Step : 2333, Training Loss : 0.20483, Training Acc : 0.939, Run Time : 1.58
INFO:root:2019-05-12 04:13:28, Epoch : 1, Step : 2334, Training Loss : 0.22792, Training Acc : 0.906, Run Time : 14.37
INFO:root:2019-05-12 04:13:30, Epoch : 1, Step : 2335, Training Loss : 0.24936, Training Acc : 0.850, Run Time : 1.71
INFO:root:2019-05-12 04:13:31, Epoch : 1, Step : 2336, Training Loss : 0.23139, Training Acc : 0.917, Run Time : 0.54
INFO:root:2019-05-12 04:13:36, Epoch : 1, Step : 2337, Training Loss : 0.28714, Training Acc : 0.878, Run Time : 5.05
INFO:root:2019-05-12 04:13:36, Epoch : 1, Step : 2338, Training Loss : 0.17956, Training Acc : 0.950, Run Time : 0.61
INFO:root:2019-05-12 04:13:38, Epoch : 1, Step : 2339, Training Loss : 0.14641, Training Acc : 0.944, Run Time : 1.27
INFO:root:2019-05-12 04:13:51, Epoch : 1, Step : 2340, Training Loss : 0.27066, Training Acc : 0.911, Run Time : 13.97
INFO:root:2019-05-12 04:14:00, Epoch : 1, Step : 2341, Training Loss : 0.24594, Training Acc : 0.883, Run Time : 8.67
INFO:root:2019-05-12 04:14:12, Epoch : 1, Step : 2342, Training Loss : 0.21099, Training Acc : 0.928, Run Time : 11.63
INFO:root:2019-05-12 04:14:23, Epoch : 1, Step : 2343, Training Loss : 0.20888, Training Acc : 0.939, Run Time : 11.28
INFO:root:2019-05-12 04:14:27, Epoch : 1, Step : 2344, Training Loss : 0.34480, Training Acc : 0.828, Run Time : 3.45
INFO:root:2019-05-12 04:14:27, Epoch : 1, Step : 2345, Training Loss : 0.56782, Training Acc : 0.728, Run Time : 0.49
INFO:root:2019-05-12 04:14:37, Epoch : 1, Step : 2346, Training Loss : 0.48421, Training Acc : 0.783, Run Time : 9.92
INFO:root:2019-05-12 04:14:38, Epoch : 1, Step : 2347, Training Loss : 0.51556, Training Acc : 0.778, Run Time : 0.73
INFO:root:2019-05-12 04:14:39, Epoch : 1, Step : 2348, Training Loss : 0.46047, Training Acc : 0.783, Run Time : 1.14
INFO:root:2019-05-12 04:14:50, Epoch : 1, Step : 2349, Training Loss : 0.32538, Training Acc : 0.839, Run Time : 11.68
INFO:root:2019-05-12 04:14:51, Epoch : 1, Step : 2350, Training Loss : 0.40434, Training Acc : 0.800, Run Time : 0.53
INFO:root:2019-05-12 04:15:02, Epoch : 1, Step : 2351, Training Loss : 0.39715, Training Acc : 0.867, Run Time : 11.06
INFO:root:2019-05-12 04:15:06, Epoch : 1, Step : 2352, Training Loss : 0.32506, Training Acc : 0.861, Run Time : 4.27
INFO:root:2019-05-12 04:15:07, Epoch : 1, Step : 2353, Training Loss : 0.54441, Training Acc : 0.744, Run Time : 0.50
INFO:root:2019-05-12 04:15:17, Epoch : 1, Step : 2354, Training Loss : 0.39500, Training Acc : 0.794, Run Time : 10.43
INFO:root:2019-05-12 04:15:18, Epoch : 1, Step : 2355, Training Loss : 0.40689, Training Acc : 0.817, Run Time : 0.87
INFO:root:2019-05-12 04:15:20, Epoch : 1, Step : 2356, Training Loss : 0.26390, Training Acc : 0.883, Run Time : 1.93
INFO:root:2019-05-12 04:15:30, Epoch : 1, Step : 2357, Training Loss : 0.36499, Training Acc : 0.828, Run Time : 10.13
INFO:root:2019-05-12 04:15:31, Epoch : 1, Step : 2358, Training Loss : 0.48364, Training Acc : 0.794, Run Time : 0.61
INFO:root:2019-05-12 04:15:31, Epoch : 1, Step : 2359, Training Loss : 0.49421, Training Acc : 0.767, Run Time : 0.55
INFO:root:2019-05-12 04:15:42, Epoch : 1, Step : 2360, Training Loss : 0.46758, Training Acc : 0.756, Run Time : 10.32
INFO:root:2019-05-12 04:15:43, Epoch : 1, Step : 2361, Training Loss : 0.45533, Training Acc : 0.783, Run Time : 1.38
INFO:root:2019-05-12 04:15:44, Epoch : 1, Step : 2362, Training Loss : 0.77126, Training Acc : 0.583, Run Time : 0.52
INFO:root:2019-05-12 04:16:00, Epoch : 1, Step : 2363, Training Loss : 0.42292, Training Acc : 0.800, Run Time : 16.33
INFO:root:2019-05-12 04:16:11, Epoch : 1, Step : 2364, Training Loss : 0.37118, Training Acc : 0.822, Run Time : 10.60
INFO:root:2019-05-12 04:16:28, Epoch : 1, Step : 2365, Training Loss : 0.43109, Training Acc : 0.761, Run Time : 17.95
INFO:root:2019-05-12 04:16:33, Epoch : 1, Step : 2366, Training Loss : 0.42277, Training Acc : 0.811, Run Time : 4.94
INFO:root:2019-05-12 04:16:34, Epoch : 1, Step : 2367, Training Loss : 0.29260, Training Acc : 0.889, Run Time : 0.67
INFO:root:2019-05-12 04:16:46, Epoch : 1, Step : 2368, Training Loss : 0.37006, Training Acc : 0.867, Run Time : 11.82
INFO:root:2019-05-12 04:16:47, Epoch : 1, Step : 2369, Training Loss : 0.31714, Training Acc : 0.889, Run Time : 0.87
INFO:root:2019-05-12 04:17:04, Epoch : 1, Step : 2370, Training Loss : 0.28840, Training Acc : 0.867, Run Time : 17.62
INFO:root:2019-05-12 04:17:09, Epoch : 1, Step : 2371, Training Loss : 0.20938, Training Acc : 0.944, Run Time : 4.84
INFO:root:2019-05-12 04:17:10, Epoch : 1, Step : 2372, Training Loss : 0.38246, Training Acc : 0.850, Run Time : 0.57
INFO:root:2019-05-12 04:17:23, Epoch : 1, Step : 2373, Training Loss : 0.30103, Training Acc : 0.883, Run Time : 13.44
INFO:root:2019-05-12 04:17:24, Epoch : 1, Step : 2374, Training Loss : 0.36891, Training Acc : 0.817, Run Time : 0.74
INFO:root:2019-05-12 04:17:43, Epoch : 1, Step : 2375, Training Loss : 0.33863, Training Acc : 0.839, Run Time : 18.99
INFO:root:2019-05-12 04:17:44, Epoch : 1, Step : 2376, Training Loss : 0.32007, Training Acc : 0.850, Run Time : 1.51
INFO:root:2019-05-12 04:17:46, Epoch : 1, Step : 2377, Training Loss : 0.32950, Training Acc : 0.828, Run Time : 1.73
INFO:root:2019-05-12 04:17:56, Epoch : 1, Step : 2378, Training Loss : 0.36622, Training Acc : 0.889, Run Time : 10.14
INFO:root:2019-05-12 04:17:57, Epoch : 1, Step : 2379, Training Loss : 0.30724, Training Acc : 0.867, Run Time : 0.86
INFO:root:2019-05-12 04:18:09, Epoch : 1, Step : 2380, Training Loss : 0.29274, Training Acc : 0.822, Run Time : 12.12
INFO:root:2019-05-12 04:18:10, Epoch : 1, Step : 2381, Training Loss : 0.24920, Training Acc : 0.906, Run Time : 0.51
INFO:root:2019-05-12 04:18:12, Epoch : 1, Step : 2382, Training Loss : 0.23414, Training Acc : 0.900, Run Time : 1.85
INFO:root:2019-05-12 04:18:24, Epoch : 1, Step : 2383, Training Loss : 0.28576, Training Acc : 0.861, Run Time : 12.14
INFO:root:2019-05-12 04:18:25, Epoch : 1, Step : 2384, Training Loss : 0.31037, Training Acc : 0.856, Run Time : 1.13
INFO:root:2019-05-12 04:18:36, Epoch : 1, Step : 2385, Training Loss : 0.25503, Training Acc : 0.917, Run Time : 11.48
INFO:root:2019-05-12 04:18:37, Epoch : 1, Step : 2386, Training Loss : 0.40515, Training Acc : 0.817, Run Time : 0.45
INFO:root:2019-05-12 04:18:38, Epoch : 1, Step : 2387, Training Loss : 0.44536, Training Acc : 0.778, Run Time : 0.76
INFO:root:2019-05-12 04:18:48, Epoch : 1, Step : 2388, Training Loss : 0.39981, Training Acc : 0.811, Run Time : 10.08
INFO:root:2019-05-12 04:18:50, Epoch : 1, Step : 2389, Training Loss : 0.43825, Training Acc : 0.739, Run Time : 1.82
INFO:root:2019-05-12 04:19:00, Epoch : 1, Step : 2390, Training Loss : 0.38008, Training Acc : 0.800, Run Time : 10.74
INFO:root:2019-05-12 04:19:01, Epoch : 1, Step : 2391, Training Loss : 0.32483, Training Acc : 0.828, Run Time : 0.57
INFO:root:2019-05-12 04:19:05, Epoch : 1, Step : 2392, Training Loss : 0.35378, Training Acc : 0.800, Run Time : 4.08
INFO:root:2019-05-12 04:19:15, Epoch : 1, Step : 2393, Training Loss : 0.36190, Training Acc : 0.833, Run Time : 10.44
INFO:root:2019-05-12 04:19:17, Epoch : 1, Step : 2394, Training Loss : 0.34396, Training Acc : 0.839, Run Time : 1.91
INFO:root:2019-05-12 04:19:33, Epoch : 1, Step : 2395, Training Loss : 0.36350, Training Acc : 0.811, Run Time : 15.81
INFO:root:2019-05-12 04:19:46, Epoch : 1, Step : 2396, Training Loss : 0.31242, Training Acc : 0.839, Run Time : 12.64
INFO:root:2019-05-12 04:19:59, Epoch : 1, Step : 2397, Training Loss : 0.32365, Training Acc : 0.817, Run Time : 12.85
INFO:root:2019-05-12 04:20:02, Epoch : 1, Step : 2398, Training Loss : 0.27536, Training Acc : 0.844, Run Time : 3.17
INFO:root:2019-05-12 04:20:02, Epoch : 1, Step : 2399, Training Loss : 0.25842, Training Acc : 0.861, Run Time : 0.63
INFO:root:2019-05-12 04:20:15, Epoch : 1, Step : 2400, Training Loss : 0.23649, Training Acc : 0.906, Run Time : 12.67
INFO:root:2019-05-12 04:20:24, Epoch : 1, Step : 2401, Training Loss : 0.27183, Training Acc : 0.856, Run Time : 9.24
INFO:root:2019-05-12 04:20:26, Epoch : 1, Step : 2402, Training Loss : 0.23922, Training Acc : 0.906, Run Time : 1.26
INFO:root:2019-05-12 04:20:26, Epoch : 1, Step : 2403, Training Loss : 0.25792, Training Acc : 0.889, Run Time : 0.62
INFO:root:2019-05-12 04:20:36, Epoch : 1, Step : 2404, Training Loss : 0.26892, Training Acc : 0.844, Run Time : 10.26
INFO:root:2019-05-12 04:20:37, Epoch : 1, Step : 2405, Training Loss : 0.37192, Training Acc : 0.839, Run Time : 0.53
INFO:root:2019-05-12 04:20:38, Epoch : 1, Step : 2406, Training Loss : 0.35014, Training Acc : 0.839, Run Time : 0.60
INFO:root:2019-05-12 04:20:49, Epoch : 1, Step : 2407, Training Loss : 0.50896, Training Acc : 0.794, Run Time : 11.78
INFO:root:2019-05-12 04:20:50, Epoch : 1, Step : 2408, Training Loss : 0.42065, Training Acc : 0.817, Run Time : 1.02
INFO:root:2019-05-12 04:21:02, Epoch : 1, Step : 2409, Training Loss : 0.38929, Training Acc : 0.822, Run Time : 12.07
INFO:root:2019-05-12 04:21:03, Epoch : 1, Step : 2410, Training Loss : 0.34113, Training Acc : 0.817, Run Time : 0.69
INFO:root:2019-05-12 04:21:05, Epoch : 1, Step : 2411, Training Loss : 0.50190, Training Acc : 0.783, Run Time : 2.28
INFO:root:2019-05-12 04:21:18, Epoch : 1, Step : 2412, Training Loss : 0.40434, Training Acc : 0.783, Run Time : 12.67
INFO:root:2019-05-12 04:21:19, Epoch : 1, Step : 2413, Training Loss : 0.32103, Training Acc : 0.850, Run Time : 0.77
INFO:root:2019-05-12 04:21:20, Epoch : 1, Step : 2414, Training Loss : 0.30439, Training Acc : 0.883, Run Time : 0.77
INFO:root:2019-05-12 04:21:21, Epoch : 1, Step : 2415, Training Loss : 0.37609, Training Acc : 0.811, Run Time : 1.54
INFO:root:2019-05-12 04:21:39, Epoch : 1, Step : 2416, Training Loss : 0.21883, Training Acc : 0.917, Run Time : 17.44
INFO:root:2019-05-12 04:21:46, Epoch : 1, Step : 2417, Training Loss : 0.35258, Training Acc : 0.867, Run Time : 7.48
INFO:root:2019-05-12 04:21:47, Epoch : 1, Step : 2418, Training Loss : 0.22594, Training Acc : 0.900, Run Time : 0.49
INFO:root:2019-05-12 04:22:00, Epoch : 1, Step : 2419, Training Loss : 0.31917, Training Acc : 0.861, Run Time : 13.29
INFO:root:2019-05-12 04:22:01, Epoch : 1, Step : 2420, Training Loss : 0.18374, Training Acc : 0.906, Run Time : 0.77
INFO:root:2019-05-12 04:22:02, Epoch : 1, Step : 2421, Training Loss : 0.21308, Training Acc : 0.928, Run Time : 1.55
INFO:root:2019-05-12 04:22:13, Epoch : 1, Step : 2422, Training Loss : 0.27969, Training Acc : 0.900, Run Time : 10.88
INFO:root:2019-05-12 04:22:14, Epoch : 1, Step : 2423, Training Loss : 0.22084, Training Acc : 0.906, Run Time : 0.51
INFO:root:2019-05-12 04:22:15, Epoch : 1, Step : 2424, Training Loss : 0.31620, Training Acc : 0.883, Run Time : 1.84
INFO:root:2019-05-12 04:22:24, Epoch : 1, Step : 2425, Training Loss : 0.30292, Training Acc : 0.900, Run Time : 8.14
INFO:root:2019-05-12 04:22:24, Epoch : 1, Step : 2426, Training Loss : 0.21961, Training Acc : 0.933, Run Time : 0.56
INFO:root:2019-05-12 04:22:25, Epoch : 1, Step : 2427, Training Loss : 0.16979, Training Acc : 0.944, Run Time : 1.10
INFO:root:2019-05-12 04:22:37, Epoch : 1, Step : 2428, Training Loss : 0.37725, Training Acc : 0.850, Run Time : 11.97
INFO:root:2019-05-12 04:22:38, Epoch : 1, Step : 2429, Training Loss : 0.21160, Training Acc : 0.922, Run Time : 0.43
INFO:root:2019-05-12 04:22:40, Epoch : 1, Step : 2430, Training Loss : 0.16347, Training Acc : 0.928, Run Time : 1.91
INFO:root:2019-05-12 04:22:50, Epoch : 1, Step : 2431, Training Loss : 0.34467, Training Acc : 0.883, Run Time : 10.84
INFO:root:2019-05-12 04:22:51, Epoch : 1, Step : 2432, Training Loss : 0.16587, Training Acc : 0.944, Run Time : 0.64
INFO:root:2019-05-12 04:23:00, Epoch : 1, Step : 2433, Training Loss : 0.26584, Training Acc : 0.894, Run Time : 9.23
INFO:root:2019-05-12 04:23:03, Epoch : 1, Step : 2434, Training Loss : 0.21262, Training Acc : 0.917, Run Time : 3.25
INFO:root:2019-05-12 04:23:04, Epoch : 1, Step : 2435, Training Loss : 0.20177, Training Acc : 0.911, Run Time : 0.79
INFO:root:2019-05-12 04:23:14, Epoch : 1, Step : 2436, Training Loss : 0.20701, Training Acc : 0.911, Run Time : 10.10
INFO:root:2019-05-12 04:23:15, Epoch : 1, Step : 2437, Training Loss : 0.20521, Training Acc : 0.911, Run Time : 0.82
INFO:root:2019-05-12 04:23:17, Epoch : 1, Step : 2438, Training Loss : 0.29571, Training Acc : 0.856, Run Time : 2.05
INFO:root:2019-05-12 04:23:29, Epoch : 1, Step : 2439, Training Loss : 0.16654, Training Acc : 0.939, Run Time : 12.22
INFO:root:2019-05-12 04:23:30, Epoch : 1, Step : 2440, Training Loss : 0.31635, Training Acc : 0.872, Run Time : 0.64
INFO:root:2019-05-12 04:23:32, Epoch : 1, Step : 2441, Training Loss : 0.24813, Training Acc : 0.911, Run Time : 1.74
INFO:root:2019-05-12 04:23:43, Epoch : 1, Step : 2442, Training Loss : 0.25626, Training Acc : 0.900, Run Time : 11.59
INFO:root:2019-05-12 04:23:45, Epoch : 1, Step : 2443, Training Loss : 0.24853, Training Acc : 0.911, Run Time : 1.34
INFO:root:2019-05-12 04:23:57, Epoch : 1, Step : 2444, Training Loss : 0.37525, Training Acc : 0.850, Run Time : 12.64
INFO:root:2019-05-12 04:23:59, Epoch : 1, Step : 2445, Training Loss : 0.29812, Training Acc : 0.883, Run Time : 1.50
INFO:root:2019-05-12 04:24:11, Epoch : 1, Step : 2446, Training Loss : 0.28517, Training Acc : 0.856, Run Time : 12.54
INFO:root:2019-05-12 04:24:13, Epoch : 1, Step : 2447, Training Loss : 0.18829, Training Acc : 0.917, Run Time : 1.42
INFO:root:2019-05-12 04:24:23, Epoch : 1, Step : 2448, Training Loss : 0.44267, Training Acc : 0.822, Run Time : 10.14
INFO:root:2019-05-12 04:24:28, Epoch : 1, Step : 2449, Training Loss : 0.34655, Training Acc : 0.872, Run Time : 5.00
INFO:root:2019-05-12 04:24:30, Epoch : 1, Step : 2450, Training Loss : 0.26033, Training Acc : 0.894, Run Time : 1.68
INFO:root:2019-05-12 04:24:40, Epoch : 1, Step : 2451, Training Loss : 0.23072, Training Acc : 0.883, Run Time : 10.03
INFO:root:2019-05-12 04:24:40, Epoch : 1, Step : 2452, Training Loss : 0.25008, Training Acc : 0.894, Run Time : 0.65
INFO:root:2019-05-12 04:24:41, Epoch : 1, Step : 2453, Training Loss : 0.25931, Training Acc : 0.861, Run Time : 0.61
INFO:root:2019-05-12 04:24:54, Epoch : 1, Step : 2454, Training Loss : 0.29457, Training Acc : 0.861, Run Time : 13.08
INFO:root:2019-05-12 04:24:55, Epoch : 1, Step : 2455, Training Loss : 0.23128, Training Acc : 0.928, Run Time : 0.58
INFO:root:2019-05-12 04:24:55, Epoch : 1, Step : 2456, Training Loss : 0.30714, Training Acc : 0.861, Run Time : 0.82
INFO:root:2019-05-12 04:25:08, Epoch : 1, Step : 2457, Training Loss : 0.25051, Training Acc : 0.894, Run Time : 12.42
INFO:root:2019-05-12 04:25:09, Epoch : 1, Step : 2458, Training Loss : 0.21486, Training Acc : 0.900, Run Time : 0.82
INFO:root:2019-05-12 04:25:11, Epoch : 1, Step : 2459, Training Loss : 0.24028, Training Acc : 0.889, Run Time : 1.96
INFO:root:2019-05-12 04:25:21, Epoch : 1, Step : 2460, Training Loss : 0.17169, Training Acc : 0.917, Run Time : 10.75
INFO:root:2019-05-12 04:25:22, Epoch : 1, Step : 2461, Training Loss : 0.49332, Training Acc : 0.744, Run Time : 0.47
INFO:root:2019-05-12 04:25:24, Epoch : 1, Step : 2462, Training Loss : 1.59869, Training Acc : 0.533, Run Time : 1.70
INFO:root:2019-05-12 04:25:35, Epoch : 1, Step : 2463, Training Loss : 1.23713, Training Acc : 0.650, Run Time : 11.76
INFO:root:2019-05-12 04:25:36, Epoch : 1, Step : 2464, Training Loss : 0.67981, Training Acc : 0.656, Run Time : 0.52
INFO:root:2019-05-12 04:25:36, Epoch : 1, Step : 2465, Training Loss : 0.67670, Training Acc : 0.778, Run Time : 0.61
INFO:root:2019-05-12 04:25:38, Epoch : 1, Step : 2466, Training Loss : 0.51169, Training Acc : 0.850, Run Time : 1.28
INFO:root:2019-05-12 04:25:49, Epoch : 1, Step : 2467, Training Loss : 0.38104, Training Acc : 0.894, Run Time : 11.08
INFO:root:2019-05-12 04:25:49, Epoch : 1, Step : 2468, Training Loss : 0.31042, Training Acc : 0.856, Run Time : 0.51
INFO:root:2019-05-12 04:25:50, Epoch : 1, Step : 2469, Training Loss : 0.31361, Training Acc : 0.867, Run Time : 0.65
INFO:root:2019-05-12 04:25:51, Epoch : 1, Step : 2470, Training Loss : 0.24667, Training Acc : 0.900, Run Time : 0.58
INFO:root:2019-05-12 04:25:51, Epoch : 1, Step : 2471, Training Loss : 0.25349, Training Acc : 0.878, Run Time : 0.60
INFO:root:2019-05-12 04:26:06, Epoch : 1, Step : 2472, Training Loss : 0.20000, Training Acc : 0.917, Run Time : 15.14
INFO:root:2019-05-12 04:26:07, Epoch : 1, Step : 2473, Training Loss : 0.43669, Training Acc : 0.817, Run Time : 0.58
INFO:root:2019-05-12 04:26:09, Epoch : 1, Step : 2474, Training Loss : 0.20884, Training Acc : 0.911, Run Time : 2.48
INFO:root:2019-05-12 04:26:21, Epoch : 1, Step : 2475, Training Loss : 0.34371, Training Acc : 0.850, Run Time : 11.90
INFO:root:2019-05-12 04:26:35, Epoch : 1, Step : 2476, Training Loss : 0.29708, Training Acc : 0.922, Run Time : 13.30
INFO:root:2019-05-12 04:26:36, Epoch : 1, Step : 2477, Training Loss : 0.32414, Training Acc : 0.889, Run Time : 1.49
INFO:root:2019-05-12 04:26:37, Epoch : 1, Step : 2478, Training Loss : 0.22539, Training Acc : 0.900, Run Time : 0.56
INFO:root:2019-05-12 04:26:37, Epoch : 1, Step : 2479, Training Loss : 0.23330, Training Acc : 0.911, Run Time : 0.61
INFO:root:2019-05-12 04:26:39, Epoch : 1, Step : 2480, Training Loss : 0.35136, Training Acc : 0.839, Run Time : 1.98
INFO:root:2019-05-12 04:26:53, Epoch : 1, Step : 2481, Training Loss : 0.28495, Training Acc : 0.883, Run Time : 13.90
INFO:root:2019-05-12 04:26:54, Epoch : 1, Step : 2482, Training Loss : 0.33625, Training Acc : 0.850, Run Time : 0.73
INFO:root:2019-05-12 04:27:07, Epoch : 1, Step : 2483, Training Loss : 0.27628, Training Acc : 0.878, Run Time : 13.57
INFO:root:2019-05-12 04:27:09, Epoch : 1, Step : 2484, Training Loss : 0.21345, Training Acc : 0.911, Run Time : 1.33
INFO:root:2019-05-12 04:27:09, Epoch : 1, Step : 2485, Training Loss : 0.34919, Training Acc : 0.883, Run Time : 0.61
INFO:root:2019-05-12 04:27:11, Epoch : 1, Step : 2486, Training Loss : 0.36067, Training Acc : 0.839, Run Time : 1.43
INFO:root:2019-05-12 04:27:25, Epoch : 1, Step : 2487, Training Loss : 1.33528, Training Acc : 0.467, Run Time : 13.93
INFO:root:2019-05-12 04:27:38, Epoch : 1, Step : 2488, Training Loss : 0.72019, Training Acc : 0.706, Run Time : 13.18
INFO:root:2019-05-12 04:27:42, Epoch : 1, Step : 2489, Training Loss : 0.51425, Training Acc : 0.772, Run Time : 4.53
INFO:root:2019-05-12 04:27:44, Epoch : 1, Step : 2490, Training Loss : 0.41005, Training Acc : 0.778, Run Time : 1.22
INFO:root:2019-05-12 04:27:56, Epoch : 1, Step : 2491, Training Loss : 0.17409, Training Acc : 0.961, Run Time : 12.56
INFO:root:2019-05-12 04:27:57, Epoch : 1, Step : 2492, Training Loss : 0.29758, Training Acc : 0.872, Run Time : 0.78
INFO:root:2019-05-12 04:27:59, Epoch : 1, Step : 2493, Training Loss : 0.24292, Training Acc : 0.889, Run Time : 1.67
INFO:root:2019-05-12 04:28:09, Epoch : 1, Step : 2494, Training Loss : 0.35011, Training Acc : 0.844, Run Time : 10.65
INFO:root:2019-05-12 04:28:10, Epoch : 1, Step : 2495, Training Loss : 0.36688, Training Acc : 0.800, Run Time : 0.51
INFO:root:2019-05-12 04:28:10, Epoch : 1, Step : 2496, Training Loss : 0.47580, Training Acc : 0.778, Run Time : 0.42
INFO:root:2019-05-12 04:28:11, Epoch : 1, Step : 2497, Training Loss : 0.34624, Training Acc : 0.839, Run Time : 0.44
INFO:root:2019-05-12 04:28:23, Epoch : 1, Step : 2498, Training Loss : 0.33706, Training Acc : 0.861, Run Time : 12.21
INFO:root:2019-05-12 04:28:24, Epoch : 1, Step : 2499, Training Loss : 0.27882, Training Acc : 0.872, Run Time : 0.80
INFO:root:2019-05-12 04:28:25, Epoch : 1, Step : 2500, Training Loss : 0.27311, Training Acc : 0.889, Run Time : 1.37
INFO:root:2019-05-12 04:28:37, Epoch : 1, Step : 2501, Training Loss : 0.25753, Training Acc : 0.872, Run Time : 12.01
INFO:root:2019-05-12 04:28:38, Epoch : 1, Step : 2502, Training Loss : 0.28657, Training Acc : 0.878, Run Time : 0.54
INFO:root:2019-05-12 04:28:52, Epoch : 1, Step : 2503, Training Loss : 0.25586, Training Acc : 0.889, Run Time : 14.41
INFO:root:2019-05-12 04:29:03, Epoch : 1, Step : 2504, Training Loss : 0.23599, Training Acc : 0.939, Run Time : 10.78
INFO:root:2019-05-12 04:29:07, Epoch : 1, Step : 2505, Training Loss : 0.17761, Training Acc : 0.939, Run Time : 4.25
INFO:root:2019-05-12 04:29:17, Epoch : 1, Step : 2506, Training Loss : 0.17394, Training Acc : 0.967, Run Time : 10.24
INFO:root:2019-05-12 04:29:18, Epoch : 1, Step : 2507, Training Loss : 0.21021, Training Acc : 0.917, Run Time : 0.90
INFO:root:2019-05-12 04:29:27, Epoch : 1, Step : 2508, Training Loss : 0.32723, Training Acc : 0.839, Run Time : 9.18
INFO:root:2019-05-12 04:29:31, Epoch : 1, Step : 2509, Training Loss : 0.30741, Training Acc : 0.867, Run Time : 3.84
INFO:root:2019-05-12 04:29:32, Epoch : 1, Step : 2510, Training Loss : 0.38870, Training Acc : 0.778, Run Time : 0.46
INFO:root:2019-05-12 04:29:32, Epoch : 1, Step : 2511, Training Loss : 0.27542, Training Acc : 0.911, Run Time : 0.59
INFO:root:2019-05-12 04:29:33, Epoch : 1, Step : 2512, Training Loss : 0.22415, Training Acc : 0.933, Run Time : 0.64
INFO:root:2019-05-12 04:29:33, Epoch : 1, Step : 2513, Training Loss : 0.31126, Training Acc : 0.844, Run Time : 0.59
INFO:root:2019-05-12 04:29:48, Epoch : 1, Step : 2514, Training Loss : 0.12340, Training Acc : 0.978, Run Time : 14.84
INFO:root:2019-05-12 04:29:49, Epoch : 1, Step : 2515, Training Loss : 0.31668, Training Acc : 0.839, Run Time : 0.86
INFO:root:2019-05-12 04:29:51, Epoch : 1, Step : 2516, Training Loss : 0.19823, Training Acc : 0.967, Run Time : 1.74
INFO:root:2019-05-12 04:29:59, Epoch : 1, Step : 2517, Training Loss : 0.24775, Training Acc : 0.894, Run Time : 8.29
INFO:root:2019-05-12 04:30:00, Epoch : 1, Step : 2518, Training Loss : 0.20270, Training Acc : 0.928, Run Time : 0.63
INFO:root:2019-05-12 04:30:01, Epoch : 1, Step : 2519, Training Loss : 0.15631, Training Acc : 0.961, Run Time : 1.19
INFO:root:2019-05-12 04:30:13, Epoch : 1, Step : 2520, Training Loss : 0.27326, Training Acc : 0.839, Run Time : 11.58
INFO:root:2019-05-12 04:30:13, Epoch : 1, Step : 2521, Training Loss : 0.23533, Training Acc : 0.900, Run Time : 0.45
INFO:root:2019-05-12 04:30:14, Epoch : 1, Step : 2522, Training Loss : 0.16773, Training Acc : 0.944, Run Time : 0.67
INFO:root:2019-05-12 04:30:15, Epoch : 1, Step : 2523, Training Loss : 0.17829, Training Acc : 0.939, Run Time : 1.58
INFO:root:2019-05-12 04:30:28, Epoch : 1, Step : 2524, Training Loss : 0.18991, Training Acc : 0.939, Run Time : 13.10
INFO:root:2019-05-12 04:30:29, Epoch : 1, Step : 2525, Training Loss : 0.20137, Training Acc : 0.922, Run Time : 0.69
INFO:root:2019-05-12 04:30:40, Epoch : 1, Step : 2526, Training Loss : 0.18250, Training Acc : 0.928, Run Time : 11.11
INFO:root:2019-05-12 04:30:41, Epoch : 1, Step : 2527, Training Loss : 0.24590, Training Acc : 0.894, Run Time : 0.78
INFO:root:2019-05-12 04:30:53, Epoch : 1, Step : 2528, Training Loss : 0.16172, Training Acc : 0.956, Run Time : 11.70
INFO:root:2019-05-12 04:30:54, Epoch : 1, Step : 2529, Training Loss : 0.22673, Training Acc : 0.900, Run Time : 1.25
INFO:root:2019-05-12 04:30:55, Epoch : 1, Step : 2530, Training Loss : 0.15733, Training Acc : 0.944, Run Time : 1.25
INFO:root:2019-05-12 04:31:12, Epoch : 1, Step : 2531, Training Loss : 0.18422, Training Acc : 0.928, Run Time : 16.65
INFO:root:2019-05-12 04:31:20, Epoch : 1, Step : 2532, Training Loss : 0.20905, Training Acc : 0.906, Run Time : 8.69
INFO:root:2019-05-12 04:31:33, Epoch : 1, Step : 2533, Training Loss : 0.11657, Training Acc : 0.967, Run Time : 12.61
INFO:root:2019-05-12 04:31:35, Epoch : 1, Step : 2534, Training Loss : 0.18680, Training Acc : 0.906, Run Time : 1.63
INFO:root:2019-05-12 04:31:42, Epoch : 1, Step : 2535, Training Loss : 0.12419, Training Acc : 0.956, Run Time : 6.97
INFO:root:2019-05-12 04:31:42, Epoch : 1, Step : 2536, Training Loss : 0.16070, Training Acc : 0.933, Run Time : 0.77
INFO:root:2019-05-12 04:31:44, Epoch : 1, Step : 2537, Training Loss : 0.14607, Training Acc : 0.933, Run Time : 1.95
INFO:root:2019-05-12 04:31:55, Epoch : 1, Step : 2538, Training Loss : 0.16117, Training Acc : 0.939, Run Time : 10.84
INFO:root:2019-05-12 04:31:56, Epoch : 1, Step : 2539, Training Loss : 0.14282, Training Acc : 0.956, Run Time : 0.43
INFO:root:2019-05-12 04:31:56, Epoch : 1, Step : 2540, Training Loss : 0.16590, Training Acc : 0.939, Run Time : 0.72
INFO:root:2019-05-12 04:32:08, Epoch : 1, Step : 2541, Training Loss : 0.15089, Training Acc : 0.944, Run Time : 11.32
INFO:root:2019-05-12 04:32:08, Epoch : 1, Step : 2542, Training Loss : 0.14164, Training Acc : 0.956, Run Time : 0.53
INFO:root:2019-05-12 04:32:09, Epoch : 1, Step : 2543, Training Loss : 0.22479, Training Acc : 0.894, Run Time : 0.61
INFO:root:2019-05-12 04:32:20, Epoch : 1, Step : 2544, Training Loss : 0.14855, Training Acc : 0.961, Run Time : 11.00
INFO:root:2019-05-12 04:32:30, Epoch : 1, Step : 2545, Training Loss : 0.18858, Training Acc : 0.906, Run Time : 9.78
INFO:root:2019-05-12 04:32:31, Epoch : 1, Step : 2546, Training Loss : 0.20353, Training Acc : 0.906, Run Time : 1.48
INFO:root:2019-05-12 04:32:41, Epoch : 1, Step : 2547, Training Loss : 0.18215, Training Acc : 0.928, Run Time : 9.94
INFO:root:2019-05-12 04:32:42, Epoch : 1, Step : 2548, Training Loss : 0.27940, Training Acc : 0.889, Run Time : 1.05
INFO:root:2019-05-12 04:32:54, Epoch : 1, Step : 2549, Training Loss : 0.20929, Training Acc : 0.906, Run Time : 12.28
INFO:root:2019-05-12 04:32:55, Epoch : 1, Step : 2550, Training Loss : 0.17356, Training Acc : 0.944, Run Time : 1.03
INFO:root:2019-05-12 04:32:56, Epoch : 1, Step : 2551, Training Loss : 0.13327, Training Acc : 0.950, Run Time : 0.57
INFO:root:2019-05-12 04:33:11, Epoch : 1, Step : 2552, Training Loss : 0.15969, Training Acc : 0.928, Run Time : 15.45
INFO:root:2019-05-12 04:33:21, Epoch : 1, Step : 2553, Training Loss : 0.31323, Training Acc : 0.922, Run Time : 9.34
INFO:root:2019-05-12 04:33:22, Epoch : 1, Step : 2554, Training Loss : 0.15370, Training Acc : 0.950, Run Time : 0.93
INFO:root:2019-05-12 04:33:35, Epoch : 1, Step : 2555, Training Loss : 0.23839, Training Acc : 0.911, Run Time : 13.07
INFO:root:2019-05-12 04:33:36, Epoch : 1, Step : 2556, Training Loss : 0.35702, Training Acc : 0.894, Run Time : 1.02
INFO:root:2019-05-12 04:33:52, Epoch : 1, Step : 2557, Training Loss : 0.20222, Training Acc : 0.917, Run Time : 16.50
INFO:root:2019-05-12 04:33:58, Epoch : 1, Step : 2558, Training Loss : 0.23153, Training Acc : 0.917, Run Time : 5.24
INFO:root:2019-05-12 04:33:58, Epoch : 1, Step : 2559, Training Loss : 0.22891, Training Acc : 0.906, Run Time : 0.61
INFO:root:2019-05-12 04:34:07, Epoch : 1, Step : 2560, Training Loss : 0.26877, Training Acc : 0.872, Run Time : 9.31
INFO:root:2019-05-12 04:34:08, Epoch : 1, Step : 2561, Training Loss : 0.20433, Training Acc : 0.933, Run Time : 0.63
INFO:root:2019-05-12 04:34:09, Epoch : 1, Step : 2562, Training Loss : 0.20958, Training Acc : 0.917, Run Time : 0.42
INFO:root:2019-05-12 04:34:09, Epoch : 1, Step : 2563, Training Loss : 0.27649, Training Acc : 0.883, Run Time : 0.67
INFO:root:2019-05-12 04:34:10, Epoch : 1, Step : 2564, Training Loss : 0.20040, Training Acc : 0.889, Run Time : 0.64
INFO:root:2019-05-12 04:34:29, Epoch : 1, Step : 2565, Training Loss : 0.23515, Training Acc : 0.889, Run Time : 18.71
INFO:root:2019-05-12 04:34:39, Epoch : 1, Step : 2566, Training Loss : 0.20771, Training Acc : 0.911, Run Time : 10.04
INFO:root:2019-05-12 04:34:43, Epoch : 1, Step : 2567, Training Loss : 0.18592, Training Acc : 0.944, Run Time : 4.72
INFO:root:2019-05-12 04:34:44, Epoch : 1, Step : 2568, Training Loss : 0.23564, Training Acc : 0.889, Run Time : 0.47
INFO:root:2019-05-12 04:34:55, Epoch : 1, Step : 2569, Training Loss : 0.19984, Training Acc : 0.922, Run Time : 10.82
INFO:root:2019-05-12 04:34:55, Epoch : 1, Step : 2570, Training Loss : 0.22100, Training Acc : 0.922, Run Time : 0.60
INFO:root:2019-05-12 04:34:57, Epoch : 1, Step : 2571, Training Loss : 0.22388, Training Acc : 0.928, Run Time : 1.87
INFO:root:2019-05-12 04:35:09, Epoch : 1, Step : 2572, Training Loss : 0.35862, Training Acc : 0.867, Run Time : 12.40
INFO:root:2019-05-12 04:35:10, Epoch : 1, Step : 2573, Training Loss : 0.27202, Training Acc : 0.894, Run Time : 0.42
INFO:root:2019-05-12 04:35:10, Epoch : 1, Step : 2574, Training Loss : 0.28022, Training Acc : 0.883, Run Time : 0.53
INFO:root:2019-05-12 04:35:28, Epoch : 1, Step : 2575, Training Loss : 0.25603, Training Acc : 0.894, Run Time : 17.15
INFO:root:2019-05-12 04:35:32, Epoch : 1, Step : 2576, Training Loss : 0.30309, Training Acc : 0.867, Run Time : 4.20
INFO:root:2019-05-12 04:35:32, Epoch : 1, Step : 2577, Training Loss : 0.43770, Training Acc : 0.794, Run Time : 0.51
INFO:root:2019-05-12 04:35:44, Epoch : 1, Step : 2578, Training Loss : 0.32614, Training Acc : 0.833, Run Time : 11.68
INFO:root:2019-05-12 04:35:45, Epoch : 1, Step : 2579, Training Loss : 0.31929, Training Acc : 0.811, Run Time : 0.90
INFO:root:2019-05-12 04:35:46, Epoch : 1, Step : 2580, Training Loss : 0.17305, Training Acc : 0.900, Run Time : 1.25
INFO:root:2019-05-12 04:36:03, Epoch : 1, Step : 2581, Training Loss : 0.19836, Training Acc : 0.911, Run Time : 16.90
INFO:root:2019-05-12 04:36:04, Epoch : 1, Step : 2582, Training Loss : 0.20566, Training Acc : 0.922, Run Time : 1.28
INFO:root:2019-05-12 04:36:16, Epoch : 1, Step : 2583, Training Loss : 0.12124, Training Acc : 0.956, Run Time : 11.47
INFO:root:2019-05-12 04:36:17, Epoch : 1, Step : 2584, Training Loss : 0.27972, Training Acc : 0.883, Run Time : 0.75
INFO:root:2019-05-12 04:36:27, Epoch : 1, Step : 2585, Training Loss : 0.22600, Training Acc : 0.889, Run Time : 10.96
INFO:root:2019-05-12 04:36:28, Epoch : 1, Step : 2586, Training Loss : 0.30171, Training Acc : 0.867, Run Time : 1.00
INFO:root:2019-05-12 04:36:41, Epoch : 1, Step : 2587, Training Loss : 0.26417, Training Acc : 0.867, Run Time : 12.78
INFO:root:2019-05-12 04:36:42, Epoch : 1, Step : 2588, Training Loss : 0.38353, Training Acc : 0.800, Run Time : 0.74
INFO:root:2019-05-12 04:36:43, Epoch : 1, Step : 2589, Training Loss : 0.41197, Training Acc : 0.772, Run Time : 1.09
INFO:root:2019-05-12 04:36:57, Epoch : 1, Step : 2590, Training Loss : 0.33081, Training Acc : 0.817, Run Time : 13.42
INFO:root:2019-05-12 04:36:57, Epoch : 1, Step : 2591, Training Loss : 0.29859, Training Acc : 0.856, Run Time : 0.49
INFO:root:2019-05-12 04:36:58, Epoch : 1, Step : 2592, Training Loss : 0.20907, Training Acc : 0.878, Run Time : 0.62
INFO:root:2019-05-12 04:37:10, Epoch : 1, Step : 2593, Training Loss : 0.18155, Training Acc : 0.906, Run Time : 12.72
INFO:root:2019-05-12 04:37:12, Epoch : 1, Step : 2594, Training Loss : 0.14165, Training Acc : 0.944, Run Time : 1.21
INFO:root:2019-05-12 04:37:15, Epoch : 1, Step : 2595, Training Loss : 0.16950, Training Acc : 0.922, Run Time : 3.63
INFO:root:2019-05-12 04:37:16, Epoch : 1, Step : 2596, Training Loss : 0.21807, Training Acc : 0.894, Run Time : 0.56
INFO:root:2019-05-12 04:37:24, Epoch : 1, Step : 2597, Training Loss : 0.20110, Training Acc : 0.883, Run Time : 7.94
INFO:root:2019-05-12 04:37:27, Epoch : 1, Step : 2598, Training Loss : 0.23598, Training Acc : 0.911, Run Time : 3.31
INFO:root:2019-05-12 04:37:28, Epoch : 1, Step : 2599, Training Loss : 0.25614, Training Acc : 0.900, Run Time : 1.36
INFO:root:2019-05-12 04:37:29, Epoch : 1, Step : 2600, Training Loss : 0.27856, Training Acc : 0.861, Run Time : 0.69
INFO:root:2019-05-12 04:37:38, Epoch : 1, Step : 2601, Training Loss : 0.89938, Training Acc : 0.717, Run Time : 9.22
INFO:root:2019-05-12 04:37:39, Epoch : 1, Step : 2602, Training Loss : 0.77958, Training Acc : 0.728, Run Time : 0.45
INFO:root:2019-05-12 04:37:40, Epoch : 1, Step : 2603, Training Loss : 0.57817, Training Acc : 0.706, Run Time : 1.73
INFO:root:2019-05-12 04:37:50, Epoch : 1, Step : 2604, Training Loss : 0.64311, Training Acc : 0.767, Run Time : 10.03
INFO:root:2019-05-12 04:37:51, Epoch : 1, Step : 2605, Training Loss : 0.51994, Training Acc : 0.783, Run Time : 0.52
INFO:root:2019-05-12 04:37:52, Epoch : 1, Step : 2606, Training Loss : 0.49780, Training Acc : 0.772, Run Time : 0.92
INFO:root:2019-05-12 04:38:04, Epoch : 1, Step : 2607, Training Loss : 0.38537, Training Acc : 0.806, Run Time : 12.32
INFO:root:2019-05-12 04:38:05, Epoch : 1, Step : 2608, Training Loss : 0.35665, Training Acc : 0.789, Run Time : 0.58
INFO:root:2019-05-12 04:38:19, Epoch : 1, Step : 2609, Training Loss : 0.31944, Training Acc : 0.856, Run Time : 14.12
INFO:root:2019-05-12 04:38:33, Epoch : 1, Step : 2610, Training Loss : 0.19281, Training Acc : 0.950, Run Time : 14.21
INFO:root:2019-05-12 04:38:35, Epoch : 1, Step : 2611, Training Loss : 0.18924, Training Acc : 0.944, Run Time : 1.71
INFO:root:2019-05-12 04:38:35, Epoch : 1, Step : 2612, Training Loss : 0.40702, Training Acc : 0.794, Run Time : 0.59
INFO:root:2019-05-12 04:38:48, Epoch : 1, Step : 2613, Training Loss : 0.23331, Training Acc : 0.900, Run Time : 12.08
INFO:root:2019-05-12 04:38:49, Epoch : 1, Step : 2614, Training Loss : 0.07976, Training Acc : 0.972, Run Time : 0.99
INFO:root:2019-05-12 04:39:00, Epoch : 1, Step : 2615, Training Loss : 0.39528, Training Acc : 0.833, Run Time : 11.47
INFO:root:2019-05-12 04:39:01, Epoch : 1, Step : 2616, Training Loss : 0.19994, Training Acc : 0.900, Run Time : 0.52
INFO:root:2019-05-12 04:39:03, Epoch : 1, Step : 2617, Training Loss : 0.10859, Training Acc : 0.961, Run Time : 2.26
INFO:root:2019-05-12 04:39:15, Epoch : 1, Step : 2618, Training Loss : 0.29122, Training Acc : 0.906, Run Time : 12.45
INFO:root:2019-05-12 04:39:16, Epoch : 1, Step : 2619, Training Loss : 0.20877, Training Acc : 0.906, Run Time : 1.17
INFO:root:2019-05-12 04:39:29, Epoch : 1, Step : 2620, Training Loss : 0.23912, Training Acc : 0.917, Run Time : 12.18
INFO:root:2019-05-12 04:39:29, Epoch : 1, Step : 2621, Training Loss : 0.21592, Training Acc : 0.933, Run Time : 0.45
INFO:root:2019-05-12 04:39:30, Epoch : 1, Step : 2622, Training Loss : 0.30302, Training Acc : 0.878, Run Time : 1.35
INFO:root:2019-05-12 04:39:42, Epoch : 1, Step : 2623, Training Loss : 0.31449, Training Acc : 0.889, Run Time : 11.70
INFO:root:2019-05-12 04:39:43, Epoch : 1, Step : 2624, Training Loss : 0.27100, Training Acc : 0.889, Run Time : 0.93
INFO:root:2019-05-12 04:39:56, Epoch : 1, Step : 2625, Training Loss : 0.40807, Training Acc : 0.872, Run Time : 13.05
INFO:root:2019-05-12 04:40:08, Epoch : 1, Step : 2626, Training Loss : 0.30577, Training Acc : 0.894, Run Time : 11.63
INFO:root:2019-05-12 04:40:14, Epoch : 1, Step : 2627, Training Loss : 0.45836, Training Acc : 0.828, Run Time : 6.79
INFO:root:2019-05-12 04:40:15, Epoch : 1, Step : 2628, Training Loss : 1.27373, Training Acc : 0.617, Run Time : 0.81
INFO:root:2019-05-12 04:40:26, Epoch : 1, Step : 2629, Training Loss : 0.57011, Training Acc : 0.789, Run Time : 10.83
INFO:root:2019-05-12 04:40:27, Epoch : 1, Step : 2630, Training Loss : 0.38018, Training Acc : 0.828, Run Time : 0.82
INFO:root:2019-05-12 04:40:34, Epoch : 1, Step : 2631, Training Loss : 0.31891, Training Acc : 0.911, Run Time : 7.41
INFO:root:2019-05-12 04:40:38, Epoch : 1, Step : 2632, Training Loss : 0.66060, Training Acc : 0.711, Run Time : 3.30
INFO:root:2019-05-12 04:40:39, Epoch : 1, Step : 2633, Training Loss : 1.03037, Training Acc : 0.628, Run Time : 1.13
INFO:root:2019-05-12 04:40:52, Epoch : 1, Step : 2634, Training Loss : 0.51008, Training Acc : 0.778, Run Time : 12.89
INFO:root:2019-05-12 04:40:52, Epoch : 1, Step : 2635, Training Loss : 0.84738, Training Acc : 0.633, Run Time : 0.65
INFO:root:2019-05-12 04:40:53, Epoch : 1, Step : 2636, Training Loss : 0.70083, Training Acc : 0.689, Run Time : 0.59
INFO:root:2019-05-12 04:41:10, Epoch : 1, Step : 2637, Training Loss : 0.49812, Training Acc : 0.744, Run Time : 17.07
INFO:root:2019-05-12 04:41:12, Epoch : 1, Step : 2638, Training Loss : 0.55521, Training Acc : 0.683, Run Time : 1.58
INFO:root:2019-05-12 04:41:22, Epoch : 1, Step : 2639, Training Loss : 0.45772, Training Acc : 0.756, Run Time : 9.99
INFO:root:2019-05-12 04:41:23, Epoch : 1, Step : 2640, Training Loss : 0.57242, Training Acc : 0.761, Run Time : 1.52
INFO:root:2019-05-12 04:41:33, Epoch : 1, Step : 2641, Training Loss : 0.34972, Training Acc : 0.850, Run Time : 10.40
INFO:root:2019-05-12 04:41:34, Epoch : 1, Step : 2642, Training Loss : 0.20146, Training Acc : 0.944, Run Time : 0.74
INFO:root:2019-05-12 04:41:46, Epoch : 1, Step : 2643, Training Loss : 0.21753, Training Acc : 0.944, Run Time : 11.61
INFO:root:2019-05-12 04:41:47, Epoch : 1, Step : 2644, Training Loss : 0.52444, Training Acc : 0.811, Run Time : 1.02
INFO:root:2019-05-12 04:42:00, Epoch : 1, Step : 2645, Training Loss : 0.32385, Training Acc : 0.861, Run Time : 12.66
INFO:root:2019-05-12 04:42:00, Epoch : 1, Step : 2646, Training Loss : 0.23325, Training Acc : 0.883, Run Time : 0.85
INFO:root:2019-05-12 04:42:02, Epoch : 1, Step : 2647, Training Loss : 0.34470, Training Acc : 0.856, Run Time : 1.91
INFO:root:2019-05-12 04:42:14, Epoch : 1, Step : 2648, Training Loss : 0.23508, Training Acc : 0.906, Run Time : 12.14
INFO:root:2019-05-12 04:42:17, Epoch : 1, Step : 2649, Training Loss : 0.52353, Training Acc : 0.761, Run Time : 2.55
INFO:root:2019-05-12 04:42:28, Epoch : 1, Step : 2650, Training Loss : 0.33854, Training Acc : 0.828, Run Time : 11.48
INFO:root:2019-05-12 04:42:29, Epoch : 1, Step : 2651, Training Loss : 0.24434, Training Acc : 0.911, Run Time : 0.49
INFO:root:2019-05-12 04:42:29, Epoch : 1, Step : 2652, Training Loss : 0.17845, Training Acc : 0.961, Run Time : 0.41
INFO:root:2019-05-12 04:42:44, Epoch : 1, Step : 2653, Training Loss : 0.45013, Training Acc : 0.800, Run Time : 14.17
INFO:root:2019-05-12 04:42:45, Epoch : 1, Step : 2654, Training Loss : 0.29066, Training Acc : 0.889, Run Time : 1.16
INFO:root:2019-05-12 04:42:55, Epoch : 1, Step : 2655, Training Loss : 0.34502, Training Acc : 0.861, Run Time : 10.79
INFO:root:2019-05-12 04:42:59, Epoch : 1, Step : 2656, Training Loss : 0.15112, Training Acc : 0.972, Run Time : 3.72
INFO:root:2019-05-12 04:43:00, Epoch : 1, Step : 2657, Training Loss : 0.18985, Training Acc : 0.956, Run Time : 0.71
INFO:root:2019-05-12 04:43:09, Epoch : 1, Step : 2658, Training Loss : 0.18026, Training Acc : 0.961, Run Time : 9.04
INFO:root:2019-05-12 04:43:10, Epoch : 1, Step : 2659, Training Loss : 0.11587, Training Acc : 0.989, Run Time : 1.33
INFO:root:2019-05-12 04:43:31, Epoch : 1, Step : 2660, Training Loss : 0.18308, Training Acc : 0.978, Run Time : 20.60
INFO:root:2019-05-12 04:43:33, Epoch : 1, Step : 2661, Training Loss : 0.10806, Training Acc : 1.000, Run Time : 1.89
INFO:root:2019-05-12 04:43:46, Epoch : 1, Step : 2662, Training Loss : 0.18321, Training Acc : 0.950, Run Time : 13.23
INFO:root:2019-05-12 04:43:47, Epoch : 1, Step : 2663, Training Loss : 0.13038, Training Acc : 0.972, Run Time : 0.92
INFO:root:2019-05-12 04:43:48, Epoch : 1, Step : 2664, Training Loss : 0.28688, Training Acc : 0.894, Run Time : 1.60
INFO:root:2019-05-12 04:44:01, Epoch : 1, Step : 2665, Training Loss : 0.22027, Training Acc : 0.944, Run Time : 12.08
INFO:root:2019-05-12 04:44:02, Epoch : 1, Step : 2666, Training Loss : 0.34158, Training Acc : 0.850, Run Time : 1.62
INFO:root:2019-05-12 04:44:12, Epoch : 1, Step : 2667, Training Loss : 0.24016, Training Acc : 0.922, Run Time : 9.84
INFO:root:2019-05-12 04:44:13, Epoch : 1, Step : 2668, Training Loss : 0.17530, Training Acc : 0.944, Run Time : 0.76
INFO:root:2019-05-12 04:44:31, Epoch : 1, Step : 2669, Training Loss : 0.25616, Training Acc : 0.889, Run Time : 18.56
INFO:root:2019-05-12 04:44:47, Epoch : 1, Step : 2670, Training Loss : 0.34345, Training Acc : 0.883, Run Time : 15.25
INFO:root:2019-05-12 04:44:48, Epoch : 1, Step : 2671, Training Loss : 0.06425, Training Acc : 0.994, Run Time : 1.29
INFO:root:2019-05-12 04:44:51, Epoch : 1, Step : 2672, Training Loss : 0.26203, Training Acc : 0.922, Run Time : 2.76
INFO:root:2019-05-12 04:45:01, Epoch : 1, Step : 2673, Training Loss : 0.28683, Training Acc : 0.878, Run Time : 10.55
INFO:root:2019-05-12 04:45:02, Epoch : 1, Step : 2674, Training Loss : 0.20319, Training Acc : 0.906, Run Time : 0.53
INFO:root:2019-05-12 04:45:04, Epoch : 1, Step : 2675, Training Loss : 0.13651, Training Acc : 0.978, Run Time : 1.93
INFO:root:2019-05-12 04:45:15, Epoch : 1, Step : 2676, Training Loss : 0.32576, Training Acc : 0.878, Run Time : 11.12
INFO:root:2019-05-12 04:45:16, Epoch : 1, Step : 2677, Training Loss : 0.24062, Training Acc : 0.883, Run Time : 0.75
INFO:root:2019-05-12 04:45:26, Epoch : 1, Step : 2678, Training Loss : 0.15149, Training Acc : 0.961, Run Time : 10.20
INFO:root:2019-05-12 04:45:27, Epoch : 1, Step : 2679, Training Loss : 0.45183, Training Acc : 0.800, Run Time : 0.80
INFO:root:2019-05-12 04:45:34, Epoch : 1, Step : 2680, Training Loss : 0.23349, Training Acc : 0.900, Run Time : 6.97
INFO:root:2019-05-12 04:45:36, Epoch : 1, Step : 2681, Training Loss : 0.55777, Training Acc : 0.800, Run Time : 2.29
INFO:root:2019-05-12 04:45:37, Epoch : 1, Step : 2682, Training Loss : 0.42232, Training Acc : 0.833, Run Time : 0.93
INFO:root:2019-05-12 04:45:47, Epoch : 1, Step : 2683, Training Loss : 0.34152, Training Acc : 0.856, Run Time : 10.50
INFO:root:2019-05-12 04:45:48, Epoch : 1, Step : 2684, Training Loss : 0.54131, Training Acc : 0.806, Run Time : 1.05
INFO:root:2019-05-12 04:45:49, Epoch : 1, Step : 2685, Training Loss : 0.10712, Training Acc : 0.978, Run Time : 0.74
INFO:root:2019-05-12 04:45:50, Epoch : 1, Step : 2686, Training Loss : 0.11860, Training Acc : 0.956, Run Time : 1.46
INFO:root:2019-05-12 04:46:00, Epoch : 1, Step : 2687, Training Loss : 0.08547, Training Acc : 0.978, Run Time : 9.69
INFO:root:2019-05-12 04:46:01, Epoch : 1, Step : 2688, Training Loss : 0.10767, Training Acc : 0.983, Run Time : 0.61
INFO:root:2019-05-12 04:46:02, Epoch : 1, Step : 2689, Training Loss : 0.12033, Training Acc : 0.961, Run Time : 1.41
INFO:root:2019-05-12 04:46:11, Epoch : 1, Step : 2690, Training Loss : 0.12430, Training Acc : 0.967, Run Time : 9.08
INFO:root:2019-05-12 04:46:12, Epoch : 1, Step : 2691, Training Loss : 0.27801, Training Acc : 0.889, Run Time : 0.53
INFO:root:2019-05-12 04:46:13, Epoch : 1, Step : 2692, Training Loss : 0.11425, Training Acc : 0.967, Run Time : 1.35
INFO:root:2019-05-12 04:46:27, Epoch : 1, Step : 2693, Training Loss : 0.12903, Training Acc : 0.967, Run Time : 14.07
INFO:root:2019-05-12 04:46:31, Epoch : 1, Step : 2694, Training Loss : 0.06119, Training Acc : 0.989, Run Time : 3.64
INFO:root:2019-05-12 04:46:31, Epoch : 1, Step : 2695, Training Loss : 0.14420, Training Acc : 0.967, Run Time : 0.61
INFO:root:2019-05-12 04:46:32, Epoch : 1, Step : 2696, Training Loss : 0.08136, Training Acc : 0.994, Run Time : 0.59
INFO:root:2019-05-12 04:46:33, Epoch : 1, Step : 2697, Training Loss : 0.09133, Training Acc : 0.978, Run Time : 0.61
INFO:root:2019-05-12 04:46:33, Epoch : 1, Step : 2698, Training Loss : 0.13163, Training Acc : 0.961, Run Time : 0.63
INFO:root:2019-05-12 04:46:51, Epoch : 1, Step : 2699, Training Loss : 0.12596, Training Acc : 0.939, Run Time : 18.11
INFO:root:2019-05-12 04:46:52, Epoch : 1, Step : 2700, Training Loss : 0.06758, Training Acc : 0.983, Run Time : 0.57
INFO:root:2019-05-12 04:47:04, Epoch : 1, Step : 2701, Training Loss : 0.08856, Training Acc : 0.972, Run Time : 11.65
INFO:root:2019-05-12 04:47:04, Epoch : 1, Step : 2702, Training Loss : 0.05599, Training Acc : 1.000, Run Time : 0.66
INFO:root:2019-05-12 04:47:15, Epoch : 1, Step : 2703, Training Loss : 0.03696, Training Acc : 1.000, Run Time : 11.21
INFO:root:2019-05-12 04:47:16, Epoch : 1, Step : 2704, Training Loss : 0.05362, Training Acc : 1.000, Run Time : 0.66
INFO:root:2019-05-12 04:47:17, Epoch : 1, Step : 2705, Training Loss : 0.14704, Training Acc : 0.967, Run Time : 0.62
INFO:root:2019-05-12 04:47:28, Epoch : 1, Step : 2706, Training Loss : 0.07826, Training Acc : 0.972, Run Time : 11.49
INFO:root:2019-05-12 04:47:29, Epoch : 1, Step : 2707, Training Loss : 0.05151, Training Acc : 0.989, Run Time : 0.47
INFO:root:2019-05-12 04:47:31, Epoch : 1, Step : 2708, Training Loss : 0.08060, Training Acc : 0.978, Run Time : 1.86
INFO:root:2019-05-12 04:47:42, Epoch : 1, Step : 2709, Training Loss : 0.13329, Training Acc : 0.950, Run Time : 11.09
INFO:root:2019-05-12 04:47:42, Epoch : 1, Step : 2710, Training Loss : 0.10610, Training Acc : 0.967, Run Time : 0.79
INFO:root:2019-05-12 04:47:54, Epoch : 1, Step : 2711, Training Loss : 0.03711, Training Acc : 1.000, Run Time : 11.48
INFO:root:2019-05-12 04:47:57, Epoch : 1, Step : 2712, Training Loss : 0.03314, Training Acc : 0.994, Run Time : 2.84
INFO:root:2019-05-12 04:48:06, Epoch : 1, Step : 2713, Training Loss : 0.02952, Training Acc : 1.000, Run Time : 9.17
INFO:root:2019-05-12 04:48:07, Epoch : 1, Step : 2714, Training Loss : 0.04945, Training Acc : 1.000, Run Time : 1.24
INFO:root:2019-05-12 04:48:17, Epoch : 1, Step : 2715, Training Loss : 0.04677, Training Acc : 0.994, Run Time : 10.26
INFO:root:2019-05-12 04:48:18, Epoch : 1, Step : 2716, Training Loss : 0.03839, Training Acc : 0.989, Run Time : 0.86
INFO:root:2019-05-12 04:48:28, Epoch : 1, Step : 2717, Training Loss : 0.09672, Training Acc : 0.967, Run Time : 9.90
INFO:root:2019-05-12 04:48:29, Epoch : 1, Step : 2718, Training Loss : 0.07860, Training Acc : 0.972, Run Time : 0.67
INFO:root:2019-05-12 04:48:31, Epoch : 1, Step : 2719, Training Loss : 0.03667, Training Acc : 1.000, Run Time : 1.72
INFO:root:2019-05-12 04:48:42, Epoch : 1, Step : 2720, Training Loss : 0.10225, Training Acc : 0.956, Run Time : 11.86
INFO:root:2019-05-12 04:48:43, Epoch : 1, Step : 2721, Training Loss : 0.05802, Training Acc : 0.989, Run Time : 1.00
INFO:root:2019-05-12 04:48:54, Epoch : 1, Step : 2722, Training Loss : 0.10942, Training Acc : 0.972, Run Time : 10.81
INFO:root:2019-05-12 04:48:55, Epoch : 1, Step : 2723, Training Loss : 0.12128, Training Acc : 0.944, Run Time : 1.05
INFO:root:2019-05-12 04:49:05, Epoch : 1, Step : 2724, Training Loss : 0.03873, Training Acc : 0.989, Run Time : 9.50
INFO:root:2019-05-12 04:49:06, Epoch : 1, Step : 2725, Training Loss : 0.03325, Training Acc : 1.000, Run Time : 0.72
INFO:root:2019-05-12 04:49:07, Epoch : 1, Step : 2726, Training Loss : 0.17932, Training Acc : 0.944, Run Time : 1.70
INFO:root:2019-05-12 04:49:18, Epoch : 1, Step : 2727, Training Loss : 0.03768, Training Acc : 1.000, Run Time : 10.60
INFO:root:2019-05-12 04:49:20, Epoch : 1, Step : 2728, Training Loss : 0.12560, Training Acc : 0.950, Run Time : 2.58
INFO:root:2019-05-12 04:49:30, Epoch : 1, Step : 2729, Training Loss : 0.03504, Training Acc : 0.994, Run Time : 9.41
INFO:root:2019-05-12 04:49:30, Epoch : 1, Step : 2730, Training Loss : 0.04663, Training Acc : 0.994, Run Time : 0.55
INFO:root:2019-05-12 04:49:32, Epoch : 1, Step : 2731, Training Loss : 0.03667, Training Acc : 0.994, Run Time : 1.57
INFO:root:2019-05-12 04:49:43, Epoch : 1, Step : 2732, Training Loss : 0.03589, Training Acc : 0.994, Run Time : 10.63
INFO:root:2019-05-12 04:49:44, Epoch : 1, Step : 2733, Training Loss : 0.05188, Training Acc : 0.989, Run Time : 1.21
INFO:root:2019-05-12 04:49:54, Epoch : 1, Step : 2734, Training Loss : 0.06272, Training Acc : 0.989, Run Time : 10.66
INFO:root:2019-05-12 04:49:56, Epoch : 1, Step : 2735, Training Loss : 0.07197, Training Acc : 0.967, Run Time : 1.09
INFO:root:2019-05-12 04:50:05, Epoch : 1, Step : 2736, Training Loss : 0.05142, Training Acc : 0.989, Run Time : 9.60
INFO:root:2019-05-12 04:50:06, Epoch : 1, Step : 2737, Training Loss : 0.05566, Training Acc : 0.983, Run Time : 1.35
INFO:root:2019-05-12 04:50:08, Epoch : 1, Step : 2738, Training Loss : 0.11334, Training Acc : 0.956, Run Time : 1.02
INFO:root:2019-05-12 04:50:23, Epoch : 1, Step : 2739, Training Loss : 0.14175, Training Acc : 0.950, Run Time : 15.26
INFO:root:2019-05-12 04:50:33, Epoch : 1, Step : 2740, Training Loss : 0.80912, Training Acc : 0.728, Run Time : 10.11
INFO:root:2019-05-12 04:50:35, Epoch : 1, Step : 2741, Training Loss : 0.36320, Training Acc : 0.883, Run Time : 2.14
INFO:root:2019-05-12 04:50:36, Epoch : 1, Step : 2742, Training Loss : 0.26636, Training Acc : 0.850, Run Time : 0.64
INFO:root:2019-05-12 04:50:37, Epoch : 1, Step : 2743, Training Loss : 0.14281, Training Acc : 0.933, Run Time : 1.16
INFO:root:2019-05-12 04:50:45, Epoch : 1, Step : 2744, Training Loss : 0.31089, Training Acc : 0.894, Run Time : 7.84
INFO:root:2019-05-12 04:50:46, Epoch : 1, Step : 2745, Training Loss : 0.78499, Training Acc : 0.806, Run Time : 0.96
INFO:root:2019-05-12 04:50:47, Epoch : 1, Step : 2746, Training Loss : 0.35984, Training Acc : 0.867, Run Time : 0.91
INFO:root:2019-05-12 04:50:57, Epoch : 1, Step : 2747, Training Loss : 0.71382, Training Acc : 0.711, Run Time : 10.54
INFO:root:2019-05-12 04:50:58, Epoch : 1, Step : 2748, Training Loss : 0.20061, Training Acc : 0.911, Run Time : 1.04
INFO:root:2019-05-12 04:50:59, Epoch : 1, Step : 2749, Training Loss : 0.34832, Training Acc : 0.894, Run Time : 0.65
INFO:root:2019-05-12 04:51:00, Epoch : 1, Step : 2750, Training Loss : 0.28566, Training Acc : 0.911, Run Time : 1.57
INFO:root:2019-05-12 04:51:10, Epoch : 1, Step : 2751, Training Loss : 0.47309, Training Acc : 0.850, Run Time : 10.15
INFO:root:2019-05-12 04:51:11, Epoch : 1, Step : 2752, Training Loss : 0.47000, Training Acc : 0.822, Run Time : 0.63
INFO:root:2019-05-12 04:51:13, Epoch : 1, Step : 2753, Training Loss : 0.72994, Training Acc : 0.717, Run Time : 2.25
INFO:root:2019-05-12 04:51:25, Epoch : 1, Step : 2754, Training Loss : 0.59724, Training Acc : 0.767, Run Time : 11.86
INFO:root:2019-05-12 04:51:26, Epoch : 1, Step : 2755, Training Loss : 0.37468, Training Acc : 0.844, Run Time : 0.89
INFO:root:2019-05-12 04:51:36, Epoch : 1, Step : 2756, Training Loss : 0.20138, Training Acc : 0.917, Run Time : 10.08
INFO:root:2019-05-12 04:51:37, Epoch : 1, Step : 2757, Training Loss : 0.30360, Training Acc : 0.883, Run Time : 0.49
INFO:root:2019-05-12 04:51:48, Epoch : 1, Step : 2758, Training Loss : 0.27221, Training Acc : 0.861, Run Time : 11.16
INFO:root:2019-05-12 04:51:49, Epoch : 1, Step : 2759, Training Loss : 0.33319, Training Acc : 0.878, Run Time : 0.73
INFO:root:2019-05-12 04:51:51, Epoch : 1, Step : 2760, Training Loss : 0.33248, Training Acc : 0.856, Run Time : 2.11
INFO:root:2019-05-12 04:52:00, Epoch : 1, Step : 2761, Training Loss : 0.23743, Training Acc : 0.933, Run Time : 9.44
INFO:root:2019-05-12 04:52:01, Epoch : 1, Step : 2762, Training Loss : 0.30560, Training Acc : 0.883, Run Time : 0.56
INFO:root:2019-05-12 04:52:03, Epoch : 1, Step : 2763, Training Loss : 0.15376, Training Acc : 0.933, Run Time : 1.86
INFO:root:2019-05-12 04:52:12, Epoch : 1, Step : 2764, Training Loss : 0.24604, Training Acc : 0.894, Run Time : 9.06
INFO:root:2019-05-12 04:52:12, Epoch : 1, Step : 2765, Training Loss : 0.23015, Training Acc : 0.917, Run Time : 0.86
INFO:root:2019-05-12 04:52:14, Epoch : 1, Step : 2766, Training Loss : 0.25348, Training Acc : 0.878, Run Time : 1.13
INFO:root:2019-05-12 04:52:25, Epoch : 1, Step : 2767, Training Loss : 0.57296, Training Acc : 0.739, Run Time : 11.16
INFO:root:2019-05-12 04:52:25, Epoch : 1, Step : 2768, Training Loss : 0.24590, Training Acc : 0.911, Run Time : 0.51
INFO:root:2019-05-12 04:52:27, Epoch : 1, Step : 2769, Training Loss : 0.21752, Training Acc : 0.928, Run Time : 1.84
INFO:root:2019-05-12 04:52:37, Epoch : 1, Step : 2770, Training Loss : 0.25291, Training Acc : 0.894, Run Time : 10.19
INFO:root:2019-05-12 04:52:38, Epoch : 1, Step : 2771, Training Loss : 0.22584, Training Acc : 0.933, Run Time : 0.87
INFO:root:2019-05-12 04:52:51, Epoch : 1, Step : 2772, Training Loss : 0.23143, Training Acc : 0.894, Run Time : 12.68
INFO:root:2019-05-12 04:52:52, Epoch : 1, Step : 2773, Training Loss : 0.16422, Training Acc : 0.961, Run Time : 0.75
INFO:root:2019-05-12 04:52:53, Epoch : 1, Step : 2774, Training Loss : 0.15239, Training Acc : 0.961, Run Time : 1.34
INFO:root:2019-05-12 04:53:11, Epoch : 1, Step : 2775, Training Loss : 0.16700, Training Acc : 0.944, Run Time : 18.36
INFO:root:2019-05-12 04:53:18, Epoch : 1, Step : 2776, Training Loss : 0.19915, Training Acc : 0.928, Run Time : 7.15
INFO:root:2019-05-12 04:53:19, Epoch : 1, Step : 2777, Training Loss : 0.20326, Training Acc : 0.922, Run Time : 0.83
INFO:root:2019-05-12 04:53:31, Epoch : 1, Step : 2778, Training Loss : 0.48298, Training Acc : 0.783, Run Time : 11.73
INFO:root:2019-05-12 04:53:31, Epoch : 1, Step : 2779, Training Loss : 0.18395, Training Acc : 0.939, Run Time : 0.45
INFO:root:2019-05-12 04:53:32, Epoch : 1, Step : 2780, Training Loss : 0.21053, Training Acc : 0.922, Run Time : 0.61
INFO:root:2019-05-12 04:53:44, Epoch : 1, Step : 2781, Training Loss : 0.15385, Training Acc : 0.933, Run Time : 12.33
INFO:root:2019-05-12 04:53:45, Epoch : 1, Step : 2782, Training Loss : 0.42330, Training Acc : 0.778, Run Time : 0.63
INFO:root:2019-05-12 04:53:46, Epoch : 1, Step : 2783, Training Loss : 0.12490, Training Acc : 0.961, Run Time : 0.61
INFO:root:2019-05-12 04:53:54, Epoch : 1, Step : 2784, Training Loss : 0.64127, Training Acc : 0.728, Run Time : 8.23
INFO:root:2019-05-12 04:53:54, Epoch : 1, Step : 2785, Training Loss : 0.73417, Training Acc : 0.689, Run Time : 0.58
INFO:root:2019-05-12 04:53:56, Epoch : 1, Step : 2786, Training Loss : 0.76057, Training Acc : 0.622, Run Time : 1.70
INFO:root:2019-05-12 04:54:06, Epoch : 1, Step : 2787, Training Loss : 0.47356, Training Acc : 0.761, Run Time : 10.28
INFO:root:2019-05-12 04:54:08, Epoch : 1, Step : 2788, Training Loss : 0.34247, Training Acc : 0.850, Run Time : 1.10
INFO:root:2019-05-12 04:54:21, Epoch : 1, Step : 2789, Training Loss : 0.20178, Training Acc : 0.911, Run Time : 13.25
INFO:root:2019-05-12 04:54:23, Epoch : 1, Step : 2790, Training Loss : 0.20821, Training Acc : 0.933, Run Time : 1.81
INFO:root:2019-05-12 04:54:23, Epoch : 1, Step : 2791, Training Loss : 0.20231, Training Acc : 0.922, Run Time : 0.76
INFO:root:2019-05-12 04:54:32, Epoch : 1, Step : 2792, Training Loss : 0.33298, Training Acc : 0.811, Run Time : 8.98
INFO:root:2019-05-12 04:54:33, Epoch : 1, Step : 2793, Training Loss : 0.21745, Training Acc : 0.922, Run Time : 0.44
INFO:root:2019-05-12 04:54:34, Epoch : 1, Step : 2794, Training Loss : 0.43334, Training Acc : 0.750, Run Time : 1.70
INFO:root:2019-05-12 04:54:45, Epoch : 1, Step : 2795, Training Loss : 0.67806, Training Acc : 0.833, Run Time : 10.52
INFO:root:2019-05-12 04:54:45, Epoch : 1, Step : 2796, Training Loss : 0.36753, Training Acc : 0.806, Run Time : 0.52
INFO:root:2019-05-12 04:54:46, Epoch : 1, Step : 2797, Training Loss : 0.88470, Training Acc : 0.639, Run Time : 0.63
INFO:root:2019-05-12 04:54:48, Epoch : 1, Step : 2798, Training Loss : 0.32015, Training Acc : 0.872, Run Time : 1.59
INFO:root:2019-05-12 04:54:58, Epoch : 1, Step : 2799, Training Loss : 0.41797, Training Acc : 0.783, Run Time : 10.51
INFO:root:2019-05-12 04:54:59, Epoch : 1, Step : 2800, Training Loss : 0.41826, Training Acc : 0.783, Run Time : 0.61
INFO:root:2019-05-12 04:55:10, Epoch : 1, Step : 2801, Training Loss : 0.44215, Training Acc : 0.778, Run Time : 11.48
INFO:root:2019-05-12 04:55:11, Epoch : 1, Step : 2802, Training Loss : 0.53017, Training Acc : 0.689, Run Time : 0.52
INFO:root:2019-05-12 04:55:11, Epoch : 1, Step : 2803, Training Loss : 0.54311, Training Acc : 0.789, Run Time : 0.61
INFO:root:2019-05-12 04:55:21, Epoch : 1, Step : 2804, Training Loss : 0.41929, Training Acc : 0.828, Run Time : 9.64
INFO:root:2019-05-12 04:55:22, Epoch : 1, Step : 2805, Training Loss : 0.21367, Training Acc : 0.917, Run Time : 0.59
INFO:root:2019-05-12 04:55:23, Epoch : 1, Step : 2806, Training Loss : 0.26484, Training Acc : 0.922, Run Time : 1.74
INFO:root:2019-05-12 04:55:36, Epoch : 1, Step : 2807, Training Loss : 0.25168, Training Acc : 0.911, Run Time : 12.94
INFO:root:2019-05-12 04:55:39, Epoch : 1, Step : 2808, Training Loss : 0.25379, Training Acc : 0.906, Run Time : 2.23
INFO:root:2019-05-12 04:55:50, Epoch : 1, Step : 2809, Training Loss : 0.28703, Training Acc : 0.878, Run Time : 11.90
INFO:root:2019-05-12 04:56:00, Epoch : 1, Step : 2810, Training Loss : 0.27930, Training Acc : 0.922, Run Time : 9.78
INFO:root:2019-05-12 04:56:01, Epoch : 1, Step : 2811, Training Loss : 0.30920, Training Acc : 0.900, Run Time : 1.14
INFO:root:2019-05-12 04:56:02, Epoch : 1, Step : 2812, Training Loss : 0.23774, Training Acc : 0.894, Run Time : 0.65
INFO:root:2019-05-12 04:56:04, Epoch : 1, Step : 2813, Training Loss : 0.29202, Training Acc : 0.883, Run Time : 1.86
INFO:root:2019-05-12 04:56:13, Epoch : 1, Step : 2814, Training Loss : 0.21261, Training Acc : 0.933, Run Time : 8.82
INFO:root:2019-05-12 04:56:13, Epoch : 1, Step : 2815, Training Loss : 0.21992, Training Acc : 0.922, Run Time : 0.59
INFO:root:2019-05-12 04:56:15, Epoch : 1, Step : 2816, Training Loss : 0.20529, Training Acc : 0.917, Run Time : 1.69
INFO:root:2019-05-12 04:56:25, Epoch : 1, Step : 2817, Training Loss : 0.24357, Training Acc : 0.933, Run Time : 9.86
INFO:root:2019-05-12 04:56:26, Epoch : 1, Step : 2818, Training Loss : 0.26293, Training Acc : 0.906, Run Time : 0.84
INFO:root:2019-05-12 04:56:39, Epoch : 1, Step : 2819, Training Loss : 0.19636, Training Acc : 0.911, Run Time : 12.82
INFO:root:2019-05-12 04:56:40, Epoch : 1, Step : 2820, Training Loss : 0.18425, Training Acc : 0.911, Run Time : 1.00
INFO:root:2019-05-12 04:56:56, Epoch : 1, Step : 2821, Training Loss : 0.18753, Training Acc : 0.939, Run Time : 16.88
INFO:root:2019-05-12 04:57:03, Epoch : 1, Step : 2822, Training Loss : 0.30879, Training Acc : 0.822, Run Time : 6.14
INFO:root:2019-05-12 04:57:14, Epoch : 1, Step : 2823, Training Loss : 0.17223, Training Acc : 0.917, Run Time : 10.96
INFO:root:2019-05-12 04:57:14, Epoch : 1, Step : 2824, Training Loss : 0.14725, Training Acc : 0.956, Run Time : 0.44
INFO:root:2019-05-12 04:57:16, Epoch : 1, Step : 2825, Training Loss : 0.29666, Training Acc : 0.889, Run Time : 1.71
INFO:root:2019-05-12 04:57:26, Epoch : 1, Step : 2826, Training Loss : 0.17329, Training Acc : 0.928, Run Time : 10.51
INFO:root:2019-05-12 04:57:27, Epoch : 1, Step : 2827, Training Loss : 0.22394, Training Acc : 0.883, Run Time : 0.48
INFO:root:2019-05-12 04:57:27, Epoch : 1, Step : 2828, Training Loss : 0.10213, Training Acc : 0.978, Run Time : 0.52
INFO:root:2019-05-12 04:57:39, Epoch : 1, Step : 2829, Training Loss : 0.14975, Training Acc : 0.956, Run Time : 11.41
INFO:root:2019-05-12 04:57:40, Epoch : 1, Step : 2830, Training Loss : 0.17013, Training Acc : 0.950, Run Time : 1.50
INFO:root:2019-05-12 04:57:51, Epoch : 1, Step : 2831, Training Loss : 0.20813, Training Acc : 0.922, Run Time : 10.57
INFO:root:2019-05-12 04:57:51, Epoch : 1, Step : 2832, Training Loss : 0.25122, Training Acc : 0.889, Run Time : 0.63
INFO:root:2019-05-12 04:57:52, Epoch : 1, Step : 2833, Training Loss : 0.19117, Training Acc : 0.906, Run Time : 1.05
INFO:root:2019-05-12 04:58:04, Epoch : 1, Step : 2834, Training Loss : 0.17507, Training Acc : 0.928, Run Time : 12.06
INFO:root:2019-05-12 04:58:05, Epoch : 1, Step : 2835, Training Loss : 0.12336, Training Acc : 0.950, Run Time : 0.57
INFO:root:2019-05-12 04:58:07, Epoch : 1, Step : 2836, Training Loss : 0.15728, Training Acc : 0.922, Run Time : 1.87
INFO:root:2019-05-12 04:58:18, Epoch : 1, Step : 2837, Training Loss : 0.20645, Training Acc : 0.889, Run Time : 10.85
INFO:root:2019-05-12 04:58:31, Epoch : 1, Step : 2838, Training Loss : 0.10460, Training Acc : 0.961, Run Time : 13.09
INFO:root:2019-05-12 04:58:32, Epoch : 1, Step : 2839, Training Loss : 0.29714, Training Acc : 0.856, Run Time : 1.56
INFO:root:2019-05-12 04:58:33, Epoch : 1, Step : 2840, Training Loss : 0.34656, Training Acc : 0.844, Run Time : 0.64
INFO:root:2019-05-12 04:58:43, Epoch : 1, Step : 2841, Training Loss : 0.16150, Training Acc : 0.928, Run Time : 10.26
INFO:root:2019-05-12 04:58:44, Epoch : 1, Step : 2842, Training Loss : 0.14922, Training Acc : 0.922, Run Time : 1.03
INFO:root:2019-05-12 04:58:45, Epoch : 1, Step : 2843, Training Loss : 0.12328, Training Acc : 0.956, Run Time : 0.89
INFO:root:2019-05-12 04:58:56, Epoch : 1, Step : 2844, Training Loss : 0.13565, Training Acc : 0.928, Run Time : 11.28
INFO:root:2019-05-12 04:58:57, Epoch : 1, Step : 2845, Training Loss : 0.10574, Training Acc : 0.972, Run Time : 0.48
INFO:root:2019-05-12 04:58:58, Epoch : 1, Step : 2846, Training Loss : 0.23548, Training Acc : 0.900, Run Time : 0.64
INFO:root:2019-05-12 04:59:10, Epoch : 1, Step : 2847, Training Loss : 0.21812, Training Acc : 0.911, Run Time : 12.02
INFO:root:2019-05-12 04:59:11, Epoch : 1, Step : 2848, Training Loss : 0.15762, Training Acc : 0.928, Run Time : 1.10
INFO:root:2019-05-12 04:59:13, Epoch : 1, Step : 2849, Training Loss : 0.27300, Training Acc : 0.856, Run Time : 2.08
INFO:root:2019-05-12 04:59:24, Epoch : 1, Step : 2850, Training Loss : 0.31392, Training Acc : 0.844, Run Time : 11.21
INFO:root:2019-05-12 04:59:25, Epoch : 1, Step : 2851, Training Loss : 0.08710, Training Acc : 0.983, Run Time : 0.83
INFO:root:2019-05-12 04:59:37, Epoch : 1, Step : 2852, Training Loss : 0.19454, Training Acc : 0.911, Run Time : 11.93
INFO:root:2019-05-12 04:59:38, Epoch : 1, Step : 2853, Training Loss : 0.20382, Training Acc : 0.922, Run Time : 0.78
INFO:root:2019-05-12 04:59:39, Epoch : 1, Step : 2854, Training Loss : 0.18800, Training Acc : 0.911, Run Time : 1.93
INFO:root:2019-05-12 04:59:50, Epoch : 1, Step : 2855, Training Loss : 0.16702, Training Acc : 0.939, Run Time : 10.38
INFO:root:2019-05-12 04:59:50, Epoch : 1, Step : 2856, Training Loss : 0.24757, Training Acc : 0.867, Run Time : 0.45
INFO:root:2019-05-12 04:59:52, Epoch : 1, Step : 2857, Training Loss : 0.36410, Training Acc : 0.867, Run Time : 1.97
INFO:root:2019-05-12 05:00:03, Epoch : 1, Step : 2858, Training Loss : 0.51470, Training Acc : 0.789, Run Time : 10.75
INFO:root:2019-05-12 05:00:04, Epoch : 1, Step : 2859, Training Loss : 0.15230, Training Acc : 0.944, Run Time : 0.95
INFO:root:2019-05-12 05:00:05, Epoch : 1, Step : 2860, Training Loss : 0.28564, Training Acc : 0.856, Run Time : 1.32
INFO:root:2019-05-12 05:00:13, Epoch : 1, Step : 2861, Training Loss : 0.12287, Training Acc : 0.950, Run Time : 8.03
INFO:root:2019-05-12 05:00:14, Epoch : 1, Step : 2862, Training Loss : 0.17509, Training Acc : 0.944, Run Time : 0.81
INFO:root:2019-05-12 05:00:22, Epoch : 1, Step : 2863, Training Loss : 0.08531, Training Acc : 0.983, Run Time : 7.54
INFO:root:2019-05-12 05:00:25, Epoch : 1, Step : 2864, Training Loss : 0.14147, Training Acc : 0.956, Run Time : 3.39
INFO:root:2019-05-12 05:00:26, Epoch : 1, Step : 2865, Training Loss : 0.10137, Training Acc : 0.972, Run Time : 0.96
INFO:root:2019-05-12 05:00:36, Epoch : 1, Step : 2866, Training Loss : 0.10475, Training Acc : 0.978, Run Time : 9.58
INFO:root:2019-05-12 05:00:36, Epoch : 1, Step : 2867, Training Loss : 0.32393, Training Acc : 0.867, Run Time : 0.88
INFO:root:2019-05-12 05:00:48, Epoch : 1, Step : 2868, Training Loss : 0.17182, Training Acc : 0.944, Run Time : 11.11
INFO:root:2019-05-12 05:00:48, Epoch : 1, Step : 2869, Training Loss : 0.06626, Training Acc : 0.983, Run Time : 0.81
INFO:root:2019-05-12 05:00:49, Epoch : 1, Step : 2870, Training Loss : 0.18859, Training Acc : 0.939, Run Time : 0.42
INFO:root:2019-05-12 05:01:02, Epoch : 1, Step : 2871, Training Loss : 0.12038, Training Acc : 0.961, Run Time : 13.64
INFO:root:2019-05-12 05:01:03, Epoch : 1, Step : 2872, Training Loss : 0.21277, Training Acc : 0.933, Run Time : 0.64
INFO:root:2019-05-12 05:01:04, Epoch : 1, Step : 2873, Training Loss : 0.46176, Training Acc : 0.794, Run Time : 0.69
INFO:root:2019-05-12 05:01:05, Epoch : 1, Step : 2874, Training Loss : 0.33834, Training Acc : 0.833, Run Time : 1.29
INFO:root:2019-05-12 05:01:16, Epoch : 1, Step : 2875, Training Loss : 0.43603, Training Acc : 0.850, Run Time : 10.88
INFO:root:2019-05-12 05:01:16, Epoch : 1, Step : 2876, Training Loss : 0.13068, Training Acc : 0.922, Run Time : 0.46
INFO:root:2019-05-12 05:01:17, Epoch : 1, Step : 2877, Training Loss : 0.19156, Training Acc : 0.928, Run Time : 0.63
INFO:root:2019-05-12 05:01:19, Epoch : 1, Step : 2878, Training Loss : 0.13428, Training Acc : 0.961, Run Time : 1.69
INFO:root:2019-05-12 05:01:32, Epoch : 1, Step : 2879, Training Loss : 0.11261, Training Acc : 0.972, Run Time : 13.67
INFO:root:2019-05-12 05:01:45, Epoch : 1, Step : 2880, Training Loss : 0.22946, Training Acc : 0.894, Run Time : 12.38
INFO:root:2019-05-12 05:01:46, Epoch : 1, Step : 2881, Training Loss : 0.38957, Training Acc : 0.833, Run Time : 1.45
INFO:root:2019-05-12 05:01:48, Epoch : 1, Step : 2882, Training Loss : 0.44156, Training Acc : 0.811, Run Time : 1.53
INFO:root:2019-05-12 05:01:55, Epoch : 1, Step : 2883, Training Loss : 0.35540, Training Acc : 0.850, Run Time : 7.16
INFO:root:2019-05-12 05:01:56, Epoch : 1, Step : 2884, Training Loss : 0.38747, Training Acc : 0.850, Run Time : 0.64
INFO:root:2019-05-12 05:01:57, Epoch : 1, Step : 2885, Training Loss : 0.27418, Training Acc : 0.889, Run Time : 1.89
INFO:root:2019-05-12 05:02:10, Epoch : 1, Step : 2886, Training Loss : 0.18104, Training Acc : 0.933, Run Time : 12.95
INFO:root:2019-05-12 05:02:14, Epoch : 1, Step : 2887, Training Loss : 0.13085, Training Acc : 0.967, Run Time : 3.24
INFO:root:2019-05-12 05:02:14, Epoch : 1, Step : 2888, Training Loss : 0.15981, Training Acc : 0.939, Run Time : 0.63
INFO:root:2019-05-12 05:02:15, Epoch : 1, Step : 2889, Training Loss : 0.27929, Training Acc : 0.900, Run Time : 0.77
INFO:root:2019-05-12 05:02:16, Epoch : 1, Step : 2890, Training Loss : 0.24777, Training Acc : 0.900, Run Time : 0.61
INFO:root:2019-05-12 05:02:26, Epoch : 1, Step : 2891, Training Loss : 0.17553, Training Acc : 0.944, Run Time : 10.44
INFO:root:2019-05-12 05:02:27, Epoch : 1, Step : 2892, Training Loss : 0.19213, Training Acc : 0.933, Run Time : 0.55
INFO:root:2019-05-12 05:02:28, Epoch : 1, Step : 2893, Training Loss : 0.19989, Training Acc : 0.933, Run Time : 0.98
INFO:root:2019-05-12 05:02:39, Epoch : 1, Step : 2894, Training Loss : 0.12997, Training Acc : 0.967, Run Time : 11.36
INFO:root:2019-05-12 05:02:40, Epoch : 1, Step : 2895, Training Loss : 0.18329, Training Acc : 0.928, Run Time : 0.80
INFO:root:2019-05-12 05:02:50, Epoch : 1, Step : 2896, Training Loss : 0.14243, Training Acc : 0.967, Run Time : 10.48
INFO:root:2019-05-12 05:02:51, Epoch : 1, Step : 2897, Training Loss : 0.24403, Training Acc : 0.894, Run Time : 0.88
INFO:root:2019-05-12 05:02:52, Epoch : 1, Step : 2898, Training Loss : 0.14949, Training Acc : 0.944, Run Time : 0.66
INFO:root:2019-05-12 05:03:02, Epoch : 1, Step : 2899, Training Loss : 0.13617, Training Acc : 0.967, Run Time : 10.36
INFO:root:2019-05-12 05:03:03, Epoch : 1, Step : 2900, Training Loss : 0.34461, Training Acc : 0.850, Run Time : 0.75
INFO:root:2019-05-12 05:03:14, Epoch : 1, Step : 2901, Training Loss : 0.24367, Training Acc : 0.900, Run Time : 10.91
INFO:root:2019-05-12 05:03:14, Epoch : 1, Step : 2902, Training Loss : 0.18311, Training Acc : 0.922, Run Time : 0.48
INFO:root:2019-05-12 05:03:16, Epoch : 1, Step : 2903, Training Loss : 0.24163, Training Acc : 0.894, Run Time : 1.77
INFO:root:2019-05-12 05:03:26, Epoch : 1, Step : 2904, Training Loss : 0.34677, Training Acc : 0.833, Run Time : 10.06
INFO:root:2019-05-12 05:03:28, Epoch : 1, Step : 2905, Training Loss : 0.29322, Training Acc : 0.844, Run Time : 2.35
INFO:root:2019-05-12 05:03:39, Epoch : 1, Step : 2906, Training Loss : 0.20132, Training Acc : 0.900, Run Time : 10.29
INFO:root:2019-05-12 05:03:39, Epoch : 1, Step : 2907, Training Loss : 0.15676, Training Acc : 0.944, Run Time : 0.54
INFO:root:2019-05-12 05:03:41, Epoch : 1, Step : 2908, Training Loss : 0.27361, Training Acc : 0.883, Run Time : 1.26
INFO:root:2019-05-12 05:03:53, Epoch : 1, Step : 2909, Training Loss : 0.23248, Training Acc : 0.894, Run Time : 12.15
INFO:root:2019-05-12 05:03:53, Epoch : 1, Step : 2910, Training Loss : 0.24903, Training Acc : 0.883, Run Time : 0.63
INFO:root:2019-05-12 05:04:02, Epoch : 1, Step : 2911, Training Loss : 0.12486, Training Acc : 0.972, Run Time : 8.32
INFO:root:2019-05-12 05:04:03, Epoch : 1, Step : 2912, Training Loss : 0.16050, Training Acc : 0.939, Run Time : 0.89
INFO:root:2019-05-12 05:04:12, Epoch : 1, Step : 2913, Training Loss : 0.16279, Training Acc : 0.950, Run Time : 9.66
INFO:root:2019-05-12 05:04:13, Epoch : 1, Step : 2914, Training Loss : 0.16158, Training Acc : 0.950, Run Time : 0.63
INFO:root:2019-05-12 05:04:15, Epoch : 1, Step : 2915, Training Loss : 0.19509, Training Acc : 0.911, Run Time : 1.84
INFO:root:2019-05-12 05:04:26, Epoch : 1, Step : 2916, Training Loss : 0.18085, Training Acc : 0.906, Run Time : 11.50
INFO:root:2019-05-12 05:04:27, Epoch : 1, Step : 2917, Training Loss : 0.18719, Training Acc : 0.911, Run Time : 0.78
INFO:root:2019-05-12 05:04:39, Epoch : 1, Step : 2918, Training Loss : 0.12331, Training Acc : 0.967, Run Time : 12.21
INFO:root:2019-05-12 05:04:40, Epoch : 1, Step : 2919, Training Loss : 0.09744, Training Acc : 0.983, Run Time : 0.47
INFO:root:2019-05-12 05:04:41, Epoch : 1, Step : 2920, Training Loss : 0.13531, Training Acc : 0.961, Run Time : 1.01
INFO:root:2019-05-12 05:04:53, Epoch : 1, Step : 2921, Training Loss : 0.27498, Training Acc : 0.867, Run Time : 12.37
INFO:root:2019-05-12 05:04:54, Epoch : 1, Step : 2922, Training Loss : 0.25903, Training Acc : 0.867, Run Time : 0.61
INFO:root:2019-05-12 05:04:55, Epoch : 1, Step : 2923, Training Loss : 0.15867, Training Acc : 0.928, Run Time : 1.21
INFO:root:2019-05-12 05:05:06, Epoch : 1, Step : 2924, Training Loss : 0.26608, Training Acc : 0.922, Run Time : 11.40
INFO:root:2019-05-12 05:05:07, Epoch : 1, Step : 2925, Training Loss : 0.34643, Training Acc : 0.856, Run Time : 0.50
INFO:root:2019-05-12 05:05:10, Epoch : 1, Step : 2926, Training Loss : 0.21103, Training Acc : 0.917, Run Time : 3.30
INFO:root:2019-05-12 05:05:17, Epoch : 1, Step : 2927, Training Loss : 0.23937, Training Acc : 0.900, Run Time : 6.81
INFO:root:2019-05-12 05:05:18, Epoch : 1, Step : 2928, Training Loss : 0.36374, Training Acc : 0.833, Run Time : 0.67
INFO:root:2019-05-12 05:05:29, Epoch : 1, Step : 2929, Training Loss : 0.31755, Training Acc : 0.872, Run Time : 11.78
INFO:root:2019-05-12 05:05:30, Epoch : 1, Step : 2930, Training Loss : 0.39522, Training Acc : 0.844, Run Time : 0.86
INFO:root:2019-05-12 05:05:41, Epoch : 1, Step : 2931, Training Loss : 0.34052, Training Acc : 0.867, Run Time : 10.76
INFO:root:2019-05-12 05:05:42, Epoch : 1, Step : 2932, Training Loss : 0.30980, Training Acc : 0.867, Run Time : 0.98
INFO:root:2019-05-12 05:05:49, Epoch : 1, Step : 2933, Training Loss : 0.31372, Training Acc : 0.878, Run Time : 7.59
INFO:root:2019-05-12 05:05:52, Epoch : 1, Step : 2934, Training Loss : 0.33157, Training Acc : 0.844, Run Time : 2.13
INFO:root:2019-05-12 05:05:52, Epoch : 1, Step : 2935, Training Loss : 0.24727, Training Acc : 0.883, Run Time : 0.69
INFO:root:2019-05-12 05:05:53, Epoch : 1, Step : 2936, Training Loss : 0.23193, Training Acc : 0.917, Run Time : 0.60
INFO:root:2019-05-12 05:05:54, Epoch : 1, Step : 2937, Training Loss : 0.24171, Training Acc : 0.894, Run Time : 0.73
INFO:root:2019-05-12 05:06:05, Epoch : 1, Step : 2938, Training Loss : 0.22675, Training Acc : 0.894, Run Time : 10.97
INFO:root:2019-05-12 05:06:05, Epoch : 1, Step : 2939, Training Loss : 0.16076, Training Acc : 0.928, Run Time : 0.64
INFO:root:2019-05-12 05:06:18, Epoch : 1, Step : 2940, Training Loss : 0.18860, Training Acc : 0.922, Run Time : 13.13
INFO:root:2019-05-12 05:06:19, Epoch : 1, Step : 2941, Training Loss : 0.20149, Training Acc : 0.911, Run Time : 0.88
INFO:root:2019-05-12 05:06:20, Epoch : 1, Step : 2942, Training Loss : 0.14713, Training Acc : 0.950, Run Time : 0.61
INFO:root:2019-05-12 05:06:21, Epoch : 1, Step : 2943, Training Loss : 0.14454, Training Acc : 0.967, Run Time : 0.65
INFO:root:2019-05-12 05:06:31, Epoch : 1, Step : 2944, Training Loss : 0.08649, Training Acc : 0.989, Run Time : 10.46
INFO:root:2019-05-12 05:06:32, Epoch : 1, Step : 2945, Training Loss : 0.20749, Training Acc : 0.911, Run Time : 0.72
INFO:root:2019-05-12 05:06:38, Epoch : 1, Step : 2946, Training Loss : 0.16543, Training Acc : 0.917, Run Time : 6.23
INFO:root:2019-05-12 05:06:41, Epoch : 1, Step : 2947, Training Loss : 0.31756, Training Acc : 0.911, Run Time : 2.68
INFO:root:2019-05-12 05:06:41, Epoch : 1, Step : 2948, Training Loss : 0.13780, Training Acc : 0.933, Run Time : 0.63
INFO:root:2019-05-12 05:06:51, Epoch : 1, Step : 2949, Training Loss : 0.09012, Training Acc : 0.961, Run Time : 9.35
INFO:root:2019-05-12 05:06:51, Epoch : 1, Step : 2950, Training Loss : 0.05846, Training Acc : 0.983, Run Time : 0.67
INFO:root:2019-05-12 05:06:56, Epoch : 1, Step : 2951, Training Loss : 0.04694, Training Acc : 0.994, Run Time : 4.82
INFO:root:2019-05-12 05:06:59, Epoch : 1, Step : 2952, Training Loss : 0.07370, Training Acc : 0.983, Run Time : 3.21
INFO:root:2019-05-12 05:07:00, Epoch : 1, Step : 2953, Training Loss : 0.11623, Training Acc : 0.950, Run Time : 0.58
INFO:root:2019-05-12 05:07:07, Epoch : 1, Step : 2954, Training Loss : 0.06590, Training Acc : 0.989, Run Time : 6.72
INFO:root:2019-05-12 05:07:07, Epoch : 1, Step : 2955, Training Loss : 0.05308, Training Acc : 0.989, Run Time : 0.52
INFO:root:2019-05-12 05:07:19, Epoch : 1, Step : 2956, Training Loss : 0.11959, Training Acc : 0.950, Run Time : 11.77
INFO:root:2019-05-12 05:07:20, Epoch : 1, Step : 2957, Training Loss : 0.72413, Training Acc : 0.733, Run Time : 0.86
INFO:root:2019-05-12 05:07:32, Epoch : 1, Step : 2958, Training Loss : 0.55784, Training Acc : 0.794, Run Time : 11.83
INFO:root:2019-05-12 05:07:32, Epoch : 1, Step : 2959, Training Loss : 0.52314, Training Acc : 0.783, Run Time : 0.70
INFO:root:2019-05-12 05:07:35, Epoch : 1, Step : 2960, Training Loss : 0.19612, Training Acc : 0.928, Run Time : 2.44
INFO:root:2019-05-12 05:07:48, Epoch : 1, Step : 2961, Training Loss : 0.15161, Training Acc : 0.917, Run Time : 12.83
INFO:root:2019-05-12 05:08:01, Epoch : 1, Step : 2962, Training Loss : 0.21277, Training Acc : 0.917, Run Time : 13.52
INFO:root:2019-05-12 05:08:03, Epoch : 1, Step : 2963, Training Loss : 0.17857, Training Acc : 0.917, Run Time : 1.50
INFO:root:2019-05-12 05:08:04, Epoch : 1, Step : 2964, Training Loss : 0.16083, Training Acc : 0.922, Run Time : 1.12
INFO:root:2019-05-12 05:08:13, Epoch : 1, Step : 2965, Training Loss : 0.12419, Training Acc : 0.967, Run Time : 9.45
INFO:root:2019-05-12 05:08:14, Epoch : 1, Step : 2966, Training Loss : 0.15045, Training Acc : 0.972, Run Time : 0.48
INFO:root:2019-05-12 05:08:25, Epoch : 1, Step : 2967, Training Loss : 0.15357, Training Acc : 0.961, Run Time : 11.25
INFO:root:2019-05-12 05:08:26, Epoch : 1, Step : 2968, Training Loss : 0.16619, Training Acc : 0.956, Run Time : 0.75
INFO:root:2019-05-12 05:08:27, Epoch : 1, Step : 2969, Training Loss : 0.17250, Training Acc : 0.944, Run Time : 1.83
INFO:root:2019-05-12 05:08:38, Epoch : 1, Step : 2970, Training Loss : 0.17762, Training Acc : 0.944, Run Time : 10.62
INFO:root:2019-05-12 05:08:38, Epoch : 1, Step : 2971, Training Loss : 0.34415, Training Acc : 0.872, Run Time : 0.42
INFO:root:2019-05-12 05:08:41, Epoch : 1, Step : 2972, Training Loss : 0.29945, Training Acc : 0.867, Run Time : 2.41
INFO:root:2019-05-12 05:08:53, Epoch : 1, Step : 2973, Training Loss : 0.33782, Training Acc : 0.822, Run Time : 12.55
INFO:root:2019-05-12 05:08:54, Epoch : 1, Step : 2974, Training Loss : 0.35651, Training Acc : 0.856, Run Time : 0.47
INFO:root:2019-05-12 05:08:56, Epoch : 1, Step : 2975, Training Loss : 0.37837, Training Acc : 0.839, Run Time : 1.90
INFO:root:2019-05-12 05:09:07, Epoch : 1, Step : 2976, Training Loss : 0.33685, Training Acc : 0.844, Run Time : 11.12
INFO:root:2019-05-12 05:09:07, Epoch : 1, Step : 2977, Training Loss : 0.33633, Training Acc : 0.872, Run Time : 0.57
INFO:root:2019-05-12 05:09:09, Epoch : 1, Step : 2978, Training Loss : 0.43873, Training Acc : 0.828, Run Time : 1.70
INFO:root:2019-05-12 05:09:20, Epoch : 1, Step : 2979, Training Loss : 0.40908, Training Acc : 0.828, Run Time : 11.06
INFO:root:2019-05-12 05:09:21, Epoch : 1, Step : 2980, Training Loss : 0.43958, Training Acc : 0.794, Run Time : 0.52
INFO:root:2019-05-12 05:09:22, Epoch : 1, Step : 2981, Training Loss : 0.68164, Training Acc : 0.739, Run Time : 1.70
INFO:root:2019-05-12 05:09:32, Epoch : 1, Step : 2982, Training Loss : 0.30703, Training Acc : 0.878, Run Time : 9.73
INFO:root:2019-05-12 05:09:33, Epoch : 1, Step : 2983, Training Loss : 0.55639, Training Acc : 0.772, Run Time : 0.92
INFO:root:2019-05-12 05:09:44, Epoch : 1, Step : 2984, Training Loss : 0.35854, Training Acc : 0.850, Run Time : 10.98
INFO:root:2019-05-12 05:09:46, Epoch : 1, Step : 2985, Training Loss : 0.45869, Training Acc : 0.806, Run Time : 1.94
INFO:root:2019-05-12 05:09:56, Epoch : 1, Step : 2986, Training Loss : 0.32283, Training Acc : 0.844, Run Time : 10.17
INFO:root:2019-05-12 05:09:57, Epoch : 1, Step : 2987, Training Loss : 0.26567, Training Acc : 0.872, Run Time : 0.59
INFO:root:2019-05-12 05:09:57, Epoch : 1, Step : 2988, Training Loss : 0.38131, Training Acc : 0.850, Run Time : 0.65
INFO:root:2019-05-12 05:10:08, Epoch : 1, Step : 2989, Training Loss : 0.20179, Training Acc : 0.928, Run Time : 10.65
INFO:root:2019-05-12 05:10:09, Epoch : 1, Step : 2990, Training Loss : 0.21131, Training Acc : 0.917, Run Time : 0.43
INFO:root:2019-05-12 05:10:10, Epoch : 1, Step : 2991, Training Loss : 0.18677, Training Acc : 0.906, Run Time : 1.08
INFO:root:2019-05-12 05:10:19, Epoch : 1, Step : 2992, Training Loss : 0.24373, Training Acc : 0.883, Run Time : 9.26
INFO:root:2019-05-12 05:10:20, Epoch : 1, Step : 2993, Training Loss : 0.23281, Training Acc : 0.906, Run Time : 0.80
INFO:root:2019-05-12 05:10:34, Epoch : 1, Step : 2994, Training Loss : 0.34260, Training Acc : 0.878, Run Time : 13.93
INFO:root:2019-05-12 05:10:35, Epoch : 1, Step : 2995, Training Loss : 0.23046, Training Acc : 0.906, Run Time : 1.76
INFO:root:2019-05-12 05:10:42, Epoch : 1, Step : 2996, Training Loss : 0.27027, Training Acc : 0.883, Run Time : 6.58
INFO:root:2019-05-12 05:10:42, Epoch : 1, Step : 2997, Training Loss : 0.21688, Training Acc : 0.883, Run Time : 0.48
INFO:root:2019-05-12 05:10:56, Epoch : 1, Step : 2998, Training Loss : 0.16489, Training Acc : 0.917, Run Time : 13.25
INFO:root:2019-05-12 05:10:57, Epoch : 1, Step : 2999, Training Loss : 0.20787, Training Acc : 0.917, Run Time : 1.25
INFO:root:2019-05-12 05:11:08, Epoch : 1, Step : 3000, Training Loss : 0.13658, Training Acc : 0.944, Run Time : 11.22
INFO:root:2019-05-12 05:11:09, Epoch : 1, Step : 3001, Training Loss : 0.65894, Training Acc : 0.739, Run Time : 0.86
INFO:root:2019-05-12 05:11:21, Epoch : 1, Step : 3002, Training Loss : 0.80646, Training Acc : 0.706, Run Time : 11.91
INFO:root:2019-05-12 05:11:23, Epoch : 1, Step : 3003, Training Loss : 0.48499, Training Acc : 0.728, Run Time : 2.21
INFO:root:2019-05-12 05:11:36, Epoch : 1, Step : 3004, Training Loss : 0.54278, Training Acc : 0.717, Run Time : 12.71
INFO:root:2019-05-12 05:11:47, Epoch : 1, Step : 3005, Training Loss : 0.47780, Training Acc : 0.772, Run Time : 10.76
INFO:root:2019-05-12 05:11:51, Epoch : 1, Step : 3006, Training Loss : 0.34830, Training Acc : 0.872, Run Time : 4.79
INFO:root:2019-05-12 05:11:59, Epoch : 1, Step : 3007, Training Loss : 0.32694, Training Acc : 0.828, Run Time : 7.25
INFO:root:2019-05-12 05:12:08, Epoch : 1, Step : 3008, Training Loss : 0.16616, Training Acc : 0.944, Run Time : 8.91
INFO:root:2019-05-12 05:12:09, Epoch : 1, Step : 3009, Training Loss : 0.28015, Training Acc : 0.883, Run Time : 1.28
INFO:root:2019-05-12 05:12:09, Epoch : 1, Step : 3010, Training Loss : 0.27844, Training Acc : 0.889, Run Time : 0.65
INFO:root:2019-05-12 05:12:19, Epoch : 1, Step : 3011, Training Loss : 0.35738, Training Acc : 0.856, Run Time : 9.98
INFO:root:2019-05-12 05:12:20, Epoch : 1, Step : 3012, Training Loss : 0.39206, Training Acc : 0.883, Run Time : 0.86
INFO:root:2019-05-12 05:12:30, Epoch : 1, Step : 3013, Training Loss : 0.12302, Training Acc : 0.978, Run Time : 9.66
INFO:root:2019-05-12 05:12:30, Epoch : 1, Step : 3014, Training Loss : 0.31416, Training Acc : 0.828, Run Time : 0.46
INFO:root:2019-05-12 05:12:32, Epoch : 1, Step : 3015, Training Loss : 0.28604, Training Acc : 0.883, Run Time : 1.66
INFO:root:2019-05-12 05:12:43, Epoch : 1, Step : 3016, Training Loss : 0.26372, Training Acc : 0.911, Run Time : 10.43
INFO:root:2019-05-12 05:12:43, Epoch : 1, Step : 3017, Training Loss : 0.32127, Training Acc : 0.839, Run Time : 0.45
INFO:root:2019-05-12 05:12:43, Epoch : 1, Step : 3018, Training Loss : 0.51810, Training Acc : 0.811, Run Time : 0.46
INFO:root:2019-05-12 05:12:59, Epoch : 1, Step : 3019, Training Loss : 0.56415, Training Acc : 0.761, Run Time : 15.58
INFO:root:2019-05-12 05:13:00, Epoch : 1, Step : 3020, Training Loss : 0.63381, Training Acc : 0.700, Run Time : 0.89
INFO:root:2019-05-12 05:13:01, Epoch : 1, Step : 3021, Training Loss : 0.27950, Training Acc : 0.850, Run Time : 0.64
INFO:root:2019-05-12 05:13:13, Epoch : 1, Step : 3022, Training Loss : 0.45299, Training Acc : 0.789, Run Time : 12.73
INFO:root:2019-05-12 05:13:15, Epoch : 1, Step : 3023, Training Loss : 0.57007, Training Acc : 0.783, Run Time : 1.69
INFO:root:2019-05-12 05:13:24, Epoch : 1, Step : 3024, Training Loss : 0.22131, Training Acc : 0.906, Run Time : 8.93
INFO:root:2019-05-12 05:13:33, Epoch : 1, Step : 3025, Training Loss : 0.14745, Training Acc : 0.950, Run Time : 9.42
INFO:root:2019-05-12 05:13:35, Epoch : 1, Step : 3026, Training Loss : 0.15642, Training Acc : 0.950, Run Time : 1.34
INFO:root:2019-05-12 05:13:46, Epoch : 1, Step : 3027, Training Loss : 0.19156, Training Acc : 0.911, Run Time : 11.33
INFO:root:2019-05-12 05:13:47, Epoch : 1, Step : 3028, Training Loss : 0.15247, Training Acc : 0.944, Run Time : 1.05
INFO:root:2019-05-12 05:13:48, Epoch : 1, Step : 3029, Training Loss : 0.44848, Training Acc : 0.811, Run Time : 0.58
INFO:root:2019-05-12 05:13:49, Epoch : 1, Step : 3030, Training Loss : 0.30356, Training Acc : 0.850, Run Time : 1.90
INFO:root:2019-05-12 05:14:00, Epoch : 1, Step : 3031, Training Loss : 0.22607, Training Acc : 0.928, Run Time : 10.44
INFO:root:2019-05-12 05:14:00, Epoch : 1, Step : 3032, Training Loss : 0.17696, Training Acc : 0.933, Run Time : 0.55
INFO:root:2019-05-12 05:14:02, Epoch : 1, Step : 3033, Training Loss : 0.28397, Training Acc : 0.894, Run Time : 1.31
INFO:root:2019-05-12 05:14:13, Epoch : 1, Step : 3034, Training Loss : 0.24280, Training Acc : 0.878, Run Time : 11.45
INFO:root:2019-05-12 05:14:14, Epoch : 1, Step : 3035, Training Loss : 0.11677, Training Acc : 0.972, Run Time : 0.67
INFO:root:2019-05-12 05:14:16, Epoch : 1, Step : 3036, Training Loss : 0.23112, Training Acc : 0.933, Run Time : 1.84
INFO:root:2019-05-12 05:14:27, Epoch : 1, Step : 3037, Training Loss : 0.23049, Training Acc : 0.889, Run Time : 11.17
INFO:root:2019-05-12 05:14:28, Epoch : 1, Step : 3038, Training Loss : 0.20463, Training Acc : 0.939, Run Time : 0.80
INFO:root:2019-05-12 05:14:39, Epoch : 1, Step : 3039, Training Loss : 0.31295, Training Acc : 0.889, Run Time : 11.55
INFO:root:2019-05-12 05:14:40, Epoch : 1, Step : 3040, Training Loss : 0.26752, Training Acc : 0.906, Run Time : 0.62
INFO:root:2019-05-12 05:14:42, Epoch : 1, Step : 3041, Training Loss : 0.20950, Training Acc : 0.917, Run Time : 1.91
INFO:root:2019-05-12 05:14:52, Epoch : 1, Step : 3042, Training Loss : 0.15664, Training Acc : 0.950, Run Time : 10.65
INFO:root:2019-05-12 05:14:53, Epoch : 1, Step : 3043, Training Loss : 0.20146, Training Acc : 0.917, Run Time : 0.55
INFO:root:2019-05-12 05:14:54, Epoch : 1, Step : 3044, Training Loss : 0.27057, Training Acc : 0.844, Run Time : 1.26
INFO:root:2019-05-12 05:15:05, Epoch : 1, Step : 3045, Training Loss : 0.24286, Training Acc : 0.917, Run Time : 10.98
INFO:root:2019-05-12 05:15:06, Epoch : 1, Step : 3046, Training Loss : 0.18074, Training Acc : 0.944, Run Time : 0.48
INFO:root:2019-05-12 05:15:17, Epoch : 1, Step : 3047, Training Loss : 0.13370, Training Acc : 0.956, Run Time : 11.27
INFO:root:2019-05-12 05:15:18, Epoch : 1, Step : 3048, Training Loss : 0.19525, Training Acc : 0.939, Run Time : 1.45
INFO:root:2019-05-12 05:15:20, Epoch : 1, Step : 3049, Training Loss : 0.19976, Training Acc : 0.911, Run Time : 1.45
INFO:root:2019-05-12 05:15:31, Epoch : 1, Step : 3050, Training Loss : 0.07854, Training Acc : 0.994, Run Time : 10.88
INFO:root:2019-05-12 05:15:31, Epoch : 1, Step : 3051, Training Loss : 0.16853, Training Acc : 0.950, Run Time : 0.46
INFO:root:2019-05-12 05:15:33, Epoch : 1, Step : 3052, Training Loss : 0.10159, Training Acc : 0.967, Run Time : 1.81
INFO:root:2019-05-12 05:15:50, Epoch : 1, Step : 3053, Training Loss : 0.05663, Training Acc : 1.000, Run Time : 16.83
INFO:root:2019-05-12 05:16:01, Epoch : 1, Step : 3054, Training Loss : 0.07639, Training Acc : 0.983, Run Time : 10.82
INFO:root:2019-05-12 05:16:15, Epoch : 1, Step : 3055, Training Loss : 0.08485, Training Acc : 0.983, Run Time : 14.75
INFO:root:2019-05-12 05:16:17, Epoch : 1, Step : 3056, Training Loss : 0.09443, Training Acc : 0.967, Run Time : 1.32
INFO:root:2019-05-12 05:16:27, Epoch : 1, Step : 3057, Training Loss : 0.07567, Training Acc : 0.994, Run Time : 10.74
INFO:root:2019-05-12 05:16:29, Epoch : 1, Step : 3058, Training Loss : 0.08731, Training Acc : 0.967, Run Time : 1.45
INFO:root:2019-05-12 05:16:30, Epoch : 1, Step : 3059, Training Loss : 0.06543, Training Acc : 0.983, Run Time : 1.57
INFO:root:2019-05-12 05:16:41, Epoch : 1, Step : 3060, Training Loss : 0.13609, Training Acc : 0.972, Run Time : 10.77
INFO:root:2019-05-12 05:16:42, Epoch : 1, Step : 3061, Training Loss : 0.16432, Training Acc : 0.961, Run Time : 0.92
INFO:root:2019-05-12 05:16:44, Epoch : 1, Step : 3062, Training Loss : 0.16913, Training Acc : 0.939, Run Time : 1.86
INFO:root:2019-05-12 05:16:56, Epoch : 1, Step : 3063, Training Loss : 0.13849, Training Acc : 0.961, Run Time : 12.07
INFO:root:2019-05-12 05:17:07, Epoch : 1, Step : 3064, Training Loss : 0.07990, Training Acc : 0.978, Run Time : 11.31
INFO:root:2019-05-12 05:17:09, Epoch : 1, Step : 3065, Training Loss : 0.22912, Training Acc : 0.894, Run Time : 1.19
INFO:root:2019-05-12 05:17:10, Epoch : 1, Step : 3066, Training Loss : 0.13978, Training Acc : 0.967, Run Time : 1.48
INFO:root:2019-05-12 05:17:20, Epoch : 1, Step : 3067, Training Loss : 0.06281, Training Acc : 0.983, Run Time : 9.46
INFO:root:2019-05-12 05:17:20, Epoch : 1, Step : 3068, Training Loss : 0.52678, Training Acc : 0.833, Run Time : 0.62
INFO:root:2019-05-12 05:17:21, Epoch : 1, Step : 3069, Training Loss : 0.27340, Training Acc : 0.889, Run Time : 0.61
INFO:root:2019-05-12 05:17:21, Epoch : 1, Step : 3070, Training Loss : 0.21772, Training Acc : 0.922, Run Time : 0.61
INFO:root:2019-05-12 05:17:33, Epoch : 1, Step : 3071, Training Loss : 0.13412, Training Acc : 0.950, Run Time : 11.13
INFO:root:2019-05-12 05:17:33, Epoch : 1, Step : 3072, Training Loss : 0.14629, Training Acc : 0.933, Run Time : 0.50
INFO:root:2019-05-12 05:17:34, Epoch : 1, Step : 3073, Training Loss : 0.07002, Training Acc : 0.989, Run Time : 1.26
INFO:root:2019-05-12 05:17:44, Epoch : 1, Step : 3074, Training Loss : 0.14162, Training Acc : 0.944, Run Time : 9.82
INFO:root:2019-05-12 05:17:45, Epoch : 1, Step : 3075, Training Loss : 0.09914, Training Acc : 0.972, Run Time : 0.46
INFO:root:2019-05-12 05:17:56, Epoch : 1, Step : 3076, Training Loss : 0.11624, Training Acc : 0.956, Run Time : 11.28
INFO:root:2019-05-12 05:17:57, Epoch : 1, Step : 3077, Training Loss : 0.23825, Training Acc : 0.922, Run Time : 0.98
INFO:root:2019-05-12 05:18:09, Epoch : 1, Step : 3078, Training Loss : 0.21580, Training Acc : 0.906, Run Time : 12.01
INFO:root:2019-05-12 05:18:10, Epoch : 1, Step : 3079, Training Loss : 0.21208, Training Acc : 0.906, Run Time : 1.09
INFO:root:2019-05-12 05:18:11, Epoch : 1, Step : 3080, Training Loss : 0.14642, Training Acc : 0.939, Run Time : 1.38
INFO:root:2019-05-12 05:18:28, Epoch : 1, Step : 3081, Training Loss : 0.16664, Training Acc : 0.928, Run Time : 16.49
INFO:root:2019-05-12 05:18:29, Epoch : 1, Step : 3082, Training Loss : 0.14449, Training Acc : 0.961, Run Time : 0.72
INFO:root:2019-05-12 05:18:41, Epoch : 1, Step : 3083, Training Loss : 0.32631, Training Acc : 0.844, Run Time : 12.25
INFO:root:2019-05-12 05:18:42, Epoch : 1, Step : 3084, Training Loss : 0.52412, Training Acc : 0.794, Run Time : 1.27
INFO:root:2019-05-12 05:18:57, Epoch : 1, Step : 3085, Training Loss : 0.21913, Training Acc : 0.917, Run Time : 15.37
INFO:root:2019-05-12 05:18:59, Epoch : 1, Step : 3086, Training Loss : 0.25420, Training Acc : 0.883, Run Time : 1.72
INFO:root:2019-05-12 05:19:09, Epoch : 1, Step : 3087, Training Loss : 0.13659, Training Acc : 0.956, Run Time : 9.80
INFO:root:2019-05-12 05:19:09, Epoch : 1, Step : 3088, Training Loss : 0.08884, Training Acc : 0.978, Run Time : 0.51
INFO:root:2019-05-12 05:19:22, Epoch : 1, Step : 3089, Training Loss : 0.15440, Training Acc : 0.956, Run Time : 12.58
INFO:root:2019-05-12 05:19:23, Epoch : 1, Step : 3090, Training Loss : 0.23449, Training Acc : 0.917, Run Time : 1.12
INFO:root:2019-05-12 05:19:24, Epoch : 1, Step : 3091, Training Loss : 0.13300, Training Acc : 0.944, Run Time : 0.62
INFO:root:2019-05-12 05:19:25, Epoch : 1, Step : 3092, Training Loss : 0.25131, Training Acc : 0.917, Run Time : 0.86
INFO:root:2019-05-12 05:19:36, Epoch : 1, Step : 3093, Training Loss : 0.19321, Training Acc : 0.906, Run Time : 11.20
INFO:root:2019-05-12 05:19:37, Epoch : 1, Step : 3094, Training Loss : 0.18996, Training Acc : 0.939, Run Time : 0.81
INFO:root:2019-05-12 05:19:39, Epoch : 1, Step : 3095, Training Loss : 0.23458, Training Acc : 0.867, Run Time : 2.21
INFO:root:2019-05-12 05:19:51, Epoch : 1, Step : 3096, Training Loss : 0.35754, Training Acc : 0.889, Run Time : 11.79
INFO:root:2019-05-12 05:20:00, Epoch : 1, Step : 3097, Training Loss : 0.12155, Training Acc : 0.961, Run Time : 9.63
INFO:root:2019-05-12 05:20:11, Epoch : 1, Step : 3098, Training Loss : 0.20608, Training Acc : 0.917, Run Time : 11.08
INFO:root:2019-05-12 05:20:12, Epoch : 1, Step : 3099, Training Loss : 0.12074, Training Acc : 0.956, Run Time : 1.07
INFO:root:2019-05-12 05:20:24, Epoch : 1, Step : 3100, Training Loss : 0.08240, Training Acc : 0.983, Run Time : 11.47
INFO:root:2019-05-12 05:20:26, Epoch : 1, Step : 3101, Training Loss : 0.11869, Training Acc : 0.972, Run Time : 1.67
INFO:root:2019-05-12 05:20:37, Epoch : 1, Step : 3102, Training Loss : 0.13879, Training Acc : 0.956, Run Time : 11.15
INFO:root:2019-05-12 05:20:37, Epoch : 1, Step : 3103, Training Loss : 0.08522, Training Acc : 0.972, Run Time : 0.63
INFO:root:2019-05-12 05:20:42, Epoch : 1, Step : 3104, Training Loss : 0.19304, Training Acc : 0.928, Run Time : 4.36
INFO:root:2019-05-12 05:20:49, Epoch : 1, Step : 3105, Training Loss : 0.10123, Training Acc : 0.972, Run Time : 7.49
INFO:root:2019-05-12 05:20:50, Epoch : 1, Step : 3106, Training Loss : 0.17790, Training Acc : 0.917, Run Time : 0.62
INFO:root:2019-05-12 05:21:02, Epoch : 1, Step : 3107, Training Loss : 0.11136, Training Acc : 0.972, Run Time : 11.77
INFO:root:2019-05-12 05:21:02, Epoch : 1, Step : 3108, Training Loss : 0.14246, Training Acc : 0.950, Run Time : 0.86
INFO:root:2019-05-12 05:21:03, Epoch : 1, Step : 3109, Training Loss : 0.12063, Training Acc : 0.972, Run Time : 0.79
INFO:root:2019-05-12 05:21:04, Epoch : 1, Step : 3110, Training Loss : 0.19144, Training Acc : 0.894, Run Time : 0.63
INFO:root:2019-05-12 05:21:16, Epoch : 1, Step : 3111, Training Loss : 0.07898, Training Acc : 0.983, Run Time : 11.71
INFO:root:2019-05-12 05:21:17, Epoch : 1, Step : 3112, Training Loss : 0.15152, Training Acc : 0.950, Run Time : 1.14
INFO:root:2019-05-12 05:21:29, Epoch : 1, Step : 3113, Training Loss : 0.10181, Training Acc : 0.978, Run Time : 12.77
INFO:root:2019-05-12 05:21:30, Epoch : 1, Step : 3114, Training Loss : 0.12399, Training Acc : 0.944, Run Time : 0.45
INFO:root:2019-05-12 05:21:31, Epoch : 1, Step : 3115, Training Loss : 0.12241, Training Acc : 0.956, Run Time : 1.46
INFO:root:2019-05-12 05:21:44, Epoch : 1, Step : 3116, Training Loss : 0.11528, Training Acc : 0.967, Run Time : 12.68
INFO:root:2019-05-12 05:21:45, Epoch : 1, Step : 3117, Training Loss : 0.12713, Training Acc : 0.961, Run Time : 0.87
INFO:root:2019-05-12 05:21:56, Epoch : 1, Step : 3118, Training Loss : 0.08586, Training Acc : 0.989, Run Time : 11.09
INFO:root:2019-05-12 05:21:58, Epoch : 1, Step : 3119, Training Loss : 0.16691, Training Acc : 0.933, Run Time : 1.86
INFO:root:2019-05-12 05:22:20, Epoch : 1, Step : 3120, Training Loss : 0.17344, Training Acc : 0.928, Run Time : 22.20
INFO:root:2019-05-12 05:22:22, Epoch : 1, Step : 3121, Training Loss : 0.11690, Training Acc : 0.972, Run Time : 2.28
INFO:root:2019-05-12 05:22:23, Epoch : 1, Step : 3122, Training Loss : 0.21910, Training Acc : 0.939, Run Time : 0.62
INFO:root:2019-05-12 05:22:25, Epoch : 1, Step : 3123, Training Loss : 0.05994, Training Acc : 0.978, Run Time : 1.56
INFO:root:2019-05-12 05:22:34, Epoch : 1, Step : 3124, Training Loss : 0.11859, Training Acc : 0.967, Run Time : 9.64
INFO:root:2019-05-12 05:22:35, Epoch : 1, Step : 3125, Training Loss : 0.07737, Training Acc : 0.972, Run Time : 0.45
INFO:root:2019-05-12 05:22:36, Epoch : 1, Step : 3126, Training Loss : 0.14193, Training Acc : 0.944, Run Time : 1.49
INFO:root:2019-05-12 05:22:49, Epoch : 1, Step : 3127, Training Loss : 0.03745, Training Acc : 0.994, Run Time : 12.40
INFO:root:2019-05-12 05:22:51, Epoch : 1, Step : 3128, Training Loss : 0.09951, Training Acc : 0.961, Run Time : 2.59
INFO:root:2019-05-12 05:23:01, Epoch : 1, Step : 3129, Training Loss : 0.10094, Training Acc : 0.967, Run Time : 9.50
INFO:root:2019-05-12 05:23:01, Epoch : 1, Step : 3130, Training Loss : 0.16463, Training Acc : 0.939, Run Time : 0.66
INFO:root:2019-05-12 05:23:03, Epoch : 1, Step : 3131, Training Loss : 0.30950, Training Acc : 0.889, Run Time : 1.40
INFO:root:2019-05-12 05:23:15, Epoch : 1, Step : 3132, Training Loss : 0.18668, Training Acc : 0.933, Run Time : 12.08
INFO:root:2019-05-12 05:23:15, Epoch : 1, Step : 3133, Training Loss : 0.02929, Training Acc : 1.000, Run Time : 0.53
INFO:root:2019-05-12 05:23:17, Epoch : 1, Step : 3134, Training Loss : 0.13574, Training Acc : 0.944, Run Time : 2.23
INFO:root:2019-05-12 05:23:30, Epoch : 1, Step : 3135, Training Loss : 0.33792, Training Acc : 0.828, Run Time : 12.21
INFO:root:2019-05-12 05:23:31, Epoch : 1, Step : 3136, Training Loss : 0.23352, Training Acc : 0.900, Run Time : 1.03
INFO:root:2019-05-12 05:23:40, Epoch : 1, Step : 3137, Training Loss : 0.09470, Training Acc : 0.978, Run Time : 9.06
INFO:root:2019-05-12 05:23:41, Epoch : 1, Step : 3138, Training Loss : 0.38000, Training Acc : 0.911, Run Time : 0.75
INFO:root:2019-05-12 05:23:51, Epoch : 1, Step : 3139, Training Loss : 0.08395, Training Acc : 0.978, Run Time : 10.68
INFO:root:2019-05-12 05:23:53, Epoch : 1, Step : 3140, Training Loss : 0.17919, Training Acc : 0.922, Run Time : 1.40
INFO:root:2019-05-12 05:23:53, Epoch : 1, Step : 3141, Training Loss : 0.08597, Training Acc : 0.961, Run Time : 0.66
INFO:root:2019-05-12 05:24:03, Epoch : 1, Step : 3142, Training Loss : 0.16381, Training Acc : 0.939, Run Time : 9.59
INFO:root:2019-05-12 05:24:04, Epoch : 1, Step : 3143, Training Loss : 0.11707, Training Acc : 0.950, Run Time : 1.11
INFO:root:2019-05-12 05:24:06, Epoch : 1, Step : 3144, Training Loss : 0.15862, Training Acc : 0.956, Run Time : 1.79
INFO:root:2019-05-12 05:24:16, Epoch : 1, Step : 3145, Training Loss : 0.05401, Training Acc : 0.989, Run Time : 9.97
INFO:root:2019-05-12 05:24:26, Epoch : 1, Step : 3146, Training Loss : 0.17617, Training Acc : 0.950, Run Time : 10.20
INFO:root:2019-05-12 05:24:27, Epoch : 1, Step : 3147, Training Loss : 0.05724, Training Acc : 0.994, Run Time : 1.23
INFO:root:2019-05-12 05:24:28, Epoch : 1, Step : 3148, Training Loss : 0.06054, Training Acc : 0.983, Run Time : 0.59
INFO:root:2019-05-12 05:24:39, Epoch : 1, Step : 3149, Training Loss : 0.14286, Training Acc : 0.922, Run Time : 11.10
INFO:root:2019-05-12 05:24:39, Epoch : 1, Step : 3150, Training Loss : 0.08558, Training Acc : 0.967, Run Time : 0.46
INFO:root:2019-05-12 05:24:41, Epoch : 1, Step : 3151, Training Loss : 0.06869, Training Acc : 0.983, Run Time : 1.24
INFO:root:2019-05-12 05:24:53, Epoch : 1, Step : 3152, Training Loss : 0.06200, Training Acc : 0.972, Run Time : 12.42
INFO:root:2019-05-12 05:24:54, Epoch : 1, Step : 3153, Training Loss : 0.04045, Training Acc : 1.000, Run Time : 0.68
INFO:root:2019-05-12 05:25:06, Epoch : 1, Step : 3154, Training Loss : 0.08446, Training Acc : 0.972, Run Time : 11.95
INFO:root:2019-05-12 05:25:06, Epoch : 1, Step : 3155, Training Loss : 0.03377, Training Acc : 0.989, Run Time : 0.60
INFO:root:2019-05-12 05:25:07, Epoch : 1, Step : 3156, Training Loss : 0.03000, Training Acc : 1.000, Run Time : 0.78
INFO:root:2019-05-12 05:25:20, Epoch : 1, Step : 3157, Training Loss : 0.07345, Training Acc : 0.972, Run Time : 13.16
INFO:root:2019-05-12 05:25:21, Epoch : 1, Step : 3158, Training Loss : 0.03331, Training Acc : 0.989, Run Time : 1.06
INFO:root:2019-05-12 05:25:22, Epoch : 1, Step : 3159, Training Loss : 0.12770, Training Acc : 0.939, Run Time : 0.60
INFO:root:2019-05-12 05:25:33, Epoch : 1, Step : 3160, Training Loss : 0.09786, Training Acc : 0.967, Run Time : 10.83
INFO:root:2019-05-12 05:25:34, Epoch : 1, Step : 3161, Training Loss : 0.04154, Training Acc : 0.989, Run Time : 1.05
INFO:root:2019-05-12 05:25:35, Epoch : 1, Step : 3162, Training Loss : 0.05949, Training Acc : 0.983, Run Time : 1.64
INFO:root:2019-05-12 05:25:46, Epoch : 1, Step : 3163, Training Loss : 0.08458, Training Acc : 0.978, Run Time : 10.69
INFO:root:2019-05-12 05:25:47, Epoch : 1, Step : 3164, Training Loss : 0.08687, Training Acc : 0.978, Run Time : 0.88
INFO:root:2019-05-12 05:25:47, Epoch : 1, Step : 3165, Training Loss : 0.08144, Training Acc : 0.967, Run Time : 0.62
INFO:root:2019-05-12 05:25:59, Epoch : 1, Step : 3166, Training Loss : 0.03954, Training Acc : 0.994, Run Time : 11.79
INFO:root:2019-05-12 05:26:01, Epoch : 1, Step : 3167, Training Loss : 0.05384, Training Acc : 0.978, Run Time : 1.63
INFO:root:2019-05-12 05:26:15, Epoch : 1, Step : 3168, Training Loss : 0.01809, Training Acc : 0.994, Run Time : 13.84
INFO:root:2019-05-12 05:26:16, Epoch : 1, Step : 3169, Training Loss : 0.09130, Training Acc : 0.950, Run Time : 1.15
INFO:root:2019-05-12 05:26:16, Epoch : 1, Step : 3170, Training Loss : 0.02307, Training Acc : 1.000, Run Time : 0.59
INFO:root:2019-05-12 05:26:33, Epoch : 1, Step : 3171, Training Loss : 0.04497, Training Acc : 0.989, Run Time : 16.69
INFO:root:2019-05-12 05:26:41, Epoch : 1, Step : 3172, Training Loss : 0.01829, Training Acc : 1.000, Run Time : 7.93
INFO:root:2019-05-12 05:26:45, Epoch : 1, Step : 3173, Training Loss : 0.03058, Training Acc : 0.994, Run Time : 3.88
INFO:root:2019-05-12 05:26:46, Epoch : 1, Step : 3174, Training Loss : 0.07293, Training Acc : 0.983, Run Time : 0.62
INFO:root:2019-05-12 05:26:56, Epoch : 1, Step : 3175, Training Loss : 0.03312, Training Acc : 1.000, Run Time : 10.64
INFO:root:2019-05-12 05:27:06, Epoch : 1, Step : 3176, Training Loss : 0.06150, Training Acc : 0.983, Run Time : 9.47
INFO:root:2019-05-12 05:27:17, Epoch : 1, Step : 3177, Training Loss : 0.11529, Training Acc : 0.950, Run Time : 11.07
INFO:root:2019-05-12 05:27:18, Epoch : 1, Step : 3178, Training Loss : 0.07494, Training Acc : 0.978, Run Time : 1.12
INFO:root:2019-05-12 05:27:19, Epoch : 1, Step : 3179, Training Loss : 0.05330, Training Acc : 0.972, Run Time : 0.60
INFO:root:2019-05-12 05:27:31, Epoch : 1, Step : 3180, Training Loss : 0.07584, Training Acc : 0.983, Run Time : 12.99
INFO:root:2019-05-12 05:27:32, Epoch : 1, Step : 3181, Training Loss : 0.13840, Training Acc : 0.956, Run Time : 0.48
INFO:root:2019-05-12 05:27:35, Epoch : 1, Step : 3182, Training Loss : 0.60507, Training Acc : 0.717, Run Time : 2.66
INFO:root:2019-05-12 05:27:54, Epoch : 1, Step : 3183, Training Loss : 0.23857, Training Acc : 0.922, Run Time : 19.41
INFO:root:2019-05-12 05:27:55, Epoch : 1, Step : 3184, Training Loss : 0.44669, Training Acc : 0.833, Run Time : 1.00
INFO:root:2019-05-12 05:28:09, Epoch : 1, Step : 3185, Training Loss : 0.15050, Training Acc : 0.950, Run Time : 13.95
INFO:root:2019-05-12 05:28:20, Epoch : 1, Step : 3186, Training Loss : 0.09180, Training Acc : 0.972, Run Time : 11.30
INFO:root:2019-05-12 05:28:22, Epoch : 1, Step : 3187, Training Loss : 0.03440, Training Acc : 1.000, Run Time : 2.04
INFO:root:2019-05-12 05:28:33, Epoch : 1, Step : 3188, Training Loss : 0.19791, Training Acc : 0.906, Run Time : 11.03
INFO:root:2019-05-12 05:28:46, Epoch : 1, Step : 3189, Training Loss : 0.24640, Training Acc : 0.950, Run Time : 12.44
INFO:root:2019-05-12 05:29:03, Epoch : 1, Step : 3190, Training Loss : 0.17714, Training Acc : 0.950, Run Time : 17.27
INFO:root:2019-05-12 05:29:08, Epoch : 1, Step : 3191, Training Loss : 0.12055, Training Acc : 0.956, Run Time : 4.97
INFO:root:2019-05-12 05:29:09, Epoch : 1, Step : 3192, Training Loss : 0.19628, Training Acc : 0.944, Run Time : 0.62
INFO:root:2019-05-12 05:29:20, Epoch : 1, Step : 3193, Training Loss : 0.11990, Training Acc : 0.956, Run Time : 11.28
INFO:root:2019-05-12 05:29:21, Epoch : 1, Step : 3194, Training Loss : 0.28387, Training Acc : 0.911, Run Time : 1.36
INFO:root:2019-05-12 05:29:34, Epoch : 1, Step : 3195, Training Loss : 0.37848, Training Acc : 0.889, Run Time : 12.42
INFO:root:2019-05-12 05:29:35, Epoch : 1, Step : 3196, Training Loss : 0.24870, Training Acc : 0.911, Run Time : 0.83
INFO:root:2019-05-12 05:29:36, Epoch : 1, Step : 3197, Training Loss : 0.34758, Training Acc : 0.872, Run Time : 1.57
INFO:root:2019-05-12 05:29:46, Epoch : 1, Step : 3198, Training Loss : 0.19349, Training Acc : 0.950, Run Time : 10.15
INFO:root:2019-05-12 05:29:47, Epoch : 1, Step : 3199, Training Loss : 0.27985, Training Acc : 0.928, Run Time : 0.95
INFO:root:2019-05-12 05:29:48, Epoch : 1, Step : 3200, Training Loss : 0.42713, Training Acc : 0.878, Run Time : 0.62
INFO:root:2019-05-12 05:30:00, Epoch : 1, Step : 3201, Training Loss : 1.27864, Training Acc : 0.689, Run Time : 12.46
INFO:root:2019-05-12 05:30:01, Epoch : 1, Step : 3202, Training Loss : 0.99305, Training Acc : 0.672, Run Time : 0.78
INFO:root:2019-05-12 05:30:13, Epoch : 1, Step : 3203, Training Loss : 1.04438, Training Acc : 0.706, Run Time : 11.44
INFO:root:2019-05-12 05:30:14, Epoch : 1, Step : 3204, Training Loss : 0.92340, Training Acc : 0.672, Run Time : 1.29
INFO:root:2019-05-12 05:30:16, Epoch : 1, Step : 3205, Training Loss : 0.66943, Training Acc : 0.733, Run Time : 2.15
INFO:root:2019-05-12 05:30:29, Epoch : 1, Step : 3206, Training Loss : 0.41038, Training Acc : 0.783, Run Time : 12.61
INFO:root:2019-05-12 05:30:30, Epoch : 1, Step : 3207, Training Loss : 0.33689, Training Acc : 0.833, Run Time : 1.11
INFO:root:2019-05-12 05:30:42, Epoch : 1, Step : 3208, Training Loss : 0.20470, Training Acc : 0.889, Run Time : 11.97
INFO:root:2019-05-12 05:30:42, Epoch : 1, Step : 3209, Training Loss : 0.27434, Training Acc : 0.856, Run Time : 0.68
INFO:root:2019-05-12 05:30:43, Epoch : 1, Step : 3210, Training Loss : 0.19602, Training Acc : 0.917, Run Time : 0.88
INFO:root:2019-05-12 05:30:56, Epoch : 1, Step : 3211, Training Loss : 0.16814, Training Acc : 0.933, Run Time : 12.57
INFO:root:2019-05-12 05:30:57, Epoch : 1, Step : 3212, Training Loss : 0.13971, Training Acc : 0.944, Run Time : 0.73
INFO:root:2019-05-12 05:31:07, Epoch : 1, Step : 3213, Training Loss : 0.22867, Training Acc : 0.939, Run Time : 10.77
INFO:root:2019-05-12 05:31:08, Epoch : 1, Step : 3214, Training Loss : 0.19052, Training Acc : 0.939, Run Time : 0.51
INFO:root:2019-05-12 05:31:08, Epoch : 1, Step : 3215, Training Loss : 0.29879, Training Acc : 0.922, Run Time : 0.66
INFO:root:2019-05-12 05:31:19, Epoch : 1, Step : 3216, Training Loss : 0.21294, Training Acc : 0.922, Run Time : 10.57
INFO:root:2019-05-12 05:31:21, Epoch : 1, Step : 3217, Training Loss : 0.23308, Training Acc : 0.933, Run Time : 1.55
INFO:root:2019-05-12 05:31:31, Epoch : 1, Step : 3218, Training Loss : 0.20710, Training Acc : 0.933, Run Time : 10.09
INFO:root:2019-05-12 05:31:31, Epoch : 1, Step : 3219, Training Loss : 0.30266, Training Acc : 0.911, Run Time : 0.74
INFO:root:2019-05-12 05:31:32, Epoch : 1, Step : 3220, Training Loss : 0.25293, Training Acc : 0.917, Run Time : 0.96
INFO:root:2019-05-12 05:31:47, Epoch : 1, Step : 3221, Training Loss : 0.28521, Training Acc : 0.917, Run Time : 14.74
INFO:root:2019-05-12 05:32:00, Epoch : 1, Step : 3222, Training Loss : 0.27031, Training Acc : 0.917, Run Time : 12.69
INFO:root:2019-05-12 05:32:01, Epoch : 1, Step : 3223, Training Loss : 0.23904, Training Acc : 0.922, Run Time : 1.66
INFO:root:2019-05-12 05:32:03, Epoch : 1, Step : 3224, Training Loss : 0.22249, Training Acc : 0.928, Run Time : 1.59
INFO:root:2019-05-12 05:32:15, Epoch : 1, Step : 3225, Training Loss : 0.27580, Training Acc : 0.911, Run Time : 11.66
INFO:root:2019-05-12 05:32:16, Epoch : 1, Step : 3226, Training Loss : 0.21218, Training Acc : 0.928, Run Time : 1.12
INFO:root:2019-05-12 05:32:26, Epoch : 1, Step : 3227, Training Loss : 0.26905, Training Acc : 0.917, Run Time : 10.00
INFO:root:2019-05-12 05:32:29, Epoch : 1, Step : 3228, Training Loss : 0.20156, Training Acc : 0.922, Run Time : 3.00
INFO:root:2019-05-12 05:32:30, Epoch : 1, Step : 3229, Training Loss : 0.21589, Training Acc : 0.928, Run Time : 0.76
INFO:root:2019-05-12 05:32:40, Epoch : 1, Step : 3230, Training Loss : 0.33074, Training Acc : 0.906, Run Time : 10.48
INFO:root:2019-05-12 05:32:41, Epoch : 1, Step : 3231, Training Loss : 0.17708, Training Acc : 0.944, Run Time : 0.48
INFO:root:2019-05-12 05:32:42, Epoch : 1, Step : 3232, Training Loss : 0.18342, Training Acc : 0.933, Run Time : 1.47
INFO:root:2019-05-12 05:32:53, Epoch : 1, Step : 3233, Training Loss : 0.15462, Training Acc : 0.944, Run Time : 11.12
INFO:root:2019-05-12 05:32:54, Epoch : 1, Step : 3234, Training Loss : 0.12498, Training Acc : 0.944, Run Time : 0.53
INFO:root:2019-05-12 05:32:54, Epoch : 1, Step : 3235, Training Loss : 0.15920, Training Acc : 0.950, Run Time : 0.79
INFO:root:2019-05-12 05:33:07, Epoch : 1, Step : 3236, Training Loss : 0.11053, Training Acc : 0.950, Run Time : 12.30
INFO:root:2019-05-12 05:33:08, Epoch : 1, Step : 3237, Training Loss : 0.14987, Training Acc : 0.922, Run Time : 0.88
INFO:root:2019-05-12 05:33:19, Epoch : 1, Step : 3238, Training Loss : 0.18674, Training Acc : 0.933, Run Time : 11.44
INFO:root:2019-05-12 05:33:20, Epoch : 1, Step : 3239, Training Loss : 0.10886, Training Acc : 0.944, Run Time : 0.97
INFO:root:2019-05-12 05:33:21, Epoch : 1, Step : 3240, Training Loss : 0.15158, Training Acc : 0.939, Run Time : 0.46
INFO:root:2019-05-12 05:33:21, Epoch : 1, Step : 3241, Training Loss : 0.11942, Training Acc : 0.944, Run Time : 0.62
INFO:root:2019-05-12 05:33:35, Epoch : 1, Step : 3242, Training Loss : 0.17309, Training Acc : 0.933, Run Time : 13.51
INFO:root:2019-05-12 05:33:35, Epoch : 1, Step : 3243, Training Loss : 0.20509, Training Acc : 0.894, Run Time : 0.62
INFO:root:2019-05-12 05:33:36, Epoch : 1, Step : 3244, Training Loss : 0.11583, Training Acc : 0.939, Run Time : 0.98
INFO:root:2019-05-12 05:33:49, Epoch : 1, Step : 3245, Training Loss : 0.12495, Training Acc : 0.944, Run Time : 12.75
INFO:root:2019-05-12 05:33:50, Epoch : 1, Step : 3246, Training Loss : 0.20803, Training Acc : 0.933, Run Time : 1.33
INFO:root:2019-05-12 05:33:52, Epoch : 1, Step : 3247, Training Loss : 0.13477, Training Acc : 0.944, Run Time : 1.51
INFO:root:2019-05-12 05:34:03, Epoch : 1, Step : 3248, Training Loss : 0.15372, Training Acc : 0.928, Run Time : 11.18
INFO:root:2019-05-12 05:34:14, Epoch : 1, Step : 3249, Training Loss : 0.15715, Training Acc : 0.922, Run Time : 11.23
INFO:root:2019-05-12 05:34:26, Epoch : 1, Step : 3250, Training Loss : 0.14696, Training Acc : 0.933, Run Time : 12.10
INFO:root:2019-05-12 05:34:27, Epoch : 1, Step : 3251, Training Loss : 0.16109, Training Acc : 0.939, Run Time : 0.84
INFO:root:2019-05-12 05:34:29, Epoch : 1, Step : 3252, Training Loss : 0.18285, Training Acc : 0.922, Run Time : 2.27
INFO:root:2019-05-12 05:34:40, Epoch : 1, Step : 3253, Training Loss : 0.11292, Training Acc : 0.933, Run Time : 10.23
INFO:root:2019-05-12 05:34:41, Epoch : 1, Step : 3254, Training Loss : 0.19546, Training Acc : 0.906, Run Time : 0.84
INFO:root:2019-05-12 05:34:42, Epoch : 1, Step : 3255, Training Loss : 0.17433, Training Acc : 0.933, Run Time : 1.40
INFO:root:2019-05-12 05:35:06, Epoch : 1, Step : 3256, Training Loss : 0.17719, Training Acc : 0.917, Run Time : 23.90
INFO:root:2019-05-12 05:35:10, Epoch : 1, Step : 3257, Training Loss : 0.16622, Training Acc : 0.933, Run Time : 4.40
INFO:root:2019-05-12 05:35:11, Epoch : 1, Step : 3258, Training Loss : 0.19060, Training Acc : 0.922, Run Time : 0.51
INFO:root:2019-05-12 05:35:22, Epoch : 1, Step : 3259, Training Loss : 0.16363, Training Acc : 0.933, Run Time : 11.31
INFO:root:2019-05-12 05:35:23, Epoch : 1, Step : 3260, Training Loss : 0.14668, Training Acc : 0.939, Run Time : 0.97
INFO:root:2019-05-12 05:35:24, Epoch : 1, Step : 3261, Training Loss : 0.16110, Training Acc : 0.939, Run Time : 0.98
INFO:root:2019-05-12 05:35:36, Epoch : 1, Step : 3262, Training Loss : 0.17341, Training Acc : 0.917, Run Time : 12.41
INFO:root:2019-05-12 05:35:37, Epoch : 1, Step : 3263, Training Loss : 0.18821, Training Acc : 0.878, Run Time : 0.56
INFO:root:2019-05-12 05:35:39, Epoch : 1, Step : 3264, Training Loss : 0.19436, Training Acc : 0.856, Run Time : 2.30
INFO:root:2019-05-12 05:35:52, Epoch : 1, Step : 3265, Training Loss : 0.26926, Training Acc : 0.861, Run Time : 12.41
INFO:root:2019-05-12 05:35:53, Epoch : 1, Step : 3266, Training Loss : 0.21004, Training Acc : 0.889, Run Time : 0.99
INFO:root:2019-05-12 05:35:53, Epoch : 1, Step : 3267, Training Loss : 0.24559, Training Acc : 0.872, Run Time : 0.59
INFO:root:2019-05-12 05:35:55, Epoch : 1, Step : 3268, Training Loss : 0.18625, Training Acc : 0.928, Run Time : 1.33
INFO:root:2019-05-12 05:36:05, Epoch : 1, Step : 3269, Training Loss : 0.18323, Training Acc : 0.922, Run Time : 9.87
INFO:root:2019-05-12 05:36:05, Epoch : 1, Step : 3270, Training Loss : 0.20932, Training Acc : 0.889, Run Time : 0.59
INFO:root:2019-05-12 05:36:06, Epoch : 1, Step : 3271, Training Loss : 0.15568, Training Acc : 0.922, Run Time : 0.69
INFO:root:2019-05-12 05:36:20, Epoch : 1, Step : 3272, Training Loss : 0.16027, Training Acc : 0.928, Run Time : 13.91
INFO:root:2019-05-12 05:36:21, Epoch : 1, Step : 3273, Training Loss : 0.15087, Training Acc : 0.922, Run Time : 1.28
INFO:root:2019-05-12 05:36:38, Epoch : 1, Step : 3274, Training Loss : 0.15405, Training Acc : 0.922, Run Time : 17.08
INFO:root:2019-05-12 05:36:42, Epoch : 1, Step : 3275, Training Loss : 0.13711, Training Acc : 0.944, Run Time : 3.71
INFO:root:2019-05-12 05:36:42, Epoch : 1, Step : 3276, Training Loss : 0.14631, Training Acc : 0.933, Run Time : 0.66
INFO:root:2019-05-12 05:36:57, Epoch : 1, Step : 3277, Training Loss : 0.20705, Training Acc : 0.872, Run Time : 14.08
INFO:root:2019-05-12 05:36:57, Epoch : 1, Step : 3278, Training Loss : 0.18235, Training Acc : 0.917, Run Time : 0.98
INFO:root:2019-05-12 05:37:12, Epoch : 1, Step : 3279, Training Loss : 0.19870, Training Acc : 0.894, Run Time : 14.46
INFO:root:2019-05-12 05:37:13, Epoch : 1, Step : 3280, Training Loss : 0.17423, Training Acc : 0.911, Run Time : 1.06
INFO:root:2019-05-12 05:37:14, Epoch : 1, Step : 3281, Training Loss : 0.14672, Training Acc : 0.939, Run Time : 0.62
INFO:root:2019-05-12 05:37:27, Epoch : 1, Step : 3282, Training Loss : 0.16880, Training Acc : 0.933, Run Time : 13.16
INFO:root:2019-05-12 05:37:30, Epoch : 1, Step : 3283, Training Loss : 0.15260, Training Acc : 0.911, Run Time : 2.93
INFO:root:2019-05-12 05:37:39, Epoch : 1, Step : 3284, Training Loss : 0.16488, Training Acc : 0.911, Run Time : 8.99
INFO:root:2019-05-12 05:37:39, Epoch : 1, Step : 3285, Training Loss : 0.14510, Training Acc : 0.911, Run Time : 0.62
INFO:root:2019-05-12 05:37:42, Epoch : 1, Step : 3286, Training Loss : 0.15392, Training Acc : 0.922, Run Time : 2.27
INFO:root:2019-05-12 05:37:53, Epoch : 1, Step : 3287, Training Loss : 0.13119, Training Acc : 0.944, Run Time : 11.62
INFO:root:2019-05-12 05:37:54, Epoch : 1, Step : 3288, Training Loss : 0.12790, Training Acc : 0.956, Run Time : 0.54
INFO:root:2019-05-12 05:37:54, Epoch : 1, Step : 3289, Training Loss : 0.13265, Training Acc : 0.922, Run Time : 0.59
INFO:root:2019-05-12 05:38:05, Epoch : 1, Step : 3290, Training Loss : 0.16644, Training Acc : 0.917, Run Time : 10.99
INFO:root:2019-05-12 05:38:07, Epoch : 1, Step : 3291, Training Loss : 0.14451, Training Acc : 0.950, Run Time : 1.52
INFO:root:2019-05-12 05:38:08, Epoch : 1, Step : 3292, Training Loss : 0.14866, Training Acc : 0.922, Run Time : 0.75
INFO:root:2019-05-12 05:38:19, Epoch : 1, Step : 3293, Training Loss : 0.10969, Training Acc : 0.950, Run Time : 11.49
INFO:root:2019-05-12 05:38:20, Epoch : 1, Step : 3294, Training Loss : 0.14984, Training Acc : 0.933, Run Time : 0.96
INFO:root:2019-05-12 05:38:22, Epoch : 1, Step : 3295, Training Loss : 0.11877, Training Acc : 0.950, Run Time : 1.84
INFO:root:2019-05-12 05:38:35, Epoch : 1, Step : 3296, Training Loss : 0.13113, Training Acc : 0.933, Run Time : 13.33
INFO:root:2019-05-12 05:38:37, Epoch : 1, Step : 3297, Training Loss : 0.15492, Training Acc : 0.911, Run Time : 2.15
INFO:root:2019-05-12 05:38:46, Epoch : 1, Step : 3298, Training Loss : 0.12825, Training Acc : 0.956, Run Time : 8.51
INFO:root:2019-05-12 05:38:47, Epoch : 1, Step : 3299, Training Loss : 0.11958, Training Acc : 0.939, Run Time : 0.83
INFO:root:2019-05-12 05:39:02, Epoch : 1, Step : 3300, Training Loss : 0.11727, Training Acc : 0.950, Run Time : 14.91
INFO:root:2019-05-12 05:39:05, Epoch : 1, Step : 3301, Training Loss : 0.09981, Training Acc : 0.961, Run Time : 3.08
INFO:root:2019-05-12 05:39:13, Epoch : 1, Step : 3302, Training Loss : 0.12025, Training Acc : 0.961, Run Time : 8.06
INFO:root:2019-05-12 05:39:13, Epoch : 1, Step : 3303, Training Loss : 0.10947, Training Acc : 0.944, Run Time : 0.64
INFO:root:2019-05-12 05:39:15, Epoch : 1, Step : 3304, Training Loss : 0.11543, Training Acc : 0.939, Run Time : 1.46
INFO:root:2019-05-12 05:39:35, Epoch : 1, Step : 3305, Training Loss : 0.12461, Training Acc : 0.956, Run Time : 19.74
INFO:root:2019-05-12 05:39:43, Epoch : 1, Step : 3306, Training Loss : 0.08616, Training Acc : 0.983, Run Time : 8.27
INFO:root:2019-05-12 05:39:43, Epoch : 1, Step : 3307, Training Loss : 0.11222, Training Acc : 0.961, Run Time : 0.44
INFO:root:2019-05-12 05:39:45, Epoch : 1, Step : 3308, Training Loss : 0.12528, Training Acc : 0.978, Run Time : 1.80
INFO:root:2019-05-12 05:39:56, Epoch : 1, Step : 3309, Training Loss : 0.11045, Training Acc : 0.967, Run Time : 11.17
INFO:root:2019-05-12 05:39:57, Epoch : 1, Step : 3310, Training Loss : 0.11741, Training Acc : 0.967, Run Time : 0.48
INFO:root:2019-05-12 05:39:58, Epoch : 1, Step : 3311, Training Loss : 0.12590, Training Acc : 0.950, Run Time : 1.73
INFO:root:2019-05-12 05:40:10, Epoch : 1, Step : 3312, Training Loss : 0.10969, Training Acc : 0.983, Run Time : 11.07
INFO:root:2019-05-12 05:40:10, Epoch : 1, Step : 3313, Training Loss : 0.13241, Training Acc : 0.956, Run Time : 0.85
INFO:root:2019-05-12 05:40:11, Epoch : 1, Step : 3314, Training Loss : 0.09053, Training Acc : 0.994, Run Time : 0.63
INFO:root:2019-05-12 05:40:21, Epoch : 1, Step : 3315, Training Loss : 0.10002, Training Acc : 0.972, Run Time : 10.32
INFO:root:2019-05-12 05:40:22, Epoch : 1, Step : 3316, Training Loss : 0.10967, Training Acc : 0.972, Run Time : 0.68
INFO:root:2019-05-12 05:40:23, Epoch : 1, Step : 3317, Training Loss : 0.10002, Training Acc : 0.961, Run Time : 1.43
INFO:root:2019-05-12 05:40:34, Epoch : 1, Step : 3318, Training Loss : 0.11831, Training Acc : 0.967, Run Time : 10.07
INFO:root:2019-05-12 05:40:35, Epoch : 1, Step : 3319, Training Loss : 0.09484, Training Acc : 0.967, Run Time : 1.07
INFO:root:2019-05-12 05:40:39, Epoch : 1, Step : 3320, Training Loss : 0.29464, Training Acc : 0.828, Run Time : 4.39
INFO:root:2019-05-12 05:40:45, Epoch : 1, Step : 3321, Training Loss : 0.20111, Training Acc : 0.900, Run Time : 5.76
INFO:root:2019-05-12 05:40:45, Epoch : 1, Step : 3322, Training Loss : 0.10268, Training Acc : 0.978, Run Time : 0.64
INFO:root:2019-05-12 05:40:46, Epoch : 1, Step : 3323, Training Loss : 0.14627, Training Acc : 0.961, Run Time : 0.63
INFO:root:2019-05-12 05:40:57, Epoch : 1, Step : 3324, Training Loss : 0.09783, Training Acc : 0.972, Run Time : 10.84
INFO:root:2019-05-12 05:40:57, Epoch : 1, Step : 3325, Training Loss : 0.09369, Training Acc : 0.972, Run Time : 0.50
INFO:root:2019-05-12 05:40:59, Epoch : 1, Step : 3326, Training Loss : 0.14276, Training Acc : 0.972, Run Time : 1.84
INFO:root:2019-05-12 05:41:09, Epoch : 1, Step : 3327, Training Loss : 0.08933, Training Acc : 0.956, Run Time : 9.96
INFO:root:2019-05-12 05:41:10, Epoch : 1, Step : 3328, Training Loss : 0.10744, Training Acc : 0.956, Run Time : 0.61
INFO:root:2019-05-12 05:41:12, Epoch : 1, Step : 3329, Training Loss : 0.10165, Training Acc : 0.967, Run Time : 2.29
INFO:root:2019-05-12 05:41:20, Epoch : 1, Step : 3330, Training Loss : 0.14304, Training Acc : 0.922, Run Time : 7.65
INFO:root:2019-05-12 05:41:21, Epoch : 1, Step : 3331, Training Loss : 0.10137, Training Acc : 0.956, Run Time : 0.88
INFO:root:2019-05-12 05:41:22, Epoch : 1, Step : 3332, Training Loss : 0.08461, Training Acc : 0.978, Run Time : 1.48
INFO:root:2019-05-12 05:41:33, Epoch : 1, Step : 3333, Training Loss : 0.11709, Training Acc : 0.967, Run Time : 11.19
INFO:root:2019-05-12 05:41:34, Epoch : 1, Step : 3334, Training Loss : 0.12416, Training Acc : 0.944, Run Time : 0.92
INFO:root:2019-05-12 05:41:35, Epoch : 1, Step : 3335, Training Loss : 0.09562, Training Acc : 0.972, Run Time : 0.65
INFO:root:2019-05-12 05:41:45, Epoch : 1, Step : 3336, Training Loss : 0.10354, Training Acc : 0.961, Run Time : 10.49
INFO:root:2019-05-12 05:41:46, Epoch : 1, Step : 3337, Training Loss : 0.10014, Training Acc : 0.961, Run Time : 0.66
INFO:root:2019-05-12 05:41:48, Epoch : 1, Step : 3338, Training Loss : 0.08939, Training Acc : 0.972, Run Time : 2.22
INFO:root:2019-05-12 05:41:59, Epoch : 1, Step : 3339, Training Loss : 0.10820, Training Acc : 0.978, Run Time : 10.80
INFO:root:2019-05-12 05:42:00, Epoch : 1, Step : 3340, Training Loss : 0.09222, Training Acc : 0.983, Run Time : 0.95
INFO:root:2019-05-12 05:42:10, Epoch : 1, Step : 3341, Training Loss : 0.11840, Training Acc : 0.956, Run Time : 10.26
INFO:root:2019-05-12 05:42:11, Epoch : 1, Step : 3342, Training Loss : 0.10926, Training Acc : 0.978, Run Time : 0.80
INFO:root:2019-05-12 05:42:13, Epoch : 1, Step : 3343, Training Loss : 0.11222, Training Acc : 0.956, Run Time : 1.51
INFO:root:2019-05-12 05:42:23, Epoch : 1, Step : 3344, Training Loss : 0.13175, Training Acc : 0.939, Run Time : 10.41
INFO:root:2019-05-12 05:42:24, Epoch : 1, Step : 3345, Training Loss : 0.15166, Training Acc : 0.933, Run Time : 0.78
INFO:root:2019-05-12 05:42:37, Epoch : 1, Step : 3346, Training Loss : 0.15079, Training Acc : 0.933, Run Time : 13.31
INFO:root:2019-05-12 05:42:38, Epoch : 1, Step : 3347, Training Loss : 0.12530, Training Acc : 0.944, Run Time : 0.70
INFO:root:2019-05-12 05:42:38, Epoch : 1, Step : 3348, Training Loss : 0.12620, Training Acc : 0.944, Run Time : 0.46
INFO:root:2019-05-12 05:42:39, Epoch : 1, Step : 3349, Training Loss : 0.13455, Training Acc : 0.956, Run Time : 0.61
INFO:root:2019-05-12 05:42:57, Epoch : 1, Step : 3350, Training Loss : 0.12372, Training Acc : 0.939, Run Time : 18.10
INFO:root:2019-05-12 05:43:06, Epoch : 1, Step : 3351, Training Loss : 0.11182, Training Acc : 0.967, Run Time : 9.53
INFO:root:2019-05-12 05:43:16, Epoch : 1, Step : 3352, Training Loss : 0.14498, Training Acc : 0.922, Run Time : 9.96
INFO:root:2019-05-12 05:43:17, Epoch : 1, Step : 3353, Training Loss : 0.14027, Training Acc : 0.939, Run Time : 0.47
INFO:root:2019-05-12 05:43:21, Epoch : 1, Step : 3354, Training Loss : 0.12658, Training Acc : 0.950, Run Time : 4.51
INFO:root:2019-05-12 05:43:28, Epoch : 1, Step : 3355, Training Loss : 0.12822, Training Acc : 0.961, Run Time : 7.05
INFO:root:2019-05-12 05:43:30, Epoch : 1, Step : 3356, Training Loss : 0.15280, Training Acc : 0.911, Run Time : 1.31
INFO:root:2019-05-12 05:43:31, Epoch : 1, Step : 3357, Training Loss : 0.15265, Training Acc : 0.956, Run Time : 1.20
INFO:root:2019-05-12 05:43:42, Epoch : 1, Step : 3358, Training Loss : 0.12221, Training Acc : 0.961, Run Time : 11.35
INFO:root:2019-05-12 05:43:43, Epoch : 1, Step : 3359, Training Loss : 0.16088, Training Acc : 0.944, Run Time : 0.85
INFO:root:2019-05-12 05:43:44, Epoch : 1, Step : 3360, Training Loss : 0.15720, Training Acc : 0.922, Run Time : 0.91
INFO:root:2019-05-12 05:43:53, Epoch : 1, Step : 3361, Training Loss : 0.13993, Training Acc : 0.939, Run Time : 8.52
INFO:root:2019-05-12 05:43:55, Epoch : 1, Step : 3362, Training Loss : 0.10193, Training Acc : 0.978, Run Time : 2.43
INFO:root:2019-05-12 05:43:56, Epoch : 1, Step : 3363, Training Loss : 0.12355, Training Acc : 0.950, Run Time : 1.10
INFO:root:2019-05-12 05:44:02, Epoch : 1, Step : 3364, Training Loss : 0.12221, Training Acc : 0.933, Run Time : 5.80
INFO:root:2019-05-12 05:44:03, Epoch : 1, Step : 3365, Training Loss : 0.16287, Training Acc : 0.894, Run Time : 0.70
INFO:root:2019-05-12 05:44:05, Epoch : 1, Step : 3366, Training Loss : 0.09776, Training Acc : 0.983, Run Time : 1.95
INFO:root:2019-05-12 05:44:15, Epoch : 1, Step : 3367, Training Loss : 0.10736, Training Acc : 0.967, Run Time : 10.70
INFO:root:2019-05-12 05:44:16, Epoch : 1, Step : 3368, Training Loss : 0.15949, Training Acc : 0.933, Run Time : 0.44
INFO:root:2019-05-12 05:44:18, Epoch : 1, Step : 3369, Training Loss : 0.12169, Training Acc : 0.944, Run Time : 1.95
INFO:root:2019-05-12 05:44:29, Epoch : 1, Step : 3370, Training Loss : 0.16400, Training Acc : 0.939, Run Time : 11.03
INFO:root:2019-05-12 05:44:29, Epoch : 1, Step : 3371, Training Loss : 0.11520, Training Acc : 0.956, Run Time : 0.66
INFO:root:2019-05-12 05:44:31, Epoch : 1, Step : 3372, Training Loss : 0.13635, Training Acc : 0.944, Run Time : 1.61
INFO:root:2019-05-12 05:44:42, Epoch : 1, Step : 3373, Training Loss : 0.17425, Training Acc : 0.911, Run Time : 10.96
INFO:root:2019-05-12 05:44:43, Epoch : 1, Step : 3374, Training Loss : 0.18277, Training Acc : 0.900, Run Time : 0.68
INFO:root:2019-05-12 05:44:44, Epoch : 1, Step : 3375, Training Loss : 0.10702, Training Acc : 0.967, Run Time : 1.95
INFO:root:2019-05-12 05:44:58, Epoch : 1, Step : 3376, Training Loss : 0.13974, Training Acc : 0.944, Run Time : 13.87
INFO:root:2019-05-12 05:45:14, Epoch : 1, Step : 3377, Training Loss : 0.15356, Training Acc : 0.933, Run Time : 15.69
INFO:root:2019-05-12 05:45:16, Epoch : 1, Step : 3378, Training Loss : 0.11952, Training Acc : 0.950, Run Time : 1.81
INFO:root:2019-05-12 05:45:17, Epoch : 1, Step : 3379, Training Loss : 0.14435, Training Acc : 0.939, Run Time : 0.64
INFO:root:2019-05-12 05:45:25, Epoch : 1, Step : 3380, Training Loss : 0.13735, Training Acc : 0.961, Run Time : 8.99
INFO:root:2019-05-12 05:45:26, Epoch : 1, Step : 3381, Training Loss : 0.10822, Training Acc : 0.972, Run Time : 0.81
INFO:root:2019-05-12 05:45:27, Epoch : 1, Step : 3382, Training Loss : 0.17447, Training Acc : 0.928, Run Time : 0.92
INFO:root:2019-05-12 05:45:39, Epoch : 1, Step : 3383, Training Loss : 0.10890, Training Acc : 0.972, Run Time : 11.62
INFO:root:2019-05-12 05:45:40, Epoch : 1, Step : 3384, Training Loss : 0.10924, Training Acc : 0.950, Run Time : 0.80
INFO:root:2019-05-12 05:45:51, Epoch : 1, Step : 3385, Training Loss : 0.13443, Training Acc : 0.933, Run Time : 11.21
INFO:root:2019-05-12 05:45:51, Epoch : 1, Step : 3386, Training Loss : 0.16350, Training Acc : 0.922, Run Time : 0.63
INFO:root:2019-05-12 05:45:54, Epoch : 1, Step : 3387, Training Loss : 0.18864, Training Acc : 0.900, Run Time : 2.12
INFO:root:2019-05-12 05:46:06, Epoch : 1, Step : 3388, Training Loss : 0.16498, Training Acc : 0.917, Run Time : 12.39
INFO:root:2019-05-12 05:46:06, Epoch : 1, Step : 3389, Training Loss : 0.16499, Training Acc : 0.922, Run Time : 0.46
INFO:root:2019-05-12 05:46:07, Epoch : 1, Step : 3390, Training Loss : 0.12204, Training Acc : 0.950, Run Time : 0.60
INFO:root:2019-05-12 05:46:08, Epoch : 1, Step : 3391, Training Loss : 0.13768, Training Acc : 0.944, Run Time : 0.58
INFO:root:2019-05-12 05:46:21, Epoch : 1, Step : 3392, Training Loss : 0.22573, Training Acc : 0.900, Run Time : 13.24
INFO:root:2019-05-12 05:46:22, Epoch : 1, Step : 3393, Training Loss : 0.18152, Training Acc : 0.911, Run Time : 1.18
INFO:root:2019-05-12 05:46:23, Epoch : 1, Step : 3394, Training Loss : 0.14735, Training Acc : 0.950, Run Time : 0.61
INFO:root:2019-05-12 05:46:23, Epoch : 1, Step : 3395, Training Loss : 0.14928, Training Acc : 0.917, Run Time : 0.67
INFO:root:2019-05-12 05:46:24, Epoch : 1, Step : 3396, Training Loss : 0.13056, Training Acc : 0.933, Run Time : 0.74
INFO:root:2019-05-12 05:46:38, Epoch : 1, Step : 3397, Training Loss : 0.13315, Training Acc : 0.928, Run Time : 13.61
INFO:root:2019-05-12 05:46:38, Epoch : 1, Step : 3398, Training Loss : 0.09343, Training Acc : 0.983, Run Time : 0.73
INFO:root:2019-05-12 05:46:50, Epoch : 1, Step : 3399, Training Loss : 0.10242, Training Acc : 0.956, Run Time : 11.22
INFO:root:2019-05-12 05:46:50, Epoch : 1, Step : 3400, Training Loss : 0.11745, Training Acc : 0.967, Run Time : 0.71
INFO:root:2019-05-12 05:47:02, Epoch : 1, Step : 3401, Training Loss : 0.56776, Training Acc : 0.822, Run Time : 11.23
INFO:root:2019-05-12 05:47:02, Epoch : 1, Step : 3402, Training Loss : 0.69855, Training Acc : 0.761, Run Time : 0.44
INFO:root:2019-05-12 05:47:03, Epoch : 1, Step : 3403, Training Loss : 0.43571, Training Acc : 0.822, Run Time : 1.15
INFO:root:2019-05-12 05:47:14, Epoch : 1, Step : 3404, Training Loss : 0.54896, Training Acc : 0.811, Run Time : 11.31
INFO:root:2019-05-12 05:47:15, Epoch : 1, Step : 3405, Training Loss : 0.61103, Training Acc : 0.783, Run Time : 0.77
INFO:root:2019-05-12 05:47:26, Epoch : 1, Step : 3406, Training Loss : 0.30666, Training Acc : 0.856, Run Time : 10.31
INFO:root:2019-05-12 05:47:29, Epoch : 1, Step : 3407, Training Loss : 0.29984, Training Acc : 0.883, Run Time : 2.95
INFO:root:2019-05-12 05:47:33, Epoch : 1, Step : 3408, Training Loss : 0.32607, Training Acc : 0.844, Run Time : 4.67
INFO:root:2019-05-12 05:47:40, Epoch : 1, Step : 3409, Training Loss : 0.44255, Training Acc : 0.861, Run Time : 6.59
INFO:root:2019-05-12 05:47:40, Epoch : 1, Step : 3410, Training Loss : 0.32260, Training Acc : 0.883, Run Time : 0.50
INFO:root:2019-05-12 05:47:41, Epoch : 1, Step : 3411, Training Loss : 0.98117, Training Acc : 0.689, Run Time : 0.65
INFO:root:2019-05-12 05:47:56, Epoch : 1, Step : 3412, Training Loss : 0.94061, Training Acc : 0.694, Run Time : 14.68
INFO:root:2019-05-12 05:47:57, Epoch : 1, Step : 3413, Training Loss : 0.55019, Training Acc : 0.794, Run Time : 1.49
INFO:root:2019-05-12 05:48:08, Epoch : 1, Step : 3414, Training Loss : 0.40749, Training Acc : 0.833, Run Time : 11.26
INFO:root:2019-05-12 05:48:10, Epoch : 1, Step : 3415, Training Loss : 0.60838, Training Acc : 0.783, Run Time : 1.24
INFO:root:2019-05-12 05:48:20, Epoch : 1, Step : 3416, Training Loss : 0.30289, Training Acc : 0.889, Run Time : 10.72
INFO:root:2019-05-12 05:48:34, Epoch : 1, Step : 3417, Training Loss : 0.22018, Training Acc : 0.867, Run Time : 13.98
INFO:root:2019-05-12 05:48:36, Epoch : 1, Step : 3418, Training Loss : 0.19170, Training Acc : 0.933, Run Time : 1.36
INFO:root:2019-05-12 05:48:36, Epoch : 1, Step : 3419, Training Loss : 0.13604, Training Acc : 0.944, Run Time : 0.67
INFO:root:2019-05-12 05:48:48, Epoch : 1, Step : 3420, Training Loss : 0.14796, Training Acc : 0.939, Run Time : 11.35
INFO:root:2019-05-12 05:48:48, Epoch : 1, Step : 3421, Training Loss : 0.15573, Training Acc : 0.928, Run Time : 0.69
INFO:root:2019-05-12 05:49:00, Epoch : 1, Step : 3422, Training Loss : 0.14366, Training Acc : 0.933, Run Time : 11.22
INFO:root:2019-05-12 05:49:03, Epoch : 1, Step : 3423, Training Loss : 0.24606, Training Acc : 0.906, Run Time : 3.62
INFO:root:2019-05-12 05:49:04, Epoch : 1, Step : 3424, Training Loss : 0.09015, Training Acc : 0.956, Run Time : 0.81
INFO:root:2019-05-12 05:49:14, Epoch : 1, Step : 3425, Training Loss : 0.13051, Training Acc : 0.950, Run Time : 9.57
INFO:root:2019-05-12 05:49:14, Epoch : 1, Step : 3426, Training Loss : 0.14768, Training Acc : 0.933, Run Time : 0.76
INFO:root:2019-05-12 05:49:16, Epoch : 1, Step : 3427, Training Loss : 0.13520, Training Acc : 0.950, Run Time : 1.47
INFO:root:2019-05-12 05:49:27, Epoch : 1, Step : 3428, Training Loss : 0.27899, Training Acc : 0.900, Run Time : 11.23
INFO:root:2019-05-12 05:49:28, Epoch : 1, Step : 3429, Training Loss : 0.20052, Training Acc : 0.911, Run Time : 1.14
INFO:root:2019-05-12 05:49:38, Epoch : 1, Step : 3430, Training Loss : 0.26192, Training Acc : 0.900, Run Time : 9.98
INFO:root:2019-05-12 05:49:39, Epoch : 1, Step : 3431, Training Loss : 0.25733, Training Acc : 0.917, Run Time : 1.10
INFO:root:2019-05-12 05:49:42, Epoch : 1, Step : 3432, Training Loss : 0.16338, Training Acc : 0.956, Run Time : 2.75
INFO:root:2019-05-12 05:49:49, Epoch : 1, Step : 3433, Training Loss : 0.12150, Training Acc : 0.967, Run Time : 7.40
INFO:root:2019-05-12 05:49:50, Epoch : 1, Step : 3434, Training Loss : 0.31197, Training Acc : 0.861, Run Time : 0.46
INFO:root:2019-05-12 05:50:00, Epoch : 1, Step : 3435, Training Loss : 0.24875, Training Acc : 0.911, Run Time : 10.57
INFO:root:2019-05-12 05:50:01, Epoch : 1, Step : 3436, Training Loss : 0.08171, Training Acc : 0.978, Run Time : 0.53
INFO:root:2019-05-12 05:50:03, Epoch : 1, Step : 3437, Training Loss : 0.15265, Training Acc : 0.944, Run Time : 2.17
INFO:root:2019-05-12 05:50:14, Epoch : 1, Step : 3438, Training Loss : 0.25654, Training Acc : 0.894, Run Time : 10.82
INFO:root:2019-05-12 05:50:16, Epoch : 1, Step : 3439, Training Loss : 0.19677, Training Acc : 0.939, Run Time : 1.74
INFO:root:2019-05-12 05:50:26, Epoch : 1, Step : 3440, Training Loss : 0.18797, Training Acc : 0.928, Run Time : 10.52
INFO:root:2019-05-12 05:50:27, Epoch : 1, Step : 3441, Training Loss : 0.30195, Training Acc : 0.906, Run Time : 0.77
INFO:root:2019-05-12 05:50:28, Epoch : 1, Step : 3442, Training Loss : 0.46966, Training Acc : 0.856, Run Time : 0.68
INFO:root:2019-05-12 05:50:41, Epoch : 1, Step : 3443, Training Loss : 0.59859, Training Acc : 0.739, Run Time : 12.96
INFO:root:2019-05-12 05:50:42, Epoch : 1, Step : 3444, Training Loss : 0.21908, Training Acc : 0.906, Run Time : 1.03
INFO:root:2019-05-12 05:50:43, Epoch : 1, Step : 3445, Training Loss : 0.29345, Training Acc : 0.894, Run Time : 1.31
INFO:root:2019-05-12 05:50:55, Epoch : 1, Step : 3446, Training Loss : 0.20993, Training Acc : 0.928, Run Time : 11.71
INFO:root:2019-05-12 05:50:55, Epoch : 1, Step : 3447, Training Loss : 0.15049, Training Acc : 0.933, Run Time : 0.70
INFO:root:2019-05-12 05:50:56, Epoch : 1, Step : 3448, Training Loss : 0.17081, Training Acc : 0.944, Run Time : 0.65
INFO:root:2019-05-12 05:51:08, Epoch : 1, Step : 3449, Training Loss : 0.38647, Training Acc : 0.833, Run Time : 12.15
INFO:root:2019-05-12 05:51:09, Epoch : 1, Step : 3450, Training Loss : 0.50862, Training Acc : 0.817, Run Time : 0.61
INFO:root:2019-05-12 05:51:10, Epoch : 1, Step : 3451, Training Loss : 0.41419, Training Acc : 0.833, Run Time : 1.34
INFO:root:2019-05-12 05:51:22, Epoch : 1, Step : 3452, Training Loss : 0.42511, Training Acc : 0.828, Run Time : 11.77
INFO:root:2019-05-12 05:51:22, Epoch : 1, Step : 3453, Training Loss : 0.15351, Training Acc : 0.956, Run Time : 0.52
INFO:root:2019-05-12 05:51:24, Epoch : 1, Step : 3454, Training Loss : 0.25067, Training Acc : 0.917, Run Time : 2.07
INFO:root:2019-05-12 05:51:49, Epoch : 1, Step : 3455, Training Loss : 0.22934, Training Acc : 0.911, Run Time : 24.42
INFO:root:2019-05-12 05:51:53, Epoch : 1, Step : 3456, Training Loss : 0.25681, Training Acc : 0.917, Run Time : 3.98
INFO:root:2019-05-12 05:51:53, Epoch : 1, Step : 3457, Training Loss : 0.43505, Training Acc : 0.867, Run Time : 0.56
INFO:root:2019-05-12 05:51:54, Epoch : 1, Step : 3458, Training Loss : 0.52584, Training Acc : 0.794, Run Time : 0.60
INFO:root:2019-05-12 05:52:04, Epoch : 1, Step : 3459, Training Loss : 0.20083, Training Acc : 0.939, Run Time : 9.60
INFO:root:2019-05-12 05:52:04, Epoch : 1, Step : 3460, Training Loss : 0.15393, Training Acc : 0.944, Run Time : 0.63
INFO:root:2019-05-12 05:52:06, Epoch : 1, Step : 3461, Training Loss : 0.12862, Training Acc : 0.972, Run Time : 1.37
INFO:root:2019-05-12 05:52:16, Epoch : 1, Step : 3462, Training Loss : 0.15619, Training Acc : 0.950, Run Time : 10.09
INFO:root:2019-05-12 05:52:16, Epoch : 1, Step : 3463, Training Loss : 0.16347, Training Acc : 0.956, Run Time : 0.57
INFO:root:2019-05-12 05:52:17, Epoch : 1, Step : 3464, Training Loss : 0.10013, Training Acc : 0.967, Run Time : 0.81
INFO:root:2019-05-12 05:52:29, Epoch : 1, Step : 3465, Training Loss : 0.17104, Training Acc : 0.944, Run Time : 11.80
INFO:root:2019-05-12 05:52:30, Epoch : 1, Step : 3466, Training Loss : 0.09538, Training Acc : 0.978, Run Time : 0.72
INFO:root:2019-05-12 05:52:40, Epoch : 1, Step : 3467, Training Loss : 0.16626, Training Acc : 0.933, Run Time : 10.54
INFO:root:2019-05-12 05:52:42, Epoch : 1, Step : 3468, Training Loss : 0.31551, Training Acc : 0.806, Run Time : 1.44
INFO:root:2019-05-12 05:52:43, Epoch : 1, Step : 3469, Training Loss : 0.12005, Training Acc : 0.972, Run Time : 1.56
INFO:root:2019-05-12 05:52:54, Epoch : 1, Step : 3470, Training Loss : 0.19929, Training Acc : 0.933, Run Time : 11.27
INFO:root:2019-05-12 05:52:55, Epoch : 1, Step : 3471, Training Loss : 0.15148, Training Acc : 0.950, Run Time : 0.79
INFO:root:2019-05-12 05:53:10, Epoch : 1, Step : 3472, Training Loss : 0.15380, Training Acc : 0.961, Run Time : 15.06
INFO:root:2019-05-12 05:53:11, Epoch : 1, Step : 3473, Training Loss : 0.08218, Training Acc : 0.983, Run Time : 0.49
INFO:root:2019-05-12 05:53:12, Epoch : 1, Step : 3474, Training Loss : 0.05960, Training Acc : 1.000, Run Time : 1.12
INFO:root:2019-05-12 05:53:15, Epoch : 1, Step : 3475, Training Loss : 0.17691, Training Acc : 0.906, Run Time : 3.50
INFO:root:2019-05-12 05:53:24, Epoch : 1, Step : 3476, Training Loss : 0.14702, Training Acc : 0.950, Run Time : 8.57
INFO:root:2019-05-12 05:53:26, Epoch : 1, Step : 3477, Training Loss : 0.12718, Training Acc : 0.956, Run Time : 1.66
INFO:root:2019-05-12 05:53:36, Epoch : 1, Step : 3478, Training Loss : 0.19248, Training Acc : 0.939, Run Time : 10.59
INFO:root:2019-05-12 05:53:37, Epoch : 1, Step : 3479, Training Loss : 0.22704, Training Acc : 0.917, Run Time : 0.94
INFO:root:2019-05-12 05:53:50, Epoch : 1, Step : 3480, Training Loss : 0.11359, Training Acc : 0.961, Run Time : 12.77
INFO:root:2019-05-12 05:53:51, Epoch : 1, Step : 3481, Training Loss : 0.15706, Training Acc : 0.950, Run Time : 0.65
INFO:root:2019-05-12 05:53:52, Epoch : 1, Step : 3482, Training Loss : 0.09690, Training Acc : 0.961, Run Time : 1.88
INFO:root:2019-05-12 05:54:03, Epoch : 1, Step : 3483, Training Loss : 0.36678, Training Acc : 0.889, Run Time : 10.02
INFO:root:2019-05-12 05:54:03, Epoch : 1, Step : 3484, Training Loss : 0.13066, Training Acc : 0.944, Run Time : 0.51
INFO:root:2019-05-12 05:54:04, Epoch : 1, Step : 3485, Training Loss : 0.29683, Training Acc : 0.900, Run Time : 1.43
INFO:root:2019-05-12 05:54:16, Epoch : 1, Step : 3486, Training Loss : 0.16658, Training Acc : 0.933, Run Time : 11.21
INFO:root:2019-05-12 05:54:16, Epoch : 1, Step : 3487, Training Loss : 0.12044, Training Acc : 0.961, Run Time : 0.46
INFO:root:2019-05-12 05:54:17, Epoch : 1, Step : 3488, Training Loss : 0.20721, Training Acc : 0.917, Run Time : 0.42
INFO:root:2019-05-12 05:54:31, Epoch : 1, Step : 3489, Training Loss : 0.10279, Training Acc : 0.961, Run Time : 14.83
INFO:root:2019-05-12 05:54:32, Epoch : 1, Step : 3490, Training Loss : 0.09630, Training Acc : 0.950, Run Time : 0.57
INFO:root:2019-05-12 05:54:43, Epoch : 1, Step : 3491, Training Loss : 0.09465, Training Acc : 0.967, Run Time : 10.57
INFO:root:2019-05-12 05:54:44, Epoch : 1, Step : 3492, Training Loss : 0.08417, Training Acc : 0.967, Run Time : 1.73
INFO:root:2019-05-12 05:54:58, Epoch : 1, Step : 3493, Training Loss : 0.07850, Training Acc : 0.978, Run Time : 13.31
INFO:root:2019-05-12 05:54:59, Epoch : 1, Step : 3494, Training Loss : 0.05989, Training Acc : 0.994, Run Time : 1.56
INFO:root:2019-05-12 05:55:10, Epoch : 1, Step : 3495, Training Loss : 0.07258, Training Acc : 0.983, Run Time : 11.03
INFO:root:2019-05-12 05:55:12, Epoch : 1, Step : 3496, Training Loss : 0.07826, Training Acc : 0.978, Run Time : 1.80
INFO:root:2019-05-12 05:55:23, Epoch : 1, Step : 3497, Training Loss : 0.04229, Training Acc : 0.989, Run Time : 11.33
INFO:root:2019-05-12 05:55:24, Epoch : 1, Step : 3498, Training Loss : 0.15855, Training Acc : 0.939, Run Time : 0.62
INFO:root:2019-05-12 05:55:25, Epoch : 1, Step : 3499, Training Loss : 0.09389, Training Acc : 0.956, Run Time : 0.64
INFO:root:2019-05-12 05:55:25, Epoch : 1, Step : 3500, Training Loss : 0.16559, Training Acc : 0.928, Run Time : 0.67
INFO:root:2019-05-12 05:55:37, Epoch : 1, Step : 3501, Training Loss : 0.23425, Training Acc : 0.911, Run Time : 12.26
INFO:root:2019-05-12 05:55:38, Epoch : 1, Step : 3502, Training Loss : 0.21408, Training Acc : 0.922, Run Time : 0.55
INFO:root:2019-05-12 05:55:40, Epoch : 1, Step : 3503, Training Loss : 0.17257, Training Acc : 0.928, Run Time : 1.90
INFO:root:2019-05-12 05:55:50, Epoch : 1, Step : 3504, Training Loss : 0.22282, Training Acc : 0.906, Run Time : 10.40
INFO:root:2019-05-12 05:55:51, Epoch : 1, Step : 3505, Training Loss : 0.35251, Training Acc : 0.844, Run Time : 0.55
INFO:root:2019-05-12 05:55:57, Epoch : 1, Step : 3506, Training Loss : 0.24179, Training Acc : 0.928, Run Time : 5.75
INFO:root:2019-05-12 05:56:01, Epoch : 1, Step : 3507, Training Loss : 0.28825, Training Acc : 0.900, Run Time : 4.30
INFO:root:2019-05-12 05:56:01, Epoch : 1, Step : 3508, Training Loss : 0.05134, Training Acc : 0.989, Run Time : 0.49
INFO:root:2019-05-12 05:56:03, Epoch : 1, Step : 3509, Training Loss : 0.21782, Training Acc : 0.928, Run Time : 1.90
INFO:root:2019-05-12 05:56:14, Epoch : 1, Step : 3510, Training Loss : 0.12133, Training Acc : 0.939, Run Time : 10.81
INFO:root:2019-05-12 05:56:15, Epoch : 1, Step : 3511, Training Loss : 0.07536, Training Acc : 0.967, Run Time : 0.70
INFO:root:2019-05-12 05:56:17, Epoch : 1, Step : 3512, Training Loss : 0.06437, Training Acc : 0.978, Run Time : 1.74
INFO:root:2019-05-12 05:56:27, Epoch : 1, Step : 3513, Training Loss : 0.10333, Training Acc : 0.956, Run Time : 10.39
INFO:root:2019-05-12 05:56:28, Epoch : 1, Step : 3514, Training Loss : 0.14459, Training Acc : 0.939, Run Time : 0.75
INFO:root:2019-05-12 05:56:39, Epoch : 1, Step : 3515, Training Loss : 0.13137, Training Acc : 0.944, Run Time : 11.09
INFO:root:2019-05-12 05:56:40, Epoch : 1, Step : 3516, Training Loss : 0.17931, Training Acc : 0.911, Run Time : 1.08
INFO:root:2019-05-12 05:56:41, Epoch : 1, Step : 3517, Training Loss : 0.22415, Training Acc : 0.900, Run Time : 0.91
INFO:root:2019-05-12 05:56:51, Epoch : 1, Step : 3518, Training Loss : 0.15732, Training Acc : 0.933, Run Time : 10.51
INFO:root:2019-05-12 05:56:53, Epoch : 1, Step : 3519, Training Loss : 0.18583, Training Acc : 0.922, Run Time : 1.50
INFO:root:2019-05-12 05:57:04, Epoch : 1, Step : 3520, Training Loss : 0.21921, Training Acc : 0.900, Run Time : 11.18
INFO:root:2019-05-12 05:57:05, Epoch : 1, Step : 3521, Training Loss : 0.19988, Training Acc : 0.933, Run Time : 0.89
INFO:root:2019-05-12 05:57:18, Epoch : 1, Step : 3522, Training Loss : 0.16665, Training Acc : 0.939, Run Time : 13.46
INFO:root:2019-05-12 05:57:26, Epoch : 1, Step : 3523, Training Loss : 0.12138, Training Acc : 0.956, Run Time : 7.94
INFO:root:2019-05-12 05:57:43, Epoch : 1, Step : 3524, Training Loss : 0.06335, Training Acc : 0.978, Run Time : 16.94
INFO:root:2019-05-12 05:57:48, Epoch : 1, Step : 3525, Training Loss : 0.09279, Training Acc : 0.956, Run Time : 4.75
INFO:root:2019-05-12 05:57:49, Epoch : 1, Step : 3526, Training Loss : 0.06778, Training Acc : 0.994, Run Time : 0.66
INFO:root:2019-05-12 05:57:59, Epoch : 1, Step : 3527, Training Loss : 0.11083, Training Acc : 0.961, Run Time : 10.20
INFO:root:2019-05-12 05:57:59, Epoch : 1, Step : 3528, Training Loss : 0.04415, Training Acc : 0.983, Run Time : 0.68
INFO:root:2019-05-12 05:58:01, Epoch : 1, Step : 3529, Training Loss : 0.10876, Training Acc : 0.967, Run Time : 1.73
INFO:root:2019-05-12 05:58:11, Epoch : 1, Step : 3530, Training Loss : 0.06597, Training Acc : 0.972, Run Time : 10.27
INFO:root:2019-05-12 05:58:12, Epoch : 1, Step : 3531, Training Loss : 0.08033, Training Acc : 0.967, Run Time : 0.52
INFO:root:2019-05-12 05:58:13, Epoch : 1, Step : 3532, Training Loss : 0.10371, Training Acc : 0.961, Run Time : 0.94
INFO:root:2019-05-12 05:58:24, Epoch : 1, Step : 3533, Training Loss : 0.06238, Training Acc : 0.983, Run Time : 10.59
INFO:root:2019-05-12 05:58:24, Epoch : 1, Step : 3534, Training Loss : 0.16031, Training Acc : 0.939, Run Time : 0.52
INFO:root:2019-05-12 05:58:26, Epoch : 1, Step : 3535, Training Loss : 0.11642, Training Acc : 0.961, Run Time : 1.82
INFO:root:2019-05-12 05:58:37, Epoch : 1, Step : 3536, Training Loss : 0.10668, Training Acc : 0.961, Run Time : 10.65
INFO:root:2019-05-12 05:58:38, Epoch : 1, Step : 3537, Training Loss : 0.05830, Training Acc : 0.978, Run Time : 1.00
INFO:root:2019-05-12 05:58:50, Epoch : 1, Step : 3538, Training Loss : 0.03987, Training Acc : 0.994, Run Time : 11.99
INFO:root:2019-05-12 05:58:50, Epoch : 1, Step : 3539, Training Loss : 0.06807, Training Acc : 0.989, Run Time : 0.47
INFO:root:2019-05-12 05:58:51, Epoch : 1, Step : 3540, Training Loss : 0.11533, Training Acc : 0.956, Run Time : 1.09
INFO:root:2019-05-12 05:59:03, Epoch : 1, Step : 3541, Training Loss : 0.07077, Training Acc : 0.972, Run Time : 11.59
INFO:root:2019-05-12 05:59:03, Epoch : 1, Step : 3542, Training Loss : 0.29330, Training Acc : 0.878, Run Time : 0.47
INFO:root:2019-05-12 05:59:07, Epoch : 1, Step : 3543, Training Loss : 0.13489, Training Acc : 0.950, Run Time : 4.26
INFO:root:2019-05-12 05:59:10, Epoch : 1, Step : 3544, Training Loss : 0.06585, Training Acc : 0.978, Run Time : 2.57
INFO:root:2019-05-12 05:59:10, Epoch : 1, Step : 3545, Training Loss : 0.09741, Training Acc : 0.967, Run Time : 0.49
INFO:root:2019-05-12 05:59:16, Epoch : 1, Step : 3546, Training Loss : 0.03903, Training Acc : 0.978, Run Time : 5.65
INFO:root:2019-05-12 05:59:17, Epoch : 1, Step : 3547, Training Loss : 0.15503, Training Acc : 0.944, Run Time : 0.63
INFO:root:2019-05-12 05:59:17, Epoch : 1, Step : 3548, Training Loss : 0.12212, Training Acc : 0.956, Run Time : 0.63
INFO:root:2019-05-12 05:59:20, Epoch : 1, Step : 3549, Training Loss : 0.10958, Training Acc : 0.944, Run Time : 3.05
INFO:root:2019-05-12 05:59:30, Epoch : 1, Step : 3550, Training Loss : 0.15527, Training Acc : 0.933, Run Time : 9.70
INFO:root:2019-05-12 05:59:31, Epoch : 1, Step : 3551, Training Loss : 0.12357, Training Acc : 0.956, Run Time : 0.45
INFO:root:2019-05-12 05:59:31, Epoch : 1, Step : 3552, Training Loss : 0.07314, Training Acc : 0.983, Run Time : 0.62
INFO:root:2019-05-12 05:59:33, Epoch : 1, Step : 3553, Training Loss : 0.16077, Training Acc : 0.939, Run Time : 2.14
INFO:root:2019-05-12 05:59:46, Epoch : 1, Step : 3554, Training Loss : 0.11171, Training Acc : 0.961, Run Time : 13.04
INFO:root:2019-05-12 05:59:49, Epoch : 1, Step : 3555, Training Loss : 0.16183, Training Acc : 0.939, Run Time : 2.80
INFO:root:2019-05-12 06:00:01, Epoch : 1, Step : 3556, Training Loss : 0.18785, Training Acc : 0.939, Run Time : 12.11
INFO:root:2019-05-12 06:00:02, Epoch : 1, Step : 3557, Training Loss : 0.09533, Training Acc : 0.967, Run Time : 0.75
INFO:root:2019-05-12 06:00:04, Epoch : 1, Step : 3558, Training Loss : 0.11727, Training Acc : 0.956, Run Time : 1.71
INFO:root:2019-05-12 06:00:15, Epoch : 1, Step : 3559, Training Loss : 0.08586, Training Acc : 0.967, Run Time : 11.13
INFO:root:2019-05-12 06:00:15, Epoch : 1, Step : 3560, Training Loss : 0.15762, Training Acc : 0.939, Run Time : 0.47
INFO:root:2019-05-12 06:00:18, Epoch : 1, Step : 3561, Training Loss : 0.13504, Training Acc : 0.928, Run Time : 2.77
INFO:root:2019-05-12 06:00:32, Epoch : 1, Step : 3562, Training Loss : 0.13924, Training Acc : 0.933, Run Time : 13.62
INFO:root:2019-05-12 06:00:33, Epoch : 1, Step : 3563, Training Loss : 0.29392, Training Acc : 0.894, Run Time : 1.19
INFO:root:2019-05-12 06:00:44, Epoch : 1, Step : 3564, Training Loss : 0.14567, Training Acc : 0.933, Run Time : 11.47
INFO:root:2019-05-12 06:00:45, Epoch : 1, Step : 3565, Training Loss : 0.12885, Training Acc : 0.950, Run Time : 0.73
INFO:root:2019-05-12 06:00:47, Epoch : 1, Step : 3566, Training Loss : 0.14896, Training Acc : 0.933, Run Time : 1.93
INFO:root:2019-05-12 06:01:00, Epoch : 1, Step : 3567, Training Loss : 0.11921, Training Acc : 0.961, Run Time : 12.96
INFO:root:2019-05-12 06:01:02, Epoch : 1, Step : 3568, Training Loss : 0.15416, Training Acc : 0.939, Run Time : 2.22
INFO:root:2019-05-12 06:01:13, Epoch : 1, Step : 3569, Training Loss : 0.24506, Training Acc : 0.900, Run Time : 10.43
INFO:root:2019-05-12 06:01:16, Epoch : 1, Step : 3570, Training Loss : 0.15643, Training Acc : 0.933, Run Time : 3.48
INFO:root:2019-05-12 06:01:18, Epoch : 1, Step : 3571, Training Loss : 0.08905, Training Acc : 0.961, Run Time : 1.65
INFO:root:2019-05-12 06:01:19, Epoch : 1, Step : 3572, Training Loss : 0.13589, Training Acc : 0.939, Run Time : 1.08
INFO:root:2019-05-12 06:01:26, Epoch : 1, Step : 3573, Training Loss : 0.09209, Training Acc : 0.967, Run Time : 7.16
INFO:root:2019-05-12 06:01:27, Epoch : 1, Step : 3574, Training Loss : 0.07053, Training Acc : 0.972, Run Time : 0.91
INFO:root:2019-05-12 06:01:38, Epoch : 1, Step : 3575, Training Loss : 0.10983, Training Acc : 0.972, Run Time : 10.61
INFO:root:2019-05-12 06:01:39, Epoch : 1, Step : 3576, Training Loss : 0.17624, Training Acc : 0.922, Run Time : 1.02
INFO:root:2019-05-12 06:01:39, Epoch : 1, Step : 3577, Training Loss : 0.11371, Training Acc : 0.967, Run Time : 0.73
INFO:root:2019-05-12 06:01:51, Epoch : 1, Step : 3578, Training Loss : 0.16059, Training Acc : 0.944, Run Time : 11.27
INFO:root:2019-05-12 06:01:51, Epoch : 1, Step : 3579, Training Loss : 0.16226, Training Acc : 0.922, Run Time : 0.48
INFO:root:2019-05-12 06:02:03, Epoch : 1, Step : 3580, Training Loss : 0.10454, Training Acc : 0.961, Run Time : 11.62
INFO:root:2019-05-12 06:02:04, Epoch : 1, Step : 3581, Training Loss : 0.11841, Training Acc : 0.967, Run Time : 0.89
INFO:root:2019-05-12 06:02:04, Epoch : 1, Step : 3582, Training Loss : 0.18038, Training Acc : 0.900, Run Time : 0.67
INFO:root:2019-05-12 06:02:18, Epoch : 1, Step : 3583, Training Loss : 0.20806, Training Acc : 0.906, Run Time : 13.77
INFO:root:2019-05-12 06:02:19, Epoch : 1, Step : 3584, Training Loss : 0.14192, Training Acc : 0.950, Run Time : 0.58
INFO:root:2019-05-12 06:02:26, Epoch : 1, Step : 3585, Training Loss : 0.21020, Training Acc : 0.917, Run Time : 7.09
INFO:root:2019-05-12 06:02:39, Epoch : 1, Step : 3586, Training Loss : 0.04678, Training Acc : 0.994, Run Time : 12.98
INFO:root:2019-05-12 06:02:40, Epoch : 1, Step : 3587, Training Loss : 0.25599, Training Acc : 0.911, Run Time : 1.33
INFO:root:2019-05-12 06:02:40, Epoch : 1, Step : 3588, Training Loss : 0.19237, Training Acc : 0.922, Run Time : 0.40
INFO:root:2019-05-12 06:03:00, Epoch : 1, Step : 3589, Training Loss : 0.11239, Training Acc : 0.961, Run Time : 19.19
INFO:root:2019-05-12 06:03:01, Epoch : 1, Step : 3590, Training Loss : 0.14169, Training Acc : 0.944, Run Time : 1.59
INFO:root:2019-05-12 06:03:27, Epoch : 1, Step : 3591, Training Loss : 0.12401, Training Acc : 0.939, Run Time : 25.46
INFO:root:2019-05-12 06:03:33, Epoch : 1, Step : 3592, Training Loss : 0.09191, Training Acc : 0.972, Run Time : 6.22
INFO:root:2019-05-12 06:03:34, Epoch : 1, Step : 3593, Training Loss : 0.14150, Training Acc : 0.944, Run Time : 0.78
INFO:root:2019-05-12 06:03:45, Epoch : 1, Step : 3594, Training Loss : 0.08883, Training Acc : 0.961, Run Time : 10.87
INFO:root:2019-05-12 06:03:45, Epoch : 1, Step : 3595, Training Loss : 0.11768, Training Acc : 0.961, Run Time : 0.77
INFO:root:2019-05-12 06:03:46, Epoch : 1, Step : 3596, Training Loss : 0.27888, Training Acc : 0.850, Run Time : 0.65
INFO:root:2019-05-12 06:03:58, Epoch : 1, Step : 3597, Training Loss : 0.23478, Training Acc : 0.911, Run Time : 11.86
INFO:root:2019-05-12 06:03:58, Epoch : 1, Step : 3598, Training Loss : 0.20286, Training Acc : 0.911, Run Time : 0.50
INFO:root:2019-05-12 06:04:00, Epoch : 1, Step : 3599, Training Loss : 0.31703, Training Acc : 0.867, Run Time : 1.26
INFO:root:2019-05-12 06:04:12, Epoch : 1, Step : 3600, Training Loss : 0.39962, Training Acc : 0.844, Run Time : 12.08
INFO:root:2019-05-12 06:04:27, Epoch : 1, Step : 3601, Training Loss : 0.59700, Training Acc : 0.800, Run Time : 15.85
INFO:root:2019-05-12 06:04:29, Epoch : 1, Step : 3602, Training Loss : 0.85884, Training Acc : 0.706, Run Time : 1.90
INFO:root:2019-05-12 06:04:40, Epoch : 1, Step : 3603, Training Loss : 0.66338, Training Acc : 0.711, Run Time : 10.35
INFO:root:2019-05-12 06:04:40, Epoch : 1, Step : 3604, Training Loss : 1.33400, Training Acc : 0.589, Run Time : 0.67
INFO:root:2019-05-12 06:04:55, Epoch : 1, Step : 3605, Training Loss : 0.68781, Training Acc : 0.717, Run Time : 14.96
INFO:root:2019-05-12 06:05:17, Epoch : 1, Step : 3606, Training Loss : 0.79974, Training Acc : 0.644, Run Time : 21.64
INFO:root:2019-05-12 06:05:36, Epoch : 1, Step : 3607, Training Loss : 0.77845, Training Acc : 0.617, Run Time : 18.75
INFO:root:2019-05-12 06:05:47, Epoch : 1, Step : 3608, Training Loss : 0.73187, Training Acc : 0.689, Run Time : 11.61
INFO:root:2019-05-12 06:05:49, Epoch : 1, Step : 3609, Training Loss : 0.54965, Training Acc : 0.711, Run Time : 1.25
INFO:root:2019-05-12 06:06:03, Epoch : 1, Step : 3610, Training Loss : 0.72350, Training Acc : 0.700, Run Time : 14.75
INFO:root:2019-05-12 06:06:19, Epoch : 1, Step : 3611, Training Loss : 0.57351, Training Acc : 0.722, Run Time : 15.47
INFO:root:2019-05-12 06:06:36, Epoch : 1, Step : 3612, Training Loss : 0.26156, Training Acc : 0.900, Run Time : 17.46
INFO:root:2019-05-12 06:06:47, Epoch : 1, Step : 3613, Training Loss : 0.42336, Training Acc : 0.794, Run Time : 10.98
INFO:root:2019-05-12 06:06:52, Epoch : 1, Step : 3614, Training Loss : 0.45174, Training Acc : 0.806, Run Time : 4.72
INFO:root:2019-05-12 06:07:02, Epoch : 1, Step : 3615, Training Loss : 0.47542, Training Acc : 0.733, Run Time : 10.38
INFO:root:2019-05-12 06:07:03, Epoch : 1, Step : 3616, Training Loss : 0.48629, Training Acc : 0.739, Run Time : 0.51
INFO:root:2019-05-12 06:07:04, Epoch : 1, Step : 3617, Training Loss : 0.60440, Training Acc : 0.644, Run Time : 1.52
INFO:root:2019-05-12 06:07:17, Epoch : 1, Step : 3618, Training Loss : 0.48062, Training Acc : 0.750, Run Time : 12.96
INFO:root:2019-05-12 06:07:19, Epoch : 1, Step : 3619, Training Loss : 0.49916, Training Acc : 0.772, Run Time : 1.21
INFO:root:2019-05-12 06:07:19, Epoch : 1, Step : 3620, Training Loss : 0.35651, Training Acc : 0.800, Run Time : 0.64
INFO:root:2019-05-12 06:07:30, Epoch : 1, Step : 3621, Training Loss : 0.56130, Training Acc : 0.706, Run Time : 10.94
INFO:root:2019-05-12 06:07:31, Epoch : 1, Step : 3622, Training Loss : 0.54849, Training Acc : 0.717, Run Time : 0.68
INFO:root:2019-05-12 06:07:31, Epoch : 1, Step : 3623, Training Loss : 0.54805, Training Acc : 0.694, Run Time : 0.46
INFO:root:2019-05-12 06:07:46, Epoch : 1, Step : 3624, Training Loss : 0.49519, Training Acc : 0.728, Run Time : 14.89
INFO:root:2019-05-12 06:07:47, Epoch : 1, Step : 3625, Training Loss : 0.53896, Training Acc : 0.711, Run Time : 0.95
INFO:root:2019-05-12 06:08:02, Epoch : 1, Step : 3626, Training Loss : 0.46941, Training Acc : 0.806, Run Time : 14.52
INFO:root:2019-05-12 06:08:10, Epoch : 1, Step : 3627, Training Loss : 0.40321, Training Acc : 0.806, Run Time : 8.02
INFO:root:2019-05-12 06:08:10, Epoch : 1, Step : 3628, Training Loss : 0.39216, Training Acc : 0.800, Run Time : 0.73
INFO:root:2019-05-12 06:08:11, Epoch : 1, Step : 3629, Training Loss : 0.35246, Training Acc : 0.844, Run Time : 0.59
INFO:root:2019-05-12 06:08:19, Epoch : 1, Step : 3630, Training Loss : 0.37501, Training Acc : 0.828, Run Time : 7.85
INFO:root:2019-05-12 06:08:19, Epoch : 1, Step : 3631, Training Loss : 0.37444, Training Acc : 0.822, Run Time : 0.54
INFO:root:2019-05-12 06:08:20, Epoch : 1, Step : 3632, Training Loss : 0.26021, Training Acc : 0.917, Run Time : 0.44
INFO:root:2019-05-12 06:08:23, Epoch : 1, Step : 3633, Training Loss : 0.42039, Training Acc : 0.794, Run Time : 2.99
INFO:root:2019-05-12 06:08:24, Epoch : 1, Step : 3634, Training Loss : 0.41407, Training Acc : 0.794, Run Time : 1.32
INFO:root:2019-05-12 06:08:25, Epoch : 1, Step : 3635, Training Loss : 0.37737, Training Acc : 0.828, Run Time : 0.56
INFO:root:2019-05-12 06:08:34, Epoch : 1, Step : 3636, Training Loss : 0.37521, Training Acc : 0.833, Run Time : 9.75
INFO:root:2019-05-12 06:08:35, Epoch : 1, Step : 3637, Training Loss : 0.30980, Training Acc : 0.911, Run Time : 0.49
INFO:root:2019-05-12 06:08:36, Epoch : 1, Step : 3638, Training Loss : 0.40524, Training Acc : 0.811, Run Time : 1.33
INFO:root:2019-05-12 06:08:47, Epoch : 1, Step : 3639, Training Loss : 0.42527, Training Acc : 0.783, Run Time : 10.41
INFO:root:2019-05-12 06:08:48, Epoch : 1, Step : 3640, Training Loss : 0.70753, Training Acc : 0.656, Run Time : 0.86
INFO:root:2019-05-12 06:08:49, Epoch : 1, Step : 3641, Training Loss : 0.27648, Training Acc : 0.906, Run Time : 1.85
INFO:root:2019-05-12 06:09:02, Epoch : 1, Step : 3642, Training Loss : 0.37103, Training Acc : 0.844, Run Time : 12.12
INFO:root:2019-05-12 06:09:02, Epoch : 1, Step : 3643, Training Loss : 0.49643, Training Acc : 0.767, Run Time : 0.47
INFO:root:2019-05-12 06:09:04, Epoch : 1, Step : 3644, Training Loss : 0.39131, Training Acc : 0.822, Run Time : 2.33
INFO:root:2019-05-12 06:09:18, Epoch : 1, Step : 3645, Training Loss : 0.30773, Training Acc : 0.883, Run Time : 13.74
INFO:root:2019-05-12 06:09:32, Epoch : 1, Step : 3646, Training Loss : 0.33913, Training Acc : 0.850, Run Time : 14.09
INFO:root:2019-05-12 06:09:42, Epoch : 1, Step : 3647, Training Loss : 0.33603, Training Acc : 0.850, Run Time : 10.22
INFO:root:2019-05-12 06:09:43, Epoch : 1, Step : 3648, Training Loss : 0.26101, Training Acc : 0.872, Run Time : 0.78
INFO:root:2019-05-12 06:09:44, Epoch : 1, Step : 3649, Training Loss : 0.34403, Training Acc : 0.850, Run Time : 0.74
INFO:root:2019-05-12 06:09:57, Epoch : 1, Step : 3650, Training Loss : 0.34775, Training Acc : 0.839, Run Time : 13.36
INFO:root:2019-05-12 06:09:59, Epoch : 1, Step : 3651, Training Loss : 0.27926, Training Acc : 0.933, Run Time : 1.53
INFO:root:2019-05-12 06:10:10, Epoch : 1, Step : 3652, Training Loss : 0.35288, Training Acc : 0.844, Run Time : 11.19
INFO:root:2019-05-12 06:10:10, Epoch : 1, Step : 3653, Training Loss : 0.55533, Training Acc : 0.683, Run Time : 0.43
INFO:root:2019-05-12 06:10:11, Epoch : 1, Step : 3654, Training Loss : 0.49246, Training Acc : 0.733, Run Time : 0.82
INFO:root:2019-05-12 06:10:26, Epoch : 1, Step : 3655, Training Loss : 0.35796, Training Acc : 0.833, Run Time : 14.56
INFO:root:2019-05-12 06:10:26, Epoch : 1, Step : 3656, Training Loss : 0.40516, Training Acc : 0.833, Run Time : 0.53
INFO:root:2019-05-12 06:10:27, Epoch : 1, Step : 3657, Training Loss : 0.33838, Training Acc : 0.850, Run Time : 0.65
INFO:root:2019-05-12 06:10:39, Epoch : 1, Step : 3658, Training Loss : 0.30584, Training Acc : 0.856, Run Time : 12.52
INFO:root:2019-05-12 06:10:40, Epoch : 1, Step : 3659, Training Loss : 0.35060, Training Acc : 0.850, Run Time : 0.88
INFO:root:2019-05-12 06:10:41, Epoch : 1, Step : 3660, Training Loss : 0.25681, Training Acc : 0.933, Run Time : 0.65
INFO:root:2019-05-12 06:10:56, Epoch : 1, Step : 3661, Training Loss : 0.33150, Training Acc : 0.856, Run Time : 14.60
INFO:root:2019-05-12 06:10:57, Epoch : 1, Step : 3662, Training Loss : 0.48527, Training Acc : 0.767, Run Time : 1.21
INFO:root:2019-05-12 06:10:57, Epoch : 1, Step : 3663, Training Loss : 0.36023, Training Acc : 0.844, Run Time : 0.58
INFO:root:2019-05-12 06:10:59, Epoch : 1, Step : 3664, Training Loss : 0.36950, Training Acc : 0.839, Run Time : 1.96
INFO:root:2019-05-12 06:11:09, Epoch : 1, Step : 3665, Training Loss : 0.30554, Training Acc : 0.906, Run Time : 10.01
INFO:root:2019-05-12 06:11:10, Epoch : 1, Step : 3666, Training Loss : 0.41708, Training Acc : 0.817, Run Time : 0.45
INFO:root:2019-05-12 06:11:12, Epoch : 1, Step : 3667, Training Loss : 0.31901, Training Acc : 0.889, Run Time : 1.79
INFO:root:2019-05-12 06:11:26, Epoch : 1, Step : 3668, Training Loss : 0.26173, Training Acc : 0.906, Run Time : 14.36
INFO:root:2019-05-12 06:11:33, Epoch : 1, Step : 3669, Training Loss : 0.44538, Training Acc : 0.783, Run Time : 7.06
INFO:root:2019-05-12 06:11:36, Epoch : 1, Step : 3670, Training Loss : 0.37275, Training Acc : 0.789, Run Time : 2.91
INFO:root:2019-05-12 06:11:37, Epoch : 1, Step : 3671, Training Loss : 0.44186, Training Acc : 0.733, Run Time : 1.32
INFO:root:2019-05-12 06:11:45, Epoch : 1, Step : 3672, Training Loss : 0.34535, Training Acc : 0.828, Run Time : 8.08
INFO:root:2019-05-12 06:11:56, Epoch : 1, Step : 3673, Training Loss : 0.29463, Training Acc : 0.883, Run Time : 10.41
INFO:root:2019-05-12 06:11:57, Epoch : 1, Step : 3674, Training Loss : 0.31123, Training Acc : 0.894, Run Time : 1.16
INFO:root:2019-05-12 06:12:06, Epoch : 1, Step : 3675, Training Loss : 0.28155, Training Acc : 0.894, Run Time : 9.17
INFO:root:2019-05-12 06:12:07, Epoch : 1, Step : 3676, Training Loss : 0.41844, Training Acc : 0.767, Run Time : 1.06
INFO:root:2019-05-12 06:12:08, Epoch : 1, Step : 3677, Training Loss : 0.32530, Training Acc : 0.844, Run Time : 0.62
INFO:root:2019-05-12 06:12:17, Epoch : 1, Step : 3678, Training Loss : 0.38489, Training Acc : 0.767, Run Time : 9.65
INFO:root:2019-05-12 06:12:18, Epoch : 1, Step : 3679, Training Loss : 0.28629, Training Acc : 0.894, Run Time : 0.81
INFO:root:2019-05-12 06:12:19, Epoch : 1, Step : 3680, Training Loss : 0.37587, Training Acc : 0.828, Run Time : 0.59
INFO:root:2019-05-12 06:12:21, Epoch : 1, Step : 3681, Training Loss : 0.41354, Training Acc : 0.806, Run Time : 2.15
INFO:root:2019-05-12 06:12:31, Epoch : 1, Step : 3682, Training Loss : 0.29298, Training Acc : 0.878, Run Time : 9.74
INFO:root:2019-05-12 06:12:31, Epoch : 1, Step : 3683, Training Loss : 0.22389, Training Acc : 0.911, Run Time : 0.46
INFO:root:2019-05-12 06:12:32, Epoch : 1, Step : 3684, Training Loss : 0.24969, Training Acc : 0.911, Run Time : 0.77
INFO:root:2019-05-12 06:12:33, Epoch : 1, Step : 3685, Training Loss : 0.23298, Training Acc : 0.922, Run Time : 1.38
INFO:root:2019-05-12 06:12:43, Epoch : 1, Step : 3686, Training Loss : 0.43011, Training Acc : 0.739, Run Time : 10.15
INFO:root:2019-05-12 06:12:44, Epoch : 1, Step : 3687, Training Loss : 0.37715, Training Acc : 0.850, Run Time : 0.75
INFO:root:2019-05-12 06:12:54, Epoch : 1, Step : 3688, Training Loss : 0.34748, Training Acc : 0.850, Run Time : 9.68
INFO:root:2019-05-12 06:12:54, Epoch : 1, Step : 3689, Training Loss : 0.31724, Training Acc : 0.878, Run Time : 0.49
INFO:root:2019-05-12 06:12:55, Epoch : 1, Step : 3690, Training Loss : 0.25997, Training Acc : 0.917, Run Time : 0.56
INFO:root:2019-05-12 06:13:07, Epoch : 1, Step : 3691, Training Loss : 0.44144, Training Acc : 0.806, Run Time : 11.82
INFO:root:2019-05-12 06:13:07, Epoch : 1, Step : 3692, Training Loss : 0.33201, Training Acc : 0.844, Run Time : 0.49
INFO:root:2019-05-12 06:13:09, Epoch : 1, Step : 3693, Training Loss : 0.58596, Training Acc : 0.761, Run Time : 1.75
INFO:root:2019-05-12 06:13:20, Epoch : 1, Step : 3694, Training Loss : 0.41388, Training Acc : 0.817, Run Time : 11.06
INFO:root:2019-05-12 06:13:21, Epoch : 1, Step : 3695, Training Loss : 0.37265, Training Acc : 0.822, Run Time : 0.71
INFO:root:2019-05-12 06:13:21, Epoch : 1, Step : 3696, Training Loss : 0.42950, Training Acc : 0.783, Run Time : 0.58
INFO:root:2019-05-12 06:13:32, Epoch : 1, Step : 3697, Training Loss : 0.25671, Training Acc : 0.872, Run Time : 10.93
INFO:root:2019-05-12 06:13:34, Epoch : 1, Step : 3698, Training Loss : 0.67903, Training Acc : 0.672, Run Time : 1.41
INFO:root:2019-05-12 06:13:45, Epoch : 1, Step : 3699, Training Loss : 0.39170, Training Acc : 0.811, Run Time : 11.68
INFO:root:2019-05-12 06:13:48, Epoch : 1, Step : 3700, Training Loss : 0.64306, Training Acc : 0.667, Run Time : 3.09
INFO:root:2019-05-12 06:13:49, Epoch : 1, Step : 3701, Training Loss : 0.36260, Training Acc : 0.811, Run Time : 0.91
INFO:root:2019-05-12 06:13:56, Epoch : 1, Step : 3702, Training Loss : 0.55337, Training Acc : 0.739, Run Time : 6.99
INFO:root:2019-05-12 06:13:57, Epoch : 1, Step : 3703, Training Loss : 0.36878, Training Acc : 0.844, Run Time : 0.60
INFO:root:2019-05-12 06:14:09, Epoch : 1, Step : 3704, Training Loss : 0.61034, Training Acc : 0.739, Run Time : 11.80
INFO:root:2019-05-12 06:14:11, Epoch : 1, Step : 3705, Training Loss : 0.39096, Training Acc : 0.794, Run Time : 1.78
INFO:root:2019-05-12 06:14:17, Epoch : 1, Step : 3706, Training Loss : 0.61955, Training Acc : 0.694, Run Time : 6.40
INFO:root:2019-05-12 06:14:18, Epoch : 1, Step : 3707, Training Loss : 0.49620, Training Acc : 0.744, Run Time : 0.92
INFO:root:2019-05-12 06:14:19, Epoch : 1, Step : 3708, Training Loss : 0.50638, Training Acc : 0.756, Run Time : 1.59
INFO:root:2019-05-12 06:14:27, Epoch : 1, Step : 3709, Training Loss : 0.36472, Training Acc : 0.856, Run Time : 7.55
INFO:root:2019-05-12 06:14:28, Epoch : 1, Step : 3710, Training Loss : 0.65380, Training Acc : 0.578, Run Time : 0.64
INFO:root:2019-05-12 06:14:29, Epoch : 1, Step : 3711, Training Loss : 0.40433, Training Acc : 0.767, Run Time : 1.57
INFO:root:2019-05-12 06:14:44, Epoch : 1, Step : 3712, Training Loss : 0.40408, Training Acc : 0.794, Run Time : 14.81
INFO:root:2019-05-12 06:14:51, Epoch : 1, Step : 3713, Training Loss : 0.31050, Training Acc : 0.922, Run Time : 6.67
INFO:root:2019-05-12 06:14:51, Epoch : 1, Step : 3714, Training Loss : 0.31384, Training Acc : 0.889, Run Time : 0.60
INFO:root:2019-05-12 06:15:04, Epoch : 1, Step : 3715, Training Loss : 0.27133, Training Acc : 0.906, Run Time : 12.25
INFO:root:2019-05-12 06:15:04, Epoch : 1, Step : 3716, Training Loss : 0.37927, Training Acc : 0.817, Run Time : 0.81
INFO:root:2019-05-12 06:15:18, Epoch : 1, Step : 3717, Training Loss : 0.42544, Training Acc : 0.772, Run Time : 13.18
INFO:root:2019-05-12 06:15:19, Epoch : 1, Step : 3718, Training Loss : 0.30565, Training Acc : 0.889, Run Time : 1.73
INFO:root:2019-05-12 06:15:30, Epoch : 1, Step : 3719, Training Loss : 0.43620, Training Acc : 0.761, Run Time : 10.40
INFO:root:2019-05-12 06:15:31, Epoch : 1, Step : 3720, Training Loss : 0.44931, Training Acc : 0.828, Run Time : 1.31
INFO:root:2019-05-12 06:15:42, Epoch : 1, Step : 3721, Training Loss : 0.25135, Training Acc : 0.922, Run Time : 10.70
INFO:root:2019-05-12 06:15:43, Epoch : 1, Step : 3722, Training Loss : 0.19289, Training Acc : 0.961, Run Time : 1.57
INFO:root:2019-05-12 06:15:44, Epoch : 1, Step : 3723, Training Loss : 0.21114, Training Acc : 0.944, Run Time : 0.57
INFO:root:2019-05-12 06:15:57, Epoch : 1, Step : 3724, Training Loss : 0.34882, Training Acc : 0.844, Run Time : 13.14
INFO:root:2019-05-12 06:15:57, Epoch : 1, Step : 3725, Training Loss : 0.22436, Training Acc : 0.922, Run Time : 0.49
INFO:root:2019-05-12 06:16:11, Epoch : 1, Step : 3726, Training Loss : 0.21808, Training Acc : 0.928, Run Time : 13.63
INFO:root:2019-05-12 06:16:12, Epoch : 1, Step : 3727, Training Loss : 0.24602, Training Acc : 0.928, Run Time : 0.87
INFO:root:2019-05-12 06:16:23, Epoch : 1, Step : 3728, Training Loss : 0.26988, Training Acc : 0.883, Run Time : 10.57
INFO:root:2019-05-12 06:16:23, Epoch : 1, Step : 3729, Training Loss : 0.16068, Training Acc : 0.978, Run Time : 0.69
INFO:root:2019-05-12 06:16:30, Epoch : 1, Step : 3730, Training Loss : 0.25574, Training Acc : 0.894, Run Time : 6.76
INFO:root:2019-05-12 06:16:33, Epoch : 1, Step : 3731, Training Loss : 0.16447, Training Acc : 0.950, Run Time : 2.76
INFO:root:2019-05-12 06:16:34, Epoch : 1, Step : 3732, Training Loss : 0.33867, Training Acc : 0.856, Run Time : 1.33
INFO:root:2019-05-12 06:16:43, Epoch : 1, Step : 3733, Training Loss : 0.31821, Training Acc : 0.883, Run Time : 8.81
INFO:root:2019-05-12 06:16:44, Epoch : 1, Step : 3734, Training Loss : 0.22156, Training Acc : 0.900, Run Time : 0.97
INFO:root:2019-05-12 06:16:46, Epoch : 1, Step : 3735, Training Loss : 0.17826, Training Acc : 0.928, Run Time : 1.64
INFO:root:2019-05-12 06:17:04, Epoch : 1, Step : 3736, Training Loss : 0.34900, Training Acc : 0.811, Run Time : 18.34
INFO:root:2019-05-12 06:17:24, Epoch : 1, Step : 3737, Training Loss : 0.59906, Training Acc : 0.750, Run Time : 20.23
INFO:root:2019-05-12 06:17:38, Epoch : 1, Step : 3738, Training Loss : 0.28659, Training Acc : 0.861, Run Time : 13.68
INFO:root:2019-05-12 06:17:38, Epoch : 1, Step : 3739, Training Loss : 0.35685, Training Acc : 0.800, Run Time : 0.43
INFO:root:2019-05-12 06:17:39, Epoch : 1, Step : 3740, Training Loss : 0.31047, Training Acc : 0.856, Run Time : 0.41
INFO:root:2019-05-12 06:17:51, Epoch : 1, Step : 3741, Training Loss : 0.31176, Training Acc : 0.867, Run Time : 12.19
INFO:root:2019-05-12 06:17:52, Epoch : 1, Step : 3742, Training Loss : 0.35554, Training Acc : 0.844, Run Time : 0.79
INFO:root:2019-05-12 06:17:53, Epoch : 1, Step : 3743, Training Loss : 0.20093, Training Acc : 0.922, Run Time : 1.78
INFO:root:2019-05-12 06:18:03, Epoch : 1, Step : 3744, Training Loss : 0.23365, Training Acc : 0.928, Run Time : 9.46
INFO:root:2019-05-12 06:18:04, Epoch : 1, Step : 3745, Training Loss : 0.31515, Training Acc : 0.861, Run Time : 0.78
INFO:root:2019-05-12 06:18:05, Epoch : 1, Step : 3746, Training Loss : 0.27943, Training Acc : 0.883, Run Time : 1.84
INFO:root:2019-05-12 06:18:18, Epoch : 1, Step : 3747, Training Loss : 0.33656, Training Acc : 0.844, Run Time : 12.40
INFO:root:2019-05-12 06:18:19, Epoch : 1, Step : 3748, Training Loss : 0.36818, Training Acc : 0.811, Run Time : 0.67
INFO:root:2019-05-12 06:18:27, Epoch : 1, Step : 3749, Training Loss : 0.20362, Training Acc : 0.972, Run Time : 8.62
INFO:root:2019-05-12 06:18:30, Epoch : 1, Step : 3750, Training Loss : 0.36909, Training Acc : 0.844, Run Time : 3.20
INFO:root:2019-05-12 06:18:32, Epoch : 1, Step : 3751, Training Loss : 0.41626, Training Acc : 0.839, Run Time : 1.28
INFO:root:2019-05-12 06:18:40, Epoch : 1, Step : 3752, Training Loss : 0.41682, Training Acc : 0.789, Run Time : 8.87
INFO:root:2019-05-12 06:18:41, Epoch : 1, Step : 3753, Training Loss : 0.29091, Training Acc : 0.906, Run Time : 0.96
INFO:root:2019-05-12 06:18:43, Epoch : 1, Step : 3754, Training Loss : 0.30221, Training Acc : 0.867, Run Time : 1.07
INFO:root:2019-05-12 06:18:55, Epoch : 1, Step : 3755, Training Loss : 0.33080, Training Acc : 0.878, Run Time : 12.48
INFO:root:2019-05-12 06:18:56, Epoch : 1, Step : 3756, Training Loss : 0.35659, Training Acc : 0.844, Run Time : 1.30
INFO:root:2019-05-12 06:18:57, Epoch : 1, Step : 3757, Training Loss : 0.33129, Training Acc : 0.850, Run Time : 0.62
INFO:root:2019-05-12 06:19:09, Epoch : 1, Step : 3758, Training Loss : 0.24841, Training Acc : 0.917, Run Time : 11.84
INFO:root:2019-05-12 06:19:09, Epoch : 1, Step : 3759, Training Loss : 0.31888, Training Acc : 0.861, Run Time : 0.70
INFO:root:2019-05-12 06:19:11, Epoch : 1, Step : 3760, Training Loss : 0.33905, Training Acc : 0.822, Run Time : 1.60
INFO:root:2019-05-12 06:19:22, Epoch : 1, Step : 3761, Training Loss : 0.37196, Training Acc : 0.828, Run Time : 11.26
INFO:root:2019-05-12 06:19:23, Epoch : 1, Step : 3762, Training Loss : 0.33443, Training Acc : 0.856, Run Time : 0.72
INFO:root:2019-05-12 06:19:25, Epoch : 1, Step : 3763, Training Loss : 0.34256, Training Acc : 0.856, Run Time : 2.05
INFO:root:2019-05-12 06:19:39, Epoch : 1, Step : 3764, Training Loss : 0.25827, Training Acc : 0.900, Run Time : 13.75
INFO:root:2019-05-12 06:19:42, Epoch : 1, Step : 3765, Training Loss : 0.36616, Training Acc : 0.828, Run Time : 2.78
INFO:root:2019-05-12 06:20:07, Epoch : 1, Step : 3766, Training Loss : 0.47534, Training Acc : 0.767, Run Time : 25.76
INFO:root:2019-05-12 06:20:11, Epoch : 1, Step : 3767, Training Loss : 0.35890, Training Acc : 0.811, Run Time : 3.38
INFO:root:2019-05-12 06:20:12, Epoch : 1, Step : 3768, Training Loss : 0.35617, Training Acc : 0.828, Run Time : 1.04
INFO:root:2019-05-12 06:20:23, Epoch : 1, Step : 3769, Training Loss : 0.31707, Training Acc : 0.850, Run Time : 10.90
INFO:root:2019-05-12 06:20:24, Epoch : 1, Step : 3770, Training Loss : 0.36239, Training Acc : 0.867, Run Time : 1.11
INFO:root:2019-05-12 06:20:37, Epoch : 1, Step : 3771, Training Loss : 0.37735, Training Acc : 0.828, Run Time : 13.27
INFO:root:2019-05-12 06:20:38, Epoch : 1, Step : 3772, Training Loss : 0.43838, Training Acc : 0.778, Run Time : 0.52
INFO:root:2019-05-12 06:20:38, Epoch : 1, Step : 3773, Training Loss : 0.46763, Training Acc : 0.739, Run Time : 0.44
INFO:root:2019-05-12 06:20:54, Epoch : 1, Step : 3774, Training Loss : 0.36294, Training Acc : 0.811, Run Time : 15.59
INFO:root:2019-05-12 06:20:55, Epoch : 1, Step : 3775, Training Loss : 0.31274, Training Acc : 0.839, Run Time : 1.56
INFO:root:2019-05-12 06:21:10, Epoch : 1, Step : 3776, Training Loss : 0.46961, Training Acc : 0.733, Run Time : 15.31
INFO:root:2019-05-12 06:21:11, Epoch : 1, Step : 3777, Training Loss : 0.25806, Training Acc : 0.900, Run Time : 0.52
INFO:root:2019-05-12 06:21:12, Epoch : 1, Step : 3778, Training Loss : 0.40718, Training Acc : 0.789, Run Time : 0.98
INFO:root:2019-05-12 06:21:25, Epoch : 1, Step : 3779, Training Loss : 0.32251, Training Acc : 0.833, Run Time : 12.81
INFO:root:2019-05-12 06:21:26, Epoch : 1, Step : 3780, Training Loss : 0.27625, Training Acc : 0.900, Run Time : 0.88
INFO:root:2019-05-12 06:21:26, Epoch : 1, Step : 3781, Training Loss : 0.37416, Training Acc : 0.800, Run Time : 0.56
INFO:root:2019-05-12 06:21:27, Epoch : 1, Step : 3782, Training Loss : 0.33456, Training Acc : 0.828, Run Time : 0.65
INFO:root:2019-05-12 06:21:42, Epoch : 1, Step : 3783, Training Loss : 0.28901, Training Acc : 0.894, Run Time : 14.68
INFO:root:2019-05-12 06:21:42, Epoch : 1, Step : 3784, Training Loss : 0.28658, Training Acc : 0.856, Run Time : 0.77
INFO:root:2019-05-12 06:21:54, Epoch : 1, Step : 3785, Training Loss : 0.32114, Training Acc : 0.872, Run Time : 11.35
INFO:root:2019-05-12 06:22:03, Epoch : 1, Step : 3786, Training Loss : 0.31553, Training Acc : 0.850, Run Time : 9.29
INFO:root:2019-05-12 06:22:08, Epoch : 1, Step : 3787, Training Loss : 0.32855, Training Acc : 0.850, Run Time : 4.68
INFO:root:2019-05-12 06:22:31, Epoch : 1, Step : 3788, Training Loss : 0.33445, Training Acc : 0.839, Run Time : 23.06
INFO:root:2019-05-12 06:22:46, Epoch : 1, Step : 3789, Training Loss : 0.33773, Training Acc : 0.806, Run Time : 15.63
INFO:root:2019-05-12 06:22:52, Epoch : 1, Step : 3790, Training Loss : 0.34175, Training Acc : 0.850, Run Time : 5.78
INFO:root:2019-05-12 06:22:53, Epoch : 1, Step : 3791, Training Loss : 0.27953, Training Acc : 0.867, Run Time : 0.41
INFO:root:2019-05-12 06:22:53, Epoch : 1, Step : 3792, Training Loss : 0.42425, Training Acc : 0.761, Run Time : 0.55
INFO:root:2019-05-12 06:23:02, Epoch : 1, Step : 3793, Training Loss : 0.30387, Training Acc : 0.850, Run Time : 8.58
INFO:root:2019-05-12 06:23:02, Epoch : 1, Step : 3794, Training Loss : 0.36829, Training Acc : 0.783, Run Time : 0.66
INFO:root:2019-05-12 06:23:05, Epoch : 1, Step : 3795, Training Loss : 0.38942, Training Acc : 0.789, Run Time : 2.36
INFO:root:2019-05-12 06:23:16, Epoch : 1, Step : 3796, Training Loss : 0.35171, Training Acc : 0.811, Run Time : 11.22
INFO:root:2019-05-12 06:23:31, Epoch : 1, Step : 3797, Training Loss : 0.47239, Training Acc : 0.772, Run Time : 14.75
INFO:root:2019-05-12 06:23:32, Epoch : 1, Step : 3798, Training Loss : 0.28408, Training Acc : 0.844, Run Time : 0.89
INFO:root:2019-05-12 06:23:45, Epoch : 1, Step : 3799, Training Loss : 0.32294, Training Acc : 0.850, Run Time : 13.81
INFO:root:2019-05-12 06:24:01, Epoch : 1, Step : 3800, Training Loss : 0.28993, Training Acc : 0.861, Run Time : 15.18
INFO:root:2019-05-12 06:24:02, Epoch : 1, Step : 3801, Training Loss : 0.94425, Training Acc : 0.700, Run Time : 1.71
INFO:root:2019-05-12 06:24:05, Epoch : 1, Step : 3802, Training Loss : 1.06765, Training Acc : 0.628, Run Time : 2.36
INFO:root:2019-05-12 06:24:24, Epoch : 1, Step : 3803, Training Loss : 0.86429, Training Acc : 0.644, Run Time : 19.54
INFO:root:2019-05-12 06:24:26, Epoch : 1, Step : 3804, Training Loss : 0.75691, Training Acc : 0.728, Run Time : 2.15
INFO:root:2019-05-12 06:24:27, Epoch : 1, Step : 3805, Training Loss : 0.51173, Training Acc : 0.717, Run Time : 0.64
INFO:root:2019-05-12 06:24:37, Epoch : 1, Step : 3806, Training Loss : 0.60546, Training Acc : 0.761, Run Time : 10.08
INFO:root:2019-05-12 06:24:38, Epoch : 1, Step : 3807, Training Loss : 0.51898, Training Acc : 0.750, Run Time : 0.73
INFO:root:2019-05-12 06:24:40, Epoch : 1, Step : 3808, Training Loss : 0.28759, Training Acc : 0.883, Run Time : 1.77
INFO:root:2019-05-12 06:24:55, Epoch : 1, Step : 3809, Training Loss : 0.27868, Training Acc : 0.878, Run Time : 14.98
INFO:root:2019-05-12 06:25:09, Epoch : 1, Step : 3810, Training Loss : 0.27904, Training Acc : 0.894, Run Time : 14.34
INFO:root:2019-05-12 06:25:10, Epoch : 1, Step : 3811, Training Loss : 0.28778, Training Acc : 0.906, Run Time : 1.57
INFO:root:2019-05-12 06:25:11, Epoch : 1, Step : 3812, Training Loss : 0.43526, Training Acc : 0.828, Run Time : 0.59
INFO:root:2019-05-12 06:25:12, Epoch : 1, Step : 3813, Training Loss : 0.29116, Training Acc : 0.872, Run Time : 0.54
INFO:root:2019-05-12 06:25:12, Epoch : 1, Step : 3814, Training Loss : 0.42484, Training Acc : 0.861, Run Time : 0.57
INFO:root:2019-05-12 06:25:35, Epoch : 1, Step : 3815, Training Loss : 0.30374, Training Acc : 0.856, Run Time : 22.38
INFO:root:2019-05-12 06:25:46, Epoch : 1, Step : 3816, Training Loss : 0.37548, Training Acc : 0.828, Run Time : 11.09
INFO:root:2019-05-12 06:26:05, Epoch : 1, Step : 3817, Training Loss : 0.24459, Training Acc : 0.900, Run Time : 19.53
INFO:root:2019-05-12 06:26:22, Epoch : 1, Step : 3818, Training Loss : 0.46155, Training Acc : 0.750, Run Time : 16.93
INFO:root:2019-05-12 06:26:25, Epoch : 1, Step : 3819, Training Loss : 0.30601, Training Acc : 0.883, Run Time : 2.61
INFO:root:2019-05-12 06:26:25, Epoch : 1, Step : 3820, Training Loss : 0.35017, Training Acc : 0.867, Run Time : 0.60
INFO:root:2019-05-12 06:26:27, Epoch : 1, Step : 3821, Training Loss : 0.51446, Training Acc : 0.778, Run Time : 1.60
INFO:root:2019-05-12 06:26:36, Epoch : 1, Step : 3822, Training Loss : 0.48815, Training Acc : 0.794, Run Time : 9.26
INFO:root:2019-05-12 06:26:37, Epoch : 1, Step : 3823, Training Loss : 0.59741, Training Acc : 0.728, Run Time : 0.79
INFO:root:2019-05-12 06:26:39, Epoch : 1, Step : 3824, Training Loss : 0.31322, Training Acc : 0.878, Run Time : 1.70
INFO:root:2019-05-12 06:26:49, Epoch : 1, Step : 3825, Training Loss : 0.35967, Training Acc : 0.811, Run Time : 10.86
INFO:root:2019-05-12 06:26:50, Epoch : 1, Step : 3826, Training Loss : 0.40050, Training Acc : 0.811, Run Time : 0.63
INFO:root:2019-05-12 06:26:52, Epoch : 1, Step : 3827, Training Loss : 0.39554, Training Acc : 0.789, Run Time : 1.78
INFO:root:2019-05-12 06:27:05, Epoch : 1, Step : 3828, Training Loss : 0.29939, Training Acc : 0.878, Run Time : 13.28
INFO:root:2019-05-12 06:27:06, Epoch : 1, Step : 3829, Training Loss : 0.26023, Training Acc : 0.872, Run Time : 0.54
INFO:root:2019-05-12 06:27:06, Epoch : 1, Step : 3830, Training Loss : 0.34809, Training Acc : 0.811, Run Time : 0.65
INFO:root:2019-05-12 06:27:14, Epoch : 1, Step : 3831, Training Loss : 0.38871, Training Acc : 0.778, Run Time : 7.63
INFO:root:2019-05-12 06:27:15, Epoch : 1, Step : 3832, Training Loss : 0.31210, Training Acc : 0.844, Run Time : 0.68
INFO:root:2019-05-12 06:27:16, Epoch : 1, Step : 3833, Training Loss : 0.41914, Training Acc : 0.833, Run Time : 1.59
INFO:root:2019-05-12 06:27:31, Epoch : 1, Step : 3834, Training Loss : 0.40135, Training Acc : 0.822, Run Time : 15.11
INFO:root:2019-05-12 06:27:53, Epoch : 1, Step : 3835, Training Loss : 0.30474, Training Acc : 0.889, Run Time : 21.68
INFO:root:2019-05-12 06:28:04, Epoch : 1, Step : 3836, Training Loss : 0.42985, Training Acc : 0.844, Run Time : 10.62
INFO:root:2019-05-12 06:28:15, Epoch : 1, Step : 3837, Training Loss : 0.34872, Training Acc : 0.800, Run Time : 11.43
INFO:root:2019-05-12 06:28:16, Epoch : 1, Step : 3838, Training Loss : 0.28618, Training Acc : 0.889, Run Time : 1.10
INFO:root:2019-05-12 06:28:25, Epoch : 1, Step : 3839, Training Loss : 0.38771, Training Acc : 0.806, Run Time : 8.85
INFO:root:2019-05-12 06:28:26, Epoch : 1, Step : 3840, Training Loss : 0.33127, Training Acc : 0.833, Run Time : 1.12
INFO:root:2019-05-12 06:28:27, Epoch : 1, Step : 3841, Training Loss : 0.35365, Training Acc : 0.850, Run Time : 0.62
INFO:root:2019-05-12 06:28:40, Epoch : 1, Step : 3842, Training Loss : 0.47307, Training Acc : 0.811, Run Time : 13.70
INFO:root:2019-05-12 06:28:42, Epoch : 1, Step : 3843, Training Loss : 0.52900, Training Acc : 0.778, Run Time : 1.16
INFO:root:2019-05-12 06:28:43, Epoch : 1, Step : 3844, Training Loss : 0.54836, Training Acc : 0.728, Run Time : 1.57
INFO:root:2019-05-12 06:28:57, Epoch : 1, Step : 3845, Training Loss : 0.40679, Training Acc : 0.822, Run Time : 13.56
INFO:root:2019-05-12 06:29:02, Epoch : 1, Step : 3846, Training Loss : 0.39647, Training Acc : 0.794, Run Time : 4.91
INFO:root:2019-05-12 06:29:02, Epoch : 1, Step : 3847, Training Loss : 0.41148, Training Acc : 0.817, Run Time : 0.62
INFO:root:2019-05-12 06:29:16, Epoch : 1, Step : 3848, Training Loss : 0.52546, Training Acc : 0.744, Run Time : 13.36
INFO:root:2019-05-12 06:29:27, Epoch : 1, Step : 3849, Training Loss : 0.40036, Training Acc : 0.794, Run Time : 11.19
INFO:root:2019-05-12 06:29:28, Epoch : 1, Step : 3850, Training Loss : 0.32008, Training Acc : 0.861, Run Time : 1.38
INFO:root:2019-05-12 06:29:38, Epoch : 1, Step : 3851, Training Loss : 0.40491, Training Acc : 0.839, Run Time : 10.17
INFO:root:2019-05-12 06:29:40, Epoch : 1, Step : 3852, Training Loss : 0.38047, Training Acc : 0.839, Run Time : 1.41
INFO:root:2019-05-12 06:29:42, Epoch : 1, Step : 3853, Training Loss : 0.26627, Training Acc : 0.872, Run Time : 1.78
INFO:root:2019-05-12 06:29:55, Epoch : 1, Step : 3854, Training Loss : 0.55327, Training Acc : 0.761, Run Time : 13.73
INFO:root:2019-05-12 06:30:00, Epoch : 1, Step : 3855, Training Loss : 0.24573, Training Acc : 0.917, Run Time : 4.58
INFO:root:2019-05-12 06:30:01, Epoch : 1, Step : 3856, Training Loss : 0.35709, Training Acc : 0.828, Run Time : 0.91
INFO:root:2019-05-12 06:30:10, Epoch : 1, Step : 3857, Training Loss : 0.32541, Training Acc : 0.867, Run Time : 9.27
INFO:root:2019-05-12 06:30:11, Epoch : 1, Step : 3858, Training Loss : 0.51691, Training Acc : 0.789, Run Time : 1.17
INFO:root:2019-05-12 06:30:12, Epoch : 1, Step : 3859, Training Loss : 0.55262, Training Acc : 0.717, Run Time : 0.59
INFO:root:2019-05-12 06:30:26, Epoch : 1, Step : 3860, Training Loss : 0.41508, Training Acc : 0.806, Run Time : 14.51
INFO:root:2019-05-12 06:30:42, Epoch : 1, Step : 3861, Training Loss : 0.64350, Training Acc : 0.678, Run Time : 15.54
INFO:root:2019-05-12 06:30:50, Epoch : 1, Step : 3862, Training Loss : 0.48711, Training Acc : 0.761, Run Time : 7.73
INFO:root:2019-05-12 06:31:01, Epoch : 1, Step : 3863, Training Loss : 0.33217, Training Acc : 0.833, Run Time : 11.78
INFO:root:2019-05-12 06:31:02, Epoch : 1, Step : 3864, Training Loss : 0.51665, Training Acc : 0.733, Run Time : 0.95
INFO:root:2019-05-12 06:31:04, Epoch : 1, Step : 3865, Training Loss : 0.54321, Training Acc : 0.750, Run Time : 1.67
INFO:root:2019-05-12 06:31:14, Epoch : 1, Step : 3866, Training Loss : 0.27773, Training Acc : 0.878, Run Time : 9.99
INFO:root:2019-05-12 06:31:15, Epoch : 1, Step : 3867, Training Loss : 0.36689, Training Acc : 0.822, Run Time : 0.69
INFO:root:2019-05-12 06:31:15, Epoch : 1, Step : 3868, Training Loss : 0.28963, Training Acc : 0.883, Run Time : 0.56
INFO:root:2019-05-12 06:31:28, Epoch : 1, Step : 3869, Training Loss : 0.37681, Training Acc : 0.806, Run Time : 12.51
INFO:root:2019-05-12 06:31:33, Epoch : 1, Step : 3870, Training Loss : 0.35753, Training Acc : 0.811, Run Time : 5.36
INFO:root:2019-05-12 06:31:36, Epoch : 1, Step : 3871, Training Loss : 0.18385, Training Acc : 0.950, Run Time : 3.22
INFO:root:2019-05-12 06:31:37, Epoch : 1, Step : 3872, Training Loss : 0.17824, Training Acc : 0.933, Run Time : 1.00
INFO:root:2019-05-12 06:31:46, Epoch : 1, Step : 3873, Training Loss : 0.21448, Training Acc : 0.950, Run Time : 8.80
INFO:root:2019-05-12 06:31:47, Epoch : 1, Step : 3874, Training Loss : 0.17736, Training Acc : 0.950, Run Time : 0.72
INFO:root:2019-05-12 06:31:48, Epoch : 1, Step : 3875, Training Loss : 0.20583, Training Acc : 0.911, Run Time : 1.26
INFO:root:2019-05-12 06:32:00, Epoch : 1, Step : 3876, Training Loss : 0.18509, Training Acc : 0.939, Run Time : 11.39
INFO:root:2019-05-12 06:32:00, Epoch : 1, Step : 3877, Training Loss : 0.20012, Training Acc : 0.900, Run Time : 0.47
INFO:root:2019-05-12 06:32:01, Epoch : 1, Step : 3878, Training Loss : 0.15113, Training Acc : 0.972, Run Time : 0.58
INFO:root:2019-05-12 06:32:03, Epoch : 1, Step : 3879, Training Loss : 0.21551, Training Acc : 0.922, Run Time : 2.29
INFO:root:2019-05-12 06:32:04, Epoch : 1, Step : 3880, Training Loss : 0.20859, Training Acc : 0.928, Run Time : 0.63
INFO:root:2019-05-12 06:32:07, Epoch : 1, Step : 3881, Training Loss : 0.33916, Training Acc : 0.833, Run Time : 3.95
INFO:root:2019-05-12 06:32:09, Epoch : 1, Step : 3882, Training Loss : 0.18177, Training Acc : 0.961, Run Time : 1.96
INFO:root:2019-05-12 06:32:23, Epoch : 1, Step : 3883, Training Loss : 0.29539, Training Acc : 0.922, Run Time : 13.34
INFO:root:2019-05-12 06:32:25, Epoch : 1, Step : 3884, Training Loss : 0.26417, Training Acc : 0.900, Run Time : 2.05
INFO:root:2019-05-12 06:32:32, Epoch : 1, Step : 3885, Training Loss : 0.38226, Training Acc : 0.833, Run Time : 7.13
INFO:root:2019-05-12 06:32:33, Epoch : 1, Step : 3886, Training Loss : 0.27844, Training Acc : 0.889, Run Time : 1.46
INFO:root:2019-05-12 06:32:46, Epoch : 1, Step : 3887, Training Loss : 0.26354, Training Acc : 0.889, Run Time : 12.32
INFO:root:2019-05-12 06:32:47, Epoch : 1, Step : 3888, Training Loss : 0.27100, Training Acc : 0.872, Run Time : 0.83
INFO:root:2019-05-12 06:32:47, Epoch : 1, Step : 3889, Training Loss : 0.42950, Training Acc : 0.761, Run Time : 0.58
INFO:root:2019-05-12 06:32:59, Epoch : 1, Step : 3890, Training Loss : 0.24722, Training Acc : 0.933, Run Time : 11.51
INFO:root:2019-05-12 06:32:59, Epoch : 1, Step : 3891, Training Loss : 0.37010, Training Acc : 0.822, Run Time : 0.48
INFO:root:2019-05-12 06:33:00, Epoch : 1, Step : 3892, Training Loss : 0.27056, Training Acc : 0.872, Run Time : 1.22
INFO:root:2019-05-12 06:33:14, Epoch : 1, Step : 3893, Training Loss : 0.29108, Training Acc : 0.861, Run Time : 13.33
INFO:root:2019-05-12 06:33:16, Epoch : 1, Step : 3894, Training Loss : 0.42218, Training Acc : 0.772, Run Time : 2.09
INFO:root:2019-05-12 06:33:16, Epoch : 1, Step : 3895, Training Loss : 0.36565, Training Acc : 0.828, Run Time : 0.68
INFO:root:2019-05-12 06:33:27, Epoch : 1, Step : 3896, Training Loss : 0.30312, Training Acc : 0.850, Run Time : 10.34
INFO:root:2019-05-12 06:33:28, Epoch : 1, Step : 3897, Training Loss : 0.26243, Training Acc : 0.911, Run Time : 0.72
INFO:root:2019-05-12 06:33:29, Epoch : 1, Step : 3898, Training Loss : 0.36986, Training Acc : 0.811, Run Time : 1.53
INFO:root:2019-05-12 06:33:41, Epoch : 1, Step : 3899, Training Loss : 0.23056, Training Acc : 0.911, Run Time : 11.62
INFO:root:2019-05-12 06:33:41, Epoch : 1, Step : 3900, Training Loss : 0.30136, Training Acc : 0.839, Run Time : 0.48
INFO:root:2019-05-12 06:33:43, Epoch : 1, Step : 3901, Training Loss : 0.39853, Training Acc : 0.783, Run Time : 2.35
INFO:root:2019-05-12 06:33:55, Epoch : 1, Step : 3902, Training Loss : 0.37761, Training Acc : 0.794, Run Time : 11.43
INFO:root:2019-05-12 06:33:56, Epoch : 1, Step : 3903, Training Loss : 0.31511, Training Acc : 0.861, Run Time : 1.14
INFO:root:2019-05-12 06:34:08, Epoch : 1, Step : 3904, Training Loss : 0.21718, Training Acc : 0.911, Run Time : 12.15
INFO:root:2019-05-12 06:34:09, Epoch : 1, Step : 3905, Training Loss : 0.18940, Training Acc : 0.939, Run Time : 0.58
INFO:root:2019-05-12 06:34:13, Epoch : 1, Step : 3906, Training Loss : 0.28329, Training Acc : 0.883, Run Time : 3.81
INFO:root:2019-05-12 06:34:22, Epoch : 1, Step : 3907, Training Loss : 0.32714, Training Acc : 0.844, Run Time : 9.67
INFO:root:2019-05-12 06:34:23, Epoch : 1, Step : 3908, Training Loss : 0.26093, Training Acc : 0.889, Run Time : 0.55
INFO:root:2019-05-12 06:34:24, Epoch : 1, Step : 3909, Training Loss : 0.30062, Training Acc : 0.872, Run Time : 1.55
INFO:root:2019-05-12 06:34:34, Epoch : 1, Step : 3910, Training Loss : 0.26867, Training Acc : 0.872, Run Time : 9.38
INFO:root:2019-05-12 06:34:34, Epoch : 1, Step : 3911, Training Loss : 0.26933, Training Acc : 0.878, Run Time : 0.45
INFO:root:2019-05-12 06:34:35, Epoch : 1, Step : 3912, Training Loss : 0.22376, Training Acc : 0.906, Run Time : 0.71
INFO:root:2019-05-12 06:34:36, Epoch : 1, Step : 3913, Training Loss : 0.20031, Training Acc : 0.933, Run Time : 0.81
INFO:root:2019-05-12 06:34:56, Epoch : 1, Step : 3914, Training Loss : 0.18101, Training Acc : 0.956, Run Time : 20.62
INFO:root:2019-05-12 06:34:58, Epoch : 1, Step : 3915, Training Loss : 0.25717, Training Acc : 0.911, Run Time : 1.29
INFO:root:2019-05-12 06:35:16, Epoch : 1, Step : 3916, Training Loss : 0.22355, Training Acc : 0.900, Run Time : 18.35
INFO:root:2019-05-12 06:35:17, Epoch : 1, Step : 3917, Training Loss : 0.24189, Training Acc : 0.861, Run Time : 1.43
INFO:root:2019-05-12 06:35:18, Epoch : 1, Step : 3918, Training Loss : 0.18513, Training Acc : 0.950, Run Time : 0.63
INFO:root:2019-05-12 06:35:30, Epoch : 1, Step : 3919, Training Loss : 0.19294, Training Acc : 0.928, Run Time : 11.55
INFO:root:2019-05-12 06:35:30, Epoch : 1, Step : 3920, Training Loss : 0.26614, Training Acc : 0.856, Run Time : 0.42
INFO:root:2019-05-12 06:35:32, Epoch : 1, Step : 3921, Training Loss : 0.16985, Training Acc : 0.928, Run Time : 2.43
INFO:root:2019-05-12 06:35:45, Epoch : 1, Step : 3922, Training Loss : 0.17515, Training Acc : 0.911, Run Time : 12.53
INFO:root:2019-05-12 06:35:46, Epoch : 1, Step : 3923, Training Loss : 0.18561, Training Acc : 0.933, Run Time : 0.89
INFO:root:2019-05-12 06:35:46, Epoch : 1, Step : 3924, Training Loss : 0.14845, Training Acc : 0.939, Run Time : 0.61
INFO:root:2019-05-12 06:35:59, Epoch : 1, Step : 3925, Training Loss : 0.10738, Training Acc : 0.961, Run Time : 12.79
INFO:root:2019-05-12 06:36:01, Epoch : 1, Step : 3926, Training Loss : 0.18075, Training Acc : 0.939, Run Time : 1.60
INFO:root:2019-05-12 06:36:01, Epoch : 1, Step : 3927, Training Loss : 0.22915, Training Acc : 0.922, Run Time : 0.62
INFO:root:2019-05-12 06:36:15, Epoch : 1, Step : 3928, Training Loss : 0.29827, Training Acc : 0.856, Run Time : 13.87
INFO:root:2019-05-12 06:36:16, Epoch : 1, Step : 3929, Training Loss : 0.28414, Training Acc : 0.889, Run Time : 1.03
INFO:root:2019-05-12 06:36:28, Epoch : 1, Step : 3930, Training Loss : 0.33414, Training Acc : 0.850, Run Time : 11.98
INFO:root:2019-05-12 06:36:29, Epoch : 1, Step : 3931, Training Loss : 0.39027, Training Acc : 0.811, Run Time : 0.89
INFO:root:2019-05-12 06:36:42, Epoch : 1, Step : 3932, Training Loss : 0.45290, Training Acc : 0.789, Run Time : 12.85
INFO:root:2019-05-12 06:36:43, Epoch : 1, Step : 3933, Training Loss : 0.47297, Training Acc : 0.811, Run Time : 0.46
INFO:root:2019-05-12 06:36:43, Epoch : 1, Step : 3934, Training Loss : 0.48342, Training Acc : 0.772, Run Time : 0.54
INFO:root:2019-05-12 06:36:55, Epoch : 1, Step : 3935, Training Loss : 0.31581, Training Acc : 0.878, Run Time : 11.93
INFO:root:2019-05-12 06:36:56, Epoch : 1, Step : 3936, Training Loss : 0.25233, Training Acc : 0.900, Run Time : 1.41
INFO:root:2019-05-12 06:36:58, Epoch : 1, Step : 3937, Training Loss : 0.47901, Training Acc : 0.800, Run Time : 2.04
INFO:root:2019-05-12 06:37:16, Epoch : 1, Step : 3938, Training Loss : 0.46342, Training Acc : 0.800, Run Time : 17.85
INFO:root:2019-05-12 06:37:39, Epoch : 1, Step : 3939, Training Loss : 0.32802, Training Acc : 0.861, Run Time : 22.25
INFO:root:2019-05-12 06:37:45, Epoch : 1, Step : 3940, Training Loss : 0.34877, Training Acc : 0.878, Run Time : 6.07
INFO:root:2019-05-12 06:37:45, Epoch : 1, Step : 3941, Training Loss : 0.38526, Training Acc : 0.856, Run Time : 0.67
INFO:root:2019-05-12 06:37:46, Epoch : 1, Step : 3942, Training Loss : 0.39172, Training Acc : 0.856, Run Time : 0.59
INFO:root:2019-05-12 06:37:58, Epoch : 1, Step : 3943, Training Loss : 0.43435, Training Acc : 0.811, Run Time : 12.53
INFO:root:2019-05-12 06:38:00, Epoch : 1, Step : 3944, Training Loss : 0.48477, Training Acc : 0.750, Run Time : 1.50
INFO:root:2019-05-12 06:38:01, Epoch : 1, Step : 3945, Training Loss : 0.31579, Training Acc : 0.867, Run Time : 0.61
INFO:root:2019-05-12 06:38:01, Epoch : 1, Step : 3946, Training Loss : 0.30023, Training Acc : 0.900, Run Time : 0.61
INFO:root:2019-05-12 06:38:11, Epoch : 1, Step : 3947, Training Loss : 0.30009, Training Acc : 0.861, Run Time : 10.28
INFO:root:2019-05-12 06:38:12, Epoch : 1, Step : 3948, Training Loss : 0.28172, Training Acc : 0.911, Run Time : 0.47
INFO:root:2019-05-12 06:38:13, Epoch : 1, Step : 3949, Training Loss : 0.33207, Training Acc : 0.850, Run Time : 0.73
INFO:root:2019-05-12 06:38:15, Epoch : 1, Step : 3950, Training Loss : 0.27903, Training Acc : 0.906, Run Time : 1.95
INFO:root:2019-05-12 06:38:26, Epoch : 1, Step : 3951, Training Loss : 0.84210, Training Acc : 0.678, Run Time : 11.55
INFO:root:2019-05-12 06:38:27, Epoch : 1, Step : 3952, Training Loss : 0.63004, Training Acc : 0.717, Run Time : 0.71
INFO:root:2019-05-12 06:38:33, Epoch : 1, Step : 3953, Training Loss : 0.43845, Training Acc : 0.828, Run Time : 5.68
INFO:root:2019-05-12 06:38:33, Epoch : 1, Step : 3954, Training Loss : 0.17779, Training Acc : 0.944, Run Time : 0.76
INFO:root:2019-05-12 06:38:41, Epoch : 1, Step : 3955, Training Loss : 0.26880, Training Acc : 0.906, Run Time : 8.15
INFO:root:2019-05-12 06:38:42, Epoch : 1, Step : 3956, Training Loss : 0.32285, Training Acc : 0.861, Run Time : 0.61
INFO:root:2019-05-12 06:38:43, Epoch : 1, Step : 3957, Training Loss : 0.21145, Training Acc : 0.917, Run Time : 1.21
INFO:root:2019-05-12 06:38:56, Epoch : 1, Step : 3958, Training Loss : 0.25947, Training Acc : 0.894, Run Time : 12.71
INFO:root:2019-05-12 06:38:57, Epoch : 1, Step : 3959, Training Loss : 0.19429, Training Acc : 0.933, Run Time : 1.07
INFO:root:2019-05-12 06:38:58, Epoch : 1, Step : 3960, Training Loss : 0.38901, Training Acc : 0.817, Run Time : 0.61
INFO:root:2019-05-12 06:39:09, Epoch : 1, Step : 3961, Training Loss : 0.34791, Training Acc : 0.822, Run Time : 11.84
INFO:root:2019-05-12 06:39:10, Epoch : 1, Step : 3962, Training Loss : 0.22712, Training Acc : 0.900, Run Time : 0.70
INFO:root:2019-05-12 06:39:12, Epoch : 1, Step : 3963, Training Loss : 0.13098, Training Acc : 0.956, Run Time : 1.61
INFO:root:2019-05-12 06:39:24, Epoch : 1, Step : 3964, Training Loss : 0.29423, Training Acc : 0.917, Run Time : 11.88
INFO:root:2019-05-12 06:39:25, Epoch : 1, Step : 3965, Training Loss : 0.16460, Training Acc : 0.939, Run Time : 1.15
INFO:root:2019-05-12 06:39:38, Epoch : 1, Step : 3966, Training Loss : 0.12053, Training Acc : 0.972, Run Time : 12.68
INFO:root:2019-05-12 06:39:39, Epoch : 1, Step : 3967, Training Loss : 0.18041, Training Acc : 0.939, Run Time : 1.02
INFO:root:2019-05-12 06:40:00, Epoch : 1, Step : 3968, Training Loss : 0.33423, Training Acc : 0.844, Run Time : 21.00
INFO:root:2019-05-12 06:40:10, Epoch : 1, Step : 3969, Training Loss : 0.27509, Training Acc : 0.900, Run Time : 10.65
INFO:root:2019-05-12 06:40:11, Epoch : 1, Step : 3970, Training Loss : 0.22553, Training Acc : 0.911, Run Time : 0.61
INFO:root:2019-05-12 06:40:13, Epoch : 1, Step : 3971, Training Loss : 0.15705, Training Acc : 0.944, Run Time : 2.55
INFO:root:2019-05-12 06:40:28, Epoch : 1, Step : 3972, Training Loss : 0.19005, Training Acc : 0.950, Run Time : 14.87
INFO:root:2019-05-12 06:40:41, Epoch : 1, Step : 3973, Training Loss : 0.30140, Training Acc : 0.889, Run Time : 13.29
INFO:root:2019-05-12 06:40:42, Epoch : 1, Step : 3974, Training Loss : 0.16204, Training Acc : 0.950, Run Time : 0.87
INFO:root:2019-05-12 06:40:43, Epoch : 1, Step : 3975, Training Loss : 0.17267, Training Acc : 0.950, Run Time : 0.89
INFO:root:2019-05-12 06:40:54, Epoch : 1, Step : 3976, Training Loss : 0.10832, Training Acc : 0.967, Run Time : 11.13
INFO:root:2019-05-12 06:40:55, Epoch : 1, Step : 3977, Training Loss : 0.14601, Training Acc : 0.944, Run Time : 0.81
INFO:root:2019-05-12 06:41:07, Epoch : 1, Step : 3978, Training Loss : 0.21549, Training Acc : 0.900, Run Time : 11.59
INFO:root:2019-05-12 06:41:08, Epoch : 1, Step : 3979, Training Loss : 0.20322, Training Acc : 0.933, Run Time : 0.95
INFO:root:2019-05-12 06:41:19, Epoch : 1, Step : 3980, Training Loss : 0.32690, Training Acc : 0.883, Run Time : 11.14
INFO:root:2019-05-12 06:41:20, Epoch : 1, Step : 3981, Training Loss : 0.30389, Training Acc : 0.833, Run Time : 0.75
INFO:root:2019-05-12 06:41:20, Epoch : 1, Step : 3982, Training Loss : 0.30005, Training Acc : 0.878, Run Time : 0.44
INFO:root:2019-05-12 06:41:33, Epoch : 1, Step : 3983, Training Loss : 0.20529, Training Acc : 0.911, Run Time : 13.38
INFO:root:2019-05-12 06:41:35, Epoch : 1, Step : 3984, Training Loss : 0.14643, Training Acc : 0.939, Run Time : 1.63
INFO:root:2019-05-12 06:41:47, Epoch : 1, Step : 3985, Training Loss : 0.15988, Training Acc : 0.967, Run Time : 11.56
INFO:root:2019-05-12 06:41:47, Epoch : 1, Step : 3986, Training Loss : 0.16052, Training Acc : 0.944, Run Time : 0.58
INFO:root:2019-05-12 06:41:49, Epoch : 1, Step : 3987, Training Loss : 0.16499, Training Acc : 0.933, Run Time : 1.34
INFO:root:2019-05-12 06:42:04, Epoch : 1, Step : 3988, Training Loss : 0.14511, Training Acc : 0.961, Run Time : 14.94
INFO:root:2019-05-12 06:42:05, Epoch : 1, Step : 3989, Training Loss : 0.17370, Training Acc : 0.939, Run Time : 1.43
INFO:root:2019-05-12 06:42:06, Epoch : 1, Step : 3990, Training Loss : 0.19293, Training Acc : 0.917, Run Time : 0.79
INFO:root:2019-05-12 06:42:07, Epoch : 1, Step : 3991, Training Loss : 0.20932, Training Acc : 0.917, Run Time : 0.85
INFO:root:2019-05-12 06:42:08, Epoch : 1, Step : 3992, Training Loss : 0.28288, Training Acc : 0.856, Run Time : 1.62
INFO:root:2019-05-12 06:42:20, Epoch : 1, Step : 3993, Training Loss : 0.15054, Training Acc : 0.950, Run Time : 12.08
INFO:root:2019-05-12 06:42:33, Epoch : 1, Step : 3994, Training Loss : 0.13905, Training Acc : 0.950, Run Time : 12.57
INFO:root:2019-05-12 06:42:36, Epoch : 1, Step : 3995, Training Loss : 0.15021, Training Acc : 0.922, Run Time : 2.67
INFO:root:2019-05-12 06:42:36, Epoch : 1, Step : 3996, Training Loss : 0.19550, Training Acc : 0.939, Run Time : 0.82
INFO:root:2019-05-12 06:42:47, Epoch : 1, Step : 3997, Training Loss : 0.10541, Training Acc : 0.961, Run Time : 10.62
INFO:root:2019-05-12 06:42:48, Epoch : 1, Step : 3998, Training Loss : 0.17093, Training Acc : 0.933, Run Time : 0.72
INFO:root:2019-05-12 06:43:02, Epoch : 1, Step : 3999, Training Loss : 0.22160, Training Acc : 0.894, Run Time : 14.06
INFO:root:2019-05-12 06:43:03, Epoch : 1, Step : 4000, Training Loss : 0.25578, Training Acc : 0.883, Run Time : 1.26
INFO:root:2019-05-12 06:43:15, Epoch : 1, Step : 4001, Training Loss : 0.51489, Training Acc : 0.739, Run Time : 12.08
INFO:root:2019-05-12 06:43:16, Epoch : 1, Step : 4002, Training Loss : 0.38042, Training Acc : 0.828, Run Time : 0.88
INFO:root:2019-05-12 06:43:30, Epoch : 1, Step : 4003, Training Loss : 0.34355, Training Acc : 0.828, Run Time : 13.65
INFO:root:2019-05-12 06:43:31, Epoch : 1, Step : 4004, Training Loss : 0.30232, Training Acc : 0.878, Run Time : 1.37
INFO:root:2019-05-12 06:43:43, Epoch : 1, Step : 4005, Training Loss : 0.22883, Training Acc : 0.922, Run Time : 11.60
INFO:root:2019-05-12 06:43:44, Epoch : 1, Step : 4006, Training Loss : 0.24291, Training Acc : 0.906, Run Time : 1.02
INFO:root:2019-05-12 06:43:56, Epoch : 1, Step : 4007, Training Loss : 0.28196, Training Acc : 0.883, Run Time : 12.43
INFO:root:2019-05-12 06:43:57, Epoch : 1, Step : 4008, Training Loss : 0.20761, Training Acc : 0.900, Run Time : 1.12
INFO:root:2019-05-12 06:43:59, Epoch : 1, Step : 4009, Training Loss : 0.24774, Training Acc : 0.889, Run Time : 2.14
INFO:root:2019-05-12 06:44:11, Epoch : 1, Step : 4010, Training Loss : 0.30329, Training Acc : 0.861, Run Time : 12.07
INFO:root:2019-05-12 06:44:20, Epoch : 1, Step : 4011, Training Loss : 0.26874, Training Acc : 0.894, Run Time : 8.68
INFO:root:2019-05-12 06:44:21, Epoch : 1, Step : 4012, Training Loss : 0.20618, Training Acc : 0.894, Run Time : 0.77
INFO:root:2019-05-12 06:44:21, Epoch : 1, Step : 4013, Training Loss : 0.18512, Training Acc : 0.894, Run Time : 0.64
INFO:root:2019-05-12 06:44:34, Epoch : 1, Step : 4014, Training Loss : 0.30985, Training Acc : 0.867, Run Time : 12.40
INFO:root:2019-05-12 06:44:34, Epoch : 1, Step : 4015, Training Loss : 0.38603, Training Acc : 0.833, Run Time : 0.48
INFO:root:2019-05-12 06:44:35, Epoch : 1, Step : 4016, Training Loss : 0.25509, Training Acc : 0.911, Run Time : 1.19
INFO:root:2019-05-12 06:44:48, Epoch : 1, Step : 4017, Training Loss : 0.22504, Training Acc : 0.911, Run Time : 12.33
INFO:root:2019-05-12 06:44:49, Epoch : 1, Step : 4018, Training Loss : 0.22760, Training Acc : 0.944, Run Time : 0.93
INFO:root:2019-05-12 06:44:59, Epoch : 1, Step : 4019, Training Loss : 0.22862, Training Acc : 0.917, Run Time : 10.39
INFO:root:2019-05-12 06:45:00, Epoch : 1, Step : 4020, Training Loss : 0.29604, Training Acc : 0.867, Run Time : 1.23
INFO:root:2019-05-12 06:45:17, Epoch : 1, Step : 4021, Training Loss : 0.22891, Training Acc : 0.922, Run Time : 16.87
INFO:root:2019-05-12 06:45:21, Epoch : 1, Step : 4022, Training Loss : 0.18982, Training Acc : 0.917, Run Time : 3.54
INFO:root:2019-05-12 06:45:32, Epoch : 1, Step : 4023, Training Loss : 0.37483, Training Acc : 0.856, Run Time : 11.31
INFO:root:2019-05-12 06:45:33, Epoch : 1, Step : 4024, Training Loss : 0.19747, Training Acc : 0.906, Run Time : 1.33
INFO:root:2019-05-12 06:45:34, Epoch : 1, Step : 4025, Training Loss : 0.09709, Training Acc : 0.972, Run Time : 0.75
INFO:root:2019-05-12 06:45:36, Epoch : 1, Step : 4026, Training Loss : 0.36732, Training Acc : 0.844, Run Time : 1.69
INFO:root:2019-05-12 06:45:46, Epoch : 1, Step : 4027, Training Loss : 0.25094, Training Acc : 0.894, Run Time : 10.25
INFO:root:2019-05-12 06:45:47, Epoch : 1, Step : 4028, Training Loss : 0.16640, Training Acc : 0.928, Run Time : 0.56
INFO:root:2019-05-12 06:45:47, Epoch : 1, Step : 4029, Training Loss : 0.16340, Training Acc : 0.933, Run Time : 0.60
INFO:root:2019-05-12 06:45:50, Epoch : 1, Step : 4030, Training Loss : 0.15816, Training Acc : 0.950, Run Time : 2.57
INFO:root:2019-05-12 06:46:03, Epoch : 1, Step : 4031, Training Loss : 0.17410, Training Acc : 0.933, Run Time : 12.67
INFO:root:2019-05-12 06:46:03, Epoch : 1, Step : 4032, Training Loss : 0.21381, Training Acc : 0.922, Run Time : 0.86
INFO:root:2019-05-12 06:46:04, Epoch : 1, Step : 4033, Training Loss : 0.26592, Training Acc : 0.911, Run Time : 0.59
INFO:root:2019-05-12 06:46:16, Epoch : 1, Step : 4034, Training Loss : 0.13379, Training Acc : 0.956, Run Time : 12.23
INFO:root:2019-05-12 06:46:17, Epoch : 1, Step : 4035, Training Loss : 0.16417, Training Acc : 0.944, Run Time : 0.47
INFO:root:2019-05-12 06:46:17, Epoch : 1, Step : 4036, Training Loss : 0.22315, Training Acc : 0.878, Run Time : 0.61
INFO:root:2019-05-12 06:46:20, Epoch : 1, Step : 4037, Training Loss : 0.25151, Training Acc : 0.894, Run Time : 2.81
INFO:root:2019-05-12 06:46:31, Epoch : 1, Step : 4038, Training Loss : 0.24768, Training Acc : 0.922, Run Time : 10.44
INFO:root:2019-05-12 06:46:31, Epoch : 1, Step : 4039, Training Loss : 0.24910, Training Acc : 0.889, Run Time : 0.68
INFO:root:2019-05-12 06:46:43, Epoch : 1, Step : 4040, Training Loss : 0.34104, Training Acc : 0.867, Run Time : 11.86
INFO:root:2019-05-12 06:46:44, Epoch : 1, Step : 4041, Training Loss : 0.25968, Training Acc : 0.883, Run Time : 0.92
INFO:root:2019-05-12 06:46:45, Epoch : 1, Step : 4042, Training Loss : 0.55842, Training Acc : 0.811, Run Time : 0.61
INFO:root:2019-05-12 06:46:45, Epoch : 1, Step : 4043, Training Loss : 0.53133, Training Acc : 0.811, Run Time : 0.63
INFO:root:2019-05-12 06:46:46, Epoch : 1, Step : 4044, Training Loss : 0.32461, Training Acc : 0.883, Run Time : 0.59
INFO:root:2019-05-12 06:47:02, Epoch : 1, Step : 4045, Training Loss : 0.30784, Training Acc : 0.844, Run Time : 16.47
INFO:root:2019-05-12 06:47:03, Epoch : 1, Step : 4046, Training Loss : 0.24892, Training Acc : 0.878, Run Time : 0.50
INFO:root:2019-05-12 06:47:03, Epoch : 1, Step : 4047, Training Loss : 0.28599, Training Acc : 0.878, Run Time : 0.66
INFO:root:2019-05-12 06:47:05, Epoch : 1, Step : 4048, Training Loss : 0.32372, Training Acc : 0.867, Run Time : 1.22
INFO:root:2019-05-12 06:47:05, Epoch : 1, Step : 4049, Training Loss : 0.37998, Training Acc : 0.839, Run Time : 0.64
INFO:root:2019-05-12 06:47:20, Epoch : 1, Step : 4050, Training Loss : 0.34942, Training Acc : 0.872, Run Time : 14.37
INFO:root:2019-05-12 06:47:21, Epoch : 1, Step : 4051, Training Loss : 0.21166, Training Acc : 0.911, Run Time : 1.51
INFO:root:2019-05-12 06:47:23, Epoch : 1, Step : 4052, Training Loss : 0.34352, Training Acc : 0.878, Run Time : 1.94
INFO:root:2019-05-12 06:47:24, Epoch : 1, Step : 4053, Training Loss : 0.35368, Training Acc : 0.856, Run Time : 0.91
INFO:root:2019-05-12 06:47:25, Epoch : 1, Step : 4054, Training Loss : 0.40596, Training Acc : 0.828, Run Time : 0.84
INFO:root:2019-05-12 06:47:33, Epoch : 1, Step : 4055, Training Loss : 0.32539, Training Acc : 0.861, Run Time : 8.45
INFO:root:2019-05-12 06:47:34, Epoch : 1, Step : 4056, Training Loss : 0.52738, Training Acc : 0.778, Run Time : 1.02
INFO:root:2019-05-12 06:47:45, Epoch : 1, Step : 4057, Training Loss : 0.49172, Training Acc : 0.806, Run Time : 10.68
INFO:root:2019-05-12 06:47:46, Epoch : 1, Step : 4058, Training Loss : 0.37166, Training Acc : 0.850, Run Time : 1.02
INFO:root:2019-05-12 06:47:48, Epoch : 1, Step : 4059, Training Loss : 0.33680, Training Acc : 0.850, Run Time : 1.62
INFO:root:2019-05-12 06:47:58, Epoch : 1, Step : 4060, Training Loss : 0.36958, Training Acc : 0.839, Run Time : 10.31
INFO:root:2019-05-12 06:47:58, Epoch : 1, Step : 4061, Training Loss : 0.30094, Training Acc : 0.867, Run Time : 0.41
INFO:root:2019-05-12 06:47:59, Epoch : 1, Step : 4062, Training Loss : 0.27009, Training Acc : 0.894, Run Time : 0.50
INFO:root:2019-05-12 06:48:10, Epoch : 1, Step : 4063, Training Loss : 0.41070, Training Acc : 0.828, Run Time : 11.27
INFO:root:2019-05-12 06:48:11, Epoch : 1, Step : 4064, Training Loss : 0.41832, Training Acc : 0.811, Run Time : 0.97
INFO:root:2019-05-12 06:48:24, Epoch : 1, Step : 4065, Training Loss : 0.46018, Training Acc : 0.789, Run Time : 12.42
INFO:root:2019-05-12 06:48:24, Epoch : 1, Step : 4066, Training Loss : 0.41864, Training Acc : 0.772, Run Time : 0.54
INFO:root:2019-05-12 06:48:25, Epoch : 1, Step : 4067, Training Loss : 0.42588, Training Acc : 0.778, Run Time : 1.02
INFO:root:2019-05-12 06:48:37, Epoch : 1, Step : 4068, Training Loss : 0.29577, Training Acc : 0.878, Run Time : 12.07
INFO:root:2019-05-12 06:48:38, Epoch : 1, Step : 4069, Training Loss : 0.28593, Training Acc : 0.883, Run Time : 0.57
INFO:root:2019-05-12 06:48:39, Epoch : 1, Step : 4070, Training Loss : 0.25762, Training Acc : 0.889, Run Time : 1.59
INFO:root:2019-05-12 06:48:50, Epoch : 1, Step : 4071, Training Loss : 0.36614, Training Acc : 0.844, Run Time : 10.60
INFO:root:2019-05-12 06:48:50, Epoch : 1, Step : 4072, Training Loss : 0.32687, Training Acc : 0.883, Run Time : 0.48
INFO:root:2019-05-12 06:48:53, Epoch : 1, Step : 4073, Training Loss : 0.28855, Training Acc : 0.911, Run Time : 2.13
INFO:root:2019-05-12 06:49:04, Epoch : 1, Step : 4074, Training Loss : 0.24153, Training Acc : 0.922, Run Time : 11.25
INFO:root:2019-05-12 06:49:05, Epoch : 1, Step : 4075, Training Loss : 0.23857, Training Acc : 0.922, Run Time : 0.92
INFO:root:2019-05-12 06:49:15, Epoch : 1, Step : 4076, Training Loss : 0.34019, Training Acc : 0.867, Run Time : 10.21
INFO:root:2019-05-12 06:49:16, Epoch : 1, Step : 4077, Training Loss : 0.24964, Training Acc : 0.911, Run Time : 1.24
INFO:root:2019-05-12 06:49:17, Epoch : 1, Step : 4078, Training Loss : 0.42323, Training Acc : 0.811, Run Time : 0.54
INFO:root:2019-05-12 06:49:28, Epoch : 1, Step : 4079, Training Loss : 0.30602, Training Acc : 0.878, Run Time : 10.81
INFO:root:2019-05-12 06:49:29, Epoch : 1, Step : 4080, Training Loss : 0.31803, Training Acc : 0.844, Run Time : 1.17
INFO:root:2019-05-12 06:49:29, Epoch : 1, Step : 4081, Training Loss : 0.35693, Training Acc : 0.833, Run Time : 0.68
INFO:root:2019-05-12 06:49:30, Epoch : 1, Step : 4082, Training Loss : 0.27931, Training Acc : 0.878, Run Time : 0.75
INFO:root:2019-05-12 06:49:31, Epoch : 1, Step : 4083, Training Loss : 0.24477, Training Acc : 0.900, Run Time : 1.35
INFO:root:2019-05-12 06:49:44, Epoch : 1, Step : 4084, Training Loss : 0.27917, Training Acc : 0.867, Run Time : 13.02
INFO:root:2019-05-12 06:49:45, Epoch : 1, Step : 4085, Training Loss : 0.37654, Training Acc : 0.833, Run Time : 0.45
INFO:root:2019-05-12 06:49:46, Epoch : 1, Step : 4086, Training Loss : 0.22315, Training Acc : 0.894, Run Time : 1.52
INFO:root:2019-05-12 06:49:57, Epoch : 1, Step : 4087, Training Loss : 0.33145, Training Acc : 0.850, Run Time : 10.62
INFO:root:2019-05-12 06:49:58, Epoch : 1, Step : 4088, Training Loss : 0.14241, Training Acc : 0.961, Run Time : 0.43
INFO:root:2019-05-12 06:50:00, Epoch : 1, Step : 4089, Training Loss : 0.21541, Training Acc : 0.894, Run Time : 2.19
INFO:root:2019-05-12 06:50:13, Epoch : 1, Step : 4090, Training Loss : 0.26895, Training Acc : 0.883, Run Time : 13.09
INFO:root:2019-05-12 06:50:28, Epoch : 1, Step : 4091, Training Loss : 0.25948, Training Acc : 0.883, Run Time : 15.22
INFO:root:2019-05-12 06:50:29, Epoch : 1, Step : 4092, Training Loss : 0.25837, Training Acc : 0.894, Run Time : 1.30
INFO:root:2019-05-12 06:50:30, Epoch : 1, Step : 4093, Training Loss : 0.25141, Training Acc : 0.900, Run Time : 0.58
INFO:root:2019-05-12 06:50:40, Epoch : 1, Step : 4094, Training Loss : 0.26062, Training Acc : 0.900, Run Time : 10.41
INFO:root:2019-05-12 06:50:41, Epoch : 1, Step : 4095, Training Loss : 0.16301, Training Acc : 0.950, Run Time : 0.45
INFO:root:2019-05-12 06:50:43, Epoch : 1, Step : 4096, Training Loss : 0.25208, Training Acc : 0.917, Run Time : 1.86
INFO:root:2019-05-12 06:50:55, Epoch : 1, Step : 4097, Training Loss : 0.26640, Training Acc : 0.939, Run Time : 12.28
INFO:root:2019-05-12 06:50:57, Epoch : 1, Step : 4098, Training Loss : 0.24942, Training Acc : 0.900, Run Time : 1.76
INFO:root:2019-05-12 06:51:07, Epoch : 1, Step : 4099, Training Loss : 0.31581, Training Acc : 0.872, Run Time : 10.44
INFO:root:2019-05-12 06:51:08, Epoch : 1, Step : 4100, Training Loss : 0.26618, Training Acc : 0.922, Run Time : 0.48
INFO:root:2019-05-12 06:51:20, Epoch : 1, Step : 4101, Training Loss : 0.28801, Training Acc : 0.900, Run Time : 11.93
INFO:root:2019-05-12 06:51:20, Epoch : 1, Step : 4102, Training Loss : 0.18742, Training Acc : 0.928, Run Time : 0.41
INFO:root:2019-05-12 06:51:21, Epoch : 1, Step : 4103, Training Loss : 0.19172, Training Acc : 0.922, Run Time : 0.61
INFO:root:2019-05-12 06:51:32, Epoch : 1, Step : 4104, Training Loss : 0.20999, Training Acc : 0.928, Run Time : 11.29
INFO:root:2019-05-12 06:51:33, Epoch : 1, Step : 4105, Training Loss : 0.31094, Training Acc : 0.867, Run Time : 1.32
INFO:root:2019-05-12 06:51:35, Epoch : 1, Step : 4106, Training Loss : 0.35709, Training Acc : 0.844, Run Time : 1.76
INFO:root:2019-05-12 06:51:46, Epoch : 1, Step : 4107, Training Loss : 0.26456, Training Acc : 0.878, Run Time : 10.66
INFO:root:2019-05-12 06:51:46, Epoch : 1, Step : 4108, Training Loss : 0.23204, Training Acc : 0.917, Run Time : 0.89
INFO:root:2019-05-12 06:51:49, Epoch : 1, Step : 4109, Training Loss : 0.26792, Training Acc : 0.894, Run Time : 2.12
INFO:root:2019-05-12 06:52:10, Epoch : 1, Step : 4110, Training Loss : 0.25086, Training Acc : 0.878, Run Time : 21.01
INFO:root:2019-05-12 06:52:11, Epoch : 1, Step : 4111, Training Loss : 0.26551, Training Acc : 0.900, Run Time : 1.78
INFO:root:2019-05-12 06:52:12, Epoch : 1, Step : 4112, Training Loss : 0.25563, Training Acc : 0.911, Run Time : 0.62
INFO:root:2019-05-12 06:52:13, Epoch : 1, Step : 4113, Training Loss : 0.33669, Training Acc : 0.850, Run Time : 0.57
INFO:root:2019-05-12 06:52:14, Epoch : 1, Step : 4114, Training Loss : 0.29871, Training Acc : 0.867, Run Time : 1.27
INFO:root:2019-05-12 06:52:30, Epoch : 1, Step : 4115, Training Loss : 0.20866, Training Acc : 0.906, Run Time : 15.90
INFO:root:2019-05-12 06:52:31, Epoch : 1, Step : 4116, Training Loss : 0.22489, Training Acc : 0.906, Run Time : 1.00
INFO:root:2019-05-12 06:52:31, Epoch : 1, Step : 4117, Training Loss : 0.30919, Training Acc : 0.906, Run Time : 0.69
INFO:root:2019-05-12 06:52:48, Epoch : 1, Step : 4118, Training Loss : 0.29959, Training Acc : 0.872, Run Time : 17.05
INFO:root:2019-05-12 06:52:49, Epoch : 1, Step : 4119, Training Loss : 0.18237, Training Acc : 0.928, Run Time : 0.86
INFO:root:2019-05-12 06:52:51, Epoch : 1, Step : 4120, Training Loss : 0.33813, Training Acc : 0.861, Run Time : 1.41
INFO:root:2019-05-12 06:53:02, Epoch : 1, Step : 4121, Training Loss : 0.24050, Training Acc : 0.894, Run Time : 11.31
INFO:root:2019-05-12 06:53:03, Epoch : 1, Step : 4122, Training Loss : 0.41776, Training Acc : 0.839, Run Time : 1.31
INFO:root:2019-05-12 06:53:05, Epoch : 1, Step : 4123, Training Loss : 0.25032, Training Acc : 0.906, Run Time : 1.23
INFO:root:2019-05-12 06:53:17, Epoch : 1, Step : 4124, Training Loss : 0.31464, Training Acc : 0.856, Run Time : 12.74
INFO:root:2019-05-12 06:53:18, Epoch : 1, Step : 4125, Training Loss : 0.23949, Training Acc : 0.906, Run Time : 0.78
INFO:root:2019-05-12 06:53:33, Epoch : 1, Step : 4126, Training Loss : 0.19945, Training Acc : 0.928, Run Time : 15.07
INFO:root:2019-05-12 06:53:34, Epoch : 1, Step : 4127, Training Loss : 0.16872, Training Acc : 0.939, Run Time : 0.49
INFO:root:2019-05-12 06:53:35, Epoch : 1, Step : 4128, Training Loss : 0.14845, Training Acc : 0.950, Run Time : 1.17
INFO:root:2019-05-12 06:53:47, Epoch : 1, Step : 4129, Training Loss : 0.18432, Training Acc : 0.928, Run Time : 11.73
INFO:root:2019-05-12 06:53:47, Epoch : 1, Step : 4130, Training Loss : 0.24467, Training Acc : 0.894, Run Time : 0.51
INFO:root:2019-05-12 06:53:48, Epoch : 1, Step : 4131, Training Loss : 0.27887, Training Acc : 0.906, Run Time : 0.58
INFO:root:2019-05-12 06:54:00, Epoch : 1, Step : 4132, Training Loss : 0.26009, Training Acc : 0.911, Run Time : 12.05
INFO:root:2019-05-12 06:54:01, Epoch : 1, Step : 4133, Training Loss : 0.37210, Training Acc : 0.861, Run Time : 0.89
INFO:root:2019-05-12 06:54:12, Epoch : 1, Step : 4134, Training Loss : 0.38305, Training Acc : 0.850, Run Time : 11.75
INFO:root:2019-05-12 06:54:13, Epoch : 1, Step : 4135, Training Loss : 0.28914, Training Acc : 0.883, Run Time : 0.42
INFO:root:2019-05-12 06:54:14, Epoch : 1, Step : 4136, Training Loss : 0.40523, Training Acc : 0.844, Run Time : 0.84
INFO:root:2019-05-12 06:54:26, Epoch : 1, Step : 4137, Training Loss : 0.22207, Training Acc : 0.911, Run Time : 12.23
INFO:root:2019-05-12 06:54:28, Epoch : 1, Step : 4138, Training Loss : 0.21130, Training Acc : 0.917, Run Time : 1.92
INFO:root:2019-05-12 06:54:54, Epoch : 1, Step : 4139, Training Loss : 0.25483, Training Acc : 0.878, Run Time : 26.62
INFO:root:2019-05-12 06:54:59, Epoch : 1, Step : 4140, Training Loss : 0.27396, Training Acc : 0.906, Run Time : 4.87
INFO:root:2019-05-12 06:55:00, Epoch : 1, Step : 4141, Training Loss : 0.37416, Training Acc : 0.872, Run Time : 0.50
INFO:root:2019-05-12 06:55:07, Epoch : 1, Step : 4142, Training Loss : 0.23044, Training Acc : 0.889, Run Time : 7.59
INFO:root:2019-05-12 06:55:08, Epoch : 1, Step : 4143, Training Loss : 0.17693, Training Acc : 0.944, Run Time : 0.86
INFO:root:2019-05-12 06:55:09, Epoch : 1, Step : 4144, Training Loss : 0.18168, Training Acc : 0.939, Run Time : 0.58
INFO:root:2019-05-12 06:55:20, Epoch : 1, Step : 4145, Training Loss : 0.18281, Training Acc : 0.917, Run Time : 11.22
INFO:root:2019-05-12 06:55:22, Epoch : 1, Step : 4146, Training Loss : 0.16333, Training Acc : 0.939, Run Time : 1.63
INFO:root:2019-05-12 06:55:39, Epoch : 1, Step : 4147, Training Loss : 0.18500, Training Acc : 0.939, Run Time : 17.20
INFO:root:2019-05-12 06:55:44, Epoch : 1, Step : 4148, Training Loss : 0.28803, Training Acc : 0.872, Run Time : 5.57
INFO:root:2019-05-12 06:55:45, Epoch : 1, Step : 4149, Training Loss : 0.44354, Training Acc : 0.822, Run Time : 0.64
INFO:root:2019-05-12 06:55:54, Epoch : 1, Step : 4150, Training Loss : 0.39898, Training Acc : 0.833, Run Time : 9.27
INFO:root:2019-05-12 06:55:55, Epoch : 1, Step : 4151, Training Loss : 0.33333, Training Acc : 0.900, Run Time : 0.69
INFO:root:2019-05-12 06:55:57, Epoch : 1, Step : 4152, Training Loss : 0.34216, Training Acc : 0.878, Run Time : 1.88
INFO:root:2019-05-12 06:56:07, Epoch : 1, Step : 4153, Training Loss : 0.35407, Training Acc : 0.867, Run Time : 10.55
INFO:root:2019-05-12 06:56:09, Epoch : 1, Step : 4154, Training Loss : 0.27871, Training Acc : 0.900, Run Time : 1.06
INFO:root:2019-05-12 06:56:21, Epoch : 1, Step : 4155, Training Loss : 0.24020, Training Acc : 0.906, Run Time : 12.56
INFO:root:2019-05-12 06:56:22, Epoch : 1, Step : 4156, Training Loss : 0.18360, Training Acc : 0.944, Run Time : 0.47
INFO:root:2019-05-12 06:56:33, Epoch : 1, Step : 4157, Training Loss : 0.25452, Training Acc : 0.911, Run Time : 11.39
INFO:root:2019-05-12 06:56:34, Epoch : 1, Step : 4158, Training Loss : 0.21446, Training Acc : 0.933, Run Time : 1.26
INFO:root:2019-05-12 06:56:35, Epoch : 1, Step : 4159, Training Loss : 0.31019, Training Acc : 0.850, Run Time : 0.60
INFO:root:2019-05-12 06:56:44, Epoch : 1, Step : 4160, Training Loss : 0.26842, Training Acc : 0.906, Run Time : 9.70
INFO:root:2019-05-12 06:56:45, Epoch : 1, Step : 4161, Training Loss : 0.32289, Training Acc : 0.856, Run Time : 0.71
INFO:root:2019-05-12 06:56:46, Epoch : 1, Step : 4162, Training Loss : 0.45029, Training Acc : 0.828, Run Time : 1.08
INFO:root:2019-05-12 06:56:57, Epoch : 1, Step : 4163, Training Loss : 0.30882, Training Acc : 0.867, Run Time : 10.73
INFO:root:2019-05-12 06:56:58, Epoch : 1, Step : 4164, Training Loss : 0.28282, Training Acc : 0.894, Run Time : 0.65
INFO:root:2019-05-12 06:56:58, Epoch : 1, Step : 4165, Training Loss : 0.27866, Training Acc : 0.861, Run Time : 0.64
INFO:root:2019-05-12 06:57:00, Epoch : 1, Step : 4166, Training Loss : 0.22294, Training Acc : 0.928, Run Time : 1.79
INFO:root:2019-05-12 06:57:12, Epoch : 1, Step : 4167, Training Loss : 0.39142, Training Acc : 0.861, Run Time : 12.13
INFO:root:2019-05-12 06:57:13, Epoch : 1, Step : 4168, Training Loss : 0.21656, Training Acc : 0.906, Run Time : 1.01
INFO:root:2019-05-12 06:57:28, Epoch : 1, Step : 4169, Training Loss : 0.26095, Training Acc : 0.889, Run Time : 15.03
INFO:root:2019-05-12 06:57:30, Epoch : 1, Step : 4170, Training Loss : 0.36688, Training Acc : 0.817, Run Time : 2.05
INFO:root:2019-05-12 06:57:42, Epoch : 1, Step : 4171, Training Loss : 0.27639, Training Acc : 0.900, Run Time : 12.18
INFO:root:2019-05-12 06:57:43, Epoch : 1, Step : 4172, Training Loss : 0.40532, Training Acc : 0.861, Run Time : 0.71
INFO:root:2019-05-12 06:57:48, Epoch : 1, Step : 4173, Training Loss : 0.21211, Training Acc : 0.917, Run Time : 4.88
INFO:root:2019-05-12 06:57:53, Epoch : 1, Step : 4174, Training Loss : 0.17257, Training Acc : 0.933, Run Time : 4.51
INFO:root:2019-05-12 06:57:53, Epoch : 1, Step : 4175, Training Loss : 0.19155, Training Acc : 0.917, Run Time : 0.56
INFO:root:2019-05-12 06:57:54, Epoch : 1, Step : 4176, Training Loss : 0.28232, Training Acc : 0.900, Run Time : 0.66
INFO:root:2019-05-12 06:57:54, Epoch : 1, Step : 4177, Training Loss : 0.22347, Training Acc : 0.911, Run Time : 0.59
INFO:root:2019-05-12 06:58:08, Epoch : 1, Step : 4178, Training Loss : 0.14899, Training Acc : 0.944, Run Time : 13.17
INFO:root:2019-05-12 06:58:15, Epoch : 1, Step : 4179, Training Loss : 0.27754, Training Acc : 0.872, Run Time : 7.29
INFO:root:2019-05-12 06:58:26, Epoch : 1, Step : 4180, Training Loss : 0.17726, Training Acc : 0.933, Run Time : 11.46
INFO:root:2019-05-12 06:58:30, Epoch : 1, Step : 4181, Training Loss : 0.26107, Training Acc : 0.894, Run Time : 3.64
INFO:root:2019-05-12 06:58:31, Epoch : 1, Step : 4182, Training Loss : 0.22794, Training Acc : 0.911, Run Time : 0.59
INFO:root:2019-05-12 06:58:42, Epoch : 1, Step : 4183, Training Loss : 0.16616, Training Acc : 0.917, Run Time : 11.56
INFO:root:2019-05-12 06:58:54, Epoch : 1, Step : 4184, Training Loss : 0.15785, Training Acc : 0.933, Run Time : 12.11
INFO:root:2019-05-12 06:58:59, Epoch : 1, Step : 4185, Training Loss : 0.12770, Training Acc : 0.961, Run Time : 4.31
INFO:root:2019-05-12 06:58:59, Epoch : 1, Step : 4186, Training Loss : 0.13528, Training Acc : 0.950, Run Time : 0.46
INFO:root:2019-05-12 06:59:00, Epoch : 1, Step : 4187, Training Loss : 0.14171, Training Acc : 0.939, Run Time : 0.78
INFO:root:2019-05-12 06:59:11, Epoch : 1, Step : 4188, Training Loss : 0.13206, Training Acc : 0.939, Run Time : 11.41
INFO:root:2019-05-12 06:59:12, Epoch : 1, Step : 4189, Training Loss : 0.12697, Training Acc : 0.956, Run Time : 0.56
INFO:root:2019-05-12 06:59:12, Epoch : 1, Step : 4190, Training Loss : 0.18938, Training Acc : 0.928, Run Time : 0.65
INFO:root:2019-05-12 06:59:24, Epoch : 1, Step : 4191, Training Loss : 0.28233, Training Acc : 0.906, Run Time : 11.39
INFO:root:2019-05-12 06:59:24, Epoch : 1, Step : 4192, Training Loss : 0.23458, Training Acc : 0.917, Run Time : 0.55
INFO:root:2019-05-12 06:59:25, Epoch : 1, Step : 4193, Training Loss : 0.24246, Training Acc : 0.883, Run Time : 0.61
INFO:root:2019-05-12 06:59:37, Epoch : 1, Step : 4194, Training Loss : 0.21023, Training Acc : 0.900, Run Time : 11.90
INFO:root:2019-05-12 06:59:38, Epoch : 1, Step : 4195, Training Loss : 0.24173, Training Acc : 0.911, Run Time : 1.03
INFO:root:2019-05-12 06:59:39, Epoch : 1, Step : 4196, Training Loss : 0.12961, Training Acc : 0.972, Run Time : 0.68
INFO:root:2019-05-12 06:59:40, Epoch : 1, Step : 4197, Training Loss : 0.29034, Training Acc : 0.894, Run Time : 1.90
INFO:root:2019-05-12 06:59:51, Epoch : 1, Step : 4198, Training Loss : 0.56515, Training Acc : 0.800, Run Time : 10.37
INFO:root:2019-05-12 06:59:51, Epoch : 1, Step : 4199, Training Loss : 0.48189, Training Acc : 0.783, Run Time : 0.51
INFO:root:2019-05-12 06:59:53, Epoch : 1, Step : 4200, Training Loss : 0.45231, Training Acc : 0.828, Run Time : 2.01
INFO:root:2019-05-12 07:00:04, Epoch : 1, Step : 4201, Training Loss : 0.84559, Training Acc : 0.644, Run Time : 11.11
INFO:root:2019-05-12 07:00:05, Epoch : 1, Step : 4202, Training Loss : 0.58592, Training Acc : 0.744, Run Time : 0.55
INFO:root:2019-05-12 07:00:07, Epoch : 1, Step : 4203, Training Loss : 0.30086, Training Acc : 0.900, Run Time : 2.24
INFO:root:2019-05-12 07:00:23, Epoch : 1, Step : 4204, Training Loss : 0.34313, Training Acc : 0.833, Run Time : 15.31
INFO:root:2019-05-12 07:00:24, Epoch : 1, Step : 4205, Training Loss : 0.43423, Training Acc : 0.839, Run Time : 1.03
INFO:root:2019-05-12 07:00:24, Epoch : 1, Step : 4206, Training Loss : 0.45630, Training Acc : 0.778, Run Time : 0.70
INFO:root:2019-05-12 07:00:42, Epoch : 1, Step : 4207, Training Loss : 0.51810, Training Acc : 0.778, Run Time : 17.60
INFO:root:2019-05-12 07:00:43, Epoch : 1, Step : 4208, Training Loss : 0.45034, Training Acc : 0.822, Run Time : 1.48
INFO:root:2019-05-12 07:00:58, Epoch : 1, Step : 4209, Training Loss : 0.61628, Training Acc : 0.694, Run Time : 14.68
INFO:root:2019-05-12 07:00:59, Epoch : 1, Step : 4210, Training Loss : 0.42044, Training Acc : 0.806, Run Time : 1.14
INFO:root:2019-05-12 07:01:11, Epoch : 1, Step : 4211, Training Loss : 0.40009, Training Acc : 0.850, Run Time : 11.33
INFO:root:2019-05-12 07:01:12, Epoch : 1, Step : 4212, Training Loss : 0.29234, Training Acc : 0.900, Run Time : 1.30
INFO:root:2019-05-12 07:01:13, Epoch : 1, Step : 4213, Training Loss : 0.38738, Training Acc : 0.828, Run Time : 1.16
INFO:root:2019-05-12 07:01:26, Epoch : 1, Step : 4214, Training Loss : 0.21603, Training Acc : 0.933, Run Time : 12.61
INFO:root:2019-05-12 07:01:34, Epoch : 1, Step : 4215, Training Loss : 0.19698, Training Acc : 0.928, Run Time : 8.73
INFO:root:2019-05-12 07:01:37, Epoch : 1, Step : 4216, Training Loss : 0.16634, Training Acc : 0.933, Run Time : 2.28
INFO:root:2019-05-12 07:01:37, Epoch : 1, Step : 4217, Training Loss : 0.48012, Training Acc : 0.744, Run Time : 0.55
INFO:root:2019-05-12 07:01:55, Epoch : 1, Step : 4218, Training Loss : 0.29570, Training Acc : 0.883, Run Time : 18.07
INFO:root:2019-05-12 07:02:00, Epoch : 1, Step : 4219, Training Loss : 0.44994, Training Acc : 0.806, Run Time : 5.07
INFO:root:2019-05-12 07:02:01, Epoch : 1, Step : 4220, Training Loss : 0.32656, Training Acc : 0.850, Run Time : 0.57
INFO:root:2019-05-12 07:02:08, Epoch : 1, Step : 4221, Training Loss : 0.50742, Training Acc : 0.772, Run Time : 7.33
INFO:root:2019-05-12 07:02:13, Epoch : 1, Step : 4222, Training Loss : 0.65037, Training Acc : 0.717, Run Time : 4.57
INFO:root:2019-05-12 07:02:13, Epoch : 1, Step : 4223, Training Loss : 0.23400, Training Acc : 0.917, Run Time : 0.53
INFO:root:2019-05-12 07:02:23, Epoch : 1, Step : 4224, Training Loss : 0.44264, Training Acc : 0.817, Run Time : 9.73
INFO:root:2019-05-12 07:02:24, Epoch : 1, Step : 4225, Training Loss : 0.24960, Training Acc : 0.911, Run Time : 0.94
INFO:root:2019-05-12 07:02:40, Epoch : 1, Step : 4226, Training Loss : 0.32786, Training Acc : 0.872, Run Time : 16.42
INFO:root:2019-05-12 07:02:58, Epoch : 1, Step : 4227, Training Loss : 0.38881, Training Acc : 0.817, Run Time : 18.06
INFO:root:2019-05-12 07:03:00, Epoch : 1, Step : 4228, Training Loss : 0.37979, Training Acc : 0.817, Run Time : 1.67
INFO:root:2019-05-12 07:03:01, Epoch : 1, Step : 4229, Training Loss : 0.26933, Training Acc : 0.911, Run Time : 1.01
INFO:root:2019-05-12 07:03:14, Epoch : 1, Step : 4230, Training Loss : 0.40741, Training Acc : 0.789, Run Time : 12.68
INFO:root:2019-05-12 07:03:15, Epoch : 1, Step : 4231, Training Loss : 0.32671, Training Acc : 0.844, Run Time : 1.41
INFO:root:2019-05-12 07:03:27, Epoch : 1, Step : 4232, Training Loss : 0.39713, Training Acc : 0.833, Run Time : 12.09
INFO:root:2019-05-12 07:03:28, Epoch : 1, Step : 4233, Training Loss : 0.33388, Training Acc : 0.872, Run Time : 0.51
INFO:root:2019-05-12 07:03:30, Epoch : 1, Step : 4234, Training Loss : 0.40797, Training Acc : 0.789, Run Time : 1.98
INFO:root:2019-05-12 07:03:41, Epoch : 1, Step : 4235, Training Loss : 0.39614, Training Acc : 0.828, Run Time : 10.81
INFO:root:2019-05-12 07:03:42, Epoch : 1, Step : 4236, Training Loss : 0.26216, Training Acc : 0.889, Run Time : 0.96
INFO:root:2019-05-12 07:03:55, Epoch : 1, Step : 4237, Training Loss : 0.24468, Training Acc : 0.911, Run Time : 13.32
INFO:root:2019-05-12 07:03:56, Epoch : 1, Step : 4238, Training Loss : 0.46246, Training Acc : 0.789, Run Time : 1.29
INFO:root:2019-05-12 07:03:57, Epoch : 1, Step : 4239, Training Loss : 0.25075, Training Acc : 0.872, Run Time : 0.59
INFO:root:2019-05-12 07:04:11, Epoch : 1, Step : 4240, Training Loss : 0.40080, Training Acc : 0.800, Run Time : 14.33
INFO:root:2019-05-12 07:04:20, Epoch : 1, Step : 4241, Training Loss : 0.46183, Training Acc : 0.761, Run Time : 9.15
INFO:root:2019-05-12 07:04:23, Epoch : 1, Step : 4242, Training Loss : 0.35852, Training Acc : 0.867, Run Time : 2.76
INFO:root:2019-05-12 07:04:24, Epoch : 1, Step : 4243, Training Loss : 0.38123, Training Acc : 0.828, Run Time : 0.72
INFO:root:2019-05-12 07:04:24, Epoch : 1, Step : 4244, Training Loss : 0.35917, Training Acc : 0.817, Run Time : 0.64
INFO:root:2019-05-12 07:04:33, Epoch : 1, Step : 4245, Training Loss : 0.45674, Training Acc : 0.789, Run Time : 8.98
INFO:root:2019-05-12 07:04:34, Epoch : 1, Step : 4246, Training Loss : 0.61573, Training Acc : 0.722, Run Time : 0.47
INFO:root:2019-05-12 07:04:34, Epoch : 1, Step : 4247, Training Loss : 0.29349, Training Acc : 0.872, Run Time : 0.64
INFO:root:2019-05-12 07:04:46, Epoch : 1, Step : 4248, Training Loss : 0.45742, Training Acc : 0.772, Run Time : 11.06
INFO:root:2019-05-12 07:04:47, Epoch : 1, Step : 4249, Training Loss : 0.38772, Training Acc : 0.817, Run Time : 1.21
INFO:root:2019-05-12 07:04:57, Epoch : 1, Step : 4250, Training Loss : 0.48141, Training Acc : 0.739, Run Time : 10.47
INFO:root:2019-05-12 07:04:58, Epoch : 1, Step : 4251, Training Loss : 0.51887, Training Acc : 0.772, Run Time : 1.14
INFO:root:2019-05-12 07:04:59, Epoch : 1, Step : 4252, Training Loss : 0.37674, Training Acc : 0.811, Run Time : 0.61
INFO:root:2019-05-12 07:05:10, Epoch : 1, Step : 4253, Training Loss : 0.28697, Training Acc : 0.872, Run Time : 10.93
INFO:root:2019-05-12 07:05:11, Epoch : 1, Step : 4254, Training Loss : 0.34016, Training Acc : 0.856, Run Time : 0.96
INFO:root:2019-05-12 07:05:22, Epoch : 1, Step : 4255, Training Loss : 0.33205, Training Acc : 0.861, Run Time : 10.96
INFO:root:2019-05-12 07:05:23, Epoch : 1, Step : 4256, Training Loss : 0.23415, Training Acc : 0.922, Run Time : 1.52
INFO:root:2019-05-12 07:05:24, Epoch : 1, Step : 4257, Training Loss : 0.31319, Training Acc : 0.872, Run Time : 0.64
INFO:root:2019-05-12 07:05:25, Epoch : 1, Step : 4258, Training Loss : 0.25613, Training Acc : 0.906, Run Time : 0.95
INFO:root:2019-05-12 07:05:35, Epoch : 1, Step : 4259, Training Loss : 0.27956, Training Acc : 0.894, Run Time : 10.32
INFO:root:2019-05-12 07:05:36, Epoch : 1, Step : 4260, Training Loss : 0.33476, Training Acc : 0.800, Run Time : 0.51
INFO:root:2019-05-12 07:05:37, Epoch : 1, Step : 4261, Training Loss : 0.49497, Training Acc : 0.744, Run Time : 0.95
INFO:root:2019-05-12 07:05:47, Epoch : 1, Step : 4262, Training Loss : 0.32623, Training Acc : 0.889, Run Time : 10.79
INFO:root:2019-05-12 07:05:48, Epoch : 1, Step : 4263, Training Loss : 0.29550, Training Acc : 0.883, Run Time : 0.58
INFO:root:2019-05-12 07:05:49, Epoch : 1, Step : 4264, Training Loss : 0.33392, Training Acc : 0.850, Run Time : 0.68
INFO:root:2019-05-12 07:05:50, Epoch : 1, Step : 4265, Training Loss : 0.42154, Training Acc : 0.800, Run Time : 0.94
INFO:root:2019-05-12 07:06:00, Epoch : 1, Step : 4266, Training Loss : 0.29269, Training Acc : 0.889, Run Time : 10.49
INFO:root:2019-05-12 07:06:01, Epoch : 1, Step : 4267, Training Loss : 0.33201, Training Acc : 0.872, Run Time : 1.15
INFO:root:2019-05-12 07:06:14, Epoch : 1, Step : 4268, Training Loss : 0.31923, Training Acc : 0.867, Run Time : 12.64
INFO:root:2019-05-12 07:06:15, Epoch : 1, Step : 4269, Training Loss : 0.38726, Training Acc : 0.833, Run Time : 1.43
INFO:root:2019-05-12 07:06:25, Epoch : 1, Step : 4270, Training Loss : 0.26453, Training Acc : 0.900, Run Time : 10.01
INFO:root:2019-05-12 07:06:26, Epoch : 1, Step : 4271, Training Loss : 0.34146, Training Acc : 0.856, Run Time : 0.80
INFO:root:2019-05-12 07:06:27, Epoch : 1, Step : 4272, Training Loss : 0.28471, Training Acc : 0.861, Run Time : 0.61
INFO:root:2019-05-12 07:06:29, Epoch : 1, Step : 4273, Training Loss : 0.34653, Training Acc : 0.844, Run Time : 1.86
INFO:root:2019-05-12 07:06:37, Epoch : 1, Step : 4274, Training Loss : 0.44366, Training Acc : 0.800, Run Time : 7.87
INFO:root:2019-05-12 07:06:37, Epoch : 1, Step : 4275, Training Loss : 0.32884, Training Acc : 0.844, Run Time : 0.77
INFO:root:2019-05-12 07:06:39, Epoch : 1, Step : 4276, Training Loss : 0.66461, Training Acc : 0.811, Run Time : 1.58
INFO:root:2019-05-12 07:06:51, Epoch : 1, Step : 4277, Training Loss : 0.40195, Training Acc : 0.844, Run Time : 11.95
INFO:root:2019-05-12 07:06:52, Epoch : 1, Step : 4278, Training Loss : 0.25425, Training Acc : 0.900, Run Time : 0.93
INFO:root:2019-05-12 07:07:04, Epoch : 1, Step : 4279, Training Loss : 0.39714, Training Acc : 0.856, Run Time : 12.64
INFO:root:2019-05-12 07:07:06, Epoch : 1, Step : 4280, Training Loss : 0.28739, Training Acc : 0.878, Run Time : 1.36
INFO:root:2019-05-12 07:07:17, Epoch : 1, Step : 4281, Training Loss : 0.33048, Training Acc : 0.872, Run Time : 11.12
INFO:root:2019-05-12 07:07:19, Epoch : 1, Step : 4282, Training Loss : 0.53844, Training Acc : 0.739, Run Time : 1.73
INFO:root:2019-05-12 07:07:31, Epoch : 1, Step : 4283, Training Loss : 0.39282, Training Acc : 0.839, Run Time : 12.04
INFO:root:2019-05-12 07:07:32, Epoch : 1, Step : 4284, Training Loss : 0.40077, Training Acc : 0.861, Run Time : 0.93
INFO:root:2019-05-12 07:07:42, Epoch : 1, Step : 4285, Training Loss : 0.43088, Training Acc : 0.789, Run Time : 10.73
INFO:root:2019-05-12 07:07:43, Epoch : 1, Step : 4286, Training Loss : 0.40010, Training Acc : 0.811, Run Time : 0.59
INFO:root:2019-05-12 07:07:45, Epoch : 1, Step : 4287, Training Loss : 0.47313, Training Acc : 0.789, Run Time : 1.82
INFO:root:2019-05-12 07:07:56, Epoch : 1, Step : 4288, Training Loss : 0.64265, Training Acc : 0.706, Run Time : 11.47
INFO:root:2019-05-12 07:07:57, Epoch : 1, Step : 4289, Training Loss : 0.36883, Training Acc : 0.828, Run Time : 0.54
INFO:root:2019-05-12 07:07:59, Epoch : 1, Step : 4290, Training Loss : 0.30893, Training Acc : 0.883, Run Time : 1.95
INFO:root:2019-05-12 07:08:12, Epoch : 1, Step : 4291, Training Loss : 0.33651, Training Acc : 0.872, Run Time : 13.14
INFO:root:2019-05-12 07:08:22, Epoch : 1, Step : 4292, Training Loss : 0.36553, Training Acc : 0.789, Run Time : 10.01
INFO:root:2019-05-12 07:08:31, Epoch : 1, Step : 4293, Training Loss : 0.33370, Training Acc : 0.828, Run Time : 8.91
INFO:root:2019-05-12 07:08:32, Epoch : 1, Step : 4294, Training Loss : 0.28192, Training Acc : 0.883, Run Time : 0.78
INFO:root:2019-05-12 07:08:43, Epoch : 1, Step : 4295, Training Loss : 0.37543, Training Acc : 0.844, Run Time : 11.16
INFO:root:2019-05-12 07:08:43, Epoch : 1, Step : 4296, Training Loss : 0.35650, Training Acc : 0.872, Run Time : 0.55
INFO:root:2019-05-12 07:08:44, Epoch : 1, Step : 4297, Training Loss : 0.27552, Training Acc : 0.883, Run Time : 1.03
INFO:root:2019-05-12 07:08:46, Epoch : 1, Step : 4298, Training Loss : 0.29201, Training Acc : 0.883, Run Time : 1.36
INFO:root:2019-05-12 07:08:56, Epoch : 1, Step : 4299, Training Loss : 0.25068, Training Acc : 0.894, Run Time : 10.50
INFO:root:2019-05-12 07:08:57, Epoch : 1, Step : 4300, Training Loss : 0.43368, Training Acc : 0.806, Run Time : 0.71
INFO:root:2019-05-12 07:09:10, Epoch : 1, Step : 4301, Training Loss : 0.49923, Training Acc : 0.761, Run Time : 12.72
INFO:root:2019-05-12 07:09:10, Epoch : 1, Step : 4302, Training Loss : 0.65448, Training Acc : 0.717, Run Time : 0.51
INFO:root:2019-05-12 07:09:11, Epoch : 1, Step : 4303, Training Loss : 0.97031, Training Acc : 0.572, Run Time : 1.04
INFO:root:2019-05-12 07:09:22, Epoch : 1, Step : 4304, Training Loss : 0.81707, Training Acc : 0.633, Run Time : 10.72
INFO:root:2019-05-12 07:09:23, Epoch : 1, Step : 4305, Training Loss : 0.73973, Training Acc : 0.617, Run Time : 0.72
INFO:root:2019-05-12 07:09:36, Epoch : 1, Step : 4306, Training Loss : 0.60444, Training Acc : 0.694, Run Time : 13.05
INFO:root:2019-05-12 07:09:36, Epoch : 1, Step : 4307, Training Loss : 0.44688, Training Acc : 0.772, Run Time : 0.90
INFO:root:2019-05-12 07:09:37, Epoch : 1, Step : 4308, Training Loss : 0.40500, Training Acc : 0.817, Run Time : 0.59
INFO:root:2019-05-12 07:09:39, Epoch : 1, Step : 4309, Training Loss : 0.52703, Training Acc : 0.694, Run Time : 1.65
INFO:root:2019-05-12 07:09:51, Epoch : 1, Step : 4310, Training Loss : 0.61651, Training Acc : 0.661, Run Time : 12.17
INFO:root:2019-05-12 07:09:52, Epoch : 1, Step : 4311, Training Loss : 0.41640, Training Acc : 0.828, Run Time : 0.83
INFO:root:2019-05-12 07:09:54, Epoch : 1, Step : 4312, Training Loss : 0.46952, Training Acc : 0.744, Run Time : 1.83
INFO:root:2019-05-12 07:10:06, Epoch : 1, Step : 4313, Training Loss : 0.40266, Training Acc : 0.811, Run Time : 12.64
INFO:root:2019-05-12 07:10:07, Epoch : 1, Step : 4314, Training Loss : 0.33618, Training Acc : 0.911, Run Time : 0.70
INFO:root:2019-05-12 07:10:10, Epoch : 1, Step : 4315, Training Loss : 0.47701, Training Acc : 0.806, Run Time : 2.92
INFO:root:2019-05-12 07:10:25, Epoch : 1, Step : 4316, Training Loss : 0.50920, Training Acc : 0.756, Run Time : 14.91
INFO:root:2019-05-12 07:10:26, Epoch : 1, Step : 4317, Training Loss : 0.41530, Training Acc : 0.822, Run Time : 0.87
INFO:root:2019-05-12 07:10:35, Epoch : 1, Step : 4318, Training Loss : 0.42953, Training Acc : 0.828, Run Time : 9.57
INFO:root:2019-05-12 07:10:36, Epoch : 1, Step : 4319, Training Loss : 0.47099, Training Acc : 0.811, Run Time : 0.63
INFO:root:2019-05-12 07:10:36, Epoch : 1, Step : 4320, Training Loss : 0.49011, Training Acc : 0.744, Run Time : 0.60
INFO:root:2019-05-12 07:10:47, Epoch : 1, Step : 4321, Training Loss : 0.48627, Training Acc : 0.744, Run Time : 10.77
INFO:root:2019-05-12 07:10:49, Epoch : 1, Step : 4322, Training Loss : 0.52737, Training Acc : 0.711, Run Time : 1.60
INFO:root:2019-05-12 07:10:50, Epoch : 1, Step : 4323, Training Loss : 0.57752, Training Acc : 0.700, Run Time : 1.49
INFO:root:2019-05-12 07:10:59, Epoch : 1, Step : 4324, Training Loss : 0.61177, Training Acc : 0.678, Run Time : 8.62
INFO:root:2019-05-12 07:10:59, Epoch : 1, Step : 4325, Training Loss : 0.48772, Training Acc : 0.722, Run Time : 0.59
INFO:root:2019-05-12 07:11:08, Epoch : 1, Step : 4326, Training Loss : 0.50631, Training Acc : 0.706, Run Time : 8.54
INFO:root:2019-05-12 07:11:09, Epoch : 1, Step : 4327, Training Loss : 0.52781, Training Acc : 0.678, Run Time : 0.91
INFO:root:2019-05-12 07:11:10, Epoch : 1, Step : 4328, Training Loss : 0.42827, Training Acc : 0.767, Run Time : 0.62
INFO:root:2019-05-12 07:11:22, Epoch : 1, Step : 4329, Training Loss : 0.48663, Training Acc : 0.728, Run Time : 12.35
INFO:root:2019-05-12 07:11:23, Epoch : 1, Step : 4330, Training Loss : 0.40606, Training Acc : 0.806, Run Time : 1.01
INFO:root:2019-05-12 07:11:35, Epoch : 1, Step : 4331, Training Loss : 0.42599, Training Acc : 0.789, Run Time : 11.75
INFO:root:2019-05-12 07:11:35, Epoch : 1, Step : 4332, Training Loss : 0.44974, Training Acc : 0.800, Run Time : 0.65
INFO:root:2019-05-12 07:11:44, Epoch : 1, Step : 4333, Training Loss : 0.44300, Training Acc : 0.772, Run Time : 8.76
INFO:root:2019-05-12 07:11:45, Epoch : 1, Step : 4334, Training Loss : 0.47026, Training Acc : 0.778, Run Time : 1.02
INFO:root:2019-05-12 07:11:56, Epoch : 1, Step : 4335, Training Loss : 0.37069, Training Acc : 0.822, Run Time : 10.83
INFO:root:2019-05-12 07:11:57, Epoch : 1, Step : 4336, Training Loss : 0.47316, Training Acc : 0.761, Run Time : 0.73
INFO:root:2019-05-12 07:12:07, Epoch : 1, Step : 4337, Training Loss : 0.44459, Training Acc : 0.800, Run Time : 10.80
INFO:root:2019-05-12 07:12:08, Epoch : 1, Step : 4338, Training Loss : 0.48367, Training Acc : 0.728, Run Time : 0.71
INFO:root:2019-05-12 07:12:09, Epoch : 1, Step : 4339, Training Loss : 0.37447, Training Acc : 0.861, Run Time : 1.16
INFO:root:2019-05-12 07:12:24, Epoch : 1, Step : 4340, Training Loss : 0.38087, Training Acc : 0.833, Run Time : 14.80
INFO:root:2019-05-12 07:12:43, Epoch : 1, Step : 4341, Training Loss : 0.30837, Training Acc : 0.922, Run Time : 18.96
INFO:root:2019-05-12 07:12:49, Epoch : 1, Step : 4342, Training Loss : 0.42935, Training Acc : 0.817, Run Time : 5.92
INFO:root:2019-05-12 07:12:55, Epoch : 1, Step : 4343, Training Loss : 0.40504, Training Acc : 0.794, Run Time : 5.94
INFO:root:2019-05-12 07:12:56, Epoch : 1, Step : 4344, Training Loss : 0.43254, Training Acc : 0.811, Run Time : 0.98
INFO:root:2019-05-12 07:12:57, Epoch : 1, Step : 4345, Training Loss : 0.36160, Training Acc : 0.867, Run Time : 0.63
INFO:root:2019-05-12 07:13:13, Epoch : 1, Step : 4346, Training Loss : 0.47072, Training Acc : 0.772, Run Time : 16.79
INFO:root:2019-05-12 07:13:20, Epoch : 1, Step : 4347, Training Loss : 0.49989, Training Acc : 0.744, Run Time : 6.72
INFO:root:2019-05-12 07:13:21, Epoch : 1, Step : 4348, Training Loss : 0.57373, Training Acc : 0.706, Run Time : 0.76
INFO:root:2019-05-12 07:13:31, Epoch : 1, Step : 4349, Training Loss : 0.46060, Training Acc : 0.761, Run Time : 10.27
INFO:root:2019-05-12 07:13:33, Epoch : 1, Step : 4350, Training Loss : 0.39073, Training Acc : 0.867, Run Time : 2.03
INFO:root:2019-05-12 07:13:45, Epoch : 1, Step : 4351, Training Loss : 0.37258, Training Acc : 0.828, Run Time : 12.30
INFO:root:2019-05-12 07:13:58, Epoch : 1, Step : 4352, Training Loss : 0.40550, Training Acc : 0.800, Run Time : 13.05
INFO:root:2019-05-12 07:14:00, Epoch : 1, Step : 4353, Training Loss : 0.31163, Training Acc : 0.856, Run Time : 1.08
INFO:root:2019-05-12 07:14:10, Epoch : 1, Step : 4354, Training Loss : 0.33428, Training Acc : 0.911, Run Time : 10.30
INFO:root:2019-05-12 07:14:11, Epoch : 1, Step : 4355, Training Loss : 0.29112, Training Acc : 0.906, Run Time : 1.08
INFO:root:2019-05-12 07:14:12, Epoch : 1, Step : 4356, Training Loss : 0.34155, Training Acc : 0.889, Run Time : 0.60
INFO:root:2019-05-12 07:14:23, Epoch : 1, Step : 4357, Training Loss : 0.47385, Training Acc : 0.744, Run Time : 11.47
INFO:root:2019-05-12 07:14:24, Epoch : 1, Step : 4358, Training Loss : 0.38989, Training Acc : 0.794, Run Time : 1.21
INFO:root:2019-05-12 07:14:32, Epoch : 1, Step : 4359, Training Loss : 0.37076, Training Acc : 0.856, Run Time : 7.63
INFO:root:2019-05-12 07:14:33, Epoch : 1, Step : 4360, Training Loss : 0.42068, Training Acc : 0.806, Run Time : 0.90
INFO:root:2019-05-12 07:14:38, Epoch : 1, Step : 4361, Training Loss : 0.44474, Training Acc : 0.833, Run Time : 5.73
INFO:root:2019-05-12 07:14:42, Epoch : 1, Step : 4362, Training Loss : 0.27942, Training Acc : 0.911, Run Time : 3.89
INFO:root:2019-05-12 07:14:43, Epoch : 1, Step : 4363, Training Loss : 0.31094, Training Acc : 0.861, Run Time : 0.47
INFO:root:2019-05-12 07:14:44, Epoch : 1, Step : 4364, Training Loss : 0.35965, Training Acc : 0.861, Run Time : 0.67
INFO:root:2019-05-12 07:14:58, Epoch : 1, Step : 4365, Training Loss : 0.35552, Training Acc : 0.828, Run Time : 14.26
INFO:root:2019-05-12 07:14:59, Epoch : 1, Step : 4366, Training Loss : 0.25868, Training Acc : 0.894, Run Time : 1.68
INFO:root:2019-05-12 07:15:15, Epoch : 1, Step : 4367, Training Loss : 0.27670, Training Acc : 0.883, Run Time : 15.33
INFO:root:2019-05-12 07:15:17, Epoch : 1, Step : 4368, Training Loss : 0.25433, Training Acc : 0.889, Run Time : 2.37
INFO:root:2019-05-12 07:15:42, Epoch : 1, Step : 4369, Training Loss : 0.26413, Training Acc : 0.878, Run Time : 24.65
INFO:root:2019-05-12 07:15:47, Epoch : 1, Step : 4370, Training Loss : 0.20942, Training Acc : 0.922, Run Time : 4.79
INFO:root:2019-05-12 07:15:47, Epoch : 1, Step : 4371, Training Loss : 0.23884, Training Acc : 0.911, Run Time : 0.59
INFO:root:2019-05-12 07:15:48, Epoch : 1, Step : 4372, Training Loss : 0.23867, Training Acc : 0.922, Run Time : 0.61
INFO:root:2019-05-12 07:16:02, Epoch : 1, Step : 4373, Training Loss : 0.19417, Training Acc : 0.944, Run Time : 13.99
INFO:root:2019-05-12 07:16:02, Epoch : 1, Step : 4374, Training Loss : 0.24873, Training Acc : 0.894, Run Time : 0.62
INFO:root:2019-05-12 07:16:04, Epoch : 1, Step : 4375, Training Loss : 0.26687, Training Acc : 0.856, Run Time : 1.80
INFO:root:2019-05-12 07:16:18, Epoch : 1, Step : 4376, Training Loss : 0.29698, Training Acc : 0.839, Run Time : 13.62
INFO:root:2019-05-12 07:16:31, Epoch : 1, Step : 4377, Training Loss : 0.29578, Training Acc : 0.839, Run Time : 12.97
INFO:root:2019-05-12 07:16:31, Epoch : 1, Step : 4378, Training Loss : 0.28495, Training Acc : 0.861, Run Time : 0.71
INFO:root:2019-05-12 07:16:33, Epoch : 1, Step : 4379, Training Loss : 0.25952, Training Acc : 0.894, Run Time : 1.22
INFO:root:2019-05-12 07:16:43, Epoch : 1, Step : 4380, Training Loss : 0.27051, Training Acc : 0.850, Run Time : 10.51
INFO:root:2019-05-12 07:16:44, Epoch : 1, Step : 4381, Training Loss : 0.31055, Training Acc : 0.822, Run Time : 0.80
INFO:root:2019-05-12 07:16:55, Epoch : 1, Step : 4382, Training Loss : 0.22421, Training Acc : 0.889, Run Time : 11.01
INFO:root:2019-05-12 07:16:56, Epoch : 1, Step : 4383, Training Loss : 0.22951, Training Acc : 0.917, Run Time : 0.54
INFO:root:2019-05-12 07:16:56, Epoch : 1, Step : 4384, Training Loss : 0.44796, Training Acc : 0.800, Run Time : 0.78
INFO:root:2019-05-12 07:16:57, Epoch : 1, Step : 4385, Training Loss : 0.49757, Training Acc : 0.778, Run Time : 0.58
INFO:root:2019-05-12 07:16:58, Epoch : 1, Step : 4386, Training Loss : 0.43868, Training Acc : 0.794, Run Time : 0.71
INFO:root:2019-05-12 07:17:13, Epoch : 1, Step : 4387, Training Loss : 0.39420, Training Acc : 0.778, Run Time : 15.14
INFO:root:2019-05-12 07:17:13, Epoch : 1, Step : 4388, Training Loss : 0.44296, Training Acc : 0.778, Run Time : 0.47
INFO:root:2019-05-12 07:17:14, Epoch : 1, Step : 4389, Training Loss : 0.29414, Training Acc : 0.839, Run Time : 0.62
INFO:root:2019-05-12 07:17:15, Epoch : 1, Step : 4390, Training Loss : 0.32120, Training Acc : 0.822, Run Time : 0.90
INFO:root:2019-05-12 07:17:24, Epoch : 1, Step : 4391, Training Loss : 0.38172, Training Acc : 0.806, Run Time : 9.20
INFO:root:2019-05-12 07:17:25, Epoch : 1, Step : 4392, Training Loss : 0.49521, Training Acc : 0.767, Run Time : 1.45
INFO:root:2019-05-12 07:17:26, Epoch : 1, Step : 4393, Training Loss : 0.43600, Training Acc : 0.783, Run Time : 0.60
INFO:root:2019-05-12 07:17:33, Epoch : 1, Step : 4394, Training Loss : 0.30120, Training Acc : 0.844, Run Time : 7.23
INFO:root:2019-05-12 07:17:34, Epoch : 1, Step : 4395, Training Loss : 0.32434, Training Acc : 0.844, Run Time : 0.88
INFO:root:2019-05-12 07:17:46, Epoch : 1, Step : 4396, Training Loss : 0.27918, Training Acc : 0.878, Run Time : 12.23
INFO:root:2019-05-12 07:17:47, Epoch : 1, Step : 4397, Training Loss : 0.27995, Training Acc : 0.872, Run Time : 0.63
INFO:root:2019-05-12 07:17:48, Epoch : 1, Step : 4398, Training Loss : 0.19528, Training Acc : 0.917, Run Time : 0.68
INFO:root:2019-05-12 07:18:01, Epoch : 1, Step : 4399, Training Loss : 0.29232, Training Acc : 0.856, Run Time : 13.02
INFO:root:2019-05-12 07:18:01, Epoch : 1, Step : 4400, Training Loss : 0.23108, Training Acc : 0.894, Run Time : 0.72
INFO:root:2019-05-12 07:18:16, Epoch : 1, Step : 4401, Training Loss : 0.35444, Training Acc : 0.811, Run Time : 14.24
INFO:root:2019-05-12 07:18:17, Epoch : 1, Step : 4402, Training Loss : 0.37025, Training Acc : 0.817, Run Time : 1.07
INFO:root:2019-05-12 07:18:30, Epoch : 1, Step : 4403, Training Loss : 0.30384, Training Acc : 0.828, Run Time : 13.68
INFO:root:2019-05-12 07:18:32, Epoch : 1, Step : 4404, Training Loss : 0.33816, Training Acc : 0.833, Run Time : 1.95
INFO:root:2019-05-12 07:18:43, Epoch : 1, Step : 4405, Training Loss : 0.28091, Training Acc : 0.839, Run Time : 10.18
INFO:root:2019-05-12 07:18:55, Epoch : 1, Step : 4406, Training Loss : 0.28332, Training Acc : 0.883, Run Time : 12.41
INFO:root:2019-05-12 07:18:56, Epoch : 1, Step : 4407, Training Loss : 0.25336, Training Acc : 0.894, Run Time : 0.92
INFO:root:2019-05-12 07:18:57, Epoch : 1, Step : 4408, Training Loss : 0.22095, Training Acc : 0.911, Run Time : 0.82
INFO:root:2019-05-12 07:19:08, Epoch : 1, Step : 4409, Training Loss : 0.25516, Training Acc : 0.861, Run Time : 11.77
INFO:root:2019-05-12 07:19:19, Epoch : 1, Step : 4410, Training Loss : 0.28289, Training Acc : 0.856, Run Time : 10.14
INFO:root:2019-05-12 07:19:21, Epoch : 1, Step : 4411, Training Loss : 0.25732, Training Acc : 0.872, Run Time : 2.11
INFO:root:2019-05-12 07:19:29, Epoch : 1, Step : 4412, Training Loss : 0.33019, Training Acc : 0.822, Run Time : 8.17
INFO:root:2019-05-12 07:19:30, Epoch : 1, Step : 4413, Training Loss : 0.31479, Training Acc : 0.828, Run Time : 1.04
INFO:root:2019-05-12 07:19:42, Epoch : 1, Step : 4414, Training Loss : 0.29162, Training Acc : 0.839, Run Time : 12.24
INFO:root:2019-05-12 07:19:44, Epoch : 1, Step : 4415, Training Loss : 0.30654, Training Acc : 0.850, Run Time : 2.04
INFO:root:2019-05-12 07:19:45, Epoch : 1, Step : 4416, Training Loss : 0.28998, Training Acc : 0.867, Run Time : 0.59
INFO:root:2019-05-12 07:19:54, Epoch : 1, Step : 4417, Training Loss : 0.35118, Training Acc : 0.850, Run Time : 9.14
INFO:root:2019-05-12 07:19:55, Epoch : 1, Step : 4418, Training Loss : 0.24408, Training Acc : 0.883, Run Time : 0.73
INFO:root:2019-05-12 07:19:55, Epoch : 1, Step : 4419, Training Loss : 0.22286, Training Acc : 0.900, Run Time : 0.64
INFO:root:2019-05-12 07:20:09, Epoch : 1, Step : 4420, Training Loss : 0.23351, Training Acc : 0.883, Run Time : 13.67
INFO:root:2019-05-12 07:20:10, Epoch : 1, Step : 4421, Training Loss : 0.23820, Training Acc : 0.911, Run Time : 0.84
INFO:root:2019-05-12 07:20:11, Epoch : 1, Step : 4422, Training Loss : 0.23676, Training Acc : 0.889, Run Time : 1.11
INFO:root:2019-05-12 07:20:21, Epoch : 1, Step : 4423, Training Loss : 0.25360, Training Acc : 0.878, Run Time : 10.37
INFO:root:2019-05-12 07:20:22, Epoch : 1, Step : 4424, Training Loss : 0.29458, Training Acc : 0.839, Run Time : 0.77
INFO:root:2019-05-12 07:20:24, Epoch : 1, Step : 4425, Training Loss : 0.27557, Training Acc : 0.844, Run Time : 1.94
INFO:root:2019-05-12 07:20:41, Epoch : 1, Step : 4426, Training Loss : 0.16666, Training Acc : 0.944, Run Time : 17.41
INFO:root:2019-05-12 07:20:58, Epoch : 1, Step : 4427, Training Loss : 0.20299, Training Acc : 0.906, Run Time : 17.03
INFO:root:2019-05-12 07:21:13, Epoch : 1, Step : 4428, Training Loss : 0.19528, Training Acc : 0.889, Run Time : 14.11
INFO:root:2019-05-12 07:21:20, Epoch : 1, Step : 4429, Training Loss : 0.16593, Training Acc : 0.933, Run Time : 7.65
INFO:root:2019-05-12 07:21:34, Epoch : 1, Step : 4430, Training Loss : 0.17136, Training Acc : 0.967, Run Time : 14.17
INFO:root:2019-05-12 07:21:41, Epoch : 1, Step : 4431, Training Loss : 0.12648, Training Acc : 0.967, Run Time : 6.59
INFO:root:2019-05-12 07:21:42, Epoch : 1, Step : 4432, Training Loss : 0.13860, Training Acc : 0.967, Run Time : 1.22
INFO:root:2019-05-12 07:21:43, Epoch : 1, Step : 4433, Training Loss : 0.15059, Training Acc : 0.956, Run Time : 0.60
INFO:root:2019-05-12 07:21:43, Epoch : 1, Step : 4434, Training Loss : 0.18451, Training Acc : 0.906, Run Time : 0.62
INFO:root:2019-05-12 07:21:44, Epoch : 1, Step : 4435, Training Loss : 0.20560, Training Acc : 0.900, Run Time : 0.63
INFO:root:2019-05-12 07:21:52, Epoch : 1, Step : 4436, Training Loss : 0.21975, Training Acc : 0.894, Run Time : 8.05
INFO:root:2019-05-12 07:21:53, Epoch : 1, Step : 4437, Training Loss : 0.20029, Training Acc : 0.911, Run Time : 1.00
INFO:root:2019-05-12 07:21:54, Epoch : 1, Step : 4438, Training Loss : 0.22593, Training Acc : 0.889, Run Time : 0.58
INFO:root:2019-05-12 07:21:54, Epoch : 1, Step : 4439, Training Loss : 0.18609, Training Acc : 0.906, Run Time : 0.64
INFO:root:2019-05-12 07:22:06, Epoch : 1, Step : 4440, Training Loss : 0.22170, Training Acc : 0.906, Run Time : 11.67
INFO:root:2019-05-12 07:22:06, Epoch : 1, Step : 4441, Training Loss : 0.25985, Training Acc : 0.872, Run Time : 0.45
INFO:root:2019-05-12 07:22:08, Epoch : 1, Step : 4442, Training Loss : 0.28216, Training Acc : 0.861, Run Time : 1.17
INFO:root:2019-05-12 07:22:20, Epoch : 1, Step : 4443, Training Loss : 0.25719, Training Acc : 0.850, Run Time : 12.55
INFO:root:2019-05-12 07:22:21, Epoch : 1, Step : 4444, Training Loss : 0.25653, Training Acc : 0.867, Run Time : 0.77
INFO:root:2019-05-12 07:22:33, Epoch : 1, Step : 4445, Training Loss : 0.15064, Training Acc : 0.944, Run Time : 11.81
INFO:root:2019-05-12 07:22:34, Epoch : 1, Step : 4446, Training Loss : 0.17418, Training Acc : 0.933, Run Time : 1.02
INFO:root:2019-05-12 07:22:34, Epoch : 1, Step : 4447, Training Loss : 0.15911, Training Acc : 0.939, Run Time : 0.59
INFO:root:2019-05-12 07:22:35, Epoch : 1, Step : 4448, Training Loss : 0.21918, Training Acc : 0.917, Run Time : 0.87
INFO:root:2019-05-12 07:22:46, Epoch : 1, Step : 4449, Training Loss : 0.27440, Training Acc : 0.889, Run Time : 10.48
INFO:root:2019-05-12 07:22:47, Epoch : 1, Step : 4450, Training Loss : 0.26800, Training Acc : 0.861, Run Time : 1.49
INFO:root:2019-05-12 07:22:57, Epoch : 1, Step : 4451, Training Loss : 0.33078, Training Acc : 0.844, Run Time : 9.97
INFO:root:2019-05-12 07:22:58, Epoch : 1, Step : 4452, Training Loss : 0.30097, Training Acc : 0.839, Run Time : 1.09
INFO:root:2019-05-12 07:23:05, Epoch : 1, Step : 4453, Training Loss : 0.29514, Training Acc : 0.844, Run Time : 6.58
INFO:root:2019-05-12 07:23:05, Epoch : 1, Step : 4454, Training Loss : 0.24001, Training Acc : 0.889, Run Time : 0.55
INFO:root:2019-05-12 07:23:07, Epoch : 1, Step : 4455, Training Loss : 0.31566, Training Acc : 0.878, Run Time : 1.65
INFO:root:2019-05-12 07:23:18, Epoch : 1, Step : 4456, Training Loss : 0.16678, Training Acc : 0.950, Run Time : 11.05
INFO:root:2019-05-12 07:23:18, Epoch : 1, Step : 4457, Training Loss : 0.24496, Training Acc : 0.889, Run Time : 0.43
INFO:root:2019-05-12 07:23:19, Epoch : 1, Step : 4458, Training Loss : 0.23767, Training Acc : 0.900, Run Time : 0.80
INFO:root:2019-05-12 07:23:32, Epoch : 1, Step : 4459, Training Loss : 0.22156, Training Acc : 0.900, Run Time : 12.92
INFO:root:2019-05-12 07:23:34, Epoch : 1, Step : 4460, Training Loss : 0.19602, Training Acc : 0.911, Run Time : 1.61
INFO:root:2019-05-12 07:23:43, Epoch : 1, Step : 4461, Training Loss : 0.20030, Training Acc : 0.894, Run Time : 9.09
INFO:root:2019-05-12 07:23:44, Epoch : 1, Step : 4462, Training Loss : 0.25679, Training Acc : 0.889, Run Time : 0.70
INFO:root:2019-05-12 07:23:49, Epoch : 1, Step : 4463, Training Loss : 0.19560, Training Acc : 0.911, Run Time : 5.09
INFO:root:2019-05-12 07:24:03, Epoch : 1, Step : 4464, Training Loss : 0.29244, Training Acc : 0.878, Run Time : 14.58
INFO:root:2019-05-12 07:24:11, Epoch : 1, Step : 4465, Training Loss : 0.27756, Training Acc : 0.883, Run Time : 7.55
INFO:root:2019-05-12 07:24:22, Epoch : 1, Step : 4466, Training Loss : 0.22634, Training Acc : 0.883, Run Time : 11.56
INFO:root:2019-05-12 07:24:23, Epoch : 1, Step : 4467, Training Loss : 0.23601, Training Acc : 0.906, Run Time : 0.84
INFO:root:2019-05-12 07:24:25, Epoch : 1, Step : 4468, Training Loss : 0.36648, Training Acc : 0.828, Run Time : 1.40
INFO:root:2019-05-12 07:24:25, Epoch : 1, Step : 4469, Training Loss : 0.17172, Training Acc : 0.922, Run Time : 0.61
INFO:root:2019-05-12 07:24:26, Epoch : 1, Step : 4470, Training Loss : 0.21326, Training Acc : 0.922, Run Time : 1.04
INFO:root:2019-05-12 07:24:34, Epoch : 1, Step : 4471, Training Loss : 0.22108, Training Acc : 0.878, Run Time : 7.90
INFO:root:2019-05-12 07:24:35, Epoch : 1, Step : 4472, Training Loss : 0.24001, Training Acc : 0.906, Run Time : 0.69
INFO:root:2019-05-12 07:24:42, Epoch : 1, Step : 4473, Training Loss : 0.45758, Training Acc : 0.817, Run Time : 7.49
INFO:root:2019-05-12 07:24:46, Epoch : 1, Step : 4474, Training Loss : 0.22688, Training Acc : 0.906, Run Time : 3.17
INFO:root:2019-05-12 07:24:46, Epoch : 1, Step : 4475, Training Loss : 0.24089, Training Acc : 0.900, Run Time : 0.45
INFO:root:2019-05-12 07:24:57, Epoch : 1, Step : 4476, Training Loss : 0.32322, Training Acc : 0.839, Run Time : 10.78
INFO:root:2019-05-12 07:24:58, Epoch : 1, Step : 4477, Training Loss : 0.26832, Training Acc : 0.872, Run Time : 1.12
INFO:root:2019-05-12 07:25:10, Epoch : 1, Step : 4478, Training Loss : 0.10932, Training Acc : 0.978, Run Time : 12.11
INFO:root:2019-05-12 07:25:11, Epoch : 1, Step : 4479, Training Loss : 0.15528, Training Acc : 0.956, Run Time : 0.58
INFO:root:2019-05-12 07:25:12, Epoch : 1, Step : 4480, Training Loss : 0.25170, Training Acc : 0.883, Run Time : 1.70
INFO:root:2019-05-12 07:25:24, Epoch : 1, Step : 4481, Training Loss : 0.11806, Training Acc : 0.967, Run Time : 11.64
INFO:root:2019-05-12 07:25:24, Epoch : 1, Step : 4482, Training Loss : 0.15463, Training Acc : 0.950, Run Time : 0.48
INFO:root:2019-05-12 07:25:26, Epoch : 1, Step : 4483, Training Loss : 0.15972, Training Acc : 0.950, Run Time : 2.14
INFO:root:2019-05-12 07:25:36, Epoch : 1, Step : 4484, Training Loss : 0.17109, Training Acc : 0.933, Run Time : 9.75
INFO:root:2019-05-12 07:25:37, Epoch : 1, Step : 4485, Training Loss : 0.33765, Training Acc : 0.872, Run Time : 0.60
INFO:root:2019-05-12 07:25:38, Epoch : 1, Step : 4486, Training Loss : 0.21078, Training Acc : 0.911, Run Time : 0.99
INFO:root:2019-05-12 07:25:49, Epoch : 1, Step : 4487, Training Loss : 0.24423, Training Acc : 0.900, Run Time : 11.13
INFO:root:2019-05-12 07:25:50, Epoch : 1, Step : 4488, Training Loss : 0.16974, Training Acc : 0.922, Run Time : 0.81
INFO:root:2019-05-12 07:25:51, Epoch : 1, Step : 4489, Training Loss : 0.21022, Training Acc : 0.922, Run Time : 1.65
INFO:root:2019-05-12 07:26:01, Epoch : 1, Step : 4490, Training Loss : 0.45580, Training Acc : 0.811, Run Time : 9.89
INFO:root:2019-05-12 07:26:02, Epoch : 1, Step : 4491, Training Loss : 0.27234, Training Acc : 0.867, Run Time : 0.73
INFO:root:2019-05-12 07:26:04, Epoch : 1, Step : 4492, Training Loss : 0.26237, Training Acc : 0.883, Run Time : 1.91
INFO:root:2019-05-12 07:26:15, Epoch : 1, Step : 4493, Training Loss : 0.34642, Training Acc : 0.850, Run Time : 10.91
INFO:root:2019-05-12 07:26:15, Epoch : 1, Step : 4494, Training Loss : 0.21043, Training Acc : 0.917, Run Time : 0.47
INFO:root:2019-05-12 07:26:17, Epoch : 1, Step : 4495, Training Loss : 0.19481, Training Acc : 0.906, Run Time : 1.82
INFO:root:2019-05-12 07:26:30, Epoch : 1, Step : 4496, Training Loss : 0.17478, Training Acc : 0.950, Run Time : 12.50
INFO:root:2019-05-12 07:26:30, Epoch : 1, Step : 4497, Training Loss : 0.23143, Training Acc : 0.906, Run Time : 0.54
INFO:root:2019-05-12 07:26:31, Epoch : 1, Step : 4498, Training Loss : 0.26143, Training Acc : 0.883, Run Time : 0.69
INFO:root:2019-05-12 07:26:40, Epoch : 1, Step : 4499, Training Loss : 0.16120, Training Acc : 0.944, Run Time : 9.40
INFO:root:2019-05-12 07:26:41, Epoch : 1, Step : 4500, Training Loss : 0.26428, Training Acc : 0.906, Run Time : 0.57
INFO:root:2019-05-12 07:26:56, Epoch : 1, Step : 4501, Training Loss : 0.22643, Training Acc : 0.894, Run Time : 14.95
INFO:root:2019-05-12 07:26:57, Epoch : 1, Step : 4502, Training Loss : 0.19908, Training Acc : 0.933, Run Time : 1.45
INFO:root:2019-05-12 07:27:12, Epoch : 1, Step : 4503, Training Loss : 0.18293, Training Acc : 0.944, Run Time : 14.51
INFO:root:2019-05-12 07:27:14, Epoch : 1, Step : 4504, Training Loss : 0.28369, Training Acc : 0.883, Run Time : 1.90
INFO:root:2019-05-12 07:27:14, Epoch : 1, Step : 4505, Training Loss : 0.24379, Training Acc : 0.878, Run Time : 0.67
INFO:root:2019-05-12 07:27:23, Epoch : 1, Step : 4506, Training Loss : 0.24510, Training Acc : 0.889, Run Time : 8.67
INFO:root:2019-05-12 07:27:24, Epoch : 1, Step : 4507, Training Loss : 0.27967, Training Acc : 0.867, Run Time : 0.62
INFO:root:2019-05-12 07:27:37, Epoch : 1, Step : 4508, Training Loss : 0.27901, Training Acc : 0.867, Run Time : 13.04
INFO:root:2019-05-12 07:27:37, Epoch : 1, Step : 4509, Training Loss : 0.18856, Training Acc : 0.933, Run Time : 0.78
INFO:root:2019-05-12 07:27:39, Epoch : 1, Step : 4510, Training Loss : 0.19205, Training Acc : 0.950, Run Time : 1.12
INFO:root:2019-05-12 07:27:52, Epoch : 1, Step : 4511, Training Loss : 0.18037, Training Acc : 0.922, Run Time : 13.09
INFO:root:2019-05-12 07:27:53, Epoch : 1, Step : 4512, Training Loss : 0.18598, Training Acc : 0.944, Run Time : 1.14
INFO:root:2019-05-12 07:28:06, Epoch : 1, Step : 4513, Training Loss : 0.23972, Training Acc : 0.900, Run Time : 12.90
INFO:root:2019-05-12 07:28:08, Epoch : 1, Step : 4514, Training Loss : 0.20090, Training Acc : 0.944, Run Time : 2.45
INFO:root:2019-05-12 07:28:09, Epoch : 1, Step : 4515, Training Loss : 0.13227, Training Acc : 0.956, Run Time : 0.81
INFO:root:2019-05-12 07:28:21, Epoch : 1, Step : 4516, Training Loss : 0.17788, Training Acc : 0.917, Run Time : 11.68
INFO:root:2019-05-12 07:28:22, Epoch : 1, Step : 4517, Training Loss : 0.19577, Training Acc : 0.922, Run Time : 1.30
INFO:root:2019-05-12 07:28:23, Epoch : 1, Step : 4518, Training Loss : 0.18417, Training Acc : 0.911, Run Time : 0.60
INFO:root:2019-05-12 07:28:34, Epoch : 1, Step : 4519, Training Loss : 0.17102, Training Acc : 0.939, Run Time : 11.65
INFO:root:2019-05-12 07:28:35, Epoch : 1, Step : 4520, Training Loss : 0.24212, Training Acc : 0.922, Run Time : 0.66
INFO:root:2019-05-12 07:28:36, Epoch : 1, Step : 4521, Training Loss : 0.25169, Training Acc : 0.906, Run Time : 0.67
INFO:root:2019-05-12 07:28:48, Epoch : 1, Step : 4522, Training Loss : 0.22337, Training Acc : 0.911, Run Time : 12.25
INFO:root:2019-05-12 07:28:48, Epoch : 1, Step : 4523, Training Loss : 0.22476, Training Acc : 0.894, Run Time : 0.69
INFO:root:2019-05-12 07:28:49, Epoch : 1, Step : 4524, Training Loss : 0.18542, Training Acc : 0.900, Run Time : 0.54
INFO:root:2019-05-12 07:28:49, Epoch : 1, Step : 4525, Training Loss : 0.16647, Training Acc : 0.933, Run Time : 0.38
INFO:root:2019-05-12 07:28:51, Epoch : 1, Step : 4526, Training Loss : 0.17671, Training Acc : 0.950, Run Time : 1.64
INFO:root:2019-05-12 07:29:07, Epoch : 1, Step : 4527, Training Loss : 0.20112, Training Acc : 0.917, Run Time : 15.74
INFO:root:2019-05-12 07:29:09, Epoch : 1, Step : 4528, Training Loss : 0.28965, Training Acc : 0.906, Run Time : 2.40
INFO:root:2019-05-12 07:29:21, Epoch : 1, Step : 4529, Training Loss : 0.30010, Training Acc : 0.850, Run Time : 12.10
INFO:root:2019-05-12 07:29:22, Epoch : 1, Step : 4530, Training Loss : 0.18223, Training Acc : 0.906, Run Time : 0.48
INFO:root:2019-05-12 07:29:22, Epoch : 1, Step : 4531, Training Loss : 0.28277, Training Acc : 0.867, Run Time : 0.47
INFO:root:2019-05-12 07:29:24, Epoch : 1, Step : 4532, Training Loss : 0.20309, Training Acc : 0.922, Run Time : 1.55
INFO:root:2019-05-12 07:29:35, Epoch : 1, Step : 4533, Training Loss : 0.24827, Training Acc : 0.889, Run Time : 11.06
INFO:root:2019-05-12 07:29:36, Epoch : 1, Step : 4534, Training Loss : 0.36374, Training Acc : 0.811, Run Time : 0.80
INFO:root:2019-05-12 07:29:49, Epoch : 1, Step : 4535, Training Loss : 0.18809, Training Acc : 0.928, Run Time : 13.20
INFO:root:2019-05-12 07:29:51, Epoch : 1, Step : 4536, Training Loss : 0.24980, Training Acc : 0.878, Run Time : 2.51
INFO:root:2019-05-12 07:30:05, Epoch : 1, Step : 4537, Training Loss : 0.27126, Training Acc : 0.894, Run Time : 13.73
INFO:root:2019-05-12 07:30:19, Epoch : 1, Step : 4538, Training Loss : 0.24119, Training Acc : 0.872, Run Time : 13.80
INFO:root:2019-05-12 07:30:20, Epoch : 1, Step : 4539, Training Loss : 0.25370, Training Acc : 0.906, Run Time : 0.99
INFO:root:2019-05-12 07:30:21, Epoch : 1, Step : 4540, Training Loss : 0.27893, Training Acc : 0.894, Run Time : 1.02
INFO:root:2019-05-12 07:30:33, Epoch : 1, Step : 4541, Training Loss : 0.47804, Training Acc : 0.778, Run Time : 12.51
INFO:root:2019-05-12 07:30:34, Epoch : 1, Step : 4542, Training Loss : 0.22051, Training Acc : 0.922, Run Time : 0.47
INFO:root:2019-05-12 07:30:35, Epoch : 1, Step : 4543, Training Loss : 0.29423, Training Acc : 0.883, Run Time : 0.82
INFO:root:2019-05-12 07:30:35, Epoch : 1, Step : 4544, Training Loss : 0.45315, Training Acc : 0.783, Run Time : 0.69
INFO:root:2019-05-12 07:30:36, Epoch : 1, Step : 4545, Training Loss : 0.42593, Training Acc : 0.817, Run Time : 0.61
INFO:root:2019-05-12 07:30:48, Epoch : 1, Step : 4546, Training Loss : 0.46274, Training Acc : 0.828, Run Time : 11.86
INFO:root:2019-05-12 07:30:49, Epoch : 1, Step : 4547, Training Loss : 0.24045, Training Acc : 0.906, Run Time : 0.68
INFO:root:2019-05-12 07:30:50, Epoch : 1, Step : 4548, Training Loss : 0.25348, Training Acc : 0.900, Run Time : 1.52
INFO:root:2019-05-12 07:31:02, Epoch : 1, Step : 4549, Training Loss : 0.29348, Training Acc : 0.883, Run Time : 11.52
INFO:root:2019-05-12 07:31:03, Epoch : 1, Step : 4550, Training Loss : 0.31776, Training Acc : 0.872, Run Time : 1.23
INFO:root:2019-05-12 07:31:03, Epoch : 1, Step : 4551, Training Loss : 0.37221, Training Acc : 0.839, Run Time : 0.63
INFO:root:2019-05-12 07:31:15, Epoch : 1, Step : 4552, Training Loss : 0.24790, Training Acc : 0.894, Run Time : 12.04
INFO:root:2019-05-12 07:31:16, Epoch : 1, Step : 4553, Training Loss : 0.30998, Training Acc : 0.839, Run Time : 0.63
INFO:root:2019-05-12 07:31:17, Epoch : 1, Step : 4554, Training Loss : 0.37155, Training Acc : 0.839, Run Time : 0.59
INFO:root:2019-05-12 07:31:32, Epoch : 1, Step : 4555, Training Loss : 0.45860, Training Acc : 0.817, Run Time : 15.75
INFO:root:2019-05-12 07:31:33, Epoch : 1, Step : 4556, Training Loss : 0.29739, Training Acc : 0.856, Run Time : 1.02
INFO:root:2019-05-12 07:31:34, Epoch : 1, Step : 4557, Training Loss : 0.27784, Training Acc : 0.878, Run Time : 1.05
INFO:root:2019-05-12 07:31:37, Epoch : 1, Step : 4558, Training Loss : 0.39647, Training Acc : 0.856, Run Time : 2.76
INFO:root:2019-05-12 07:31:47, Epoch : 1, Step : 4559, Training Loss : 0.23013, Training Acc : 0.878, Run Time : 9.47
INFO:root:2019-05-12 07:31:48, Epoch : 1, Step : 4560, Training Loss : 0.33732, Training Acc : 0.828, Run Time : 0.87
INFO:root:2019-05-12 07:31:55, Epoch : 1, Step : 4561, Training Loss : 0.29451, Training Acc : 0.861, Run Time : 7.70
INFO:root:2019-05-12 07:31:58, Epoch : 1, Step : 4562, Training Loss : 0.39639, Training Acc : 0.817, Run Time : 2.89
INFO:root:2019-05-12 07:31:59, Epoch : 1, Step : 4563, Training Loss : 0.32747, Training Acc : 0.844, Run Time : 0.52
INFO:root:2019-05-12 07:32:08, Epoch : 1, Step : 4564, Training Loss : 0.29008, Training Acc : 0.839, Run Time : 9.02
INFO:root:2019-05-12 07:32:12, Epoch : 1, Step : 4565, Training Loss : 0.21724, Training Acc : 0.906, Run Time : 4.21
INFO:root:2019-05-12 07:32:14, Epoch : 1, Step : 4566, Training Loss : 0.20169, Training Acc : 0.917, Run Time : 2.52
INFO:root:2019-05-12 07:32:15, Epoch : 1, Step : 4567, Training Loss : 0.21881, Training Acc : 0.911, Run Time : 0.58
INFO:root:2019-05-12 07:32:23, Epoch : 1, Step : 4568, Training Loss : 0.25404, Training Acc : 0.883, Run Time : 8.01
INFO:root:2019-05-12 07:32:33, Epoch : 1, Step : 4569, Training Loss : 0.15792, Training Acc : 0.978, Run Time : 10.15
INFO:root:2019-05-12 07:32:35, Epoch : 1, Step : 4570, Training Loss : 0.16402, Training Acc : 0.944, Run Time : 1.92
INFO:root:2019-05-12 07:32:36, Epoch : 1, Step : 4571, Training Loss : 0.18231, Training Acc : 0.906, Run Time : 0.62
INFO:root:2019-05-12 07:32:36, Epoch : 1, Step : 4572, Training Loss : 0.13691, Training Acc : 0.956, Run Time : 0.62
INFO:root:2019-05-12 07:32:38, Epoch : 1, Step : 4573, Training Loss : 0.15376, Training Acc : 0.928, Run Time : 1.53
INFO:root:2019-05-12 07:32:51, Epoch : 1, Step : 4574, Training Loss : 0.14267, Training Acc : 0.944, Run Time : 12.91
INFO:root:2019-05-12 07:32:52, Epoch : 1, Step : 4575, Training Loss : 0.19412, Training Acc : 0.922, Run Time : 1.51
INFO:root:2019-05-12 07:32:53, Epoch : 1, Step : 4576, Training Loss : 0.17520, Training Acc : 0.961, Run Time : 0.96
INFO:root:2019-05-12 07:33:09, Epoch : 1, Step : 4577, Training Loss : 0.12309, Training Acc : 0.956, Run Time : 15.59
INFO:root:2019-05-12 07:33:21, Epoch : 1, Step : 4578, Training Loss : 0.16380, Training Acc : 0.950, Run Time : 12.56
INFO:root:2019-05-12 07:33:24, Epoch : 1, Step : 4579, Training Loss : 0.15786, Training Acc : 0.939, Run Time : 2.16
INFO:root:2019-05-12 07:33:25, Epoch : 1, Step : 4580, Training Loss : 0.14245, Training Acc : 0.956, Run Time : 1.46
INFO:root:2019-05-12 07:33:39, Epoch : 1, Step : 4581, Training Loss : 0.21139, Training Acc : 0.894, Run Time : 13.52
INFO:root:2019-05-12 07:33:39, Epoch : 1, Step : 4582, Training Loss : 0.21112, Training Acc : 0.911, Run Time : 0.58
INFO:root:2019-05-12 07:33:40, Epoch : 1, Step : 4583, Training Loss : 0.15335, Training Acc : 0.928, Run Time : 0.75
INFO:root:2019-05-12 07:33:52, Epoch : 1, Step : 4584, Training Loss : 0.11343, Training Acc : 0.956, Run Time : 12.40
INFO:root:2019-05-12 07:33:53, Epoch : 1, Step : 4585, Training Loss : 0.08271, Training Acc : 0.983, Run Time : 0.90
INFO:root:2019-05-12 07:33:55, Epoch : 1, Step : 4586, Training Loss : 0.14754, Training Acc : 0.939, Run Time : 2.22
INFO:root:2019-05-12 07:34:07, Epoch : 1, Step : 4587, Training Loss : 0.08055, Training Acc : 0.961, Run Time : 11.55
INFO:root:2019-05-12 07:34:08, Epoch : 1, Step : 4588, Training Loss : 0.11498, Training Acc : 0.944, Run Time : 0.89
INFO:root:2019-05-12 07:34:21, Epoch : 1, Step : 4589, Training Loss : 0.12746, Training Acc : 0.967, Run Time : 12.81
INFO:root:2019-05-12 07:34:21, Epoch : 1, Step : 4590, Training Loss : 0.18669, Training Acc : 0.906, Run Time : 0.41
INFO:root:2019-05-12 07:34:22, Epoch : 1, Step : 4591, Training Loss : 0.16609, Training Acc : 0.917, Run Time : 0.57
INFO:root:2019-05-12 07:34:24, Epoch : 1, Step : 4592, Training Loss : 0.19281, Training Acc : 0.894, Run Time : 1.92
INFO:root:2019-05-12 07:34:36, Epoch : 1, Step : 4593, Training Loss : 0.20743, Training Acc : 0.922, Run Time : 11.93
INFO:root:2019-05-12 07:34:48, Epoch : 1, Step : 4594, Training Loss : 0.27922, Training Acc : 0.878, Run Time : 12.72
INFO:root:2019-05-12 07:34:51, Epoch : 1, Step : 4595, Training Loss : 0.36965, Training Acc : 0.861, Run Time : 2.50
INFO:root:2019-05-12 07:35:03, Epoch : 1, Step : 4596, Training Loss : 0.39610, Training Acc : 0.894, Run Time : 12.03
INFO:root:2019-05-12 07:35:04, Epoch : 1, Step : 4597, Training Loss : 0.29775, Training Acc : 0.856, Run Time : 1.58
INFO:root:2019-05-12 07:35:17, Epoch : 1, Step : 4598, Training Loss : 0.34296, Training Acc : 0.867, Run Time : 12.49
INFO:root:2019-05-12 07:35:18, Epoch : 1, Step : 4599, Training Loss : 0.40562, Training Acc : 0.839, Run Time : 0.82
INFO:root:2019-05-12 07:35:30, Epoch : 1, Step : 4600, Training Loss : 0.30382, Training Acc : 0.872, Run Time : 12.16
INFO:root:2019-05-12 07:35:31, Epoch : 1, Step : 4601, Training Loss : 0.44000, Training Acc : 0.822, Run Time : 1.04
INFO:root:2019-05-12 07:35:46, Epoch : 1, Step : 4602, Training Loss : 0.26000, Training Acc : 0.894, Run Time : 14.94
INFO:root:2019-05-12 07:35:47, Epoch : 1, Step : 4603, Training Loss : 0.28908, Training Acc : 0.883, Run Time : 0.96
INFO:root:2019-05-12 07:35:49, Epoch : 1, Step : 4604, Training Loss : 0.61629, Training Acc : 0.806, Run Time : 2.73
INFO:root:2019-05-12 07:36:02, Epoch : 1, Step : 4605, Training Loss : 0.34287, Training Acc : 0.850, Run Time : 12.88
INFO:root:2019-05-12 07:36:03, Epoch : 1, Step : 4606, Training Loss : 0.40877, Training Acc : 0.861, Run Time : 0.67
INFO:root:2019-05-12 07:36:13, Epoch : 1, Step : 4607, Training Loss : 0.36519, Training Acc : 0.861, Run Time : 10.26
INFO:root:2019-05-12 07:36:16, Epoch : 1, Step : 4608, Training Loss : 0.44365, Training Acc : 0.806, Run Time : 2.41
INFO:root:2019-05-12 07:36:16, Epoch : 1, Step : 4609, Training Loss : 0.33836, Training Acc : 0.850, Run Time : 0.73
INFO:root:2019-05-12 07:36:19, Epoch : 1, Step : 4610, Training Loss : 0.29214, Training Acc : 0.856, Run Time : 2.26
INFO:root:2019-05-12 07:36:28, Epoch : 1, Step : 4611, Training Loss : 0.25029, Training Acc : 0.883, Run Time : 9.33
INFO:root:2019-05-12 07:36:28, Epoch : 1, Step : 4612, Training Loss : 0.27526, Training Acc : 0.872, Run Time : 0.50
INFO:root:2019-05-12 07:36:30, Epoch : 1, Step : 4613, Training Loss : 0.27863, Training Acc : 0.833, Run Time : 1.46
INFO:root:2019-05-12 07:36:42, Epoch : 1, Step : 4614, Training Loss : 0.28031, Training Acc : 0.856, Run Time : 12.11
INFO:root:2019-05-12 07:36:44, Epoch : 1, Step : 4615, Training Loss : 0.30121, Training Acc : 0.856, Run Time : 1.74
INFO:root:2019-05-12 07:36:44, Epoch : 1, Step : 4616, Training Loss : 0.22220, Training Acc : 0.900, Run Time : 0.58
INFO:root:2019-05-12 07:36:53, Epoch : 1, Step : 4617, Training Loss : 0.20586, Training Acc : 0.922, Run Time : 8.40
INFO:root:2019-05-12 07:36:57, Epoch : 1, Step : 4618, Training Loss : 0.21631, Training Acc : 0.928, Run Time : 4.22
INFO:root:2019-05-12 07:36:57, Epoch : 1, Step : 4619, Training Loss : 0.22646, Training Acc : 0.894, Run Time : 0.49
INFO:root:2019-05-12 07:37:08, Epoch : 1, Step : 4620, Training Loss : 0.20882, Training Acc : 0.911, Run Time : 10.97
INFO:root:2019-05-12 07:37:09, Epoch : 1, Step : 4621, Training Loss : 0.18956, Training Acc : 0.922, Run Time : 0.63
INFO:root:2019-05-12 07:37:10, Epoch : 1, Step : 4622, Training Loss : 0.19054, Training Acc : 0.939, Run Time : 0.54
INFO:root:2019-05-12 07:37:20, Epoch : 1, Step : 4623, Training Loss : 0.20266, Training Acc : 0.933, Run Time : 9.98
INFO:root:2019-05-12 07:37:21, Epoch : 1, Step : 4624, Training Loss : 0.20356, Training Acc : 0.911, Run Time : 1.59
INFO:root:2019-05-12 07:37:22, Epoch : 1, Step : 4625, Training Loss : 0.22846, Training Acc : 0.911, Run Time : 0.65
INFO:root:2019-05-12 07:37:33, Epoch : 1, Step : 4626, Training Loss : 0.22585, Training Acc : 0.906, Run Time : 11.41
INFO:root:2019-05-12 07:37:34, Epoch : 1, Step : 4627, Training Loss : 0.18602, Training Acc : 0.911, Run Time : 1.02
INFO:root:2019-05-12 07:37:35, Epoch : 1, Step : 4628, Training Loss : 0.31026, Training Acc : 0.856, Run Time : 0.64
INFO:root:2019-05-12 07:37:36, Epoch : 1, Step : 4629, Training Loss : 0.22667, Training Acc : 0.917, Run Time : 0.77
INFO:root:2019-05-12 07:37:49, Epoch : 1, Step : 4630, Training Loss : 0.20458, Training Acc : 0.922, Run Time : 13.12
INFO:root:2019-05-12 07:37:49, Epoch : 1, Step : 4631, Training Loss : 0.21360, Training Acc : 0.883, Run Time : 0.65
INFO:root:2019-05-12 07:37:50, Epoch : 1, Step : 4632, Training Loss : 0.17882, Training Acc : 0.939, Run Time : 0.59
INFO:root:2019-05-12 07:37:51, Epoch : 1, Step : 4633, Training Loss : 0.21035, Training Acc : 0.911, Run Time : 0.60
INFO:root:2019-05-12 07:37:51, Epoch : 1, Step : 4634, Training Loss : 0.17881, Training Acc : 0.933, Run Time : 0.77
INFO:root:2019-05-12 07:38:08, Epoch : 1, Step : 4635, Training Loss : 0.15795, Training Acc : 0.956, Run Time : 16.77
INFO:root:2019-05-12 07:38:09, Epoch : 1, Step : 4636, Training Loss : 0.22090, Training Acc : 0.917, Run Time : 0.52
INFO:root:2019-05-12 07:38:09, Epoch : 1, Step : 4637, Training Loss : 0.21186, Training Acc : 0.939, Run Time : 0.66
INFO:root:2019-05-12 07:38:23, Epoch : 1, Step : 4638, Training Loss : 0.26264, Training Acc : 0.867, Run Time : 13.39
INFO:root:2019-05-12 07:38:24, Epoch : 1, Step : 4639, Training Loss : 0.16507, Training Acc : 0.933, Run Time : 1.06
INFO:root:2019-05-12 07:38:24, Epoch : 1, Step : 4640, Training Loss : 0.14594, Training Acc : 0.939, Run Time : 0.62
INFO:root:2019-05-12 07:38:27, Epoch : 1, Step : 4641, Training Loss : 0.17851, Training Acc : 0.933, Run Time : 2.09
INFO:root:2019-05-12 07:38:38, Epoch : 1, Step : 4642, Training Loss : 0.16473, Training Acc : 0.933, Run Time : 11.21
INFO:root:2019-05-12 07:38:39, Epoch : 1, Step : 4643, Training Loss : 0.17186, Training Acc : 0.928, Run Time : 0.88
INFO:root:2019-05-12 07:38:39, Epoch : 1, Step : 4644, Training Loss : 0.17839, Training Acc : 0.933, Run Time : 0.66
INFO:root:2019-05-12 07:38:46, Epoch : 1, Step : 4645, Training Loss : 0.14103, Training Acc : 0.961, Run Time : 7.22
INFO:root:2019-05-12 07:38:52, Epoch : 1, Step : 4646, Training Loss : 0.17261, Training Acc : 0.933, Run Time : 5.43
INFO:root:2019-05-12 07:38:53, Epoch : 1, Step : 4647, Training Loss : 0.16873, Training Acc : 0.956, Run Time : 0.87
INFO:root:2019-05-12 07:39:06, Epoch : 1, Step : 4648, Training Loss : 0.19934, Training Acc : 0.906, Run Time : 13.04
INFO:root:2019-05-12 07:39:07, Epoch : 1, Step : 4649, Training Loss : 0.19545, Training Acc : 0.928, Run Time : 0.90
INFO:root:2019-05-12 07:39:16, Epoch : 1, Step : 4650, Training Loss : 0.22716, Training Acc : 0.889, Run Time : 9.74
INFO:root:2019-05-12 07:39:18, Epoch : 1, Step : 4651, Training Loss : 0.15379, Training Acc : 0.944, Run Time : 1.03
INFO:root:2019-05-12 07:39:30, Epoch : 1, Step : 4652, Training Loss : 0.13942, Training Acc : 0.950, Run Time : 12.55
INFO:root:2019-05-12 07:39:31, Epoch : 1, Step : 4653, Training Loss : 0.18716, Training Acc : 0.928, Run Time : 0.44
INFO:root:2019-05-12 07:39:32, Epoch : 1, Step : 4654, Training Loss : 0.20099, Training Acc : 0.922, Run Time : 1.58
INFO:root:2019-05-12 07:39:44, Epoch : 1, Step : 4655, Training Loss : 0.19018, Training Acc : 0.906, Run Time : 11.87
INFO:root:2019-05-12 07:39:45, Epoch : 1, Step : 4656, Training Loss : 0.19754, Training Acc : 0.917, Run Time : 1.07
INFO:root:2019-05-12 07:39:56, Epoch : 1, Step : 4657, Training Loss : 0.22558, Training Acc : 0.906, Run Time : 11.41
INFO:root:2019-05-12 07:39:57, Epoch : 1, Step : 4658, Training Loss : 0.23893, Training Acc : 0.906, Run Time : 0.55
INFO:root:2019-05-12 07:39:58, Epoch : 1, Step : 4659, Training Loss : 0.18290, Training Acc : 0.922, Run Time : 0.64
INFO:root:2019-05-12 07:40:08, Epoch : 1, Step : 4660, Training Loss : 0.28732, Training Acc : 0.872, Run Time : 10.11
INFO:root:2019-05-12 07:40:09, Epoch : 1, Step : 4661, Training Loss : 0.37155, Training Acc : 0.833, Run Time : 0.80
INFO:root:2019-05-12 07:40:10, Epoch : 1, Step : 4662, Training Loss : 0.41598, Training Acc : 0.817, Run Time : 1.39
INFO:root:2019-05-12 07:40:21, Epoch : 1, Step : 4663, Training Loss : 0.30079, Training Acc : 0.867, Run Time : 11.22
INFO:root:2019-05-12 07:40:22, Epoch : 1, Step : 4664, Training Loss : 0.33810, Training Acc : 0.844, Run Time : 0.97
INFO:root:2019-05-12 07:40:25, Epoch : 1, Step : 4665, Training Loss : 0.46151, Training Acc : 0.794, Run Time : 3.06
INFO:root:2019-05-12 07:40:32, Epoch : 1, Step : 4666, Training Loss : 0.34147, Training Acc : 0.872, Run Time : 6.81
INFO:root:2019-05-12 07:40:32, Epoch : 1, Step : 4667, Training Loss : 0.37105, Training Acc : 0.822, Run Time : 0.43
INFO:root:2019-05-12 07:40:33, Epoch : 1, Step : 4668, Training Loss : 0.41516, Training Acc : 0.828, Run Time : 0.75
INFO:root:2019-05-12 07:40:45, Epoch : 1, Step : 4669, Training Loss : 0.27088, Training Acc : 0.906, Run Time : 11.97
INFO:root:2019-05-12 07:40:46, Epoch : 1, Step : 4670, Training Loss : 0.28670, Training Acc : 0.883, Run Time : 0.60
INFO:root:2019-05-12 07:40:46, Epoch : 1, Step : 4671, Training Loss : 0.21949, Training Acc : 0.900, Run Time : 0.45
INFO:root:2019-05-12 07:40:47, Epoch : 1, Step : 4672, Training Loss : 0.21674, Training Acc : 0.911, Run Time : 0.66
INFO:root:2019-05-12 07:40:59, Epoch : 1, Step : 4673, Training Loss : 0.40067, Training Acc : 0.839, Run Time : 11.90
INFO:root:2019-05-12 07:41:00, Epoch : 1, Step : 4674, Training Loss : 0.24659, Training Acc : 0.906, Run Time : 0.84
INFO:root:2019-05-12 07:41:11, Epoch : 1, Step : 4675, Training Loss : 0.21297, Training Acc : 0.917, Run Time : 11.50
INFO:root:2019-05-12 07:41:12, Epoch : 1, Step : 4676, Training Loss : 0.21707, Training Acc : 0.911, Run Time : 0.75
INFO:root:2019-05-12 07:41:23, Epoch : 1, Step : 4677, Training Loss : 0.23092, Training Acc : 0.917, Run Time : 10.76
INFO:root:2019-05-12 07:41:24, Epoch : 1, Step : 4678, Training Loss : 0.21437, Training Acc : 0.933, Run Time : 1.31
INFO:root:2019-05-12 07:41:25, Epoch : 1, Step : 4679, Training Loss : 0.19390, Training Acc : 0.928, Run Time : 1.58
INFO:root:2019-05-12 07:41:35, Epoch : 1, Step : 4680, Training Loss : 0.34741, Training Acc : 0.861, Run Time : 9.91
INFO:root:2019-05-12 07:41:36, Epoch : 1, Step : 4681, Training Loss : 0.21529, Training Acc : 0.917, Run Time : 0.76
INFO:root:2019-05-12 07:41:37, Epoch : 1, Step : 4682, Training Loss : 0.24742, Training Acc : 0.906, Run Time : 0.63
INFO:root:2019-05-12 07:41:48, Epoch : 1, Step : 4683, Training Loss : 0.34018, Training Acc : 0.867, Run Time : 11.10
INFO:root:2019-05-12 07:41:48, Epoch : 1, Step : 4684, Training Loss : 0.21593, Training Acc : 0.928, Run Time : 0.52
INFO:root:2019-05-12 07:41:50, Epoch : 1, Step : 4685, Training Loss : 0.41641, Training Acc : 0.806, Run Time : 1.57
INFO:root:2019-05-12 07:42:00, Epoch : 1, Step : 4686, Training Loss : 0.16394, Training Acc : 0.933, Run Time : 10.41
INFO:root:2019-05-12 07:42:01, Epoch : 1, Step : 4687, Training Loss : 0.21458, Training Acc : 0.917, Run Time : 0.43
INFO:root:2019-05-12 07:42:02, Epoch : 1, Step : 4688, Training Loss : 0.20504, Training Acc : 0.928, Run Time : 1.17
INFO:root:2019-05-12 07:42:12, Epoch : 1, Step : 4689, Training Loss : 0.14823, Training Acc : 0.961, Run Time : 10.27
INFO:root:2019-05-12 07:42:13, Epoch : 1, Step : 4690, Training Loss : 0.15054, Training Acc : 0.933, Run Time : 0.52
INFO:root:2019-05-12 07:42:15, Epoch : 1, Step : 4691, Training Loss : 0.11395, Training Acc : 0.961, Run Time : 1.72
INFO:root:2019-05-12 07:42:31, Epoch : 1, Step : 4692, Training Loss : 0.13536, Training Acc : 0.956, Run Time : 16.64
INFO:root:2019-05-12 07:42:36, Epoch : 1, Step : 4693, Training Loss : 0.15518, Training Acc : 0.922, Run Time : 4.46
INFO:root:2019-05-12 07:42:36, Epoch : 1, Step : 4694, Training Loss : 0.13348, Training Acc : 0.956, Run Time : 0.52
INFO:root:2019-05-12 07:42:37, Epoch : 1, Step : 4695, Training Loss : 0.08350, Training Acc : 0.989, Run Time : 0.90
INFO:root:2019-05-12 07:42:45, Epoch : 1, Step : 4696, Training Loss : 0.10728, Training Acc : 0.972, Run Time : 8.33
INFO:root:2019-05-12 07:42:46, Epoch : 1, Step : 4697, Training Loss : 0.22952, Training Acc : 0.878, Run Time : 0.84
INFO:root:2019-05-12 07:42:47, Epoch : 1, Step : 4698, Training Loss : 0.13368, Training Acc : 0.956, Run Time : 0.72
INFO:root:2019-05-12 07:42:48, Epoch : 1, Step : 4699, Training Loss : 0.15217, Training Acc : 0.922, Run Time : 0.62
INFO:root:2019-05-12 07:42:58, Epoch : 1, Step : 4700, Training Loss : 0.15029, Training Acc : 0.939, Run Time : 10.66
INFO:root:2019-05-12 07:43:00, Epoch : 1, Step : 4701, Training Loss : 0.14723, Training Acc : 0.944, Run Time : 1.53
INFO:root:2019-05-12 07:43:12, Epoch : 1, Step : 4702, Training Loss : 0.13849, Training Acc : 0.950, Run Time : 12.21
INFO:root:2019-05-12 07:43:12, Epoch : 1, Step : 4703, Training Loss : 0.12458, Training Acc : 0.944, Run Time : 0.48
INFO:root:2019-05-12 07:43:13, Epoch : 1, Step : 4704, Training Loss : 0.13639, Training Acc : 0.933, Run Time : 0.97
INFO:root:2019-05-12 07:43:26, Epoch : 1, Step : 4705, Training Loss : 0.11761, Training Acc : 0.956, Run Time : 12.99
INFO:root:2019-05-12 07:43:38, Epoch : 1, Step : 4706, Training Loss : 0.13935, Training Acc : 0.939, Run Time : 12.12
INFO:root:2019-05-12 07:43:41, Epoch : 1, Step : 4707, Training Loss : 0.17417, Training Acc : 0.928, Run Time : 2.70
INFO:root:2019-05-12 07:43:53, Epoch : 1, Step : 4708, Training Loss : 0.09942, Training Acc : 0.950, Run Time : 11.35
INFO:root:2019-05-12 07:43:54, Epoch : 1, Step : 4709, Training Loss : 0.18765, Training Acc : 0.922, Run Time : 1.45
INFO:root:2019-05-12 07:44:04, Epoch : 1, Step : 4710, Training Loss : 0.18836, Training Acc : 0.894, Run Time : 10.29
INFO:root:2019-05-12 07:44:05, Epoch : 1, Step : 4711, Training Loss : 0.18182, Training Acc : 0.906, Run Time : 0.85
INFO:root:2019-05-12 07:44:06, Epoch : 1, Step : 4712, Training Loss : 0.20816, Training Acc : 0.911, Run Time : 1.24
INFO:root:2019-05-12 07:44:19, Epoch : 1, Step : 4713, Training Loss : 0.18018, Training Acc : 0.933, Run Time : 12.29
INFO:root:2019-05-12 07:44:19, Epoch : 1, Step : 4714, Training Loss : 0.13743, Training Acc : 0.933, Run Time : 0.51
INFO:root:2019-05-12 07:44:31, Epoch : 1, Step : 4715, Training Loss : 0.28512, Training Acc : 0.883, Run Time : 12.25
INFO:root:2019-05-12 07:44:32, Epoch : 1, Step : 4716, Training Loss : 0.18402, Training Acc : 0.928, Run Time : 0.82
INFO:root:2019-05-12 07:44:33, Epoch : 1, Step : 4717, Training Loss : 0.18401, Training Acc : 0.928, Run Time : 0.60
INFO:root:2019-05-12 07:44:33, Epoch : 1, Step : 4718, Training Loss : 0.28405, Training Acc : 0.872, Run Time : 0.62
INFO:root:2019-05-12 07:44:47, Epoch : 1, Step : 4719, Training Loss : 0.21239, Training Acc : 0.922, Run Time : 13.38
INFO:root:2019-05-12 07:44:48, Epoch : 1, Step : 4720, Training Loss : 0.23881, Training Acc : 0.906, Run Time : 1.41
INFO:root:2019-05-12 07:45:02, Epoch : 1, Step : 4721, Training Loss : 0.22892, Training Acc : 0.939, Run Time : 14.18
INFO:root:2019-05-12 07:45:04, Epoch : 1, Step : 4722, Training Loss : 0.15792, Training Acc : 0.950, Run Time : 1.80
INFO:root:2019-05-12 07:45:16, Epoch : 1, Step : 4723, Training Loss : 0.17586, Training Acc : 0.922, Run Time : 11.80
INFO:root:2019-05-12 07:45:17, Epoch : 1, Step : 4724, Training Loss : 0.12889, Training Acc : 0.939, Run Time : 1.34
INFO:root:2019-05-12 07:45:29, Epoch : 1, Step : 4725, Training Loss : 0.17688, Training Acc : 0.928, Run Time : 11.85
INFO:root:2019-05-12 07:45:30, Epoch : 1, Step : 4726, Training Loss : 0.19155, Training Acc : 0.922, Run Time : 0.81
INFO:root:2019-05-12 07:45:32, Epoch : 1, Step : 4727, Training Loss : 0.15793, Training Acc : 0.944, Run Time : 1.58
INFO:root:2019-05-12 07:45:46, Epoch : 1, Step : 4728, Training Loss : 0.12979, Training Acc : 0.956, Run Time : 14.72
INFO:root:2019-05-12 07:45:48, Epoch : 1, Step : 4729, Training Loss : 0.18753, Training Acc : 0.933, Run Time : 2.14
INFO:root:2019-05-12 07:46:02, Epoch : 1, Step : 4730, Training Loss : 0.23407, Training Acc : 0.883, Run Time : 13.27
INFO:root:2019-05-12 07:46:03, Epoch : 1, Step : 4731, Training Loss : 0.18891, Training Acc : 0.922, Run Time : 1.39
INFO:root:2019-05-12 07:46:14, Epoch : 1, Step : 4732, Training Loss : 0.20318, Training Acc : 0.922, Run Time : 11.35
INFO:root:2019-05-12 07:46:16, Epoch : 1, Step : 4733, Training Loss : 0.16297, Training Acc : 0.956, Run Time : 1.10
INFO:root:2019-05-12 07:46:17, Epoch : 1, Step : 4734, Training Loss : 0.15852, Training Acc : 0.961, Run Time : 1.86
INFO:root:2019-05-12 07:46:28, Epoch : 1, Step : 4735, Training Loss : 0.18673, Training Acc : 0.944, Run Time : 10.61
INFO:root:2019-05-12 07:46:29, Epoch : 1, Step : 4736, Training Loss : 0.16188, Training Acc : 0.967, Run Time : 0.82
INFO:root:2019-05-12 07:46:40, Epoch : 1, Step : 4737, Training Loss : 0.14666, Training Acc : 0.939, Run Time : 11.19
INFO:root:2019-05-12 07:46:41, Epoch : 1, Step : 4738, Training Loss : 0.17508, Training Acc : 0.922, Run Time : 0.44
INFO:root:2019-05-12 07:46:41, Epoch : 1, Step : 4739, Training Loss : 0.13751, Training Acc : 0.950, Run Time : 0.91
INFO:root:2019-05-12 07:46:54, Epoch : 1, Step : 4740, Training Loss : 0.25176, Training Acc : 0.883, Run Time : 12.29
INFO:root:2019-05-12 07:47:06, Epoch : 1, Step : 4741, Training Loss : 0.22258, Training Acc : 0.933, Run Time : 11.93
INFO:root:2019-05-12 07:47:07, Epoch : 1, Step : 4742, Training Loss : 0.15349, Training Acc : 0.939, Run Time : 1.27
INFO:root:2019-05-12 07:47:08, Epoch : 1, Step : 4743, Training Loss : 0.23225, Training Acc : 0.911, Run Time : 0.62
INFO:root:2019-05-12 07:47:21, Epoch : 1, Step : 4744, Training Loss : 0.21474, Training Acc : 0.906, Run Time : 13.30
INFO:root:2019-05-12 07:47:21, Epoch : 1, Step : 4745, Training Loss : 0.14837, Training Acc : 0.933, Run Time : 0.49
INFO:root:2019-05-12 07:47:23, Epoch : 1, Step : 4746, Training Loss : 0.23871, Training Acc : 0.894, Run Time : 1.85
INFO:root:2019-05-12 07:47:41, Epoch : 1, Step : 4747, Training Loss : 0.16438, Training Acc : 0.950, Run Time : 17.61
INFO:root:2019-05-12 07:47:42, Epoch : 1, Step : 4748, Training Loss : 0.17681, Training Acc : 0.939, Run Time : 1.59
INFO:root:2019-05-12 07:47:54, Epoch : 1, Step : 4749, Training Loss : 0.29088, Training Acc : 0.878, Run Time : 11.24
INFO:root:2019-05-12 07:47:55, Epoch : 1, Step : 4750, Training Loss : 0.22084, Training Acc : 0.906, Run Time : 1.01
INFO:root:2019-05-12 07:48:12, Epoch : 1, Step : 4751, Training Loss : 0.33571, Training Acc : 0.856, Run Time : 16.95
INFO:root:2019-05-12 07:48:13, Epoch : 1, Step : 4752, Training Loss : 0.20947, Training Acc : 0.900, Run Time : 1.70
INFO:root:2019-05-12 07:48:14, Epoch : 1, Step : 4753, Training Loss : 0.32169, Training Acc : 0.856, Run Time : 0.61
INFO:root:2019-05-12 07:48:14, Epoch : 1, Step : 4754, Training Loss : 0.23788, Training Acc : 0.889, Run Time : 0.57
INFO:root:2019-05-12 07:48:26, Epoch : 1, Step : 4755, Training Loss : 0.22533, Training Acc : 0.911, Run Time : 11.29
INFO:root:2019-05-12 07:48:27, Epoch : 1, Step : 4756, Training Loss : 0.32903, Training Acc : 0.856, Run Time : 0.84
INFO:root:2019-05-12 07:48:27, Epoch : 1, Step : 4757, Training Loss : 0.80603, Training Acc : 0.683, Run Time : 0.58
INFO:root:2019-05-12 07:48:28, Epoch : 1, Step : 4758, Training Loss : 0.24463, Training Acc : 0.900, Run Time : 0.69
INFO:root:2019-05-12 07:48:40, Epoch : 1, Step : 4759, Training Loss : 0.35435, Training Acc : 0.833, Run Time : 12.04
INFO:root:2019-05-12 07:48:40, Epoch : 1, Step : 4760, Training Loss : 0.22005, Training Acc : 0.906, Run Time : 0.44
INFO:root:2019-05-12 07:48:41, Epoch : 1, Step : 4761, Training Loss : 0.26162, Training Acc : 0.872, Run Time : 0.59
INFO:root:2019-05-12 07:48:42, Epoch : 1, Step : 4762, Training Loss : 0.16725, Training Acc : 0.939, Run Time : 0.61
INFO:root:2019-05-12 07:48:42, Epoch : 1, Step : 4763, Training Loss : 0.13572, Training Acc : 0.956, Run Time : 0.57
INFO:root:2019-05-12 07:48:58, Epoch : 1, Step : 4764, Training Loss : 0.21187, Training Acc : 0.917, Run Time : 15.56
INFO:root:2019-05-12 07:48:58, Epoch : 1, Step : 4765, Training Loss : 0.21049, Training Acc : 0.922, Run Time : 0.54
INFO:root:2019-05-12 07:49:09, Epoch : 1, Step : 4766, Training Loss : 0.21994, Training Acc : 0.889, Run Time : 10.76
INFO:root:2019-05-12 07:49:13, Epoch : 1, Step : 4767, Training Loss : 0.20650, Training Acc : 0.922, Run Time : 3.93
INFO:root:2019-05-12 07:49:13, Epoch : 1, Step : 4768, Training Loss : 0.22527, Training Acc : 0.917, Run Time : 0.40
INFO:root:2019-05-12 07:49:14, Epoch : 1, Step : 4769, Training Loss : 0.19418, Training Acc : 0.917, Run Time : 1.01
INFO:root:2019-05-12 07:49:25, Epoch : 1, Step : 4770, Training Loss : 0.22894, Training Acc : 0.894, Run Time : 10.42
INFO:root:2019-05-12 07:49:25, Epoch : 1, Step : 4771, Training Loss : 0.20317, Training Acc : 0.911, Run Time : 0.49
INFO:root:2019-05-12 07:49:27, Epoch : 1, Step : 4772, Training Loss : 0.21078, Training Acc : 0.906, Run Time : 2.17
INFO:root:2019-05-12 07:49:41, Epoch : 1, Step : 4773, Training Loss : 0.31443, Training Acc : 0.867, Run Time : 14.10
INFO:root:2019-05-12 07:49:50, Epoch : 1, Step : 4774, Training Loss : 0.21147, Training Acc : 0.900, Run Time : 8.95
INFO:root:2019-05-12 07:49:51, Epoch : 1, Step : 4775, Training Loss : 0.16311, Training Acc : 0.939, Run Time : 0.96
INFO:root:2019-05-12 07:49:53, Epoch : 1, Step : 4776, Training Loss : 0.16379, Training Acc : 0.933, Run Time : 1.80
INFO:root:2019-05-12 07:50:04, Epoch : 1, Step : 4777, Training Loss : 0.18611, Training Acc : 0.922, Run Time : 11.27
INFO:root:2019-05-12 07:50:05, Epoch : 1, Step : 4778, Training Loss : 0.13097, Training Acc : 0.950, Run Time : 0.58
INFO:root:2019-05-12 07:50:08, Epoch : 1, Step : 4779, Training Loss : 0.18960, Training Acc : 0.900, Run Time : 2.92
INFO:root:2019-05-12 07:50:25, Epoch : 1, Step : 4780, Training Loss : 0.19760, Training Acc : 0.906, Run Time : 17.05
INFO:root:2019-05-12 07:50:33, Epoch : 1, Step : 4781, Training Loss : 0.20018, Training Acc : 0.928, Run Time : 7.77
INFO:root:2019-05-12 07:50:44, Epoch : 1, Step : 4782, Training Loss : 0.18873, Training Acc : 0.917, Run Time : 11.46
INFO:root:2019-05-12 07:51:00, Epoch : 1, Step : 4783, Training Loss : 0.22016, Training Acc : 0.911, Run Time : 15.93
INFO:root:2019-05-12 07:51:01, Epoch : 1, Step : 4784, Training Loss : 0.25332, Training Acc : 0.911, Run Time : 1.29
INFO:root:2019-05-12 07:51:02, Epoch : 1, Step : 4785, Training Loss : 0.17246, Training Acc : 0.917, Run Time : 0.63
INFO:root:2019-05-12 07:51:17, Epoch : 1, Step : 4786, Training Loss : 0.19308, Training Acc : 0.917, Run Time : 14.79
INFO:root:2019-05-12 07:51:31, Epoch : 1, Step : 4787, Training Loss : 0.10603, Training Acc : 0.967, Run Time : 13.85
INFO:root:2019-05-12 07:51:41, Epoch : 1, Step : 4788, Training Loss : 0.11142, Training Acc : 0.961, Run Time : 10.44
INFO:root:2019-05-12 07:51:51, Epoch : 1, Step : 4789, Training Loss : 0.11278, Training Acc : 0.944, Run Time : 9.59
INFO:root:2019-05-12 07:51:52, Epoch : 1, Step : 4790, Training Loss : 0.17276, Training Acc : 0.950, Run Time : 0.95
INFO:root:2019-05-12 07:51:54, Epoch : 1, Step : 4791, Training Loss : 0.13105, Training Acc : 0.961, Run Time : 2.61
INFO:root:2019-05-12 07:52:00, Epoch : 1, Step : 4792, Training Loss : 0.18948, Training Acc : 0.928, Run Time : 6.16
INFO:root:2019-05-12 07:52:01, Epoch : 1, Step : 4793, Training Loss : 0.16111, Training Acc : 0.944, Run Time : 0.55
INFO:root:2019-05-12 07:52:06, Epoch : 1, Step : 4794, Training Loss : 0.08584, Training Acc : 0.978, Run Time : 4.88
INFO:root:2019-05-12 07:52:13, Epoch : 1, Step : 4795, Training Loss : 0.16380, Training Acc : 0.922, Run Time : 7.35
INFO:root:2019-05-12 07:52:24, Epoch : 1, Step : 4796, Training Loss : 0.14676, Training Acc : 0.922, Run Time : 10.76
INFO:root:2019-05-12 07:52:25, Epoch : 1, Step : 4797, Training Loss : 0.16138, Training Acc : 0.922, Run Time : 1.10
INFO:root:2019-05-12 07:52:27, Epoch : 1, Step : 4798, Training Loss : 0.26793, Training Acc : 0.889, Run Time : 1.66
INFO:root:2019-05-12 07:52:45, Epoch : 1, Step : 4799, Training Loss : 0.20595, Training Acc : 0.922, Run Time : 17.82
INFO:root:2019-05-12 07:52:55, Epoch : 1, Step : 4800, Training Loss : 0.28263, Training Acc : 0.900, Run Time : 10.73
INFO:root:2019-05-12 07:53:07, Epoch : 1, Step : 4801, Training Loss : 0.89633, Training Acc : 0.689, Run Time : 11.32
INFO:root:2019-05-12 07:53:07, Epoch : 1, Step : 4802, Training Loss : 0.74641, Training Acc : 0.728, Run Time : 0.66
INFO:root:2019-05-12 07:53:28, Epoch : 1, Step : 4803, Training Loss : 0.80578, Training Acc : 0.650, Run Time : 20.39
INFO:root:2019-05-12 07:53:31, Epoch : 1, Step : 4804, Training Loss : 0.53654, Training Acc : 0.778, Run Time : 3.68
INFO:root:2019-05-12 07:53:32, Epoch : 1, Step : 4805, Training Loss : 0.58131, Training Acc : 0.744, Run Time : 0.55
INFO:root:2019-05-12 07:53:33, Epoch : 1, Step : 4806, Training Loss : 0.30624, Training Acc : 0.861, Run Time : 1.38
INFO:root:2019-05-12 07:53:45, Epoch : 1, Step : 4807, Training Loss : 0.63404, Training Acc : 0.733, Run Time : 12.15
INFO:root:2019-05-12 07:53:48, Epoch : 1, Step : 4808, Training Loss : 0.21318, Training Acc : 0.906, Run Time : 2.49
INFO:root:2019-05-12 07:54:06, Epoch : 1, Step : 4809, Training Loss : 0.17795, Training Acc : 0.939, Run Time : 17.73
INFO:root:2019-05-12 07:54:10, Epoch : 1, Step : 4810, Training Loss : 0.06086, Training Acc : 0.989, Run Time : 4.30
INFO:root:2019-05-12 07:54:10, Epoch : 1, Step : 4811, Training Loss : 0.26940, Training Acc : 0.906, Run Time : 0.44
INFO:root:2019-05-12 07:54:27, Epoch : 1, Step : 4812, Training Loss : 0.12527, Training Acc : 0.956, Run Time : 16.24
INFO:root:2019-05-12 07:54:36, Epoch : 1, Step : 4813, Training Loss : 0.24384, Training Acc : 0.867, Run Time : 8.92
INFO:root:2019-05-12 07:54:37, Epoch : 1, Step : 4814, Training Loss : 0.25014, Training Acc : 0.911, Run Time : 1.73
INFO:root:2019-05-12 07:54:39, Epoch : 1, Step : 4815, Training Loss : 0.50434, Training Acc : 0.806, Run Time : 1.26
INFO:root:2019-05-12 07:54:52, Epoch : 1, Step : 4816, Training Loss : 0.12124, Training Acc : 0.956, Run Time : 13.70
INFO:root:2019-05-12 07:54:53, Epoch : 1, Step : 4817, Training Loss : 0.19930, Training Acc : 0.911, Run Time : 0.83
INFO:root:2019-05-12 07:55:04, Epoch : 1, Step : 4818, Training Loss : 0.24962, Training Acc : 0.894, Run Time : 10.69
INFO:root:2019-05-12 07:55:10, Epoch : 1, Step : 4819, Training Loss : 0.12396, Training Acc : 0.928, Run Time : 6.69
INFO:root:2019-05-12 07:55:15, Epoch : 1, Step : 4820, Training Loss : 0.28340, Training Acc : 0.922, Run Time : 4.73
INFO:root:2019-05-12 07:55:17, Epoch : 1, Step : 4821, Training Loss : 0.22412, Training Acc : 0.878, Run Time : 1.86
INFO:root:2019-05-12 07:55:18, Epoch : 1, Step : 4822, Training Loss : 0.12222, Training Acc : 0.972, Run Time : 1.35
INFO:root:2019-05-12 07:55:23, Epoch : 1, Step : 4823, Training Loss : 0.32863, Training Acc : 0.922, Run Time : 5.00
INFO:root:2019-05-12 07:55:24, Epoch : 1, Step : 4824, Training Loss : 0.50321, Training Acc : 0.906, Run Time : 0.68
INFO:root:2019-05-12 07:55:43, Epoch : 1, Step : 4825, Training Loss : 0.16484, Training Acc : 0.950, Run Time : 18.76
INFO:root:2019-05-12 07:55:52, Epoch : 1, Step : 4826, Training Loss : 0.08803, Training Acc : 0.967, Run Time : 9.07
INFO:root:2019-05-12 07:55:52, Epoch : 1, Step : 4827, Training Loss : 0.03543, Training Acc : 0.994, Run Time : 0.57
INFO:root:2019-05-12 07:55:54, Epoch : 1, Step : 4828, Training Loss : 0.12733, Training Acc : 0.956, Run Time : 1.88
INFO:root:2019-05-12 07:56:07, Epoch : 1, Step : 4829, Training Loss : 0.17011, Training Acc : 0.967, Run Time : 12.74
INFO:root:2019-05-12 07:56:07, Epoch : 1, Step : 4830, Training Loss : 0.05519, Training Acc : 0.978, Run Time : 0.41
INFO:root:2019-05-12 07:56:09, Epoch : 1, Step : 4831, Training Loss : 0.03167, Training Acc : 1.000, Run Time : 1.64
INFO:root:2019-05-12 07:56:23, Epoch : 1, Step : 4832, Training Loss : 0.11562, Training Acc : 0.972, Run Time : 14.19
INFO:root:2019-05-12 07:56:25, Epoch : 1, Step : 4833, Training Loss : 0.05550, Training Acc : 0.989, Run Time : 1.47
INFO:root:2019-05-12 07:56:27, Epoch : 1, Step : 4834, Training Loss : 0.01969, Training Acc : 1.000, Run Time : 2.29
INFO:root:2019-05-12 07:56:40, Epoch : 1, Step : 4835, Training Loss : 0.06797, Training Acc : 0.983, Run Time : 12.85
INFO:root:2019-05-12 07:56:40, Epoch : 1, Step : 4836, Training Loss : 0.10530, Training Acc : 0.961, Run Time : 0.41
INFO:root:2019-05-12 07:56:41, Epoch : 1, Step : 4837, Training Loss : 0.14387, Training Acc : 0.944, Run Time : 0.90
INFO:root:2019-05-12 07:56:42, Epoch : 1, Step : 4838, Training Loss : 0.24630, Training Acc : 0.911, Run Time : 0.57
INFO:root:2019-05-12 07:56:53, Epoch : 1, Step : 4839, Training Loss : 0.18788, Training Acc : 0.950, Run Time : 10.93
INFO:root:2019-05-12 07:56:53, Epoch : 1, Step : 4840, Training Loss : 0.18761, Training Acc : 0.950, Run Time : 0.64
INFO:root:2019-05-12 07:56:54, Epoch : 1, Step : 4841, Training Loss : 0.18517, Training Acc : 0.928, Run Time : 0.53
INFO:root:2019-05-12 07:56:55, Epoch : 1, Step : 4842, Training Loss : 0.17237, Training Acc : 0.944, Run Time : 1.11
INFO:root:2019-05-12 07:57:06, Epoch : 1, Step : 4843, Training Loss : 0.16861, Training Acc : 0.944, Run Time : 11.52
INFO:root:2019-05-12 07:57:07, Epoch : 1, Step : 4844, Training Loss : 0.25383, Training Acc : 0.928, Run Time : 0.77
INFO:root:2019-05-12 07:57:08, Epoch : 1, Step : 4845, Training Loss : 0.10506, Training Acc : 0.972, Run Time : 0.84
INFO:root:2019-05-12 07:57:19, Epoch : 1, Step : 4846, Training Loss : 0.12148, Training Acc : 0.961, Run Time : 10.51
INFO:root:2019-05-12 07:57:19, Epoch : 1, Step : 4847, Training Loss : 0.08794, Training Acc : 0.956, Run Time : 0.58
INFO:root:2019-05-12 07:57:30, Epoch : 1, Step : 4848, Training Loss : 0.29746, Training Acc : 0.894, Run Time : 10.71
INFO:root:2019-05-12 07:57:31, Epoch : 1, Step : 4849, Training Loss : 0.09406, Training Acc : 0.972, Run Time : 1.35
INFO:root:2019-05-12 07:57:41, Epoch : 1, Step : 4850, Training Loss : 0.07858, Training Acc : 0.978, Run Time : 10.18
INFO:root:2019-05-12 07:57:43, Epoch : 1, Step : 4851, Training Loss : 0.07219, Training Acc : 0.972, Run Time : 1.60
INFO:root:2019-05-12 07:57:52, Epoch : 1, Step : 4852, Training Loss : 0.14130, Training Acc : 0.939, Run Time : 8.63
INFO:root:2019-05-12 07:57:53, Epoch : 1, Step : 4853, Training Loss : 0.14166, Training Acc : 0.939, Run Time : 1.41
INFO:root:2019-05-12 07:58:02, Epoch : 1, Step : 4854, Training Loss : 0.04235, Training Acc : 1.000, Run Time : 8.64
INFO:root:2019-05-12 07:58:02, Epoch : 1, Step : 4855, Training Loss : 0.18498, Training Acc : 0.928, Run Time : 0.75
INFO:root:2019-05-12 07:58:18, Epoch : 1, Step : 4856, Training Loss : 0.09032, Training Acc : 0.983, Run Time : 15.42
INFO:root:2019-05-12 07:58:24, Epoch : 1, Step : 4857, Training Loss : 0.36401, Training Acc : 0.883, Run Time : 5.76
INFO:root:2019-05-12 07:58:26, Epoch : 1, Step : 4858, Training Loss : 0.28876, Training Acc : 0.883, Run Time : 2.04
INFO:root:2019-05-12 07:58:34, Epoch : 1, Step : 4859, Training Loss : 0.06816, Training Acc : 0.967, Run Time : 8.75
INFO:root:2019-05-12 07:58:39, Epoch : 1, Step : 4860, Training Loss : 0.11822, Training Acc : 0.961, Run Time : 4.66
INFO:root:2019-05-12 07:58:42, Epoch : 1, Step : 4861, Training Loss : 0.12931, Training Acc : 0.950, Run Time : 2.73
INFO:root:2019-05-12 07:58:42, Epoch : 1, Step : 4862, Training Loss : 0.21697, Training Acc : 0.911, Run Time : 0.64
INFO:root:2019-05-12 07:58:43, Epoch : 1, Step : 4863, Training Loss : 0.21703, Training Acc : 0.939, Run Time : 0.89
INFO:root:2019-05-12 07:58:49, Epoch : 1, Step : 4864, Training Loss : 0.48520, Training Acc : 0.872, Run Time : 6.05
INFO:root:2019-05-12 07:58:51, Epoch : 1, Step : 4865, Training Loss : 0.69758, Training Acc : 0.828, Run Time : 1.93
INFO:root:2019-05-12 07:59:01, Epoch : 1, Step : 4866, Training Loss : 0.34114, Training Acc : 0.933, Run Time : 10.08
INFO:root:2019-05-12 07:59:02, Epoch : 1, Step : 4867, Training Loss : 0.35157, Training Acc : 0.817, Run Time : 0.47
INFO:root:2019-05-12 07:59:03, Epoch : 1, Step : 4868, Training Loss : 0.57655, Training Acc : 0.800, Run Time : 1.41
INFO:root:2019-05-12 07:59:15, Epoch : 1, Step : 4869, Training Loss : 0.43057, Training Acc : 0.844, Run Time : 11.64
INFO:root:2019-05-12 07:59:16, Epoch : 1, Step : 4870, Training Loss : 0.55819, Training Acc : 0.817, Run Time : 0.82
INFO:root:2019-05-12 07:59:27, Epoch : 1, Step : 4871, Training Loss : 0.32781, Training Acc : 0.856, Run Time : 11.26
INFO:root:2019-05-12 07:59:28, Epoch : 1, Step : 4872, Training Loss : 0.37209, Training Acc : 0.894, Run Time : 0.58
INFO:root:2019-05-12 07:59:29, Epoch : 1, Step : 4873, Training Loss : 0.63193, Training Acc : 0.778, Run Time : 1.28
INFO:root:2019-05-12 07:59:40, Epoch : 1, Step : 4874, Training Loss : 0.39200, Training Acc : 0.883, Run Time : 11.38
INFO:root:2019-05-12 07:59:41, Epoch : 1, Step : 4875, Training Loss : 0.70442, Training Acc : 0.778, Run Time : 0.51
INFO:root:2019-05-12 07:59:42, Epoch : 1, Step : 4876, Training Loss : 0.42559, Training Acc : 0.856, Run Time : 1.39
INFO:root:2019-05-12 07:59:54, Epoch : 1, Step : 4877, Training Loss : 0.23403, Training Acc : 0.900, Run Time : 11.40
INFO:root:2019-05-12 07:59:56, Epoch : 1, Step : 4878, Training Loss : 0.65242, Training Acc : 0.778, Run Time : 2.71
INFO:root:2019-05-12 07:59:57, Epoch : 1, Step : 4879, Training Loss : 0.82433, Training Acc : 0.750, Run Time : 0.62
INFO:root:2019-05-12 08:00:07, Epoch : 1, Step : 4880, Training Loss : 0.87517, Training Acc : 0.733, Run Time : 9.63
INFO:root:2019-05-12 08:00:07, Epoch : 1, Step : 4881, Training Loss : 0.41674, Training Acc : 0.806, Run Time : 0.77
INFO:root:2019-05-12 08:00:12, Epoch : 1, Step : 4882, Training Loss : 0.27553, Training Acc : 0.894, Run Time : 4.51
INFO:root:2019-05-12 08:00:14, Epoch : 1, Step : 4883, Training Loss : 0.38252, Training Acc : 0.844, Run Time : 2.46
INFO:root:2019-05-12 08:00:15, Epoch : 1, Step : 4884, Training Loss : 0.20416, Training Acc : 0.917, Run Time : 0.48
INFO:root:2019-05-12 08:00:22, Epoch : 1, Step : 4885, Training Loss : 0.67865, Training Acc : 0.744, Run Time : 6.91
INFO:root:2019-05-12 08:00:22, Epoch : 1, Step : 4886, Training Loss : 0.74101, Training Acc : 0.706, Run Time : 0.55
INFO:root:2019-05-12 08:00:23, Epoch : 1, Step : 4887, Training Loss : 0.84021, Training Acc : 0.722, Run Time : 0.83
INFO:root:2019-05-12 08:00:28, Epoch : 1, Step : 4888, Training Loss : 0.73524, Training Acc : 0.700, Run Time : 4.58
INFO:root:2019-05-12 08:00:31, Epoch : 1, Step : 4889, Training Loss : 0.48644, Training Acc : 0.844, Run Time : 3.86
INFO:root:2019-05-12 08:00:32, Epoch : 1, Step : 4890, Training Loss : 0.43511, Training Acc : 0.844, Run Time : 0.63
INFO:root:2019-05-12 08:00:38, Epoch : 1, Step : 4891, Training Loss : 0.37868, Training Acc : 0.817, Run Time : 6.19
INFO:root:2019-05-12 08:00:47, Epoch : 1, Step : 4892, Training Loss : 0.26259, Training Acc : 0.894, Run Time : 9.02
INFO:root:2019-05-12 08:00:49, Epoch : 1, Step : 4893, Training Loss : 0.56400, Training Acc : 0.789, Run Time : 1.95
INFO:root:2019-05-12 08:00:54, Epoch : 1, Step : 4894, Training Loss : 0.45737, Training Acc : 0.811, Run Time : 5.05
INFO:root:2019-05-12 08:00:55, Epoch : 1, Step : 4895, Training Loss : 0.29875, Training Acc : 0.917, Run Time : 0.57
INFO:root:2019-05-12 08:00:55, Epoch : 1, Step : 4896, Training Loss : 0.17296, Training Acc : 0.967, Run Time : 0.56
INFO:root:2019-05-12 08:00:57, Epoch : 1, Step : 4897, Training Loss : 0.36487, Training Acc : 0.783, Run Time : 1.35
INFO:root:2019-05-12 08:01:05, Epoch : 1, Step : 4898, Training Loss : 0.30873, Training Acc : 0.861, Run Time : 8.58
INFO:root:2019-05-12 08:01:06, Epoch : 1, Step : 4899, Training Loss : 0.35188, Training Acc : 0.900, Run Time : 0.64
INFO:root:2019-05-12 08:01:07, Epoch : 1, Step : 4900, Training Loss : 0.34465, Training Acc : 0.850, Run Time : 1.42
INFO:root:2019-05-12 08:01:20, Epoch : 1, Step : 4901, Training Loss : 0.25351, Training Acc : 0.928, Run Time : 12.40
INFO:root:2019-05-12 08:01:21, Epoch : 1, Step : 4902, Training Loss : 0.21596, Training Acc : 0.939, Run Time : 0.80
INFO:root:2019-05-12 08:01:23, Epoch : 1, Step : 4903, Training Loss : 0.35808, Training Acc : 0.828, Run Time : 2.03
INFO:root:2019-05-12 08:01:35, Epoch : 1, Step : 4904, Training Loss : 0.27261, Training Acc : 0.922, Run Time : 12.23
INFO:root:2019-05-12 08:01:47, Epoch : 1, Step : 4905, Training Loss : 0.39701, Training Acc : 0.822, Run Time : 11.76
INFO:root:2019-05-12 08:01:48, Epoch : 1, Step : 4906, Training Loss : 0.40021, Training Acc : 0.850, Run Time : 1.47
INFO:root:2019-05-12 08:01:49, Epoch : 1, Step : 4907, Training Loss : 0.45520, Training Acc : 0.828, Run Time : 0.70
INFO:root:2019-05-12 08:01:50, Epoch : 1, Step : 4908, Training Loss : 0.27646, Training Acc : 0.917, Run Time : 1.58
INFO:root:2019-05-12 08:02:01, Epoch : 1, Step : 4909, Training Loss : 0.29735, Training Acc : 0.872, Run Time : 10.64
INFO:root:2019-05-12 08:02:02, Epoch : 1, Step : 4910, Training Loss : 0.28772, Training Acc : 0.894, Run Time : 0.51
INFO:root:2019-05-12 08:02:03, Epoch : 1, Step : 4911, Training Loss : 0.38687, Training Acc : 0.794, Run Time : 1.74
INFO:root:2019-05-12 08:02:16, Epoch : 1, Step : 4912, Training Loss : 0.51199, Training Acc : 0.756, Run Time : 12.77
INFO:root:2019-05-12 08:02:17, Epoch : 1, Step : 4913, Training Loss : 0.46526, Training Acc : 0.806, Run Time : 0.81
INFO:root:2019-05-12 08:02:31, Epoch : 1, Step : 4914, Training Loss : 0.43924, Training Acc : 0.789, Run Time : 14.31
INFO:root:2019-05-12 08:02:32, Epoch : 1, Step : 4915, Training Loss : 0.55962, Training Acc : 0.806, Run Time : 1.04
INFO:root:2019-05-12 08:02:48, Epoch : 1, Step : 4916, Training Loss : 0.52786, Training Acc : 0.717, Run Time : 15.60
INFO:root:2019-05-12 08:02:49, Epoch : 1, Step : 4917, Training Loss : 0.31559, Training Acc : 0.850, Run Time : 1.61
INFO:root:2019-05-12 08:03:03, Epoch : 1, Step : 4918, Training Loss : 0.42492, Training Acc : 0.817, Run Time : 13.07
INFO:root:2019-05-12 08:03:08, Epoch : 1, Step : 4919, Training Loss : 0.68639, Training Acc : 0.661, Run Time : 5.79
INFO:root:2019-05-12 08:03:14, Epoch : 1, Step : 4920, Training Loss : 0.56756, Training Acc : 0.722, Run Time : 5.35
INFO:root:2019-05-12 08:03:14, Epoch : 1, Step : 4921, Training Loss : 0.38711, Training Acc : 0.844, Run Time : 0.61
INFO:root:2019-05-12 08:03:15, Epoch : 1, Step : 4922, Training Loss : 0.45848, Training Acc : 0.783, Run Time : 0.62
INFO:root:2019-05-12 08:03:19, Epoch : 1, Step : 4923, Training Loss : 0.54303, Training Acc : 0.683, Run Time : 3.92
INFO:root:2019-05-12 08:03:19, Epoch : 1, Step : 4924, Training Loss : 0.45388, Training Acc : 0.783, Run Time : 0.65
INFO:root:2019-05-12 08:03:24, Epoch : 1, Step : 4925, Training Loss : 0.30542, Training Acc : 0.872, Run Time : 4.27
INFO:root:2019-05-12 08:03:31, Epoch : 1, Step : 4926, Training Loss : 0.40781, Training Acc : 0.806, Run Time : 7.37
INFO:root:2019-05-12 08:03:32, Epoch : 1, Step : 4927, Training Loss : 0.35480, Training Acc : 0.844, Run Time : 1.04
INFO:root:2019-05-12 08:03:45, Epoch : 1, Step : 4928, Training Loss : 0.29347, Training Acc : 0.900, Run Time : 12.39
INFO:root:2019-05-12 08:03:45, Epoch : 1, Step : 4929, Training Loss : 0.31066, Training Acc : 0.883, Run Time : 0.50
INFO:root:2019-05-12 08:03:47, Epoch : 1, Step : 4930, Training Loss : 0.53776, Training Acc : 0.789, Run Time : 2.43
INFO:root:2019-05-12 08:03:58, Epoch : 1, Step : 4931, Training Loss : 0.45944, Training Acc : 0.811, Run Time : 10.45
INFO:root:2019-05-12 08:03:58, Epoch : 1, Step : 4932, Training Loss : 0.56985, Training Acc : 0.728, Run Time : 0.57
INFO:root:2019-05-12 08:03:59, Epoch : 1, Step : 4933, Training Loss : 0.49511, Training Acc : 0.794, Run Time : 0.59
INFO:root:2019-05-12 08:04:12, Epoch : 1, Step : 4934, Training Loss : 0.39377, Training Acc : 0.850, Run Time : 12.62
INFO:root:2019-05-12 08:04:12, Epoch : 1, Step : 4935, Training Loss : 0.74636, Training Acc : 0.522, Run Time : 0.76
INFO:root:2019-05-12 08:04:15, Epoch : 1, Step : 4936, Training Loss : 0.61521, Training Acc : 0.678, Run Time : 2.13
INFO:root:2019-05-12 08:04:26, Epoch : 1, Step : 4937, Training Loss : 0.68667, Training Acc : 0.606, Run Time : 11.36
INFO:root:2019-05-12 08:04:27, Epoch : 1, Step : 4938, Training Loss : 0.65403, Training Acc : 0.589, Run Time : 1.18
INFO:root:2019-05-12 08:04:29, Epoch : 1, Step : 4939, Training Loss : 0.66147, Training Acc : 0.628, Run Time : 1.57
INFO:root:2019-05-12 08:04:38, Epoch : 1, Step : 4940, Training Loss : 0.63887, Training Acc : 0.706, Run Time : 8.97
INFO:root:2019-05-12 08:04:38, Epoch : 1, Step : 4941, Training Loss : 0.58814, Training Acc : 0.733, Run Time : 0.53
INFO:root:2019-05-12 08:04:39, Epoch : 1, Step : 4942, Training Loss : 0.62088, Training Acc : 0.700, Run Time : 0.64
INFO:root:2019-05-12 08:04:48, Epoch : 1, Step : 4943, Training Loss : 0.36950, Training Acc : 0.811, Run Time : 8.73
INFO:root:2019-05-12 08:04:49, Epoch : 1, Step : 4944, Training Loss : 0.50922, Training Acc : 0.817, Run Time : 1.34
INFO:root:2019-05-12 08:04:50, Epoch : 1, Step : 4945, Training Loss : 0.62606, Training Acc : 0.750, Run Time : 0.65
INFO:root:2019-05-12 08:05:03, Epoch : 1, Step : 4946, Training Loss : 0.42909, Training Acc : 0.772, Run Time : 13.94
INFO:root:2019-05-12 08:05:05, Epoch : 1, Step : 4947, Training Loss : 0.39619, Training Acc : 0.867, Run Time : 1.19
INFO:root:2019-05-12 08:05:16, Epoch : 1, Step : 4948, Training Loss : 0.59236, Training Acc : 0.667, Run Time : 11.64
INFO:root:2019-05-12 08:05:17, Epoch : 1, Step : 4949, Training Loss : 0.39333, Training Acc : 0.794, Run Time : 0.88
INFO:root:2019-05-12 08:05:32, Epoch : 1, Step : 4950, Training Loss : 0.49851, Training Acc : 0.750, Run Time : 14.60
INFO:root:2019-05-12 08:05:33, Epoch : 1, Step : 4951, Training Loss : 0.59510, Training Acc : 0.706, Run Time : 0.86
INFO:root:2019-05-12 08:05:42, Epoch : 1, Step : 4952, Training Loss : 0.39300, Training Acc : 0.811, Run Time : 9.51
INFO:root:2019-05-12 08:05:43, Epoch : 1, Step : 4953, Training Loss : 0.45306, Training Acc : 0.772, Run Time : 1.01
INFO:root:2019-05-12 08:05:45, Epoch : 1, Step : 4954, Training Loss : 0.45180, Training Acc : 0.778, Run Time : 1.31
INFO:root:2019-05-12 08:05:45, Epoch : 1, Step : 4955, Training Loss : 0.38298, Training Acc : 0.817, Run Time : 0.66
INFO:root:2019-05-12 08:05:56, Epoch : 1, Step : 4956, Training Loss : 0.56579, Training Acc : 0.722, Run Time : 10.94
INFO:root:2019-05-12 08:05:57, Epoch : 1, Step : 4957, Training Loss : 0.34111, Training Acc : 0.828, Run Time : 1.03
INFO:root:2019-05-12 08:06:08, Epoch : 1, Step : 4958, Training Loss : 0.41439, Training Acc : 0.778, Run Time : 10.85
INFO:root:2019-05-12 08:06:09, Epoch : 1, Step : 4959, Training Loss : 0.44947, Training Acc : 0.806, Run Time : 0.89
INFO:root:2019-05-12 08:06:09, Epoch : 1, Step : 4960, Training Loss : 0.46939, Training Acc : 0.761, Run Time : 0.54
INFO:root:2019-05-12 08:06:21, Epoch : 1, Step : 4961, Training Loss : 0.44859, Training Acc : 0.744, Run Time : 11.11
INFO:root:2019-05-12 08:06:21, Epoch : 1, Step : 4962, Training Loss : 0.36394, Training Acc : 0.806, Run Time : 0.64
INFO:root:2019-05-12 08:06:23, Epoch : 1, Step : 4963, Training Loss : 0.53777, Training Acc : 0.739, Run Time : 1.79
INFO:root:2019-05-12 08:06:34, Epoch : 1, Step : 4964, Training Loss : 0.35546, Training Acc : 0.839, Run Time : 11.31
INFO:root:2019-05-12 08:06:35, Epoch : 1, Step : 4965, Training Loss : 0.58678, Training Acc : 0.700, Run Time : 1.23
INFO:root:2019-05-12 08:06:46, Epoch : 1, Step : 4966, Training Loss : 0.47893, Training Acc : 0.800, Run Time : 10.59
INFO:root:2019-05-12 08:06:47, Epoch : 1, Step : 4967, Training Loss : 0.47796, Training Acc : 0.728, Run Time : 0.43
INFO:root:2019-05-12 08:06:48, Epoch : 1, Step : 4968, Training Loss : 0.64551, Training Acc : 0.672, Run Time : 1.06
INFO:root:2019-05-12 08:07:00, Epoch : 1, Step : 4969, Training Loss : 0.49170, Training Acc : 0.767, Run Time : 12.53
INFO:root:2019-05-12 08:07:01, Epoch : 1, Step : 4970, Training Loss : 0.31162, Training Acc : 0.878, Run Time : 1.28
INFO:root:2019-05-12 08:07:04, Epoch : 1, Step : 4971, Training Loss : 0.36336, Training Acc : 0.811, Run Time : 2.19
INFO:root:2019-05-12 08:07:14, Epoch : 1, Step : 4972, Training Loss : 0.61696, Training Acc : 0.689, Run Time : 10.84
INFO:root:2019-05-12 08:07:15, Epoch : 1, Step : 4973, Training Loss : 0.47533, Training Acc : 0.850, Run Time : 0.45
INFO:root:2019-05-12 08:07:15, Epoch : 1, Step : 4974, Training Loss : 0.46236, Training Acc : 0.756, Run Time : 0.62
INFO:root:2019-05-12 08:07:29, Epoch : 1, Step : 4975, Training Loss : 0.40898, Training Acc : 0.778, Run Time : 13.16
INFO:root:2019-05-12 08:07:30, Epoch : 1, Step : 4976, Training Loss : 0.37806, Training Acc : 0.850, Run Time : 0.95
INFO:root:2019-05-12 08:07:30, Epoch : 1, Step : 4977, Training Loss : 0.40709, Training Acc : 0.811, Run Time : 0.67
INFO:root:2019-05-12 08:07:47, Epoch : 1, Step : 4978, Training Loss : 0.51097, Training Acc : 0.811, Run Time : 17.07
INFO:root:2019-05-12 08:08:02, Epoch : 1, Step : 4979, Training Loss : 0.58284, Training Acc : 0.739, Run Time : 14.44
INFO:root:2019-05-12 08:08:11, Epoch : 1, Step : 4980, Training Loss : 0.67624, Training Acc : 0.628, Run Time : 9.53
INFO:root:2019-05-12 08:08:12, Epoch : 1, Step : 4981, Training Loss : 0.54784, Training Acc : 0.711, Run Time : 0.81
INFO:root:2019-05-12 08:08:13, Epoch : 1, Step : 4982, Training Loss : 0.54908, Training Acc : 0.711, Run Time : 1.18
INFO:root:2019-05-12 08:08:23, Epoch : 1, Step : 4983, Training Loss : 0.44982, Training Acc : 0.750, Run Time : 9.41
INFO:root:2019-05-12 08:08:24, Epoch : 1, Step : 4984, Training Loss : 0.54628, Training Acc : 0.733, Run Time : 1.05
INFO:root:2019-05-12 08:08:34, Epoch : 1, Step : 4985, Training Loss : 0.52896, Training Acc : 0.750, Run Time : 10.62
INFO:root:2019-05-12 08:08:35, Epoch : 1, Step : 4986, Training Loss : 0.44094, Training Acc : 0.722, Run Time : 0.67
INFO:root:2019-05-12 08:08:36, Epoch : 1, Step : 4987, Training Loss : 0.42317, Training Acc : 0.806, Run Time : 0.63
INFO:root:2019-05-12 08:08:50, Epoch : 1, Step : 4988, Training Loss : 0.45025, Training Acc : 0.750, Run Time : 14.42
INFO:root:2019-05-12 08:08:50, Epoch : 1, Step : 4989, Training Loss : 0.46046, Training Acc : 0.750, Run Time : 0.42
INFO:root:2019-05-12 08:08:51, Epoch : 1, Step : 4990, Training Loss : 0.61988, Training Acc : 0.711, Run Time : 0.75
INFO:root:2019-05-12 08:09:08, Epoch : 1, Step : 4991, Training Loss : 0.85418, Training Acc : 0.578, Run Time : 16.94
INFO:root:2019-05-12 08:09:15, Epoch : 1, Step : 4992, Training Loss : 0.51709, Training Acc : 0.739, Run Time : 6.36
INFO:root:2019-05-12 08:09:15, Epoch : 1, Step : 4993, Training Loss : 0.52302, Training Acc : 0.733, Run Time : 0.44
INFO:root:2019-05-12 08:09:17, Epoch : 1, Step : 4994, Training Loss : 0.58422, Training Acc : 0.733, Run Time : 1.73
INFO:root:2019-05-12 08:09:27, Epoch : 1, Step : 4995, Training Loss : 0.66151, Training Acc : 0.628, Run Time : 10.29
INFO:root:2019-05-12 08:09:28, Epoch : 1, Step : 4996, Training Loss : 0.66515, Training Acc : 0.678, Run Time : 0.59
INFO:root:2019-05-12 08:09:29, Epoch : 1, Step : 4997, Training Loss : 0.75552, Training Acc : 0.622, Run Time : 1.82
INFO:root:2019-05-12 08:09:39, Epoch : 1, Step : 4998, Training Loss : 0.60538, Training Acc : 0.722, Run Time : 9.86
INFO:root:2019-05-12 08:09:40, Epoch : 1, Step : 4999, Training Loss : 0.63480, Training Acc : 0.617, Run Time : 0.46
INFO:root:2019-05-12 08:09:42, Epoch : 1, Step : 5000, Training Loss : 0.52839, Training Acc : 0.733, Run Time : 1.79
INFO:root:2019-05-12 08:09:57, Epoch : 1, Step : 5001, Training Loss : 0.97389, Training Acc : 0.439, Run Time : 15.03
INFO:root:2019-05-12 08:09:59, Epoch : 1, Step : 5002, Training Loss : 0.61853, Training Acc : 0.650, Run Time : 2.00
INFO:root:2019-05-12 08:10:12, Epoch : 1, Step : 5003, Training Loss : 0.71135, Training Acc : 0.672, Run Time : 13.39
INFO:root:2019-05-12 08:10:25, Epoch : 1, Step : 5004, Training Loss : 0.79715, Training Acc : 0.556, Run Time : 13.03
INFO:root:2019-05-12 08:10:34, Epoch : 1, Step : 5005, Training Loss : 0.74900, Training Acc : 0.606, Run Time : 8.81
INFO:root:2019-05-12 08:10:37, Epoch : 1, Step : 5006, Training Loss : 0.98900, Training Acc : 0.489, Run Time : 2.84
INFO:root:2019-05-12 08:10:37, Epoch : 1, Step : 5007, Training Loss : 0.91816, Training Acc : 0.422, Run Time : 0.40
INFO:root:2019-05-12 08:10:46, Epoch : 1, Step : 5008, Training Loss : 0.65444, Training Acc : 0.644, Run Time : 8.62
INFO:root:2019-05-12 08:10:46, Epoch : 1, Step : 5009, Training Loss : 0.77683, Training Acc : 0.544, Run Time : 0.65
INFO:root:2019-05-12 08:10:47, Epoch : 1, Step : 5010, Training Loss : 0.85542, Training Acc : 0.489, Run Time : 0.71
INFO:root:2019-05-12 08:10:48, Epoch : 1, Step : 5011, Training Loss : 0.75979, Training Acc : 0.617, Run Time : 0.63
INFO:root:2019-05-12 08:11:01, Epoch : 1, Step : 5012, Training Loss : 0.74846, Training Acc : 0.594, Run Time : 13.08
INFO:root:2019-05-12 08:11:01, Epoch : 1, Step : 5013, Training Loss : 0.89107, Training Acc : 0.478, Run Time : 0.63
INFO:root:2019-05-12 08:11:02, Epoch : 1, Step : 5014, Training Loss : 0.69420, Training Acc : 0.639, Run Time : 0.61
INFO:root:2019-05-12 08:11:04, Epoch : 1, Step : 5015, Training Loss : 0.68405, Training Acc : 0.656, Run Time : 2.44
INFO:root:2019-05-12 08:11:06, Epoch : 1, Step : 5016, Training Loss : 0.65369, Training Acc : 0.533, Run Time : 1.66
INFO:root:2019-05-12 08:11:14, Epoch : 1, Step : 5017, Training Loss : 0.59223, Training Acc : 0.683, Run Time : 7.71
INFO:root:2019-05-12 08:11:14, Epoch : 1, Step : 5018, Training Loss : 0.56965, Training Acc : 0.700, Run Time : 0.70
INFO:root:2019-05-12 08:11:16, Epoch : 1, Step : 5019, Training Loss : 0.56849, Training Acc : 0.728, Run Time : 1.27
INFO:root:2019-05-12 08:11:31, Epoch : 1, Step : 5020, Training Loss : 0.55432, Training Acc : 0.717, Run Time : 15.30
INFO:root:2019-05-12 08:11:42, Epoch : 1, Step : 5021, Training Loss : 0.63398, Training Acc : 0.617, Run Time : 10.66
INFO:root:2019-05-12 08:11:42, Epoch : 1, Step : 5022, Training Loss : 0.52753, Training Acc : 0.750, Run Time : 0.83
INFO:root:2019-05-12 08:11:43, Epoch : 1, Step : 5023, Training Loss : 0.67296, Training Acc : 0.622, Run Time : 0.65
INFO:root:2019-05-12 08:11:44, Epoch : 1, Step : 5024, Training Loss : 0.65417, Training Acc : 0.650, Run Time : 1.36
INFO:root:2019-05-12 08:11:54, Epoch : 1, Step : 5025, Training Loss : 0.53378, Training Acc : 0.733, Run Time : 9.31
INFO:root:2019-05-12 08:11:54, Epoch : 1, Step : 5026, Training Loss : 0.53919, Training Acc : 0.706, Run Time : 0.55
INFO:root:2019-05-12 08:11:55, Epoch : 1, Step : 5027, Training Loss : 0.55512, Training Acc : 0.689, Run Time : 0.57
INFO:root:2019-05-12 08:11:56, Epoch : 1, Step : 5028, Training Loss : 0.59147, Training Acc : 0.644, Run Time : 0.98
INFO:root:2019-05-12 08:12:07, Epoch : 1, Step : 5029, Training Loss : 0.49594, Training Acc : 0.744, Run Time : 11.39
INFO:root:2019-05-12 08:12:19, Epoch : 1, Step : 5030, Training Loss : 0.60110, Training Acc : 0.644, Run Time : 12.01
INFO:root:2019-05-12 08:12:20, Epoch : 1, Step : 5031, Training Loss : 0.55108, Training Acc : 0.672, Run Time : 0.52
INFO:root:2019-05-12 08:12:22, Epoch : 1, Step : 5032, Training Loss : 0.56787, Training Acc : 0.683, Run Time : 1.70
INFO:root:2019-05-12 08:12:32, Epoch : 1, Step : 5033, Training Loss : 0.53249, Training Acc : 0.733, Run Time : 10.14
INFO:root:2019-05-12 08:12:32, Epoch : 1, Step : 5034, Training Loss : 0.58788, Training Acc : 0.672, Run Time : 0.58
INFO:root:2019-05-12 08:12:45, Epoch : 1, Step : 5035, Training Loss : 0.42786, Training Acc : 0.844, Run Time : 12.79
INFO:root:2019-05-12 08:12:46, Epoch : 1, Step : 5036, Training Loss : 0.57166, Training Acc : 0.700, Run Time : 1.15
INFO:root:2019-05-12 08:13:04, Epoch : 1, Step : 5037, Training Loss : 0.45257, Training Acc : 0.794, Run Time : 18.18
INFO:root:2019-05-12 08:13:05, Epoch : 1, Step : 5038, Training Loss : 0.38512, Training Acc : 0.867, Run Time : 0.51
INFO:root:2019-05-12 08:13:05, Epoch : 1, Step : 5039, Training Loss : 0.45291, Training Acc : 0.839, Run Time : 0.56
INFO:root:2019-05-12 08:13:18, Epoch : 1, Step : 5040, Training Loss : 0.47185, Training Acc : 0.756, Run Time : 12.08
INFO:root:2019-05-12 08:13:18, Epoch : 1, Step : 5041, Training Loss : 0.52043, Training Acc : 0.794, Run Time : 0.99
INFO:root:2019-05-12 08:13:20, Epoch : 1, Step : 5042, Training Loss : 0.41856, Training Acc : 0.828, Run Time : 1.35
INFO:root:2019-05-12 08:13:33, Epoch : 1, Step : 5043, Training Loss : 0.30131, Training Acc : 0.944, Run Time : 12.82
INFO:root:2019-05-12 08:13:33, Epoch : 1, Step : 5044, Training Loss : 0.58321, Training Acc : 0.733, Run Time : 0.49
INFO:root:2019-05-12 08:13:35, Epoch : 1, Step : 5045, Training Loss : 0.65448, Training Acc : 0.700, Run Time : 1.46
INFO:root:2019-05-12 08:13:44, Epoch : 1, Step : 5046, Training Loss : 0.35111, Training Acc : 0.883, Run Time : 9.09
INFO:root:2019-05-12 08:13:45, Epoch : 1, Step : 5047, Training Loss : 0.36083, Training Acc : 0.900, Run Time : 0.83
INFO:root:2019-05-12 08:13:58, Epoch : 1, Step : 5048, Training Loss : 0.51370, Training Acc : 0.722, Run Time : 13.47
INFO:root:2019-05-12 08:13:59, Epoch : 1, Step : 5049, Training Loss : 0.63967, Training Acc : 0.772, Run Time : 0.85
INFO:root:2019-05-12 08:14:00, Epoch : 1, Step : 5050, Training Loss : 0.42265, Training Acc : 0.817, Run Time : 1.13
INFO:root:2019-05-12 08:14:11, Epoch : 1, Step : 5051, Training Loss : 0.55799, Training Acc : 0.744, Run Time : 11.12
INFO:root:2019-05-12 08:14:12, Epoch : 1, Step : 5052, Training Loss : 0.32584, Training Acc : 0.906, Run Time : 0.81
INFO:root:2019-05-12 08:14:25, Epoch : 1, Step : 5053, Training Loss : 0.47950, Training Acc : 0.767, Run Time : 12.67
INFO:root:2019-05-12 08:14:25, Epoch : 1, Step : 5054, Training Loss : 0.36265, Training Acc : 0.872, Run Time : 0.71
INFO:root:2019-05-12 08:14:26, Epoch : 1, Step : 5055, Training Loss : 0.36132, Training Acc : 0.878, Run Time : 1.08
INFO:root:2019-05-12 08:14:39, Epoch : 1, Step : 5056, Training Loss : 0.30852, Training Acc : 0.872, Run Time : 12.44
INFO:root:2019-05-12 08:14:39, Epoch : 1, Step : 5057, Training Loss : 0.49481, Training Acc : 0.733, Run Time : 0.60
INFO:root:2019-05-12 08:14:46, Epoch : 1, Step : 5058, Training Loss : 0.35417, Training Acc : 0.867, Run Time : 6.66
INFO:root:2019-05-12 08:14:49, Epoch : 1, Step : 5059, Training Loss : 0.40810, Training Acc : 0.833, Run Time : 2.47
INFO:root:2019-05-12 08:14:50, Epoch : 1, Step : 5060, Training Loss : 0.36930, Training Acc : 0.811, Run Time : 1.08
INFO:root:2019-05-12 08:14:53, Epoch : 1, Step : 5061, Training Loss : 0.32191, Training Acc : 0.872, Run Time : 2.95
INFO:root:2019-05-12 08:14:53, Epoch : 1, Step : 5062, Training Loss : 0.36538, Training Acc : 0.878, Run Time : 0.56
INFO:root:2019-05-12 08:15:02, Epoch : 1, Step : 5063, Training Loss : 0.34157, Training Acc : 0.867, Run Time : 9.27
INFO:root:2019-05-12 08:15:04, Epoch : 1, Step : 5064, Training Loss : 0.67776, Training Acc : 0.728, Run Time : 1.79
INFO:root:2019-05-12 08:15:14, Epoch : 1, Step : 5065, Training Loss : 0.37347, Training Acc : 0.839, Run Time : 9.43
INFO:root:2019-05-12 08:15:14, Epoch : 1, Step : 5066, Training Loss : 0.35779, Training Acc : 0.894, Run Time : 0.67
INFO:root:2019-05-12 08:15:16, Epoch : 1, Step : 5067, Training Loss : 0.31730, Training Acc : 0.839, Run Time : 1.63
INFO:root:2019-05-12 08:15:27, Epoch : 1, Step : 5068, Training Loss : 0.27641, Training Acc : 0.917, Run Time : 10.99
INFO:root:2019-05-12 08:15:38, Epoch : 1, Step : 5069, Training Loss : 0.28521, Training Acc : 0.867, Run Time : 11.35
INFO:root:2019-05-12 08:15:40, Epoch : 1, Step : 5070, Training Loss : 0.29214, Training Acc : 0.889, Run Time : 1.80
INFO:root:2019-05-12 08:15:54, Epoch : 1, Step : 5071, Training Loss : 0.27863, Training Acc : 0.861, Run Time : 13.44
INFO:root:2019-05-12 08:15:54, Epoch : 1, Step : 5072, Training Loss : 0.25825, Training Acc : 0.939, Run Time : 0.49
INFO:root:2019-05-12 08:15:55, Epoch : 1, Step : 5073, Training Loss : 0.39835, Training Acc : 0.856, Run Time : 1.15
INFO:root:2019-05-12 08:16:07, Epoch : 1, Step : 5074, Training Loss : 0.30396, Training Acc : 0.883, Run Time : 12.28
INFO:root:2019-05-12 08:16:08, Epoch : 1, Step : 5075, Training Loss : 0.25493, Training Acc : 0.917, Run Time : 0.43
INFO:root:2019-05-12 08:16:10, Epoch : 1, Step : 5076, Training Loss : 0.35896, Training Acc : 0.839, Run Time : 1.71
INFO:root:2019-05-12 08:16:25, Epoch : 1, Step : 5077, Training Loss : 0.70403, Training Acc : 0.611, Run Time : 15.29
INFO:root:2019-05-12 08:16:26, Epoch : 1, Step : 5078, Training Loss : 0.32843, Training Acc : 0.856, Run Time : 1.37
INFO:root:2019-05-12 08:16:37, Epoch : 1, Step : 5079, Training Loss : 0.41609, Training Acc : 0.844, Run Time : 10.34
INFO:root:2019-05-12 08:16:37, Epoch : 1, Step : 5080, Training Loss : 0.31308, Training Acc : 0.828, Run Time : 0.86
INFO:root:2019-05-12 08:16:39, Epoch : 1, Step : 5081, Training Loss : 0.32537, Training Acc : 0.856, Run Time : 1.80
INFO:root:2019-05-12 08:16:50, Epoch : 1, Step : 5082, Training Loss : 0.25653, Training Acc : 0.906, Run Time : 10.65
INFO:root:2019-05-12 08:16:50, Epoch : 1, Step : 5083, Training Loss : 0.40603, Training Acc : 0.817, Run Time : 0.41
INFO:root:2019-05-12 08:16:52, Epoch : 1, Step : 5084, Training Loss : 0.67420, Training Acc : 0.667, Run Time : 1.95
INFO:root:2019-05-12 08:17:04, Epoch : 1, Step : 5085, Training Loss : 0.42457, Training Acc : 0.750, Run Time : 12.27
INFO:root:2019-05-12 08:17:05, Epoch : 1, Step : 5086, Training Loss : 0.36667, Training Acc : 0.839, Run Time : 0.86
INFO:root:2019-05-12 08:17:17, Epoch : 1, Step : 5087, Training Loss : 0.49635, Training Acc : 0.778, Run Time : 11.74
INFO:root:2019-05-12 08:17:18, Epoch : 1, Step : 5088, Training Loss : 0.33491, Training Acc : 0.822, Run Time : 1.08
INFO:root:2019-05-12 08:17:20, Epoch : 1, Step : 5089, Training Loss : 0.35635, Training Acc : 0.839, Run Time : 1.92
INFO:root:2019-05-12 08:17:32, Epoch : 1, Step : 5090, Training Loss : 0.27164, Training Acc : 0.900, Run Time : 12.12
INFO:root:2019-05-12 08:17:33, Epoch : 1, Step : 5091, Training Loss : 0.28317, Training Acc : 0.861, Run Time : 0.55
INFO:root:2019-05-12 08:17:45, Epoch : 1, Step : 5092, Training Loss : 0.21361, Training Acc : 0.950, Run Time : 12.74
INFO:root:2019-05-12 08:17:46, Epoch : 1, Step : 5093, Training Loss : 0.41110, Training Acc : 0.811, Run Time : 0.74
INFO:root:2019-05-12 08:17:58, Epoch : 1, Step : 5094, Training Loss : 0.42692, Training Acc : 0.789, Run Time : 11.28
INFO:root:2019-05-12 08:17:58, Epoch : 1, Step : 5095, Training Loss : 0.29425, Training Acc : 0.917, Run Time : 0.95
INFO:root:2019-05-12 08:17:59, Epoch : 1, Step : 5096, Training Loss : 0.22524, Training Acc : 0.922, Run Time : 0.60
INFO:root:2019-05-12 08:18:11, Epoch : 1, Step : 5097, Training Loss : 0.43243, Training Acc : 0.783, Run Time : 11.46
INFO:root:2019-05-12 08:18:12, Epoch : 1, Step : 5098, Training Loss : 0.34699, Training Acc : 0.844, Run Time : 1.90
INFO:root:2019-05-12 08:18:23, Epoch : 1, Step : 5099, Training Loss : 0.30716, Training Acc : 0.867, Run Time : 10.86
INFO:root:2019-05-12 08:18:24, Epoch : 1, Step : 5100, Training Loss : 0.38747, Training Acc : 0.856, Run Time : 0.46
INFO:root:2019-05-12 08:18:36, Epoch : 1, Step : 5101, Training Loss : 0.54848, Training Acc : 0.661, Run Time : 12.71
INFO:root:2019-05-12 08:18:37, Epoch : 1, Step : 5102, Training Loss : 0.47374, Training Acc : 0.711, Run Time : 0.86
INFO:root:2019-05-12 08:18:50, Epoch : 1, Step : 5103, Training Loss : 0.47111, Training Acc : 0.756, Run Time : 13.05
INFO:root:2019-05-12 08:18:51, Epoch : 1, Step : 5104, Training Loss : 0.32586, Training Acc : 0.878, Run Time : 0.95
INFO:root:2019-05-12 08:19:02, Epoch : 1, Step : 5105, Training Loss : 0.40700, Training Acc : 0.833, Run Time : 11.04
INFO:root:2019-05-12 08:19:03, Epoch : 1, Step : 5106, Training Loss : 0.42566, Training Acc : 0.828, Run Time : 0.81
INFO:root:2019-05-12 08:19:15, Epoch : 1, Step : 5107, Training Loss : 0.44035, Training Acc : 0.767, Run Time : 11.59
INFO:root:2019-05-12 08:19:15, Epoch : 1, Step : 5108, Training Loss : 0.51836, Training Acc : 0.744, Run Time : 0.71
INFO:root:2019-05-12 08:19:16, Epoch : 1, Step : 5109, Training Loss : 0.45830, Training Acc : 0.778, Run Time : 0.60
INFO:root:2019-05-12 08:19:18, Epoch : 1, Step : 5110, Training Loss : 0.43651, Training Acc : 0.778, Run Time : 1.58
INFO:root:2019-05-12 08:19:30, Epoch : 1, Step : 5111, Training Loss : 0.44492, Training Acc : 0.789, Run Time : 12.47
INFO:root:2019-05-12 08:19:31, Epoch : 1, Step : 5112, Training Loss : 0.46339, Training Acc : 0.756, Run Time : 0.54
INFO:root:2019-05-12 08:19:31, Epoch : 1, Step : 5113, Training Loss : 0.33806, Training Acc : 0.839, Run Time : 0.59
INFO:root:2019-05-12 08:19:49, Epoch : 1, Step : 5114, Training Loss : 0.53607, Training Acc : 0.772, Run Time : 18.16
INFO:root:2019-05-12 08:20:10, Epoch : 1, Step : 5115, Training Loss : 0.54783, Training Acc : 0.694, Run Time : 20.57
INFO:root:2019-05-12 08:20:17, Epoch : 1, Step : 5116, Training Loss : 0.51735, Training Acc : 0.700, Run Time : 7.05
INFO:root:2019-05-12 08:20:31, Epoch : 1, Step : 5117, Training Loss : 0.49524, Training Acc : 0.728, Run Time : 13.52
INFO:root:2019-05-12 08:20:33, Epoch : 1, Step : 5118, Training Loss : 0.47163, Training Acc : 0.756, Run Time : 2.20
INFO:root:2019-05-12 08:20:41, Epoch : 1, Step : 5119, Training Loss : 0.27528, Training Acc : 0.900, Run Time : 7.81
INFO:root:2019-05-12 08:20:41, Epoch : 1, Step : 5120, Training Loss : 0.60480, Training Acc : 0.667, Run Time : 0.69
INFO:root:2019-05-12 08:20:42, Epoch : 1, Step : 5121, Training Loss : 0.44790, Training Acc : 0.800, Run Time : 1.03
INFO:root:2019-05-12 08:20:56, Epoch : 1, Step : 5122, Training Loss : 0.29256, Training Acc : 0.861, Run Time : 13.59
INFO:root:2019-05-12 08:21:10, Epoch : 1, Step : 5123, Training Loss : 0.52555, Training Acc : 0.756, Run Time : 14.41
INFO:root:2019-05-12 08:21:12, Epoch : 1, Step : 5124, Training Loss : 0.49673, Training Acc : 0.750, Run Time : 1.59
INFO:root:2019-05-12 08:21:14, Epoch : 1, Step : 5125, Training Loss : 0.28636, Training Acc : 0.894, Run Time : 1.86
INFO:root:2019-05-12 08:21:24, Epoch : 1, Step : 5126, Training Loss : 0.57818, Training Acc : 0.728, Run Time : 10.63
INFO:root:2019-05-12 08:21:25, Epoch : 1, Step : 5127, Training Loss : 0.42197, Training Acc : 0.822, Run Time : 1.16
INFO:root:2019-05-12 08:21:37, Epoch : 1, Step : 5128, Training Loss : 0.98311, Training Acc : 0.517, Run Time : 11.12
INFO:root:2019-05-12 08:21:37, Epoch : 1, Step : 5129, Training Loss : 0.64522, Training Acc : 0.633, Run Time : 0.76
INFO:root:2019-05-12 08:21:39, Epoch : 1, Step : 5130, Training Loss : 0.66105, Training Acc : 0.789, Run Time : 1.85
INFO:root:2019-05-12 08:21:52, Epoch : 1, Step : 5131, Training Loss : 0.92276, Training Acc : 0.533, Run Time : 13.04
INFO:root:2019-05-12 08:21:55, Epoch : 1, Step : 5132, Training Loss : 0.58998, Training Acc : 0.689, Run Time : 2.42
INFO:root:2019-05-12 08:22:07, Epoch : 1, Step : 5133, Training Loss : 0.59521, Training Acc : 0.722, Run Time : 12.48
INFO:root:2019-05-12 08:22:31, Epoch : 1, Step : 5134, Training Loss : 0.45630, Training Acc : 0.817, Run Time : 24.28
INFO:root:2019-05-12 08:22:35, Epoch : 1, Step : 5135, Training Loss : 0.70547, Training Acc : 0.672, Run Time : 3.17
INFO:root:2019-05-12 08:22:35, Epoch : 1, Step : 5136, Training Loss : 0.50363, Training Acc : 0.811, Run Time : 0.68
INFO:root:2019-05-12 08:22:44, Epoch : 1, Step : 5137, Training Loss : 0.30086, Training Acc : 0.883, Run Time : 8.59
INFO:root:2019-05-12 08:22:45, Epoch : 1, Step : 5138, Training Loss : 0.31104, Training Acc : 0.861, Run Time : 0.98
INFO:root:2019-05-12 08:22:59, Epoch : 1, Step : 5139, Training Loss : 0.29553, Training Acc : 0.911, Run Time : 13.77
INFO:root:2019-05-12 08:23:00, Epoch : 1, Step : 5140, Training Loss : 0.61208, Training Acc : 0.633, Run Time : 1.86
INFO:root:2019-05-12 08:23:01, Epoch : 1, Step : 5141, Training Loss : 0.36152, Training Acc : 0.822, Run Time : 0.61
INFO:root:2019-05-12 08:23:03, Epoch : 1, Step : 5142, Training Loss : 0.47159, Training Acc : 0.783, Run Time : 1.97
INFO:root:2019-05-12 08:23:14, Epoch : 1, Step : 5143, Training Loss : 0.42146, Training Acc : 0.800, Run Time : 10.95
INFO:root:2019-05-12 08:23:15, Epoch : 1, Step : 5144, Training Loss : 0.72265, Training Acc : 0.622, Run Time : 0.77
INFO:root:2019-05-12 08:23:31, Epoch : 1, Step : 5145, Training Loss : 0.34853, Training Acc : 0.872, Run Time : 16.46
INFO:root:2019-05-12 08:23:33, Epoch : 1, Step : 5146, Training Loss : 0.35861, Training Acc : 0.828, Run Time : 1.96
INFO:root:2019-05-12 08:23:36, Epoch : 1, Step : 5147, Training Loss : 0.30473, Training Acc : 0.911, Run Time : 2.39
INFO:root:2019-05-12 08:23:51, Epoch : 1, Step : 5148, Training Loss : 0.32533, Training Acc : 0.867, Run Time : 15.49
INFO:root:2019-05-12 08:24:03, Epoch : 1, Step : 5149, Training Loss : 0.39890, Training Acc : 0.844, Run Time : 11.80
INFO:root:2019-05-12 08:24:05, Epoch : 1, Step : 5150, Training Loss : 0.30306, Training Acc : 0.889, Run Time : 2.10
INFO:root:2019-05-12 08:24:06, Epoch : 1, Step : 5151, Training Loss : 0.28900, Training Acc : 0.911, Run Time : 1.42
INFO:root:2019-05-12 08:24:17, Epoch : 1, Step : 5152, Training Loss : 0.39094, Training Acc : 0.822, Run Time : 10.17
INFO:root:2019-05-12 08:24:17, Epoch : 1, Step : 5153, Training Loss : 0.35023, Training Acc : 0.856, Run Time : 0.61
INFO:root:2019-05-12 08:24:18, Epoch : 1, Step : 5154, Training Loss : 0.37459, Training Acc : 0.839, Run Time : 0.43
INFO:root:2019-05-12 08:24:33, Epoch : 1, Step : 5155, Training Loss : 0.28117, Training Acc : 0.894, Run Time : 15.02
INFO:root:2019-05-12 08:24:33, Epoch : 1, Step : 5156, Training Loss : 0.43583, Training Acc : 0.783, Run Time : 0.60
INFO:root:2019-05-12 08:24:34, Epoch : 1, Step : 5157, Training Loss : 0.42758, Training Acc : 0.767, Run Time : 0.64
INFO:root:2019-05-12 08:24:50, Epoch : 1, Step : 5158, Training Loss : 0.42630, Training Acc : 0.744, Run Time : 16.09
INFO:root:2019-05-12 08:25:04, Epoch : 1, Step : 5159, Training Loss : 0.68169, Training Acc : 0.672, Run Time : 14.00
INFO:root:2019-05-12 08:25:05, Epoch : 1, Step : 5160, Training Loss : 0.64010, Training Acc : 0.667, Run Time : 0.98
INFO:root:2019-05-12 08:25:13, Epoch : 1, Step : 5161, Training Loss : 0.44303, Training Acc : 0.794, Run Time : 7.69
INFO:root:2019-05-12 08:25:15, Epoch : 1, Step : 5162, Training Loss : 0.40641, Training Acc : 0.811, Run Time : 2.66
INFO:root:2019-05-12 08:25:16, Epoch : 1, Step : 5163, Training Loss : 0.58561, Training Acc : 0.717, Run Time : 0.73
INFO:root:2019-05-12 08:25:17, Epoch : 1, Step : 5164, Training Loss : 0.43665, Training Acc : 0.789, Run Time : 0.60
INFO:root:2019-05-12 08:25:17, Epoch : 1, Step : 5165, Training Loss : 0.33685, Training Acc : 0.856, Run Time : 0.63
INFO:root:2019-05-12 08:25:18, Epoch : 1, Step : 5166, Training Loss : 0.25658, Training Acc : 0.894, Run Time : 0.64
INFO:root:2019-05-12 08:25:35, Epoch : 1, Step : 5167, Training Loss : 0.29388, Training Acc : 0.878, Run Time : 17.23
INFO:root:2019-05-12 08:25:38, Epoch : 1, Step : 5168, Training Loss : 0.41975, Training Acc : 0.789, Run Time : 2.89
INFO:root:2019-05-12 08:25:51, Epoch : 1, Step : 5169, Training Loss : 0.40884, Training Acc : 0.783, Run Time : 12.55
INFO:root:2019-05-12 08:25:52, Epoch : 1, Step : 5170, Training Loss : 0.40111, Training Acc : 0.806, Run Time : 1.01
INFO:root:2019-05-12 08:26:05, Epoch : 1, Step : 5171, Training Loss : 0.42082, Training Acc : 0.822, Run Time : 13.10
INFO:root:2019-05-12 08:26:07, Epoch : 1, Step : 5172, Training Loss : 0.31180, Training Acc : 0.839, Run Time : 2.78
INFO:root:2019-05-12 08:26:19, Epoch : 1, Step : 5173, Training Loss : 0.29733, Training Acc : 0.867, Run Time : 11.42
INFO:root:2019-05-12 08:26:20, Epoch : 1, Step : 5174, Training Loss : 0.38596, Training Acc : 0.828, Run Time : 0.89
INFO:root:2019-05-12 08:26:35, Epoch : 1, Step : 5175, Training Loss : 0.27020, Training Acc : 0.911, Run Time : 15.38
INFO:root:2019-05-12 08:26:37, Epoch : 1, Step : 5176, Training Loss : 0.40838, Training Acc : 0.861, Run Time : 1.82
INFO:root:2019-05-12 08:26:38, Epoch : 1, Step : 5177, Training Loss : 0.39009, Training Acc : 0.817, Run Time : 0.62
INFO:root:2019-05-12 08:26:49, Epoch : 1, Step : 5178, Training Loss : 0.54818, Training Acc : 0.733, Run Time : 11.57
INFO:root:2019-05-12 08:26:51, Epoch : 1, Step : 5179, Training Loss : 0.36606, Training Acc : 0.850, Run Time : 1.41
INFO:root:2019-05-12 08:27:03, Epoch : 1, Step : 5180, Training Loss : 0.27798, Training Acc : 0.900, Run Time : 12.24
INFO:root:2019-05-12 08:27:04, Epoch : 1, Step : 5181, Training Loss : 0.45010, Training Acc : 0.767, Run Time : 0.74
INFO:root:2019-05-12 08:27:05, Epoch : 1, Step : 5182, Training Loss : 0.32621, Training Acc : 0.850, Run Time : 1.31
INFO:root:2019-05-12 08:27:15, Epoch : 1, Step : 5183, Training Loss : 0.43543, Training Acc : 0.783, Run Time : 10.49
INFO:root:2019-05-12 08:27:16, Epoch : 1, Step : 5184, Training Loss : 0.33477, Training Acc : 0.878, Run Time : 0.74
INFO:root:2019-05-12 08:27:29, Epoch : 1, Step : 5185, Training Loss : 0.35711, Training Acc : 0.839, Run Time : 12.75
INFO:root:2019-05-12 08:27:31, Epoch : 1, Step : 5186, Training Loss : 0.29074, Training Acc : 0.872, Run Time : 1.72
INFO:root:2019-05-12 08:27:31, Epoch : 1, Step : 5187, Training Loss : 0.33575, Training Acc : 0.839, Run Time : 0.62
INFO:root:2019-05-12 08:27:47, Epoch : 1, Step : 5188, Training Loss : 0.31724, Training Acc : 0.856, Run Time : 15.57
INFO:root:2019-05-12 08:27:47, Epoch : 1, Step : 5189, Training Loss : 0.27223, Training Acc : 0.928, Run Time : 0.69
INFO:root:2019-05-12 08:28:03, Epoch : 1, Step : 5190, Training Loss : 0.32285, Training Acc : 0.867, Run Time : 16.00
INFO:root:2019-05-12 08:28:05, Epoch : 1, Step : 5191, Training Loss : 0.42401, Training Acc : 0.800, Run Time : 1.36
INFO:root:2019-05-12 08:28:17, Epoch : 1, Step : 5192, Training Loss : 0.40758, Training Acc : 0.839, Run Time : 12.41
INFO:root:2019-05-12 08:28:18, Epoch : 1, Step : 5193, Training Loss : 0.36353, Training Acc : 0.806, Run Time : 1.02
INFO:root:2019-05-12 08:28:31, Epoch : 1, Step : 5194, Training Loss : 0.33886, Training Acc : 0.856, Run Time : 12.90
INFO:root:2019-05-12 08:28:32, Epoch : 1, Step : 5195, Training Loss : 0.33803, Training Acc : 0.844, Run Time : 0.91
INFO:root:2019-05-12 08:28:33, Epoch : 1, Step : 5196, Training Loss : 0.27575, Training Acc : 0.894, Run Time : 1.43
INFO:root:2019-05-12 08:28:47, Epoch : 1, Step : 5197, Training Loss : 0.31909, Training Acc : 0.844, Run Time : 13.86
INFO:root:2019-05-12 08:28:49, Epoch : 1, Step : 5198, Training Loss : 0.31377, Training Acc : 0.867, Run Time : 1.55
INFO:root:2019-05-12 08:28:49, Epoch : 1, Step : 5199, Training Loss : 0.50009, Training Acc : 0.800, Run Time : 0.62
INFO:root:2019-05-12 08:29:00, Epoch : 1, Step : 5200, Training Loss : 0.54751, Training Acc : 0.750, Run Time : 10.57
INFO:root:2019-05-12 08:29:02, Epoch : 1, Step : 5201, Training Loss : 0.96478, Training Acc : 0.622, Run Time : 1.97
INFO:root:2019-05-12 08:29:03, Epoch : 1, Step : 5202, Training Loss : 0.96903, Training Acc : 0.606, Run Time : 0.63
INFO:root:2019-05-12 08:29:14, Epoch : 1, Step : 5203, Training Loss : 0.87166, Training Acc : 0.661, Run Time : 11.52
INFO:root:2019-05-12 08:29:15, Epoch : 1, Step : 5204, Training Loss : 0.88949, Training Acc : 0.633, Run Time : 0.75
INFO:root:2019-05-12 08:29:17, Epoch : 1, Step : 5205, Training Loss : 0.77798, Training Acc : 0.644, Run Time : 1.74
INFO:root:2019-05-12 08:29:28, Epoch : 1, Step : 5206, Training Loss : 0.45896, Training Acc : 0.783, Run Time : 11.50
INFO:root:2019-05-12 08:29:29, Epoch : 1, Step : 5207, Training Loss : 0.59713, Training Acc : 0.661, Run Time : 0.49
INFO:root:2019-05-12 08:29:30, Epoch : 1, Step : 5208, Training Loss : 0.42196, Training Acc : 0.806, Run Time : 1.44
INFO:root:2019-05-12 08:29:42, Epoch : 1, Step : 5209, Training Loss : 0.40654, Training Acc : 0.783, Run Time : 11.69
INFO:root:2019-05-12 08:29:43, Epoch : 1, Step : 5210, Training Loss : 0.79791, Training Acc : 0.644, Run Time : 1.29
INFO:root:2019-05-12 08:29:56, Epoch : 1, Step : 5211, Training Loss : 0.81110, Training Acc : 0.600, Run Time : 12.46
INFO:root:2019-05-12 08:29:56, Epoch : 1, Step : 5212, Training Loss : 0.56481, Training Acc : 0.711, Run Time : 0.46
INFO:root:2019-05-12 08:29:57, Epoch : 1, Step : 5213, Training Loss : 0.55560, Training Acc : 0.767, Run Time : 0.54
INFO:root:2019-05-12 08:29:59, Epoch : 1, Step : 5214, Training Loss : 0.64180, Training Acc : 0.728, Run Time : 2.02
INFO:root:2019-05-12 08:30:11, Epoch : 1, Step : 5215, Training Loss : 0.74176, Training Acc : 0.622, Run Time : 12.46
INFO:root:2019-05-12 08:30:12, Epoch : 1, Step : 5216, Training Loss : 0.52925, Training Acc : 0.722, Run Time : 1.16
INFO:root:2019-05-12 08:30:13, Epoch : 1, Step : 5217, Training Loss : 0.52553, Training Acc : 0.694, Run Time : 1.18
INFO:root:2019-05-12 08:30:27, Epoch : 1, Step : 5218, Training Loss : 0.42873, Training Acc : 0.789, Run Time : 13.31
INFO:root:2019-05-12 08:30:27, Epoch : 1, Step : 5219, Training Loss : 0.47300, Training Acc : 0.794, Run Time : 0.52
INFO:root:2019-05-12 08:30:28, Epoch : 1, Step : 5220, Training Loss : 0.46606, Training Acc : 0.822, Run Time : 0.58
INFO:root:2019-05-12 08:30:40, Epoch : 1, Step : 5221, Training Loss : 0.46606, Training Acc : 0.778, Run Time : 12.25
INFO:root:2019-05-12 08:30:41, Epoch : 1, Step : 5222, Training Loss : 0.53791, Training Acc : 0.767, Run Time : 0.73
INFO:root:2019-05-12 08:30:42, Epoch : 1, Step : 5223, Training Loss : 0.47764, Training Acc : 0.806, Run Time : 1.40
INFO:root:2019-05-12 08:30:56, Epoch : 1, Step : 5224, Training Loss : 0.47254, Training Acc : 0.833, Run Time : 13.79
INFO:root:2019-05-12 08:30:57, Epoch : 1, Step : 5225, Training Loss : 0.51146, Training Acc : 0.778, Run Time : 0.95
INFO:root:2019-05-12 08:30:58, Epoch : 1, Step : 5226, Training Loss : 0.54560, Training Acc : 0.700, Run Time : 0.69
INFO:root:2019-05-12 08:31:11, Epoch : 1, Step : 5227, Training Loss : 0.52828, Training Acc : 0.756, Run Time : 13.51
INFO:root:2019-05-12 08:31:12, Epoch : 1, Step : 5228, Training Loss : 0.54758, Training Acc : 0.717, Run Time : 0.96
INFO:root:2019-05-12 08:31:27, Epoch : 1, Step : 5229, Training Loss : 0.67947, Training Acc : 0.617, Run Time : 14.74
INFO:root:2019-05-12 08:31:28, Epoch : 1, Step : 5230, Training Loss : 0.68440, Training Acc : 0.672, Run Time : 0.94
INFO:root:2019-05-12 08:31:39, Epoch : 1, Step : 5231, Training Loss : 0.69391, Training Acc : 0.672, Run Time : 10.94
INFO:root:2019-05-12 08:31:39, Epoch : 1, Step : 5232, Training Loss : 0.53582, Training Acc : 0.756, Run Time : 0.82
INFO:root:2019-05-12 08:31:40, Epoch : 1, Step : 5233, Training Loss : 0.69056, Training Acc : 0.728, Run Time : 0.60
INFO:root:2019-05-12 08:31:42, Epoch : 1, Step : 5234, Training Loss : 0.46163, Training Acc : 0.800, Run Time : 2.23
INFO:root:2019-05-12 08:31:50, Epoch : 1, Step : 5235, Training Loss : 0.48413, Training Acc : 0.761, Run Time : 8.19
INFO:root:2019-05-12 08:31:53, Epoch : 1, Step : 5236, Training Loss : 0.52769, Training Acc : 0.733, Run Time : 2.95
INFO:root:2019-05-12 08:31:54, Epoch : 1, Step : 5237, Training Loss : 0.50697, Training Acc : 0.767, Run Time : 0.63
INFO:root:2019-05-12 08:31:55, Epoch : 1, Step : 5238, Training Loss : 0.40686, Training Acc : 0.811, Run Time : 0.55
INFO:root:2019-05-12 08:31:55, Epoch : 1, Step : 5239, Training Loss : 0.46930, Training Acc : 0.778, Run Time : 0.60
INFO:root:2019-05-12 08:31:56, Epoch : 1, Step : 5240, Training Loss : 0.45015, Training Acc : 0.772, Run Time : 0.65
INFO:root:2019-05-12 08:32:12, Epoch : 1, Step : 5241, Training Loss : 0.44542, Training Acc : 0.767, Run Time : 15.63
INFO:root:2019-05-12 08:32:12, Epoch : 1, Step : 5242, Training Loss : 0.45770, Training Acc : 0.783, Run Time : 0.44
INFO:root:2019-05-12 08:32:13, Epoch : 1, Step : 5243, Training Loss : 0.35385, Training Acc : 0.839, Run Time : 0.58
INFO:root:2019-05-12 08:32:14, Epoch : 1, Step : 5244, Training Loss : 0.32693, Training Acc : 0.856, Run Time : 1.67
INFO:root:2019-05-12 08:32:26, Epoch : 1, Step : 5245, Training Loss : 0.32152, Training Acc : 0.861, Run Time : 11.34
INFO:root:2019-05-12 08:32:26, Epoch : 1, Step : 5246, Training Loss : 0.27741, Training Acc : 0.900, Run Time : 0.76
INFO:root:2019-05-12 08:32:28, Epoch : 1, Step : 5247, Training Loss : 0.24578, Training Acc : 0.917, Run Time : 1.71
INFO:root:2019-05-12 08:32:40, Epoch : 1, Step : 5248, Training Loss : 0.30913, Training Acc : 0.861, Run Time : 11.75
INFO:root:2019-05-12 08:32:41, Epoch : 1, Step : 5249, Training Loss : 0.62082, Training Acc : 0.783, Run Time : 0.96
INFO:root:2019-05-12 08:32:49, Epoch : 1, Step : 5250, Training Loss : 0.70009, Training Acc : 0.678, Run Time : 8.03
INFO:root:2019-05-12 08:32:49, Epoch : 1, Step : 5251, Training Loss : 0.78371, Training Acc : 0.600, Run Time : 0.57
INFO:root:2019-05-12 08:32:50, Epoch : 1, Step : 5252, Training Loss : 0.64883, Training Acc : 0.717, Run Time : 0.64
INFO:root:2019-05-12 08:33:03, Epoch : 1, Step : 5253, Training Loss : 1.08762, Training Acc : 0.583, Run Time : 13.44
INFO:root:2019-05-12 08:33:04, Epoch : 1, Step : 5254, Training Loss : 0.38788, Training Acc : 0.806, Run Time : 0.62
INFO:root:2019-05-12 08:33:17, Epoch : 1, Step : 5255, Training Loss : 0.39917, Training Acc : 0.806, Run Time : 12.81
INFO:root:2019-05-12 08:33:18, Epoch : 1, Step : 5256, Training Loss : 0.32601, Training Acc : 0.878, Run Time : 1.02
INFO:root:2019-05-12 08:33:30, Epoch : 1, Step : 5257, Training Loss : 0.39809, Training Acc : 0.811, Run Time : 11.86
INFO:root:2019-05-12 08:33:30, Epoch : 1, Step : 5258, Training Loss : 0.33497, Training Acc : 0.867, Run Time : 0.65
INFO:root:2019-05-12 08:33:31, Epoch : 1, Step : 5259, Training Loss : 0.39775, Training Acc : 0.794, Run Time : 0.58
INFO:root:2019-05-12 08:33:32, Epoch : 1, Step : 5260, Training Loss : 0.46014, Training Acc : 0.750, Run Time : 0.60
INFO:root:2019-05-12 08:33:33, Epoch : 1, Step : 5261, Training Loss : 0.36725, Training Acc : 0.833, Run Time : 1.58
INFO:root:2019-05-12 08:33:47, Epoch : 1, Step : 5262, Training Loss : 0.31621, Training Acc : 0.861, Run Time : 13.99
INFO:root:2019-05-12 08:33:48, Epoch : 1, Step : 5263, Training Loss : 0.33186, Training Acc : 0.861, Run Time : 1.26
INFO:root:2019-05-12 08:33:59, Epoch : 1, Step : 5264, Training Loss : 0.31081, Training Acc : 0.867, Run Time : 10.88
INFO:root:2019-05-12 08:34:00, Epoch : 1, Step : 5265, Training Loss : 0.29932, Training Acc : 0.872, Run Time : 0.84
INFO:root:2019-05-12 08:34:13, Epoch : 1, Step : 5266, Training Loss : 0.24427, Training Acc : 0.889, Run Time : 13.08
INFO:root:2019-05-12 08:34:26, Epoch : 1, Step : 5267, Training Loss : 0.37903, Training Acc : 0.811, Run Time : 13.09
INFO:root:2019-05-12 08:34:29, Epoch : 1, Step : 5268, Training Loss : 0.25506, Training Acc : 0.911, Run Time : 2.49
INFO:root:2019-05-12 08:34:29, Epoch : 1, Step : 5269, Training Loss : 0.33955, Training Acc : 0.867, Run Time : 0.55
INFO:root:2019-05-12 08:34:38, Epoch : 1, Step : 5270, Training Loss : 0.33753, Training Acc : 0.833, Run Time : 8.87
INFO:root:2019-05-12 08:34:39, Epoch : 1, Step : 5271, Training Loss : 0.30149, Training Acc : 0.883, Run Time : 0.51
INFO:root:2019-05-12 08:34:40, Epoch : 1, Step : 5272, Training Loss : 0.33968, Training Acc : 0.867, Run Time : 1.61
INFO:root:2019-05-12 08:34:49, Epoch : 1, Step : 5273, Training Loss : 0.31709, Training Acc : 0.867, Run Time : 8.74
INFO:root:2019-05-12 08:34:50, Epoch : 1, Step : 5274, Training Loss : 0.36017, Training Acc : 0.850, Run Time : 0.82
INFO:root:2019-05-12 08:35:01, Epoch : 1, Step : 5275, Training Loss : 0.30902, Training Acc : 0.861, Run Time : 11.04
INFO:root:2019-05-12 08:35:03, Epoch : 1, Step : 5276, Training Loss : 0.29034, Training Acc : 0.883, Run Time : 1.67
INFO:root:2019-05-12 08:35:04, Epoch : 1, Step : 5277, Training Loss : 0.32827, Training Acc : 0.844, Run Time : 1.01
INFO:root:2019-05-12 08:35:13, Epoch : 1, Step : 5278, Training Loss : 0.21484, Training Acc : 0.939, Run Time : 9.06
INFO:root:2019-05-12 08:35:13, Epoch : 1, Step : 5279, Training Loss : 0.25177, Training Acc : 0.906, Run Time : 0.43
INFO:root:2019-05-12 08:35:14, Epoch : 1, Step : 5280, Training Loss : 0.25573, Training Acc : 0.906, Run Time : 1.16
INFO:root:2019-05-12 08:35:23, Epoch : 1, Step : 5281, Training Loss : 0.22605, Training Acc : 0.922, Run Time : 8.45
INFO:root:2019-05-12 08:35:23, Epoch : 1, Step : 5282, Training Loss : 0.27027, Training Acc : 0.911, Run Time : 0.41
INFO:root:2019-05-12 08:35:25, Epoch : 1, Step : 5283, Training Loss : 0.33816, Training Acc : 0.850, Run Time : 2.18
INFO:root:2019-05-12 08:35:36, Epoch : 1, Step : 5284, Training Loss : 0.23529, Training Acc : 0.933, Run Time : 10.58
INFO:root:2019-05-12 08:35:36, Epoch : 1, Step : 5285, Training Loss : 0.34380, Training Acc : 0.828, Run Time : 0.51
INFO:root:2019-05-12 08:35:38, Epoch : 1, Step : 5286, Training Loss : 0.29493, Training Acc : 0.894, Run Time : 1.43
INFO:root:2019-05-12 08:35:50, Epoch : 1, Step : 5287, Training Loss : 0.19070, Training Acc : 0.956, Run Time : 12.02
INFO:root:2019-05-12 08:35:50, Epoch : 1, Step : 5288, Training Loss : 0.20967, Training Acc : 0.928, Run Time : 0.48
INFO:root:2019-05-12 08:36:02, Epoch : 1, Step : 5289, Training Loss : 0.19476, Training Acc : 0.939, Run Time : 12.11
INFO:root:2019-05-12 08:36:04, Epoch : 1, Step : 5290, Training Loss : 0.24195, Training Acc : 0.911, Run Time : 1.48
INFO:root:2019-05-12 08:36:05, Epoch : 1, Step : 5291, Training Loss : 0.22029, Training Acc : 0.928, Run Time : 0.87
INFO:root:2019-05-12 08:36:06, Epoch : 1, Step : 5292, Training Loss : 0.22540, Training Acc : 0.922, Run Time : 1.08
INFO:root:2019-05-12 08:36:16, Epoch : 1, Step : 5293, Training Loss : 0.16763, Training Acc : 0.933, Run Time : 10.53
INFO:root:2019-05-12 08:36:17, Epoch : 1, Step : 5294, Training Loss : 0.22473, Training Acc : 0.911, Run Time : 0.59
INFO:root:2019-05-12 08:36:18, Epoch : 1, Step : 5295, Training Loss : 0.28749, Training Acc : 0.867, Run Time : 0.76
INFO:root:2019-05-12 08:36:30, Epoch : 1, Step : 5296, Training Loss : 0.34389, Training Acc : 0.839, Run Time : 12.08
INFO:root:2019-05-12 08:36:31, Epoch : 1, Step : 5297, Training Loss : 0.23332, Training Acc : 0.911, Run Time : 0.74
INFO:root:2019-05-12 08:36:41, Epoch : 1, Step : 5298, Training Loss : 0.35691, Training Acc : 0.822, Run Time : 10.82
INFO:root:2019-05-12 08:36:42, Epoch : 1, Step : 5299, Training Loss : 0.28196, Training Acc : 0.883, Run Time : 0.58
INFO:root:2019-05-12 08:36:43, Epoch : 1, Step : 5300, Training Loss : 0.30869, Training Acc : 0.872, Run Time : 1.07
INFO:root:2019-05-12 08:37:04, Epoch : 1, Step : 5301, Training Loss : 0.31466, Training Acc : 0.878, Run Time : 20.66
INFO:root:2019-05-12 08:37:17, Epoch : 1, Step : 5302, Training Loss : 0.36208, Training Acc : 0.844, Run Time : 13.13
INFO:root:2019-05-12 08:37:18, Epoch : 1, Step : 5303, Training Loss : 0.23993, Training Acc : 0.883, Run Time : 0.88
INFO:root:2019-05-12 08:37:26, Epoch : 1, Step : 5304, Training Loss : 0.29445, Training Acc : 0.867, Run Time : 8.04
INFO:root:2019-05-12 08:37:28, Epoch : 1, Step : 5305, Training Loss : 0.31986, Training Acc : 0.872, Run Time : 1.98
INFO:root:2019-05-12 08:37:36, Epoch : 1, Step : 5306, Training Loss : 0.27308, Training Acc : 0.872, Run Time : 7.79
INFO:root:2019-05-12 08:37:37, Epoch : 1, Step : 5307, Training Loss : 0.29326, Training Acc : 0.889, Run Time : 1.47
INFO:root:2019-05-12 08:37:49, Epoch : 1, Step : 5308, Training Loss : 0.31485, Training Acc : 0.844, Run Time : 12.14
INFO:root:2019-05-12 08:37:50, Epoch : 1, Step : 5309, Training Loss : 0.30019, Training Acc : 0.883, Run Time : 0.70
INFO:root:2019-05-12 08:37:50, Epoch : 1, Step : 5310, Training Loss : 0.24085, Training Acc : 0.878, Run Time : 0.56
INFO:root:2019-05-12 08:37:51, Epoch : 1, Step : 5311, Training Loss : 0.24584, Training Acc : 0.889, Run Time : 0.61
INFO:root:2019-05-12 08:37:52, Epoch : 1, Step : 5312, Training Loss : 0.25357, Training Acc : 0.867, Run Time : 0.61
INFO:root:2019-05-12 08:38:06, Epoch : 1, Step : 5313, Training Loss : 0.24563, Training Acc : 0.889, Run Time : 14.03
INFO:root:2019-05-12 08:38:06, Epoch : 1, Step : 5314, Training Loss : 0.25429, Training Acc : 0.883, Run Time : 0.61
INFO:root:2019-05-12 08:38:07, Epoch : 1, Step : 5315, Training Loss : 0.20782, Training Acc : 0.911, Run Time : 0.61
INFO:root:2019-05-12 08:38:19, Epoch : 1, Step : 5316, Training Loss : 0.23779, Training Acc : 0.872, Run Time : 12.54
INFO:root:2019-05-12 08:38:20, Epoch : 1, Step : 5317, Training Loss : 0.21995, Training Acc : 0.894, Run Time : 0.44
INFO:root:2019-05-12 08:38:21, Epoch : 1, Step : 5318, Training Loss : 0.32105, Training Acc : 0.833, Run Time : 0.78
INFO:root:2019-05-12 08:38:22, Epoch : 1, Step : 5319, Training Loss : 0.29780, Training Acc : 0.878, Run Time : 0.99
INFO:root:2019-05-12 08:38:24, Epoch : 1, Step : 5320, Training Loss : 0.30874, Training Acc : 0.878, Run Time : 2.58
INFO:root:2019-05-12 08:38:35, Epoch : 1, Step : 5321, Training Loss : 0.28790, Training Acc : 0.856, Run Time : 10.69
INFO:root:2019-05-12 08:38:35, Epoch : 1, Step : 5322, Training Loss : 0.34023, Training Acc : 0.817, Run Time : 0.52
INFO:root:2019-05-12 08:38:37, Epoch : 1, Step : 5323, Training Loss : 0.30905, Training Acc : 0.872, Run Time : 1.83
INFO:root:2019-05-12 08:38:49, Epoch : 1, Step : 5324, Training Loss : 0.28886, Training Acc : 0.861, Run Time : 11.32
INFO:root:2019-05-12 08:38:49, Epoch : 1, Step : 5325, Training Loss : 0.36819, Training Acc : 0.822, Run Time : 0.80
INFO:root:2019-05-12 08:39:00, Epoch : 1, Step : 5326, Training Loss : 0.33720, Training Acc : 0.839, Run Time : 10.89
INFO:root:2019-05-12 08:39:01, Epoch : 1, Step : 5327, Training Loss : 0.28986, Training Acc : 0.867, Run Time : 1.27
INFO:root:2019-05-12 08:39:02, Epoch : 1, Step : 5328, Training Loss : 0.31616, Training Acc : 0.883, Run Time : 0.41
INFO:root:2019-05-12 08:39:03, Epoch : 1, Step : 5329, Training Loss : 0.32422, Training Acc : 0.867, Run Time : 0.61
INFO:root:2019-05-12 08:39:14, Epoch : 1, Step : 5330, Training Loss : 0.27717, Training Acc : 0.889, Run Time : 11.08
INFO:root:2019-05-12 08:39:14, Epoch : 1, Step : 5331, Training Loss : 0.27529, Training Acc : 0.867, Run Time : 0.71
INFO:root:2019-05-12 08:39:15, Epoch : 1, Step : 5332, Training Loss : 0.23673, Training Acc : 0.900, Run Time : 0.59
INFO:root:2019-05-12 08:39:27, Epoch : 1, Step : 5333, Training Loss : 0.25559, Training Acc : 0.878, Run Time : 11.67
INFO:root:2019-05-12 08:39:27, Epoch : 1, Step : 5334, Training Loss : 0.36126, Training Acc : 0.856, Run Time : 0.62
INFO:root:2019-05-12 08:39:38, Epoch : 1, Step : 5335, Training Loss : 0.29979, Training Acc : 0.850, Run Time : 10.67
INFO:root:2019-05-12 08:39:39, Epoch : 1, Step : 5336, Training Loss : 0.31999, Training Acc : 0.822, Run Time : 0.75
INFO:root:2019-05-12 08:39:39, Epoch : 1, Step : 5337, Training Loss : 0.24768, Training Acc : 0.911, Run Time : 0.79
INFO:root:2019-05-12 08:39:53, Epoch : 1, Step : 5338, Training Loss : 0.24828, Training Acc : 0.894, Run Time : 14.01
INFO:root:2019-05-12 08:39:54, Epoch : 1, Step : 5339, Training Loss : 0.26966, Training Acc : 0.894, Run Time : 0.74
INFO:root:2019-05-12 08:39:55, Epoch : 1, Step : 5340, Training Loss : 0.27705, Training Acc : 0.872, Run Time : 0.99
INFO:root:2019-05-12 08:40:11, Epoch : 1, Step : 5341, Training Loss : 0.31657, Training Acc : 0.828, Run Time : 16.09
INFO:root:2019-05-12 08:40:12, Epoch : 1, Step : 5342, Training Loss : 0.27789, Training Acc : 0.906, Run Time : 0.53
INFO:root:2019-05-12 08:40:26, Epoch : 1, Step : 5343, Training Loss : 0.29909, Training Acc : 0.861, Run Time : 13.84
INFO:root:2019-05-12 08:40:34, Epoch : 1, Step : 5344, Training Loss : 0.25056, Training Acc : 0.889, Run Time : 8.53
INFO:root:2019-05-12 08:40:35, Epoch : 1, Step : 5345, Training Loss : 0.28804, Training Acc : 0.878, Run Time : 0.95
INFO:root:2019-05-12 08:40:53, Epoch : 1, Step : 5346, Training Loss : 0.31490, Training Acc : 0.861, Run Time : 18.03
INFO:root:2019-05-12 08:40:59, Epoch : 1, Step : 5347, Training Loss : 0.28148, Training Acc : 0.839, Run Time : 5.99
INFO:root:2019-05-12 08:41:00, Epoch : 1, Step : 5348, Training Loss : 0.26736, Training Acc : 0.878, Run Time : 0.81
INFO:root:2019-05-12 08:41:11, Epoch : 1, Step : 5349, Training Loss : 0.54737, Training Acc : 0.772, Run Time : 11.28
INFO:root:2019-05-12 08:41:13, Epoch : 1, Step : 5350, Training Loss : 0.45805, Training Acc : 0.783, Run Time : 1.49
INFO:root:2019-05-12 08:41:23, Epoch : 1, Step : 5351, Training Loss : 0.49623, Training Acc : 0.833, Run Time : 10.00
INFO:root:2019-05-12 08:41:24, Epoch : 1, Step : 5352, Training Loss : 0.39328, Training Acc : 0.828, Run Time : 1.60
INFO:root:2019-05-12 08:41:26, Epoch : 1, Step : 5353, Training Loss : 0.41778, Training Acc : 0.844, Run Time : 1.74
INFO:root:2019-05-12 08:41:27, Epoch : 1, Step : 5354, Training Loss : 0.62961, Training Acc : 0.750, Run Time : 0.95
INFO:root:2019-05-12 08:41:34, Epoch : 1, Step : 5355, Training Loss : 0.48718, Training Acc : 0.778, Run Time : 7.18
INFO:root:2019-05-12 08:41:35, Epoch : 1, Step : 5356, Training Loss : 0.46218, Training Acc : 0.778, Run Time : 0.62
INFO:root:2019-05-12 08:41:42, Epoch : 1, Step : 5357, Training Loss : 0.49302, Training Acc : 0.750, Run Time : 7.47
INFO:root:2019-05-12 08:41:45, Epoch : 1, Step : 5358, Training Loss : 0.49022, Training Acc : 0.767, Run Time : 3.07
INFO:root:2019-05-12 08:41:46, Epoch : 1, Step : 5359, Training Loss : 0.52733, Training Acc : 0.733, Run Time : 0.38
INFO:root:2019-05-12 08:41:46, Epoch : 1, Step : 5360, Training Loss : 0.61716, Training Acc : 0.683, Run Time : 0.61
INFO:root:2019-05-12 08:41:47, Epoch : 1, Step : 5361, Training Loss : 0.53922, Training Acc : 0.722, Run Time : 0.58
INFO:root:2019-05-12 08:41:47, Epoch : 1, Step : 5362, Training Loss : 0.54102, Training Acc : 0.717, Run Time : 0.65
INFO:root:2019-05-12 08:42:02, Epoch : 1, Step : 5363, Training Loss : 0.38128, Training Acc : 0.844, Run Time : 14.01
INFO:root:2019-05-12 08:42:03, Epoch : 1, Step : 5364, Training Loss : 0.38583, Training Acc : 0.817, Run Time : 1.67
INFO:root:2019-05-12 08:42:14, Epoch : 1, Step : 5365, Training Loss : 0.43366, Training Acc : 0.800, Run Time : 10.40
INFO:root:2019-05-12 08:42:14, Epoch : 1, Step : 5366, Training Loss : 0.50529, Training Acc : 0.756, Run Time : 0.66
INFO:root:2019-05-12 08:42:15, Epoch : 1, Step : 5367, Training Loss : 0.48520, Training Acc : 0.761, Run Time : 1.17
INFO:root:2019-05-12 08:42:32, Epoch : 1, Step : 5368, Training Loss : 0.50487, Training Acc : 0.811, Run Time : 16.20
INFO:root:2019-05-12 08:42:34, Epoch : 1, Step : 5369, Training Loss : 0.45097, Training Acc : 0.778, Run Time : 2.56
INFO:root:2019-05-12 08:42:35, Epoch : 1, Step : 5370, Training Loss : 0.57758, Training Acc : 0.717, Run Time : 0.65
INFO:root:2019-05-12 08:42:45, Epoch : 1, Step : 5371, Training Loss : 0.58889, Training Acc : 0.700, Run Time : 9.86
INFO:root:2019-05-12 08:42:45, Epoch : 1, Step : 5372, Training Loss : 0.50392, Training Acc : 0.750, Run Time : 0.79
INFO:root:2019-05-12 08:42:46, Epoch : 1, Step : 5373, Training Loss : 0.49032, Training Acc : 0.767, Run Time : 0.49
INFO:root:2019-05-12 08:42:48, Epoch : 1, Step : 5374, Training Loss : 0.57977, Training Acc : 0.717, Run Time : 1.78
INFO:root:2019-05-12 08:42:58, Epoch : 1, Step : 5375, Training Loss : 0.36813, Training Acc : 0.839, Run Time : 10.71
INFO:root:2019-05-12 08:42:59, Epoch : 1, Step : 5376, Training Loss : 0.49338, Training Acc : 0.756, Run Time : 0.45
INFO:root:2019-05-12 08:43:01, Epoch : 1, Step : 5377, Training Loss : 0.42207, Training Acc : 0.778, Run Time : 2.04
INFO:root:2019-05-12 08:43:13, Epoch : 1, Step : 5378, Training Loss : 0.42710, Training Acc : 0.822, Run Time : 12.25
INFO:root:2019-05-12 08:43:14, Epoch : 1, Step : 5379, Training Loss : 0.58500, Training Acc : 0.767, Run Time : 0.56
INFO:root:2019-05-12 08:43:16, Epoch : 1, Step : 5380, Training Loss : 0.48134, Training Acc : 0.778, Run Time : 2.14
INFO:root:2019-05-12 08:43:26, Epoch : 1, Step : 5381, Training Loss : 0.32356, Training Acc : 0.828, Run Time : 10.56
INFO:root:2019-05-12 08:43:27, Epoch : 1, Step : 5382, Training Loss : 0.45598, Training Acc : 0.817, Run Time : 0.99
INFO:root:2019-05-12 08:43:29, Epoch : 1, Step : 5383, Training Loss : 0.37805, Training Acc : 0.850, Run Time : 1.45
INFO:root:2019-05-12 08:43:43, Epoch : 1, Step : 5384, Training Loss : 0.45477, Training Acc : 0.817, Run Time : 14.09
INFO:root:2019-05-12 08:43:44, Epoch : 1, Step : 5385, Training Loss : 0.54253, Training Acc : 0.756, Run Time : 1.47
INFO:root:2019-05-12 08:43:57, Epoch : 1, Step : 5386, Training Loss : 0.32783, Training Acc : 0.833, Run Time : 12.09
INFO:root:2019-05-12 08:43:57, Epoch : 1, Step : 5387, Training Loss : 0.32995, Training Acc : 0.861, Run Time : 0.79
INFO:root:2019-05-12 08:44:04, Epoch : 1, Step : 5388, Training Loss : 0.61148, Training Acc : 0.750, Run Time : 6.98
INFO:root:2019-05-12 08:44:09, Epoch : 1, Step : 5389, Training Loss : 0.62071, Training Acc : 0.733, Run Time : 4.94
INFO:root:2019-05-12 08:44:10, Epoch : 1, Step : 5390, Training Loss : 0.30938, Training Acc : 0.833, Run Time : 0.44
INFO:root:2019-05-12 08:44:11, Epoch : 1, Step : 5391, Training Loss : 0.37720, Training Acc : 0.839, Run Time : 1.18
INFO:root:2019-05-12 08:44:23, Epoch : 1, Step : 5392, Training Loss : 0.44631, Training Acc : 0.761, Run Time : 11.80
INFO:root:2019-05-12 08:44:23, Epoch : 1, Step : 5393, Training Loss : 0.28016, Training Acc : 0.872, Run Time : 0.46
INFO:root:2019-05-12 08:44:24, Epoch : 1, Step : 5394, Training Loss : 0.25790, Training Acc : 0.894, Run Time : 0.62
INFO:root:2019-05-12 08:44:36, Epoch : 1, Step : 5395, Training Loss : 0.43522, Training Acc : 0.811, Run Time : 12.22
INFO:root:2019-05-12 08:44:37, Epoch : 1, Step : 5396, Training Loss : 0.34156, Training Acc : 0.828, Run Time : 0.88
INFO:root:2019-05-12 08:44:38, Epoch : 1, Step : 5397, Training Loss : 0.42087, Training Acc : 0.828, Run Time : 1.43
INFO:root:2019-05-12 08:44:50, Epoch : 1, Step : 5398, Training Loss : 0.40711, Training Acc : 0.817, Run Time : 11.84
INFO:root:2019-05-12 08:44:51, Epoch : 1, Step : 5399, Training Loss : 0.30555, Training Acc : 0.856, Run Time : 0.53
INFO:root:2019-05-12 08:44:53, Epoch : 1, Step : 5400, Training Loss : 0.29192, Training Acc : 0.917, Run Time : 2.08
INFO:root:2019-05-12 08:45:05, Epoch : 1, Step : 5401, Training Loss : 0.42774, Training Acc : 0.794, Run Time : 11.75
INFO:root:2019-05-12 08:45:05, Epoch : 1, Step : 5402, Training Loss : 0.49489, Training Acc : 0.767, Run Time : 0.67
INFO:root:2019-05-12 08:45:20, Epoch : 1, Step : 5403, Training Loss : 0.33943, Training Acc : 0.861, Run Time : 14.56
INFO:root:2019-05-12 08:45:21, Epoch : 1, Step : 5404, Training Loss : 0.35907, Training Acc : 0.828, Run Time : 0.83
INFO:root:2019-05-12 08:45:32, Epoch : 1, Step : 5405, Training Loss : 0.40430, Training Acc : 0.794, Run Time : 11.39
INFO:root:2019-05-12 08:45:33, Epoch : 1, Step : 5406, Training Loss : 0.39008, Training Acc : 0.822, Run Time : 1.30
INFO:root:2019-05-12 08:45:34, Epoch : 1, Step : 5407, Training Loss : 0.40193, Training Acc : 0.794, Run Time : 0.61
INFO:root:2019-05-12 08:45:45, Epoch : 1, Step : 5408, Training Loss : 0.31592, Training Acc : 0.844, Run Time : 11.60
INFO:root:2019-05-12 08:45:46, Epoch : 1, Step : 5409, Training Loss : 0.39539, Training Acc : 0.806, Run Time : 0.73
INFO:root:2019-05-12 08:45:58, Epoch : 1, Step : 5410, Training Loss : 0.30334, Training Acc : 0.872, Run Time : 12.03
INFO:root:2019-05-12 08:45:59, Epoch : 1, Step : 5411, Training Loss : 0.28953, Training Acc : 0.878, Run Time : 1.05
INFO:root:2019-05-12 08:46:00, Epoch : 1, Step : 5412, Training Loss : 0.27551, Training Acc : 0.856, Run Time : 0.57
INFO:root:2019-05-12 08:46:14, Epoch : 1, Step : 5413, Training Loss : 0.24444, Training Acc : 0.883, Run Time : 13.80
INFO:root:2019-05-12 08:46:26, Epoch : 1, Step : 5414, Training Loss : 0.24835, Training Acc : 0.872, Run Time : 12.59
INFO:root:2019-05-12 08:46:27, Epoch : 1, Step : 5415, Training Loss : 0.26254, Training Acc : 0.911, Run Time : 0.98
INFO:root:2019-05-12 08:46:28, Epoch : 1, Step : 5416, Training Loss : 0.26402, Training Acc : 0.872, Run Time : 0.62
INFO:root:2019-05-12 08:46:40, Epoch : 1, Step : 5417, Training Loss : 0.24071, Training Acc : 0.900, Run Time : 12.01
INFO:root:2019-05-12 08:46:40, Epoch : 1, Step : 5418, Training Loss : 0.23758, Training Acc : 0.889, Run Time : 0.43
INFO:root:2019-05-12 08:46:42, Epoch : 1, Step : 5419, Training Loss : 0.21727, Training Acc : 0.917, Run Time : 2.01
INFO:root:2019-05-12 08:46:54, Epoch : 1, Step : 5420, Training Loss : 0.31826, Training Acc : 0.889, Run Time : 12.18
INFO:root:2019-05-12 08:46:56, Epoch : 1, Step : 5421, Training Loss : 0.17774, Training Acc : 0.933, Run Time : 1.12
INFO:root:2019-05-12 08:46:56, Epoch : 1, Step : 5422, Training Loss : 0.18278, Training Acc : 0.939, Run Time : 0.62
INFO:root:2019-05-12 08:47:05, Epoch : 1, Step : 5423, Training Loss : 0.19135, Training Acc : 0.922, Run Time : 9.25
INFO:root:2019-05-12 08:47:06, Epoch : 1, Step : 5424, Training Loss : 0.20184, Training Acc : 0.911, Run Time : 0.66
INFO:root:2019-05-12 08:47:08, Epoch : 1, Step : 5425, Training Loss : 0.20170, Training Acc : 0.933, Run Time : 1.54
INFO:root:2019-05-12 08:47:16, Epoch : 1, Step : 5426, Training Loss : 0.24346, Training Acc : 0.900, Run Time : 7.95
INFO:root:2019-05-12 08:47:16, Epoch : 1, Step : 5427, Training Loss : 0.22223, Training Acc : 0.917, Run Time : 0.44
INFO:root:2019-05-12 08:47:16, Epoch : 1, Step : 5428, Training Loss : 0.23337, Training Acc : 0.906, Run Time : 0.40
INFO:root:2019-05-12 08:47:18, Epoch : 1, Step : 5429, Training Loss : 0.19004, Training Acc : 0.933, Run Time : 1.65
INFO:root:2019-05-12 08:47:28, Epoch : 1, Step : 5430, Training Loss : 0.20321, Training Acc : 0.911, Run Time : 10.41
INFO:root:2019-05-12 08:47:29, Epoch : 1, Step : 5431, Training Loss : 0.24050, Training Acc : 0.928, Run Time : 0.44
INFO:root:2019-05-12 08:47:30, Epoch : 1, Step : 5432, Training Loss : 0.17227, Training Acc : 0.944, Run Time : 1.43
INFO:root:2019-05-12 08:47:43, Epoch : 1, Step : 5433, Training Loss : 0.16613, Training Acc : 0.922, Run Time : 12.63
INFO:root:2019-05-12 08:47:44, Epoch : 1, Step : 5434, Training Loss : 0.18920, Training Acc : 0.939, Run Time : 0.53
INFO:root:2019-05-12 08:47:44, Epoch : 1, Step : 5435, Training Loss : 0.19470, Training Acc : 0.922, Run Time : 0.66
INFO:root:2019-05-12 08:47:57, Epoch : 1, Step : 5436, Training Loss : 0.17447, Training Acc : 0.911, Run Time : 13.14
INFO:root:2019-05-12 08:47:58, Epoch : 1, Step : 5437, Training Loss : 0.17335, Training Acc : 0.917, Run Time : 0.66
INFO:root:2019-05-12 08:47:58, Epoch : 1, Step : 5438, Training Loss : 0.16806, Training Acc : 0.928, Run Time : 0.43
INFO:root:2019-05-12 08:48:01, Epoch : 1, Step : 5439, Training Loss : 0.18877, Training Acc : 0.922, Run Time : 2.49
INFO:root:2019-05-12 08:48:10, Epoch : 1, Step : 5440, Training Loss : 0.24424, Training Acc : 0.883, Run Time : 8.92
INFO:root:2019-05-12 08:48:11, Epoch : 1, Step : 5441, Training Loss : 0.31543, Training Acc : 0.817, Run Time : 0.82
INFO:root:2019-05-12 08:48:12, Epoch : 1, Step : 5442, Training Loss : 0.29513, Training Acc : 0.856, Run Time : 1.82
INFO:root:2019-05-12 08:48:22, Epoch : 1, Step : 5443, Training Loss : 0.33236, Training Acc : 0.850, Run Time : 9.77
INFO:root:2019-05-12 08:48:23, Epoch : 1, Step : 5444, Training Loss : 0.37921, Training Acc : 0.811, Run Time : 0.53
INFO:root:2019-05-12 08:48:25, Epoch : 1, Step : 5445, Training Loss : 0.28168, Training Acc : 0.889, Run Time : 1.98
INFO:root:2019-05-12 08:48:37, Epoch : 1, Step : 5446, Training Loss : 0.23096, Training Acc : 0.900, Run Time : 11.82
INFO:root:2019-05-12 08:48:37, Epoch : 1, Step : 5447, Training Loss : 0.23338, Training Acc : 0.889, Run Time : 0.46
INFO:root:2019-05-12 08:48:38, Epoch : 1, Step : 5448, Training Loss : 0.24485, Training Acc : 0.867, Run Time : 0.63
INFO:root:2019-05-12 08:48:58, Epoch : 1, Step : 5449, Training Loss : 0.21175, Training Acc : 0.900, Run Time : 20.10
INFO:root:2019-05-12 08:49:03, Epoch : 1, Step : 5450, Training Loss : 0.33183, Training Acc : 0.861, Run Time : 5.37
INFO:root:2019-05-12 08:49:04, Epoch : 1, Step : 5451, Training Loss : 0.27231, Training Acc : 0.878, Run Time : 0.67
INFO:root:2019-05-12 08:49:07, Epoch : 1, Step : 5452, Training Loss : 0.27272, Training Acc : 0.883, Run Time : 3.55
INFO:root:2019-05-12 08:49:08, Epoch : 1, Step : 5453, Training Loss : 0.18596, Training Acc : 0.939, Run Time : 0.91
INFO:root:2019-05-12 08:49:41, Epoch : 1, Step : 5454, Training Loss : 0.28324, Training Acc : 0.889, Run Time : 32.53
INFO:root:2019-05-12 08:49:45, Epoch : 1, Step : 5455, Training Loss : 0.27943, Training Acc : 0.872, Run Time : 4.11
INFO:root:2019-05-12 08:49:45, Epoch : 1, Step : 5456, Training Loss : 0.34453, Training Acc : 0.844, Run Time : 0.44
INFO:root:2019-05-12 08:49:46, Epoch : 1, Step : 5457, Training Loss : 0.22642, Training Acc : 0.917, Run Time : 0.66
INFO:root:2019-05-12 08:49:48, Epoch : 1, Step : 5458, Training Loss : 0.34592, Training Acc : 0.872, Run Time : 2.04
INFO:root:2019-05-12 08:49:49, Epoch : 1, Step : 5459, Training Loss : 0.22123, Training Acc : 0.917, Run Time : 0.59
INFO:root:2019-05-12 08:49:58, Epoch : 1, Step : 5460, Training Loss : 0.24408, Training Acc : 0.872, Run Time : 9.66
INFO:root:2019-05-12 08:49:59, Epoch : 1, Step : 5461, Training Loss : 0.32380, Training Acc : 0.844, Run Time : 0.99
INFO:root:2019-05-12 08:50:01, Epoch : 1, Step : 5462, Training Loss : 0.27471, Training Acc : 0.894, Run Time : 1.52
INFO:root:2019-05-12 08:50:10, Epoch : 1, Step : 5463, Training Loss : 0.30954, Training Acc : 0.861, Run Time : 9.24
INFO:root:2019-05-12 08:50:11, Epoch : 1, Step : 5464, Training Loss : 0.29525, Training Acc : 0.850, Run Time : 0.53
INFO:root:2019-05-12 08:50:12, Epoch : 1, Step : 5465, Training Loss : 0.27126, Training Acc : 0.839, Run Time : 1.21
INFO:root:2019-05-12 08:50:22, Epoch : 1, Step : 5466, Training Loss : 0.21647, Training Acc : 0.900, Run Time : 10.12
INFO:root:2019-05-12 08:50:22, Epoch : 1, Step : 5467, Training Loss : 0.19291, Training Acc : 0.906, Run Time : 0.55
INFO:root:2019-05-12 08:50:28, Epoch : 1, Step : 5468, Training Loss : 0.15787, Training Acc : 0.928, Run Time : 5.73
INFO:root:2019-05-12 08:50:30, Epoch : 1, Step : 5469, Training Loss : 0.13955, Training Acc : 0.944, Run Time : 2.18
INFO:root:2019-05-12 08:50:31, Epoch : 1, Step : 5470, Training Loss : 0.15282, Training Acc : 0.939, Run Time : 0.63
INFO:root:2019-05-12 08:50:38, Epoch : 1, Step : 5471, Training Loss : 0.12465, Training Acc : 0.956, Run Time : 6.64
INFO:root:2019-05-12 08:50:38, Epoch : 1, Step : 5472, Training Loss : 0.12454, Training Acc : 0.944, Run Time : 0.56
INFO:root:2019-05-12 08:50:39, Epoch : 1, Step : 5473, Training Loss : 0.15406, Training Acc : 0.939, Run Time : 0.61
INFO:root:2019-05-12 08:50:50, Epoch : 1, Step : 5474, Training Loss : 0.13535, Training Acc : 0.911, Run Time : 11.20
INFO:root:2019-05-12 08:50:51, Epoch : 1, Step : 5475, Training Loss : 0.26242, Training Acc : 0.889, Run Time : 0.66
INFO:root:2019-05-12 08:51:02, Epoch : 1, Step : 5476, Training Loss : 0.25402, Training Acc : 0.878, Run Time : 11.22
INFO:root:2019-05-12 08:51:03, Epoch : 1, Step : 5477, Training Loss : 0.11680, Training Acc : 0.939, Run Time : 0.68
INFO:root:2019-05-12 08:51:14, Epoch : 1, Step : 5478, Training Loss : 0.12234, Training Acc : 0.944, Run Time : 11.76
INFO:root:2019-05-12 08:51:17, Epoch : 1, Step : 5479, Training Loss : 0.09225, Training Acc : 0.967, Run Time : 2.35
INFO:root:2019-05-12 08:51:17, Epoch : 1, Step : 5480, Training Loss : 0.12459, Training Acc : 0.956, Run Time : 0.70
INFO:root:2019-05-12 08:51:25, Epoch : 1, Step : 5481, Training Loss : 0.10018, Training Acc : 0.967, Run Time : 8.03
INFO:root:2019-05-12 08:51:26, Epoch : 1, Step : 5482, Training Loss : 0.15172, Training Acc : 0.944, Run Time : 0.51
INFO:root:2019-05-12 08:51:26, Epoch : 1, Step : 5483, Training Loss : 0.34425, Training Acc : 0.894, Run Time : 0.59
INFO:root:2019-05-12 08:51:38, Epoch : 1, Step : 5484, Training Loss : 0.40784, Training Acc : 0.878, Run Time : 11.22
INFO:root:2019-05-12 08:51:39, Epoch : 1, Step : 5485, Training Loss : 0.39276, Training Acc : 0.878, Run Time : 0.84
INFO:root:2019-05-12 08:51:51, Epoch : 1, Step : 5486, Training Loss : 0.35197, Training Acc : 0.867, Run Time : 12.05
INFO:root:2019-05-12 08:51:51, Epoch : 1, Step : 5487, Training Loss : 0.33593, Training Acc : 0.839, Run Time : 0.91
INFO:root:2019-05-12 08:52:03, Epoch : 1, Step : 5488, Training Loss : 0.38374, Training Acc : 0.839, Run Time : 11.21
INFO:root:2019-05-12 08:52:04, Epoch : 1, Step : 5489, Training Loss : 0.40535, Training Acc : 0.822, Run Time : 0.85
INFO:root:2019-05-12 08:52:04, Epoch : 1, Step : 5490, Training Loss : 0.26541, Training Acc : 0.878, Run Time : 0.56
INFO:root:2019-05-12 08:52:05, Epoch : 1, Step : 5491, Training Loss : 0.50370, Training Acc : 0.822, Run Time : 0.62
INFO:root:2019-05-12 08:52:15, Epoch : 1, Step : 5492, Training Loss : 0.42610, Training Acc : 0.833, Run Time : 10.25
INFO:root:2019-05-12 08:52:16, Epoch : 1, Step : 5493, Training Loss : 0.67525, Training Acc : 0.756, Run Time : 0.54
INFO:root:2019-05-12 08:52:16, Epoch : 1, Step : 5494, Training Loss : 0.30694, Training Acc : 0.861, Run Time : 0.70
INFO:root:2019-05-12 08:52:27, Epoch : 1, Step : 5495, Training Loss : 0.51781, Training Acc : 0.783, Run Time : 10.34
INFO:root:2019-05-12 08:52:27, Epoch : 1, Step : 5496, Training Loss : 0.25508, Training Acc : 0.906, Run Time : 0.79
INFO:root:2019-05-12 08:52:29, Epoch : 1, Step : 5497, Training Loss : 0.49168, Training Acc : 0.789, Run Time : 1.53
INFO:root:2019-05-12 08:52:38, Epoch : 1, Step : 5498, Training Loss : 0.24161, Training Acc : 0.900, Run Time : 9.32
INFO:root:2019-05-12 08:52:39, Epoch : 1, Step : 5499, Training Loss : 0.19663, Training Acc : 0.922, Run Time : 0.93
INFO:root:2019-05-12 08:52:52, Epoch : 1, Step : 5500, Training Loss : 0.20429, Training Acc : 0.928, Run Time : 12.40
INFO:root:2019-05-12 08:53:02, Epoch : 1, Step : 5501, Training Loss : 0.30491, Training Acc : 0.883, Run Time : 10.61
INFO:root:2019-05-12 08:53:03, Epoch : 1, Step : 5502, Training Loss : 0.42709, Training Acc : 0.794, Run Time : 0.89
INFO:root:2019-05-12 08:53:13, Epoch : 1, Step : 5503, Training Loss : 0.27420, Training Acc : 0.894, Run Time : 9.71
INFO:root:2019-05-12 08:53:14, Epoch : 1, Step : 5504, Training Loss : 0.22281, Training Acc : 0.900, Run Time : 0.98
INFO:root:2019-05-12 08:53:19, Epoch : 1, Step : 5505, Training Loss : 0.31986, Training Acc : 0.850, Run Time : 4.86
INFO:root:2019-05-12 08:53:20, Epoch : 1, Step : 5506, Training Loss : 0.29626, Training Acc : 0.844, Run Time : 1.08
INFO:root:2019-05-12 08:53:20, Epoch : 1, Step : 5507, Training Loss : 0.30716, Training Acc : 0.844, Run Time : 0.68
INFO:root:2019-05-12 08:53:25, Epoch : 1, Step : 5508, Training Loss : 0.27473, Training Acc : 0.878, Run Time : 4.60
INFO:root:2019-05-12 08:53:26, Epoch : 1, Step : 5509, Training Loss : 0.32433, Training Acc : 0.872, Run Time : 0.64
INFO:root:2019-05-12 08:53:27, Epoch : 1, Step : 5510, Training Loss : 0.23679, Training Acc : 0.906, Run Time : 1.33
INFO:root:2019-05-12 08:53:34, Epoch : 1, Step : 5511, Training Loss : 0.21086, Training Acc : 0.911, Run Time : 6.94
INFO:root:2019-05-12 08:53:34, Epoch : 1, Step : 5512, Training Loss : 0.27183, Training Acc : 0.900, Run Time : 0.45
INFO:root:2019-05-12 08:53:36, Epoch : 1, Step : 5513, Training Loss : 0.19922, Training Acc : 0.889, Run Time : 2.07
INFO:root:2019-05-12 08:53:47, Epoch : 1, Step : 5514, Training Loss : 0.15503, Training Acc : 0.939, Run Time : 10.23
INFO:root:2019-05-12 08:53:47, Epoch : 1, Step : 5515, Training Loss : 0.17720, Training Acc : 0.911, Run Time : 0.42
INFO:root:2019-05-12 08:53:49, Epoch : 1, Step : 5516, Training Loss : 0.20301, Training Acc : 0.894, Run Time : 1.61
INFO:root:2019-05-12 08:54:00, Epoch : 1, Step : 5517, Training Loss : 0.37771, Training Acc : 0.867, Run Time : 11.16
INFO:root:2019-05-12 08:54:00, Epoch : 1, Step : 5518, Training Loss : 0.30862, Training Acc : 0.867, Run Time : 0.43
INFO:root:2019-05-12 08:54:02, Epoch : 1, Step : 5519, Training Loss : 0.41358, Training Acc : 0.856, Run Time : 1.63
INFO:root:2019-05-12 08:54:15, Epoch : 1, Step : 5520, Training Loss : 0.21813, Training Acc : 0.911, Run Time : 12.85
INFO:root:2019-05-12 08:54:16, Epoch : 1, Step : 5521, Training Loss : 0.39258, Training Acc : 0.844, Run Time : 0.95
INFO:root:2019-05-12 08:54:31, Epoch : 1, Step : 5522, Training Loss : 0.33683, Training Acc : 0.872, Run Time : 15.63
INFO:root:2019-05-12 08:54:45, Epoch : 1, Step : 5523, Training Loss : 0.51513, Training Acc : 0.817, Run Time : 13.86
INFO:root:2019-05-12 08:54:46, Epoch : 1, Step : 5524, Training Loss : 0.37817, Training Acc : 0.822, Run Time : 0.80
INFO:root:2019-05-12 08:54:58, Epoch : 1, Step : 5525, Training Loss : 0.49519, Training Acc : 0.761, Run Time : 11.80
INFO:root:2019-05-12 08:54:58, Epoch : 1, Step : 5526, Training Loss : 0.27308, Training Acc : 0.900, Run Time : 0.53
INFO:root:2019-05-12 08:54:59, Epoch : 1, Step : 5527, Training Loss : 0.43731, Training Acc : 0.800, Run Time : 0.80
INFO:root:2019-05-12 08:55:10, Epoch : 1, Step : 5528, Training Loss : 0.60811, Training Acc : 0.750, Run Time : 11.24
INFO:root:2019-05-12 08:55:11, Epoch : 1, Step : 5529, Training Loss : 0.22467, Training Acc : 0.917, Run Time : 0.70
INFO:root:2019-05-12 08:55:12, Epoch : 1, Step : 5530, Training Loss : 0.33663, Training Acc : 0.872, Run Time : 1.34
INFO:root:2019-05-12 08:55:22, Epoch : 1, Step : 5531, Training Loss : 0.38233, Training Acc : 0.844, Run Time : 9.53
INFO:root:2019-05-12 08:55:22, Epoch : 1, Step : 5532, Training Loss : 0.29031, Training Acc : 0.878, Run Time : 0.47
INFO:root:2019-05-12 08:55:23, Epoch : 1, Step : 5533, Training Loss : 0.48232, Training Acc : 0.794, Run Time : 0.61
INFO:root:2019-05-12 08:55:27, Epoch : 1, Step : 5534, Training Loss : 0.49708, Training Acc : 0.789, Run Time : 3.63
INFO:root:2019-05-12 08:55:38, Epoch : 1, Step : 5535, Training Loss : 0.52112, Training Acc : 0.733, Run Time : 11.10
INFO:root:2019-05-12 08:55:39, Epoch : 1, Step : 5536, Training Loss : 0.30622, Training Acc : 0.861, Run Time : 1.17
INFO:root:2019-05-12 08:55:39, Epoch : 1, Step : 5537, Training Loss : 0.45593, Training Acc : 0.778, Run Time : 0.60
INFO:root:2019-05-12 08:55:42, Epoch : 1, Step : 5538, Training Loss : 0.43095, Training Acc : 0.783, Run Time : 2.11
INFO:root:2019-05-12 08:55:43, Epoch : 1, Step : 5539, Training Loss : 0.44573, Training Acc : 0.761, Run Time : 1.11
INFO:root:2019-05-12 08:55:53, Epoch : 1, Step : 5540, Training Loss : 0.57418, Training Acc : 0.722, Run Time : 10.49
INFO:root:2019-05-12 08:55:54, Epoch : 1, Step : 5541, Training Loss : 0.45790, Training Acc : 0.789, Run Time : 0.58
INFO:root:2019-05-12 08:55:58, Epoch : 1, Step : 5542, Training Loss : 0.38647, Training Acc : 0.789, Run Time : 3.99
INFO:root:2019-05-12 08:56:06, Epoch : 1, Step : 5543, Training Loss : 0.37051, Training Acc : 0.844, Run Time : 8.44
INFO:root:2019-05-12 08:56:07, Epoch : 1, Step : 5544, Training Loss : 0.51359, Training Acc : 0.722, Run Time : 0.54
INFO:root:2019-05-12 08:56:09, Epoch : 1, Step : 5545, Training Loss : 0.41614, Training Acc : 0.822, Run Time : 1.96
INFO:root:2019-05-12 08:56:17, Epoch : 1, Step : 5546, Training Loss : 0.34772, Training Acc : 0.839, Run Time : 8.47
INFO:root:2019-05-12 08:56:18, Epoch : 1, Step : 5547, Training Loss : 0.49335, Training Acc : 0.739, Run Time : 0.50
INFO:root:2019-05-12 08:56:19, Epoch : 1, Step : 5548, Training Loss : 0.30758, Training Acc : 0.872, Run Time : 1.41
INFO:root:2019-05-12 08:56:32, Epoch : 1, Step : 5549, Training Loss : 0.33374, Training Acc : 0.839, Run Time : 13.46
INFO:root:2019-05-12 08:56:34, Epoch : 1, Step : 5550, Training Loss : 0.41648, Training Acc : 0.794, Run Time : 1.44
INFO:root:2019-05-12 08:56:42, Epoch : 1, Step : 5551, Training Loss : 0.29312, Training Acc : 0.872, Run Time : 8.54
INFO:root:2019-05-12 08:56:43, Epoch : 1, Step : 5552, Training Loss : 0.23949, Training Acc : 0.911, Run Time : 1.01
INFO:root:2019-05-12 08:56:52, Epoch : 1, Step : 5553, Training Loss : 0.23054, Training Acc : 0.900, Run Time : 8.63
INFO:root:2019-05-12 08:56:53, Epoch : 1, Step : 5554, Training Loss : 0.24790, Training Acc : 0.878, Run Time : 0.91
INFO:root:2019-05-12 08:56:54, Epoch : 1, Step : 5555, Training Loss : 0.22426, Training Acc : 0.911, Run Time : 0.53
INFO:root:2019-05-12 08:57:06, Epoch : 1, Step : 5556, Training Loss : 0.21842, Training Acc : 0.906, Run Time : 12.71
INFO:root:2019-05-12 08:57:07, Epoch : 1, Step : 5557, Training Loss : 0.22291, Training Acc : 0.922, Run Time : 0.86
INFO:root:2019-05-12 08:57:20, Epoch : 1, Step : 5558, Training Loss : 0.23381, Training Acc : 0.917, Run Time : 13.20
INFO:root:2019-05-12 08:57:21, Epoch : 1, Step : 5559, Training Loss : 0.26369, Training Acc : 0.872, Run Time : 0.44
INFO:root:2019-05-12 08:57:23, Epoch : 1, Step : 5560, Training Loss : 0.30610, Training Acc : 0.878, Run Time : 2.04
INFO:root:2019-05-12 08:57:35, Epoch : 1, Step : 5561, Training Loss : 0.20372, Training Acc : 0.922, Run Time : 12.15
INFO:root:2019-05-12 08:57:39, Epoch : 1, Step : 5562, Training Loss : 0.24237, Training Acc : 0.928, Run Time : 4.53
INFO:root:2019-05-12 08:57:43, Epoch : 1, Step : 5563, Training Loss : 0.31761, Training Acc : 0.872, Run Time : 3.60
INFO:root:2019-05-12 08:57:44, Epoch : 1, Step : 5564, Training Loss : 0.26419, Training Acc : 0.911, Run Time : 0.43
INFO:root:2019-05-12 08:57:52, Epoch : 1, Step : 5565, Training Loss : 0.23490, Training Acc : 0.922, Run Time : 8.39
INFO:root:2019-05-12 08:57:55, Epoch : 1, Step : 5566, Training Loss : 0.23181, Training Acc : 0.894, Run Time : 3.39
INFO:root:2019-05-12 08:58:02, Epoch : 1, Step : 5567, Training Loss : 0.24214, Training Acc : 0.928, Run Time : 6.61
INFO:root:2019-05-12 08:58:03, Epoch : 1, Step : 5568, Training Loss : 0.23852, Training Acc : 0.917, Run Time : 1.07
INFO:root:2019-05-12 08:58:06, Epoch : 1, Step : 5569, Training Loss : 0.26191, Training Acc : 0.900, Run Time : 2.94
INFO:root:2019-05-12 08:58:07, Epoch : 1, Step : 5570, Training Loss : 0.38842, Training Acc : 0.833, Run Time : 1.53
INFO:root:2019-05-12 08:58:14, Epoch : 1, Step : 5571, Training Loss : 0.26174, Training Acc : 0.878, Run Time : 6.43
INFO:root:2019-05-12 08:58:14, Epoch : 1, Step : 5572, Training Loss : 0.38772, Training Acc : 0.794, Run Time : 0.53
INFO:root:2019-05-12 08:58:16, Epoch : 1, Step : 5573, Training Loss : 0.40797, Training Acc : 0.828, Run Time : 1.35
INFO:root:2019-05-12 08:58:18, Epoch : 1, Step : 5574, Training Loss : 0.25895, Training Acc : 0.883, Run Time : 2.70
INFO:root:2019-05-12 08:58:19, Epoch : 1, Step : 5575, Training Loss : 0.32624, Training Acc : 0.856, Run Time : 0.82
INFO:root:2019-05-12 08:58:27, Epoch : 1, Step : 5576, Training Loss : 0.36743, Training Acc : 0.850, Run Time : 8.06
INFO:root:2019-05-12 08:58:28, Epoch : 1, Step : 5577, Training Loss : 0.32504, Training Acc : 0.844, Run Time : 0.99
INFO:root:2019-05-12 08:58:30, Epoch : 1, Step : 5578, Training Loss : 0.28996, Training Acc : 0.867, Run Time : 2.12
INFO:root:2019-05-12 08:58:38, Epoch : 1, Step : 5579, Training Loss : 0.39458, Training Acc : 0.828, Run Time : 7.61
INFO:root:2019-05-12 08:58:39, Epoch : 1, Step : 5580, Training Loss : 0.35439, Training Acc : 0.828, Run Time : 1.21
INFO:root:2019-05-12 08:58:45, Epoch : 1, Step : 5581, Training Loss : 0.28156, Training Acc : 0.872, Run Time : 5.96
INFO:root:2019-05-12 08:58:46, Epoch : 1, Step : 5582, Training Loss : 0.26916, Training Acc : 0.872, Run Time : 0.60
INFO:root:2019-05-12 08:58:47, Epoch : 1, Step : 5583, Training Loss : 0.25228, Training Acc : 0.894, Run Time : 0.79
INFO:root:2019-05-12 08:58:51, Epoch : 1, Step : 5584, Training Loss : 0.29339, Training Acc : 0.844, Run Time : 4.67
INFO:root:2019-05-12 08:58:52, Epoch : 1, Step : 5585, Training Loss : 0.28619, Training Acc : 0.889, Run Time : 0.59
INFO:root:2019-05-12 08:58:53, Epoch : 1, Step : 5586, Training Loss : 0.28506, Training Acc : 0.872, Run Time : 0.75
INFO:root:2019-05-12 08:58:53, Epoch : 1, Step : 5587, Training Loss : 0.25991, Training Acc : 0.900, Run Time : 0.82
INFO:root:2019-05-12 08:59:04, Epoch : 1, Step : 5588, Training Loss : 0.26407, Training Acc : 0.889, Run Time : 10.57
INFO:root:2019-05-12 08:59:05, Epoch : 1, Step : 5589, Training Loss : 0.34790, Training Acc : 0.850, Run Time : 0.62
INFO:root:2019-05-12 08:59:05, Epoch : 1, Step : 5590, Training Loss : 0.52121, Training Acc : 0.783, Run Time : 0.56
INFO:root:2019-05-12 08:59:07, Epoch : 1, Step : 5591, Training Loss : 0.35716, Training Acc : 0.883, Run Time : 1.58
INFO:root:2019-05-12 08:59:17, Epoch : 1, Step : 5592, Training Loss : 0.20214, Training Acc : 0.906, Run Time : 10.71
INFO:root:2019-05-12 08:59:18, Epoch : 1, Step : 5593, Training Loss : 0.27668, Training Acc : 0.872, Run Time : 0.46
INFO:root:2019-05-12 08:59:29, Epoch : 1, Step : 5594, Training Loss : 0.26950, Training Acc : 0.889, Run Time : 11.26
INFO:root:2019-05-12 08:59:31, Epoch : 1, Step : 5595, Training Loss : 0.37635, Training Acc : 0.822, Run Time : 1.65
INFO:root:2019-05-12 08:59:32, Epoch : 1, Step : 5596, Training Loss : 0.29237, Training Acc : 0.889, Run Time : 1.22
INFO:root:2019-05-12 08:59:42, Epoch : 1, Step : 5597, Training Loss : 0.27853, Training Acc : 0.867, Run Time : 9.86
INFO:root:2019-05-12 08:59:42, Epoch : 1, Step : 5598, Training Loss : 0.22878, Training Acc : 0.894, Run Time : 0.47
INFO:root:2019-05-12 08:59:43, Epoch : 1, Step : 5599, Training Loss : 0.35085, Training Acc : 0.878, Run Time : 0.54
INFO:root:2019-05-12 08:59:56, Epoch : 1, Step : 5600, Training Loss : 0.35182, Training Acc : 0.828, Run Time : 12.68
INFO:root:2019-05-12 08:59:58, Epoch : 1, Step : 5601, Training Loss : 0.37767, Training Acc : 0.850, Run Time : 2.00
INFO:root:2019-05-12 08:59:58, Epoch : 1, Step : 5602, Training Loss : 0.47422, Training Acc : 0.806, Run Time : 0.66
INFO:root:2019-05-12 08:59:59, Epoch : 1, Step : 5603, Training Loss : 0.33179, Training Acc : 0.867, Run Time : 0.76
INFO:root:2019-05-12 09:00:00, Epoch : 1, Step : 5604, Training Loss : 0.37292, Training Acc : 0.783, Run Time : 1.38
INFO:root:2019-05-12 09:00:15, Epoch : 1, Step : 5605, Training Loss : 0.27231, Training Acc : 0.878, Run Time : 14.73
INFO:root:2019-05-12 09:00:16, Epoch : 1, Step : 5606, Training Loss : 0.20311, Training Acc : 0.928, Run Time : 0.69
INFO:root:2019-05-12 09:00:17, Epoch : 1, Step : 5607, Training Loss : 0.50969, Training Acc : 0.778, Run Time : 1.03
INFO:root:2019-05-12 09:00:26, Epoch : 1, Step : 5608, Training Loss : 0.66813, Training Acc : 0.694, Run Time : 9.05
INFO:root:2019-05-12 09:00:27, Epoch : 1, Step : 5609, Training Loss : 0.76024, Training Acc : 0.683, Run Time : 0.74
INFO:root:2019-05-12 09:00:27, Epoch : 1, Step : 5610, Training Loss : 0.64325, Training Acc : 0.694, Run Time : 0.58
INFO:root:2019-05-12 09:00:39, Epoch : 1, Step : 5611, Training Loss : 1.03886, Training Acc : 0.456, Run Time : 11.99
INFO:root:2019-05-12 09:00:41, Epoch : 1, Step : 5612, Training Loss : 0.58407, Training Acc : 0.700, Run Time : 1.32
INFO:root:2019-05-12 09:00:41, Epoch : 1, Step : 5613, Training Loss : 0.49152, Training Acc : 0.717, Run Time : 0.61
INFO:root:2019-05-12 09:00:46, Epoch : 1, Step : 5614, Training Loss : 0.60916, Training Acc : 0.728, Run Time : 4.66
INFO:root:2019-05-12 09:00:55, Epoch : 1, Step : 5615, Training Loss : 0.47347, Training Acc : 0.800, Run Time : 8.78
INFO:root:2019-05-12 09:01:07, Epoch : 1, Step : 5616, Training Loss : 0.60147, Training Acc : 0.717, Run Time : 12.06
INFO:root:2019-05-12 09:01:08, Epoch : 1, Step : 5617, Training Loss : 0.67892, Training Acc : 0.683, Run Time : 1.23
INFO:root:2019-05-12 09:01:10, Epoch : 1, Step : 5618, Training Loss : 0.80158, Training Acc : 0.683, Run Time : 1.70
INFO:root:2019-05-12 09:01:18, Epoch : 1, Step : 5619, Training Loss : 0.69655, Training Acc : 0.667, Run Time : 8.24
INFO:root:2019-05-12 09:01:18, Epoch : 1, Step : 5620, Training Loss : 0.67998, Training Acc : 0.694, Run Time : 0.49
INFO:root:2019-05-12 09:01:19, Epoch : 1, Step : 5621, Training Loss : 0.54249, Training Acc : 0.817, Run Time : 0.58
INFO:root:2019-05-12 09:01:20, Epoch : 1, Step : 5622, Training Loss : 0.53482, Training Acc : 0.722, Run Time : 0.89
INFO:root:2019-05-12 09:01:21, Epoch : 1, Step : 5623, Training Loss : 0.56661, Training Acc : 0.700, Run Time : 1.11
INFO:root:2019-05-12 09:01:34, Epoch : 1, Step : 5624, Training Loss : 0.39213, Training Acc : 0.822, Run Time : 13.31
INFO:root:2019-05-12 09:01:35, Epoch : 1, Step : 5625, Training Loss : 0.51644, Training Acc : 0.756, Run Time : 0.99
INFO:root:2019-05-12 09:01:47, Epoch : 1, Step : 5626, Training Loss : 0.51013, Training Acc : 0.744, Run Time : 12.22
INFO:root:2019-05-12 09:01:48, Epoch : 1, Step : 5627, Training Loss : 0.33588, Training Acc : 0.850, Run Time : 0.73
INFO:root:2019-05-12 09:01:59, Epoch : 1, Step : 5628, Training Loss : 0.67963, Training Acc : 0.667, Run Time : 11.16
INFO:root:2019-05-12 09:02:00, Epoch : 1, Step : 5629, Training Loss : 0.67487, Training Acc : 0.722, Run Time : 0.75
INFO:root:2019-05-12 09:02:02, Epoch : 1, Step : 5630, Training Loss : 0.67504, Training Acc : 0.678, Run Time : 1.95
INFO:root:2019-05-12 09:02:19, Epoch : 1, Step : 5631, Training Loss : 0.63713, Training Acc : 0.689, Run Time : 17.25
INFO:root:2019-05-12 09:02:21, Epoch : 1, Step : 5632, Training Loss : 0.45955, Training Acc : 0.778, Run Time : 1.90
INFO:root:2019-05-12 09:02:33, Epoch : 1, Step : 5633, Training Loss : 0.39888, Training Acc : 0.850, Run Time : 11.96
INFO:root:2019-05-12 09:02:44, Epoch : 1, Step : 5634, Training Loss : 0.59503, Training Acc : 0.711, Run Time : 10.40
INFO:root:2019-05-12 09:02:45, Epoch : 1, Step : 5635, Training Loss : 0.49693, Training Acc : 0.783, Run Time : 1.17
INFO:root:2019-05-12 09:02:52, Epoch : 1, Step : 5636, Training Loss : 0.64983, Training Acc : 0.717, Run Time : 7.71
INFO:root:2019-05-12 09:02:53, Epoch : 1, Step : 5637, Training Loss : 0.39383, Training Acc : 0.806, Run Time : 0.58
INFO:root:2019-05-12 09:02:55, Epoch : 1, Step : 5638, Training Loss : 0.48384, Training Acc : 0.778, Run Time : 2.01
INFO:root:2019-05-12 09:03:07, Epoch : 1, Step : 5639, Training Loss : 0.29906, Training Acc : 0.883, Run Time : 12.17
INFO:root:2019-05-12 09:03:19, Epoch : 1, Step : 5640, Training Loss : 0.44722, Training Acc : 0.828, Run Time : 11.80
INFO:root:2019-05-12 09:03:20, Epoch : 1, Step : 5641, Training Loss : 0.41843, Training Acc : 0.850, Run Time : 1.05
INFO:root:2019-05-12 09:03:22, Epoch : 1, Step : 5642, Training Loss : 0.43322, Training Acc : 0.817, Run Time : 1.73
INFO:root:2019-05-12 09:03:36, Epoch : 1, Step : 5643, Training Loss : 0.38754, Training Acc : 0.844, Run Time : 14.44
INFO:root:2019-05-12 09:03:42, Epoch : 1, Step : 5644, Training Loss : 0.40086, Training Acc : 0.817, Run Time : 5.55
INFO:root:2019-05-12 09:03:42, Epoch : 1, Step : 5645, Training Loss : 0.54424, Training Acc : 0.778, Run Time : 0.49
INFO:root:2019-05-12 09:03:43, Epoch : 1, Step : 5646, Training Loss : 0.40634, Training Acc : 0.822, Run Time : 0.62
INFO:root:2019-05-12 09:03:44, Epoch : 1, Step : 5647, Training Loss : 0.35681, Training Acc : 0.850, Run Time : 0.78
INFO:root:2019-05-12 09:03:54, Epoch : 1, Step : 5648, Training Loss : 0.53406, Training Acc : 0.744, Run Time : 9.97
INFO:root:2019-05-12 09:03:54, Epoch : 1, Step : 5649, Training Loss : 0.20099, Training Acc : 0.972, Run Time : 0.86
INFO:root:2019-05-12 09:03:56, Epoch : 1, Step : 5650, Training Loss : 0.32467, Training Acc : 0.872, Run Time : 1.58
INFO:root:2019-05-12 09:04:05, Epoch : 1, Step : 5651, Training Loss : 0.47188, Training Acc : 0.767, Run Time : 9.20
INFO:root:2019-05-12 09:04:06, Epoch : 1, Step : 5652, Training Loss : 0.41098, Training Acc : 0.811, Run Time : 0.61
INFO:root:2019-05-12 09:04:07, Epoch : 1, Step : 5653, Training Loss : 0.39208, Training Acc : 0.811, Run Time : 1.22
INFO:root:2019-05-12 09:04:20, Epoch : 1, Step : 5654, Training Loss : 0.31663, Training Acc : 0.867, Run Time : 12.89
INFO:root:2019-05-12 09:04:21, Epoch : 1, Step : 5655, Training Loss : 0.25503, Training Acc : 0.872, Run Time : 1.46
INFO:root:2019-05-12 09:04:30, Epoch : 1, Step : 5656, Training Loss : 0.31464, Training Acc : 0.861, Run Time : 8.72
INFO:root:2019-05-12 09:04:31, Epoch : 1, Step : 5657, Training Loss : 0.33180, Training Acc : 0.856, Run Time : 1.09
INFO:root:2019-05-12 09:04:32, Epoch : 1, Step : 5658, Training Loss : 0.32005, Training Acc : 0.889, Run Time : 0.60
INFO:root:2019-05-12 09:04:32, Epoch : 1, Step : 5659, Training Loss : 0.43001, Training Acc : 0.833, Run Time : 0.62
INFO:root:2019-05-12 09:04:45, Epoch : 1, Step : 5660, Training Loss : 0.37983, Training Acc : 0.850, Run Time : 12.45
INFO:root:2019-05-12 09:04:47, Epoch : 1, Step : 5661, Training Loss : 0.31043, Training Acc : 0.867, Run Time : 2.41
INFO:root:2019-05-12 09:04:58, Epoch : 1, Step : 5662, Training Loss : 0.25150, Training Acc : 0.917, Run Time : 10.47
INFO:root:2019-05-12 09:04:58, Epoch : 1, Step : 5663, Training Loss : 0.28331, Training Acc : 0.878, Run Time : 0.61
INFO:root:2019-05-12 09:04:59, Epoch : 1, Step : 5664, Training Loss : 0.33915, Training Acc : 0.839, Run Time : 0.93
INFO:root:2019-05-12 09:05:13, Epoch : 1, Step : 5665, Training Loss : 0.22287, Training Acc : 0.911, Run Time : 13.32
INFO:root:2019-05-12 09:05:14, Epoch : 1, Step : 5666, Training Loss : 0.26861, Training Acc : 0.889, Run Time : 0.91
INFO:root:2019-05-12 09:05:26, Epoch : 1, Step : 5667, Training Loss : 0.36564, Training Acc : 0.839, Run Time : 11.97
INFO:root:2019-05-12 09:05:26, Epoch : 1, Step : 5668, Training Loss : 0.16991, Training Acc : 0.922, Run Time : 0.47
INFO:root:2019-05-12 09:05:38, Epoch : 1, Step : 5669, Training Loss : 0.27827, Training Acc : 0.883, Run Time : 12.48
INFO:root:2019-05-12 09:05:39, Epoch : 1, Step : 5670, Training Loss : 0.30044, Training Acc : 0.839, Run Time : 0.84
INFO:root:2019-05-12 09:05:41, Epoch : 1, Step : 5671, Training Loss : 0.34452, Training Acc : 0.844, Run Time : 2.11
INFO:root:2019-05-12 09:05:53, Epoch : 1, Step : 5672, Training Loss : 0.34759, Training Acc : 0.856, Run Time : 11.96
INFO:root:2019-05-12 09:05:54, Epoch : 1, Step : 5673, Training Loss : 0.24436, Training Acc : 0.900, Run Time : 0.79
INFO:root:2019-05-12 09:06:06, Epoch : 1, Step : 5674, Training Loss : 0.31301, Training Acc : 0.822, Run Time : 11.92
INFO:root:2019-05-12 09:06:07, Epoch : 1, Step : 5675, Training Loss : 0.33656, Training Acc : 0.839, Run Time : 0.71
INFO:root:2019-05-12 09:06:07, Epoch : 1, Step : 5676, Training Loss : 0.20690, Training Acc : 0.933, Run Time : 0.51
INFO:root:2019-05-12 09:06:21, Epoch : 1, Step : 5677, Training Loss : 0.20917, Training Acc : 0.922, Run Time : 13.77
INFO:root:2019-05-12 09:06:22, Epoch : 1, Step : 5678, Training Loss : 0.39344, Training Acc : 0.811, Run Time : 0.69
INFO:root:2019-05-12 09:06:23, Epoch : 1, Step : 5679, Training Loss : 0.41316, Training Acc : 0.817, Run Time : 1.43
INFO:root:2019-05-12 09:06:34, Epoch : 1, Step : 5680, Training Loss : 0.13630, Training Acc : 0.961, Run Time : 10.99
INFO:root:2019-05-12 09:06:35, Epoch : 1, Step : 5681, Training Loss : 0.29231, Training Acc : 0.939, Run Time : 0.80
INFO:root:2019-05-12 09:06:36, Epoch : 1, Step : 5682, Training Loss : 0.20479, Training Acc : 0.950, Run Time : 1.41
INFO:root:2019-05-12 09:06:44, Epoch : 1, Step : 5683, Training Loss : 0.26593, Training Acc : 0.933, Run Time : 7.85
INFO:root:2019-05-12 09:06:45, Epoch : 1, Step : 5684, Training Loss : 0.19484, Training Acc : 0.944, Run Time : 0.65
INFO:root:2019-05-12 09:06:47, Epoch : 1, Step : 5685, Training Loss : 0.33364, Training Acc : 0.883, Run Time : 1.61
INFO:root:2019-05-12 09:06:54, Epoch : 1, Step : 5686, Training Loss : 0.44899, Training Acc : 0.856, Run Time : 7.26
INFO:root:2019-05-12 09:06:54, Epoch : 1, Step : 5687, Training Loss : 0.27585, Training Acc : 0.922, Run Time : 0.70
INFO:root:2019-05-12 09:07:04, Epoch : 1, Step : 5688, Training Loss : 0.28729, Training Acc : 0.889, Run Time : 9.12
INFO:root:2019-05-12 09:07:04, Epoch : 1, Step : 5689, Training Loss : 0.36931, Training Acc : 0.872, Run Time : 0.67
INFO:root:2019-05-12 09:07:06, Epoch : 1, Step : 5690, Training Loss : 0.31807, Training Acc : 0.889, Run Time : 1.35
INFO:root:2019-05-12 09:07:13, Epoch : 1, Step : 5691, Training Loss : 0.28576, Training Acc : 0.889, Run Time : 7.35
INFO:root:2019-05-12 09:07:14, Epoch : 1, Step : 5692, Training Loss : 0.27448, Training Acc : 0.900, Run Time : 0.56
INFO:root:2019-05-12 09:07:15, Epoch : 1, Step : 5693, Training Loss : 0.26131, Training Acc : 0.911, Run Time : 1.46
INFO:root:2019-05-12 09:07:23, Epoch : 1, Step : 5694, Training Loss : 0.28394, Training Acc : 0.889, Run Time : 8.45
INFO:root:2019-05-12 09:07:24, Epoch : 1, Step : 5695, Training Loss : 0.32855, Training Acc : 0.856, Run Time : 0.56
INFO:root:2019-05-12 09:07:26, Epoch : 1, Step : 5696, Training Loss : 0.37571, Training Acc : 0.861, Run Time : 1.85
INFO:root:2019-05-12 09:07:36, Epoch : 1, Step : 5697, Training Loss : 0.36603, Training Acc : 0.850, Run Time : 10.66
INFO:root:2019-05-12 09:07:38, Epoch : 1, Step : 5698, Training Loss : 0.26424, Training Acc : 0.906, Run Time : 1.45
INFO:root:2019-05-12 09:07:49, Epoch : 1, Step : 5699, Training Loss : 0.19128, Training Acc : 0.933, Run Time : 11.54
INFO:root:2019-05-12 09:07:50, Epoch : 1, Step : 5700, Training Loss : 0.29267, Training Acc : 0.878, Run Time : 0.46
INFO:root:2019-05-12 09:07:52, Epoch : 1, Step : 5701, Training Loss : 0.23532, Training Acc : 0.922, Run Time : 2.14
INFO:root:2019-05-12 09:08:06, Epoch : 1, Step : 5702, Training Loss : 0.30856, Training Acc : 0.861, Run Time : 13.91
INFO:root:2019-05-12 09:08:09, Epoch : 1, Step : 5703, Training Loss : 0.28161, Training Acc : 0.878, Run Time : 3.11
INFO:root:2019-05-12 09:08:19, Epoch : 1, Step : 5704, Training Loss : 0.23824, Training Acc : 0.900, Run Time : 10.17
INFO:root:2019-05-12 09:08:20, Epoch : 1, Step : 5705, Training Loss : 0.22465, Training Acc : 0.933, Run Time : 0.48
INFO:root:2019-05-12 09:08:20, Epoch : 1, Step : 5706, Training Loss : 0.21865, Training Acc : 0.928, Run Time : 0.57
INFO:root:2019-05-12 09:08:32, Epoch : 1, Step : 5707, Training Loss : 0.21895, Training Acc : 0.928, Run Time : 11.41
INFO:root:2019-05-12 09:08:32, Epoch : 1, Step : 5708, Training Loss : 0.19207, Training Acc : 0.928, Run Time : 0.74
INFO:root:2019-05-12 09:08:34, Epoch : 1, Step : 5709, Training Loss : 0.21393, Training Acc : 0.928, Run Time : 1.61
INFO:root:2019-05-12 09:08:45, Epoch : 1, Step : 5710, Training Loss : 0.14837, Training Acc : 0.961, Run Time : 10.98
INFO:root:2019-05-12 09:08:46, Epoch : 1, Step : 5711, Training Loss : 0.17578, Training Acc : 0.944, Run Time : 0.55
INFO:root:2019-05-12 09:08:46, Epoch : 1, Step : 5712, Training Loss : 0.13862, Training Acc : 0.972, Run Time : 0.73
INFO:root:2019-05-12 09:08:55, Epoch : 1, Step : 5713, Training Loss : 0.23604, Training Acc : 0.900, Run Time : 9.02
INFO:root:2019-05-12 09:08:56, Epoch : 1, Step : 5714, Training Loss : 0.19244, Training Acc : 0.944, Run Time : 0.43
INFO:root:2019-05-12 09:08:56, Epoch : 1, Step : 5715, Training Loss : 0.18834, Training Acc : 0.939, Run Time : 0.68
INFO:root:2019-05-12 09:09:04, Epoch : 1, Step : 5716, Training Loss : 0.26601, Training Acc : 0.922, Run Time : 7.80
INFO:root:2019-05-12 09:09:05, Epoch : 1, Step : 5717, Training Loss : 0.25802, Training Acc : 0.922, Run Time : 0.69
INFO:root:2019-05-12 09:09:06, Epoch : 1, Step : 5718, Training Loss : 0.23786, Training Acc : 0.928, Run Time : 0.58
INFO:root:2019-05-12 09:09:14, Epoch : 1, Step : 5719, Training Loss : 0.24495, Training Acc : 0.917, Run Time : 8.24
INFO:root:2019-05-12 09:09:14, Epoch : 1, Step : 5720, Training Loss : 0.16983, Training Acc : 0.933, Run Time : 0.55
INFO:root:2019-05-12 09:09:16, Epoch : 1, Step : 5721, Training Loss : 0.17791, Training Acc : 0.939, Run Time : 1.32
INFO:root:2019-05-12 09:09:25, Epoch : 1, Step : 5722, Training Loss : 0.16855, Training Acc : 0.961, Run Time : 8.94
INFO:root:2019-05-12 09:09:25, Epoch : 1, Step : 5723, Training Loss : 0.19787, Training Acc : 0.933, Run Time : 0.55
INFO:root:2019-05-12 09:09:27, Epoch : 1, Step : 5724, Training Loss : 0.12921, Training Acc : 0.956, Run Time : 1.80
INFO:root:2019-05-12 09:09:38, Epoch : 1, Step : 5725, Training Loss : 0.18954, Training Acc : 0.961, Run Time : 11.07
INFO:root:2019-05-12 09:09:39, Epoch : 1, Step : 5726, Training Loss : 0.16559, Training Acc : 0.967, Run Time : 0.72
INFO:root:2019-05-12 09:09:47, Epoch : 1, Step : 5727, Training Loss : 0.20124, Training Acc : 0.922, Run Time : 7.94
INFO:root:2019-05-12 09:09:48, Epoch : 1, Step : 5728, Training Loss : 0.23483, Training Acc : 0.906, Run Time : 0.96
INFO:root:2019-05-12 09:09:57, Epoch : 1, Step : 5729, Training Loss : 0.14291, Training Acc : 0.950, Run Time : 9.26
INFO:root:2019-05-12 09:09:58, Epoch : 1, Step : 5730, Training Loss : 0.20332, Training Acc : 0.961, Run Time : 1.49
INFO:root:2019-05-12 09:09:59, Epoch : 1, Step : 5731, Training Loss : 0.30100, Training Acc : 0.894, Run Time : 0.97
INFO:root:2019-05-12 09:10:09, Epoch : 1, Step : 5732, Training Loss : 0.27380, Training Acc : 0.906, Run Time : 10.07
INFO:root:2019-05-12 09:10:10, Epoch : 1, Step : 5733, Training Loss : 0.31673, Training Acc : 0.900, Run Time : 0.45
INFO:root:2019-05-12 09:10:11, Epoch : 1, Step : 5734, Training Loss : 0.16948, Training Acc : 0.950, Run Time : 0.65
INFO:root:2019-05-12 09:10:21, Epoch : 1, Step : 5735, Training Loss : 0.32080, Training Acc : 0.844, Run Time : 10.05
INFO:root:2019-05-12 09:10:22, Epoch : 1, Step : 5736, Training Loss : 0.21653, Training Acc : 0.894, Run Time : 0.99
INFO:root:2019-05-12 09:10:22, Epoch : 1, Step : 5737, Training Loss : 0.25250, Training Acc : 0.894, Run Time : 0.72
INFO:root:2019-05-12 09:10:36, Epoch : 1, Step : 5738, Training Loss : 0.26742, Training Acc : 0.889, Run Time : 13.53
INFO:root:2019-05-12 09:10:37, Epoch : 1, Step : 5739, Training Loss : 0.20703, Training Acc : 0.922, Run Time : 1.18
INFO:root:2019-05-12 09:10:38, Epoch : 1, Step : 5740, Training Loss : 0.31166, Training Acc : 0.867, Run Time : 1.33
INFO:root:2019-05-12 09:10:53, Epoch : 1, Step : 5741, Training Loss : 0.24240, Training Acc : 0.906, Run Time : 14.94
INFO:root:2019-05-12 09:10:58, Epoch : 1, Step : 5742, Training Loss : 0.36066, Training Acc : 0.856, Run Time : 4.39
INFO:root:2019-05-12 09:10:58, Epoch : 1, Step : 5743, Training Loss : 0.26942, Training Acc : 0.894, Run Time : 0.51
INFO:root:2019-05-12 09:11:01, Epoch : 1, Step : 5744, Training Loss : 0.40842, Training Acc : 0.828, Run Time : 2.47
INFO:root:2019-05-12 09:11:04, Epoch : 1, Step : 5745, Training Loss : 0.36739, Training Acc : 0.872, Run Time : 3.75
INFO:root:2019-05-12 09:11:05, Epoch : 1, Step : 5746, Training Loss : 0.48126, Training Acc : 0.800, Run Time : 0.64
INFO:root:2019-05-12 09:11:06, Epoch : 1, Step : 5747, Training Loss : 0.54677, Training Acc : 0.783, Run Time : 0.68
INFO:root:2019-05-12 09:11:12, Epoch : 1, Step : 5748, Training Loss : 0.36688, Training Acc : 0.817, Run Time : 6.61
INFO:root:2019-05-12 09:11:13, Epoch : 1, Step : 5749, Training Loss : 0.26032, Training Acc : 0.894, Run Time : 0.94
INFO:root:2019-05-12 09:11:18, Epoch : 1, Step : 5750, Training Loss : 0.39029, Training Acc : 0.833, Run Time : 4.90
INFO:root:2019-05-12 09:11:19, Epoch : 1, Step : 5751, Training Loss : 0.33895, Training Acc : 0.861, Run Time : 0.49
INFO:root:2019-05-12 09:11:19, Epoch : 1, Step : 5752, Training Loss : 0.30338, Training Acc : 0.894, Run Time : 0.61
INFO:root:2019-05-12 09:11:21, Epoch : 1, Step : 5753, Training Loss : 0.36567, Training Acc : 0.844, Run Time : 1.77
INFO:root:2019-05-12 09:11:22, Epoch : 1, Step : 5754, Training Loss : 0.38117, Training Acc : 0.844, Run Time : 0.72
INFO:root:2019-05-12 09:11:37, Epoch : 1, Step : 5755, Training Loss : 0.36964, Training Acc : 0.867, Run Time : 15.71
INFO:root:2019-05-12 09:11:38, Epoch : 1, Step : 5756, Training Loss : 0.36327, Training Acc : 0.839, Run Time : 0.97
INFO:root:2019-05-12 09:11:40, Epoch : 1, Step : 5757, Training Loss : 0.27753, Training Acc : 0.922, Run Time : 1.17
INFO:root:2019-05-12 09:11:54, Epoch : 1, Step : 5758, Training Loss : 0.33152, Training Acc : 0.844, Run Time : 14.08
INFO:root:2019-05-12 09:12:02, Epoch : 1, Step : 5759, Training Loss : 0.20807, Training Acc : 0.922, Run Time : 8.72
INFO:root:2019-05-12 09:12:05, Epoch : 1, Step : 5760, Training Loss : 0.28484, Training Acc : 0.894, Run Time : 3.07
INFO:root:2019-05-12 09:12:06, Epoch : 1, Step : 5761, Training Loss : 0.35483, Training Acc : 0.839, Run Time : 0.70
INFO:root:2019-05-12 09:12:14, Epoch : 1, Step : 5762, Training Loss : 0.16260, Training Acc : 0.961, Run Time : 7.92
INFO:root:2019-05-12 09:12:16, Epoch : 1, Step : 5763, Training Loss : 0.20258, Training Acc : 0.922, Run Time : 1.46
INFO:root:2019-05-12 09:12:16, Epoch : 1, Step : 5764, Training Loss : 0.14663, Training Acc : 0.956, Run Time : 0.68
INFO:root:2019-05-12 09:12:18, Epoch : 1, Step : 5765, Training Loss : 0.23260, Training Acc : 0.922, Run Time : 1.32
INFO:root:2019-05-12 09:12:27, Epoch : 1, Step : 5766, Training Loss : 0.14958, Training Acc : 0.950, Run Time : 9.16
INFO:root:2019-05-12 09:12:27, Epoch : 1, Step : 5767, Training Loss : 0.38166, Training Acc : 0.833, Run Time : 0.58
INFO:root:2019-05-12 09:12:29, Epoch : 1, Step : 5768, Training Loss : 0.34386, Training Acc : 0.872, Run Time : 1.95
INFO:root:2019-05-12 09:12:42, Epoch : 1, Step : 5769, Training Loss : 0.43606, Training Acc : 0.811, Run Time : 12.88
INFO:root:2019-05-12 09:12:43, Epoch : 1, Step : 5770, Training Loss : 0.19874, Training Acc : 0.922, Run Time : 0.43
INFO:root:2019-05-12 09:12:44, Epoch : 1, Step : 5771, Training Loss : 0.25116, Training Acc : 0.883, Run Time : 1.58
INFO:root:2019-05-12 09:12:56, Epoch : 1, Step : 5772, Training Loss : 0.31327, Training Acc : 0.889, Run Time : 12.08
INFO:root:2019-05-12 09:12:57, Epoch : 1, Step : 5773, Training Loss : 0.29020, Training Acc : 0.883, Run Time : 0.63
INFO:root:2019-05-12 09:12:57, Epoch : 1, Step : 5774, Training Loss : 0.31436, Training Acc : 0.883, Run Time : 0.61
INFO:root:2019-05-12 09:13:10, Epoch : 1, Step : 5775, Training Loss : 0.43431, Training Acc : 0.822, Run Time : 13.03
INFO:root:2019-05-12 09:13:11, Epoch : 1, Step : 5776, Training Loss : 0.45819, Training Acc : 0.822, Run Time : 0.97
INFO:root:2019-05-12 09:13:23, Epoch : 1, Step : 5777, Training Loss : 0.41120, Training Acc : 0.833, Run Time : 11.90
INFO:root:2019-05-12 09:13:24, Epoch : 1, Step : 5778, Training Loss : 0.34039, Training Acc : 0.856, Run Time : 0.69
INFO:root:2019-05-12 09:13:37, Epoch : 1, Step : 5779, Training Loss : 0.41093, Training Acc : 0.822, Run Time : 13.00
INFO:root:2019-05-12 09:13:38, Epoch : 1, Step : 5780, Training Loss : 0.31378, Training Acc : 0.889, Run Time : 1.05
INFO:root:2019-05-12 09:13:39, Epoch : 1, Step : 5781, Training Loss : 0.32979, Training Acc : 0.878, Run Time : 0.60
INFO:root:2019-05-12 09:13:39, Epoch : 1, Step : 5782, Training Loss : 0.32478, Training Acc : 0.839, Run Time : 0.63
INFO:root:2019-05-12 09:13:40, Epoch : 1, Step : 5783, Training Loss : 0.26451, Training Acc : 0.889, Run Time : 0.57
INFO:root:2019-05-12 09:13:55, Epoch : 1, Step : 5784, Training Loss : 0.26699, Training Acc : 0.894, Run Time : 15.57
INFO:root:2019-05-12 09:13:56, Epoch : 1, Step : 5785, Training Loss : 0.28216, Training Acc : 0.906, Run Time : 0.85
INFO:root:2019-05-12 09:13:57, Epoch : 1, Step : 5786, Training Loss : 0.25204, Training Acc : 0.917, Run Time : 0.70
INFO:root:2019-05-12 09:14:11, Epoch : 1, Step : 5787, Training Loss : 0.27322, Training Acc : 0.917, Run Time : 13.84
INFO:root:2019-05-12 09:14:13, Epoch : 1, Step : 5788, Training Loss : 0.18719, Training Acc : 0.956, Run Time : 2.01
INFO:root:2019-05-12 09:14:24, Epoch : 1, Step : 5789, Training Loss : 0.16310, Training Acc : 0.967, Run Time : 10.76
INFO:root:2019-05-12 09:14:24, Epoch : 1, Step : 5790, Training Loss : 0.21955, Training Acc : 0.944, Run Time : 0.74
INFO:root:2019-05-12 09:14:25, Epoch : 1, Step : 5791, Training Loss : 0.26640, Training Acc : 0.911, Run Time : 0.62
INFO:root:2019-05-12 09:14:41, Epoch : 1, Step : 5792, Training Loss : 0.14161, Training Acc : 0.967, Run Time : 16.45
INFO:root:2019-05-12 09:14:43, Epoch : 1, Step : 5793, Training Loss : 0.25590, Training Acc : 0.922, Run Time : 1.20
INFO:root:2019-05-12 09:14:57, Epoch : 1, Step : 5794, Training Loss : 0.25026, Training Acc : 0.917, Run Time : 14.23
INFO:root:2019-05-12 09:14:58, Epoch : 1, Step : 5795, Training Loss : 0.16659, Training Acc : 0.967, Run Time : 0.95
INFO:root:2019-05-12 09:15:13, Epoch : 1, Step : 5796, Training Loss : 0.21432, Training Acc : 0.922, Run Time : 14.97
INFO:root:2019-05-12 09:15:13, Epoch : 1, Step : 5797, Training Loss : 0.41564, Training Acc : 0.850, Run Time : 0.51
INFO:root:2019-05-12 09:15:14, Epoch : 1, Step : 5798, Training Loss : 0.29073, Training Acc : 0.917, Run Time : 0.45
INFO:root:2019-05-12 09:15:30, Epoch : 1, Step : 5799, Training Loss : 0.44983, Training Acc : 0.806, Run Time : 15.79
INFO:root:2019-05-12 09:15:30, Epoch : 1, Step : 5800, Training Loss : 0.50898, Training Acc : 0.800, Run Time : 0.63
INFO:root:2019-05-12 09:15:43, Epoch : 1, Step : 5801, Training Loss : 0.43282, Training Acc : 0.778, Run Time : 12.73
INFO:root:2019-05-12 09:15:43, Epoch : 1, Step : 5802, Training Loss : 0.38711, Training Acc : 0.794, Run Time : 0.46
INFO:root:2019-05-12 09:15:44, Epoch : 1, Step : 5803, Training Loss : 0.27430, Training Acc : 0.856, Run Time : 0.58
INFO:root:2019-05-12 09:15:54, Epoch : 1, Step : 5804, Training Loss : 0.30108, Training Acc : 0.872, Run Time : 10.27
INFO:root:2019-05-12 09:15:56, Epoch : 1, Step : 5805, Training Loss : 0.20726, Training Acc : 0.922, Run Time : 2.28
INFO:root:2019-05-12 09:15:57, Epoch : 1, Step : 5806, Training Loss : 0.21295, Training Acc : 0.922, Run Time : 0.58
INFO:root:2019-05-12 09:15:59, Epoch : 1, Step : 5807, Training Loss : 0.15692, Training Acc : 0.956, Run Time : 1.54
INFO:root:2019-05-12 09:16:07, Epoch : 1, Step : 5808, Training Loss : 0.13226, Training Acc : 0.972, Run Time : 8.91
INFO:root:2019-05-12 09:16:08, Epoch : 1, Step : 5809, Training Loss : 0.15141, Training Acc : 0.944, Run Time : 0.74
INFO:root:2019-05-12 09:16:20, Epoch : 1, Step : 5810, Training Loss : 0.12646, Training Acc : 0.972, Run Time : 11.85
INFO:root:2019-05-12 09:16:21, Epoch : 1, Step : 5811, Training Loss : 0.11227, Training Acc : 0.939, Run Time : 0.94
INFO:root:2019-05-12 09:16:23, Epoch : 1, Step : 5812, Training Loss : 0.14354, Training Acc : 0.933, Run Time : 1.49
INFO:root:2019-05-12 09:16:32, Epoch : 1, Step : 5813, Training Loss : 0.14755, Training Acc : 0.933, Run Time : 9.28
INFO:root:2019-05-12 09:16:32, Epoch : 1, Step : 5814, Training Loss : 0.12768, Training Acc : 0.944, Run Time : 0.41
INFO:root:2019-05-12 09:16:33, Epoch : 1, Step : 5815, Training Loss : 0.15874, Training Acc : 0.922, Run Time : 0.63
INFO:root:2019-05-12 09:16:45, Epoch : 1, Step : 5816, Training Loss : 0.14641, Training Acc : 0.928, Run Time : 11.98
INFO:root:2019-05-12 09:16:46, Epoch : 1, Step : 5817, Training Loss : 0.16123, Training Acc : 0.911, Run Time : 0.98
INFO:root:2019-05-12 09:16:48, Epoch : 1, Step : 5818, Training Loss : 0.18452, Training Acc : 0.917, Run Time : 1.88
INFO:root:2019-05-12 09:16:56, Epoch : 1, Step : 5819, Training Loss : 0.16810, Training Acc : 0.906, Run Time : 8.54
INFO:root:2019-05-12 09:16:57, Epoch : 1, Step : 5820, Training Loss : 0.16543, Training Acc : 0.917, Run Time : 0.42
INFO:root:2019-05-12 09:16:58, Epoch : 1, Step : 5821, Training Loss : 0.16270, Training Acc : 0.922, Run Time : 1.67
INFO:root:2019-05-12 09:17:10, Epoch : 1, Step : 5822, Training Loss : 0.14765, Training Acc : 0.922, Run Time : 11.55
INFO:root:2019-05-12 09:17:11, Epoch : 1, Step : 5823, Training Loss : 0.16979, Training Acc : 0.939, Run Time : 1.26
INFO:root:2019-05-12 09:17:31, Epoch : 1, Step : 5824, Training Loss : 0.12959, Training Acc : 0.939, Run Time : 19.60
INFO:root:2019-05-12 09:17:42, Epoch : 1, Step : 5825, Training Loss : 0.12511, Training Acc : 0.939, Run Time : 11.64
INFO:root:2019-05-12 09:17:55, Epoch : 1, Step : 5826, Training Loss : 0.12632, Training Acc : 0.950, Run Time : 12.95
INFO:root:2019-05-12 09:17:59, Epoch : 1, Step : 5827, Training Loss : 0.14186, Training Acc : 0.933, Run Time : 4.17
INFO:root:2019-05-12 09:18:06, Epoch : 1, Step : 5828, Training Loss : 0.11555, Training Acc : 0.956, Run Time : 6.32
INFO:root:2019-05-12 09:18:16, Epoch : 1, Step : 5829, Training Loss : 0.09861, Training Acc : 0.967, Run Time : 9.73
INFO:root:2019-05-12 09:18:17, Epoch : 1, Step : 5830, Training Loss : 0.13634, Training Acc : 0.944, Run Time : 1.19
INFO:root:2019-05-12 09:18:17, Epoch : 1, Step : 5831, Training Loss : 0.14854, Training Acc : 0.917, Run Time : 0.64
INFO:root:2019-05-12 09:18:20, Epoch : 1, Step : 5832, Training Loss : 0.16614, Training Acc : 0.933, Run Time : 2.21
INFO:root:2019-05-12 09:18:32, Epoch : 1, Step : 5833, Training Loss : 0.21127, Training Acc : 0.933, Run Time : 12.33
INFO:root:2019-05-12 09:18:33, Epoch : 1, Step : 5834, Training Loss : 0.16603, Training Acc : 0.939, Run Time : 0.71
INFO:root:2019-05-12 09:18:33, Epoch : 1, Step : 5835, Training Loss : 0.11623, Training Acc : 0.972, Run Time : 0.58
INFO:root:2019-05-12 09:18:34, Epoch : 1, Step : 5836, Training Loss : 0.11734, Training Acc : 0.956, Run Time : 1.06
INFO:root:2019-05-12 09:18:44, Epoch : 1, Step : 5837, Training Loss : 0.15968, Training Acc : 0.933, Run Time : 9.62
INFO:root:2019-05-12 09:18:45, Epoch : 1, Step : 5838, Training Loss : 0.16905, Training Acc : 0.928, Run Time : 1.05
INFO:root:2019-05-12 09:18:46, Epoch : 1, Step : 5839, Training Loss : 0.37611, Training Acc : 0.900, Run Time : 1.15
INFO:root:2019-05-12 09:18:54, Epoch : 1, Step : 5840, Training Loss : 0.22492, Training Acc : 0.917, Run Time : 7.77
INFO:root:2019-05-12 09:18:55, Epoch : 1, Step : 5841, Training Loss : 0.17606, Training Acc : 0.922, Run Time : 0.84
INFO:root:2019-05-12 09:19:00, Epoch : 1, Step : 5842, Training Loss : 0.15063, Training Acc : 0.939, Run Time : 5.75
INFO:root:2019-05-12 09:19:01, Epoch : 1, Step : 5843, Training Loss : 0.27943, Training Acc : 0.867, Run Time : 0.62
INFO:root:2019-05-12 09:19:02, Epoch : 1, Step : 5844, Training Loss : 0.31668, Training Acc : 0.850, Run Time : 0.64
INFO:root:2019-05-12 09:19:11, Epoch : 1, Step : 5845, Training Loss : 0.27327, Training Acc : 0.856, Run Time : 9.07
INFO:root:2019-05-12 09:19:11, Epoch : 1, Step : 5846, Training Loss : 0.28828, Training Acc : 0.861, Run Time : 0.45
INFO:root:2019-05-12 09:19:13, Epoch : 1, Step : 5847, Training Loss : 0.37500, Training Acc : 0.828, Run Time : 1.63
INFO:root:2019-05-12 09:19:24, Epoch : 1, Step : 5848, Training Loss : 0.28814, Training Acc : 0.906, Run Time : 11.50
INFO:root:2019-05-12 09:19:25, Epoch : 1, Step : 5849, Training Loss : 0.14178, Training Acc : 0.933, Run Time : 1.06
INFO:root:2019-05-12 09:19:42, Epoch : 1, Step : 5850, Training Loss : 0.15962, Training Acc : 0.939, Run Time : 16.61
INFO:root:2019-05-12 09:19:56, Epoch : 1, Step : 5851, Training Loss : 0.18281, Training Acc : 0.922, Run Time : 13.70
INFO:root:2019-05-12 09:20:00, Epoch : 1, Step : 5852, Training Loss : 0.20174, Training Acc : 0.911, Run Time : 3.96
INFO:root:2019-05-12 09:20:00, Epoch : 1, Step : 5853, Training Loss : 0.16461, Training Acc : 0.928, Run Time : 0.63
INFO:root:2019-05-12 09:20:11, Epoch : 1, Step : 5854, Training Loss : 0.26941, Training Acc : 0.894, Run Time : 11.11
INFO:root:2019-05-12 09:20:12, Epoch : 1, Step : 5855, Training Loss : 0.18229, Training Acc : 0.906, Run Time : 0.73
INFO:root:2019-05-12 09:20:14, Epoch : 1, Step : 5856, Training Loss : 0.16962, Training Acc : 0.928, Run Time : 2.07
INFO:root:2019-05-12 09:20:26, Epoch : 1, Step : 5857, Training Loss : 0.12916, Training Acc : 0.933, Run Time : 11.75
INFO:root:2019-05-12 09:20:27, Epoch : 1, Step : 5858, Training Loss : 0.24565, Training Acc : 0.889, Run Time : 1.31
INFO:root:2019-05-12 09:20:36, Epoch : 1, Step : 5859, Training Loss : 0.14319, Training Acc : 0.933, Run Time : 8.32
INFO:root:2019-05-12 09:20:39, Epoch : 1, Step : 5860, Training Loss : 0.15254, Training Acc : 0.950, Run Time : 2.94
INFO:root:2019-05-12 09:20:39, Epoch : 1, Step : 5861, Training Loss : 0.14650, Training Acc : 0.939, Run Time : 0.80
INFO:root:2019-05-12 09:20:52, Epoch : 1, Step : 5862, Training Loss : 0.10706, Training Acc : 0.961, Run Time : 12.57
INFO:root:2019-05-12 09:20:53, Epoch : 1, Step : 5863, Training Loss : 0.16354, Training Acc : 0.928, Run Time : 0.71
INFO:root:2019-05-12 09:20:54, Epoch : 1, Step : 5864, Training Loss : 0.19834, Training Acc : 0.928, Run Time : 1.08
INFO:root:2019-05-12 09:21:04, Epoch : 1, Step : 5865, Training Loss : 0.21078, Training Acc : 0.956, Run Time : 10.17
INFO:root:2019-05-12 09:21:05, Epoch : 1, Step : 5866, Training Loss : 0.11798, Training Acc : 0.967, Run Time : 1.14
INFO:root:2019-05-12 09:21:18, Epoch : 1, Step : 5867, Training Loss : 0.16469, Training Acc : 0.939, Run Time : 13.14
INFO:root:2019-05-12 09:21:19, Epoch : 1, Step : 5868, Training Loss : 0.14173, Training Acc : 0.950, Run Time : 0.64
INFO:root:2019-05-12 09:21:20, Epoch : 1, Step : 5869, Training Loss : 0.13916, Training Acc : 0.939, Run Time : 0.74
INFO:root:2019-05-12 09:21:31, Epoch : 1, Step : 5870, Training Loss : 0.13583, Training Acc : 0.950, Run Time : 11.52
INFO:root:2019-05-12 09:21:32, Epoch : 1, Step : 5871, Training Loss : 0.13726, Training Acc : 0.950, Run Time : 0.58
INFO:root:2019-05-12 09:21:32, Epoch : 1, Step : 5872, Training Loss : 0.18509, Training Acc : 0.911, Run Time : 0.78
INFO:root:2019-05-12 09:21:34, Epoch : 1, Step : 5873, Training Loss : 0.15723, Training Acc : 0.950, Run Time : 1.73
INFO:root:2019-05-12 09:21:44, Epoch : 1, Step : 5874, Training Loss : 0.11978, Training Acc : 0.967, Run Time : 9.93
INFO:root:2019-05-12 09:21:45, Epoch : 1, Step : 5875, Training Loss : 0.16013, Training Acc : 0.906, Run Time : 0.87
INFO:root:2019-05-12 09:21:58, Epoch : 1, Step : 5876, Training Loss : 0.17807, Training Acc : 0.911, Run Time : 12.88
INFO:root:2019-05-12 09:21:58, Epoch : 1, Step : 5877, Training Loss : 0.11189, Training Acc : 0.944, Run Time : 0.48
INFO:root:2019-05-12 09:21:59, Epoch : 1, Step : 5878, Training Loss : 0.14482, Training Acc : 0.939, Run Time : 0.61
INFO:root:2019-05-12 09:22:00, Epoch : 1, Step : 5879, Training Loss : 0.16554, Training Acc : 0.922, Run Time : 0.80
INFO:root:2019-05-12 09:22:01, Epoch : 1, Step : 5880, Training Loss : 0.11082, Training Acc : 0.950, Run Time : 1.35
INFO:root:2019-05-12 09:22:15, Epoch : 1, Step : 5881, Training Loss : 0.18248, Training Acc : 0.917, Run Time : 14.05
INFO:root:2019-05-12 09:22:20, Epoch : 1, Step : 5882, Training Loss : 0.14441, Training Acc : 0.933, Run Time : 4.47
INFO:root:2019-05-12 09:22:20, Epoch : 1, Step : 5883, Training Loss : 0.13841, Training Acc : 0.944, Run Time : 0.90
INFO:root:2019-05-12 09:22:34, Epoch : 1, Step : 5884, Training Loss : 0.16108, Training Acc : 0.928, Run Time : 13.95
INFO:root:2019-05-12 09:22:44, Epoch : 1, Step : 5885, Training Loss : 0.19552, Training Acc : 0.906, Run Time : 9.58
INFO:root:2019-05-12 09:22:46, Epoch : 1, Step : 5886, Training Loss : 0.12049, Training Acc : 0.967, Run Time : 1.82
INFO:root:2019-05-12 09:23:02, Epoch : 1, Step : 5887, Training Loss : 0.16632, Training Acc : 0.933, Run Time : 15.89
INFO:root:2019-05-12 09:23:13, Epoch : 1, Step : 5888, Training Loss : 0.11147, Training Acc : 0.939, Run Time : 11.68
INFO:root:2019-05-12 09:23:15, Epoch : 1, Step : 5889, Training Loss : 0.11340, Training Acc : 0.961, Run Time : 1.66
INFO:root:2019-05-12 09:23:16, Epoch : 1, Step : 5890, Training Loss : 0.10535, Training Acc : 0.972, Run Time : 0.60
INFO:root:2019-05-12 09:23:26, Epoch : 1, Step : 5891, Training Loss : 0.12361, Training Acc : 0.961, Run Time : 10.58
INFO:root:2019-05-12 09:23:27, Epoch : 1, Step : 5892, Training Loss : 0.10997, Training Acc : 0.972, Run Time : 0.70
INFO:root:2019-05-12 09:23:29, Epoch : 1, Step : 5893, Training Loss : 0.12066, Training Acc : 0.939, Run Time : 2.04
INFO:root:2019-05-12 09:23:42, Epoch : 1, Step : 5894, Training Loss : 0.09361, Training Acc : 0.967, Run Time : 13.40
INFO:root:2019-05-12 09:23:44, Epoch : 1, Step : 5895, Training Loss : 0.09914, Training Acc : 0.961, Run Time : 1.50
INFO:root:2019-05-12 09:23:54, Epoch : 1, Step : 5896, Training Loss : 0.09554, Training Acc : 0.978, Run Time : 10.40
INFO:root:2019-05-12 09:23:56, Epoch : 1, Step : 5897, Training Loss : 0.13413, Training Acc : 0.944, Run Time : 1.27
INFO:root:2019-05-12 09:24:08, Epoch : 1, Step : 5898, Training Loss : 0.10456, Training Acc : 0.956, Run Time : 12.01
INFO:root:2019-05-12 09:24:08, Epoch : 1, Step : 5899, Training Loss : 0.09433, Training Acc : 0.978, Run Time : 0.87
INFO:root:2019-05-12 09:24:19, Epoch : 1, Step : 5900, Training Loss : 0.10186, Training Acc : 0.961, Run Time : 10.68
INFO:root:2019-05-12 09:24:20, Epoch : 1, Step : 5901, Training Loss : 0.10171, Training Acc : 0.956, Run Time : 1.06
INFO:root:2019-05-12 09:24:21, Epoch : 1, Step : 5902, Training Loss : 0.11793, Training Acc : 0.956, Run Time : 0.57
INFO:root:2019-05-12 09:24:33, Epoch : 1, Step : 5903, Training Loss : 0.11082, Training Acc : 0.956, Run Time : 11.81
INFO:root:2019-05-12 09:24:33, Epoch : 1, Step : 5904, Training Loss : 0.11199, Training Acc : 0.961, Run Time : 0.73
INFO:root:2019-05-12 09:24:34, Epoch : 1, Step : 5905, Training Loss : 0.12902, Training Acc : 0.933, Run Time : 0.63
INFO:root:2019-05-12 09:24:47, Epoch : 1, Step : 5906, Training Loss : 0.12418, Training Acc : 0.956, Run Time : 12.99
INFO:root:2019-05-12 09:24:47, Epoch : 1, Step : 5907, Training Loss : 0.10094, Training Acc : 0.956, Run Time : 0.57
INFO:root:2019-05-12 09:24:49, Epoch : 1, Step : 5908, Training Loss : 0.10581, Training Acc : 0.978, Run Time : 1.21
INFO:root:2019-05-12 09:24:56, Epoch : 1, Step : 5909, Training Loss : 0.23255, Training Acc : 0.911, Run Time : 7.80
INFO:root:2019-05-12 09:24:57, Epoch : 1, Step : 5910, Training Loss : 0.48752, Training Acc : 0.844, Run Time : 0.65
INFO:root:2019-05-12 09:24:58, Epoch : 1, Step : 5911, Training Loss : 0.72747, Training Acc : 0.761, Run Time : 1.27
INFO:root:2019-05-12 09:25:06, Epoch : 1, Step : 5912, Training Loss : 0.80878, Training Acc : 0.739, Run Time : 7.38
INFO:root:2019-05-12 09:25:07, Epoch : 1, Step : 5913, Training Loss : 0.81699, Training Acc : 0.756, Run Time : 0.73
INFO:root:2019-05-12 09:25:12, Epoch : 1, Step : 5914, Training Loss : 0.56925, Training Acc : 0.828, Run Time : 5.17
INFO:root:2019-05-12 09:25:13, Epoch : 1, Step : 5915, Training Loss : 0.68237, Training Acc : 0.778, Run Time : 1.15
INFO:root:2019-05-12 09:25:13, Epoch : 1, Step : 5916, Training Loss : 0.56286, Training Acc : 0.817, Run Time : 0.60
INFO:root:2019-05-12 09:25:14, Epoch : 1, Step : 5917, Training Loss : 0.49600, Training Acc : 0.817, Run Time : 0.59
INFO:root:2019-05-12 09:25:15, Epoch : 1, Step : 5918, Training Loss : 0.52854, Training Acc : 0.811, Run Time : 1.08
INFO:root:2019-05-12 09:25:30, Epoch : 1, Step : 5919, Training Loss : 0.38747, Training Acc : 0.828, Run Time : 14.49
INFO:root:2019-05-12 09:25:39, Epoch : 1, Step : 5920, Training Loss : 0.30263, Training Acc : 0.889, Run Time : 9.37
INFO:root:2019-05-12 09:25:41, Epoch : 1, Step : 5921, Training Loss : 0.53889, Training Acc : 0.794, Run Time : 1.57
INFO:root:2019-05-12 09:25:41, Epoch : 1, Step : 5922, Training Loss : 0.52553, Training Acc : 0.800, Run Time : 0.63
INFO:root:2019-05-12 09:25:54, Epoch : 1, Step : 5923, Training Loss : 0.58517, Training Acc : 0.767, Run Time : 12.46
INFO:root:2019-05-12 09:25:54, Epoch : 1, Step : 5924, Training Loss : 0.34537, Training Acc : 0.828, Run Time : 0.72
INFO:root:2019-05-12 09:25:57, Epoch : 1, Step : 5925, Training Loss : 0.36653, Training Acc : 0.828, Run Time : 2.19
INFO:root:2019-05-12 09:26:08, Epoch : 1, Step : 5926, Training Loss : 0.35820, Training Acc : 0.839, Run Time : 11.83
INFO:root:2019-05-12 09:26:09, Epoch : 1, Step : 5927, Training Loss : 0.38236, Training Acc : 0.822, Run Time : 0.97
INFO:root:2019-05-12 09:26:21, Epoch : 1, Step : 5928, Training Loss : 0.40068, Training Acc : 0.817, Run Time : 11.68
INFO:root:2019-05-12 09:26:24, Epoch : 1, Step : 5929, Training Loss : 0.49211, Training Acc : 0.756, Run Time : 3.36
INFO:root:2019-05-12 09:26:32, Epoch : 1, Step : 5930, Training Loss : 0.31248, Training Acc : 0.828, Run Time : 7.90
INFO:root:2019-05-12 09:26:33, Epoch : 1, Step : 5931, Training Loss : 0.32358, Training Acc : 0.817, Run Time : 0.73
INFO:root:2019-05-12 09:26:44, Epoch : 1, Step : 5932, Training Loss : 0.42527, Training Acc : 0.817, Run Time : 10.92
INFO:root:2019-05-12 09:26:45, Epoch : 1, Step : 5933, Training Loss : 0.46434, Training Acc : 0.778, Run Time : 0.86
INFO:root:2019-05-12 09:26:45, Epoch : 1, Step : 5934, Training Loss : 0.35416, Training Acc : 0.850, Run Time : 0.54
INFO:root:2019-05-12 09:26:59, Epoch : 1, Step : 5935, Training Loss : 0.35444, Training Acc : 0.839, Run Time : 13.81
INFO:root:2019-05-12 09:27:00, Epoch : 1, Step : 5936, Training Loss : 0.28616, Training Acc : 0.883, Run Time : 0.57
INFO:root:2019-05-12 09:27:01, Epoch : 1, Step : 5937, Training Loss : 0.32684, Training Acc : 0.867, Run Time : 0.80
INFO:root:2019-05-12 09:27:11, Epoch : 1, Step : 5938, Training Loss : 0.50949, Training Acc : 0.828, Run Time : 10.93
INFO:root:2019-05-12 09:27:12, Epoch : 1, Step : 5939, Training Loss : 0.44857, Training Acc : 0.811, Run Time : 0.62
INFO:root:2019-05-12 09:27:13, Epoch : 1, Step : 5940, Training Loss : 0.70888, Training Acc : 0.750, Run Time : 0.79
INFO:root:2019-05-12 09:27:24, Epoch : 1, Step : 5941, Training Loss : 0.79502, Training Acc : 0.744, Run Time : 11.44
INFO:root:2019-05-12 09:27:25, Epoch : 1, Step : 5942, Training Loss : 0.64434, Training Acc : 0.772, Run Time : 0.86
INFO:root:2019-05-12 09:27:27, Epoch : 1, Step : 5943, Training Loss : 0.46844, Training Acc : 0.856, Run Time : 1.63
INFO:root:2019-05-12 09:27:40, Epoch : 1, Step : 5944, Training Loss : 0.31028, Training Acc : 0.883, Run Time : 13.30
INFO:root:2019-05-12 09:27:50, Epoch : 1, Step : 5945, Training Loss : 0.23780, Training Acc : 0.894, Run Time : 10.05
INFO:root:2019-05-12 09:27:52, Epoch : 1, Step : 5946, Training Loss : 0.30577, Training Acc : 0.872, Run Time : 1.84
INFO:root:2019-05-12 09:27:52, Epoch : 1, Step : 5947, Training Loss : 0.25996, Training Acc : 0.889, Run Time : 0.43
INFO:root:2019-05-12 09:28:00, Epoch : 1, Step : 5948, Training Loss : 0.11208, Training Acc : 0.961, Run Time : 7.63
INFO:root:2019-05-12 09:28:01, Epoch : 1, Step : 5949, Training Loss : 0.11177, Training Acc : 0.961, Run Time : 1.23
INFO:root:2019-05-12 09:28:15, Epoch : 1, Step : 5950, Training Loss : 0.19643, Training Acc : 0.928, Run Time : 13.68
INFO:root:2019-05-12 09:28:17, Epoch : 1, Step : 5951, Training Loss : 0.06069, Training Acc : 0.989, Run Time : 1.70
INFO:root:2019-05-12 09:28:25, Epoch : 1, Step : 5952, Training Loss : 0.21852, Training Acc : 0.894, Run Time : 8.10
INFO:root:2019-05-12 09:28:25, Epoch : 1, Step : 5953, Training Loss : 0.11701, Training Acc : 0.922, Run Time : 0.47
INFO:root:2019-05-12 09:28:35, Epoch : 1, Step : 5954, Training Loss : 0.08351, Training Acc : 0.972, Run Time : 9.64
INFO:root:2019-05-12 09:28:36, Epoch : 1, Step : 5955, Training Loss : 0.11278, Training Acc : 0.944, Run Time : 0.69
INFO:root:2019-05-12 09:28:46, Epoch : 1, Step : 5956, Training Loss : 0.14143, Training Acc : 0.944, Run Time : 10.68
INFO:root:2019-05-12 09:28:48, Epoch : 1, Step : 5957, Training Loss : 0.10250, Training Acc : 0.956, Run Time : 2.26
INFO:root:2019-05-12 09:28:50, Epoch : 1, Step : 5958, Training Loss : 0.10607, Training Acc : 0.950, Run Time : 1.23
INFO:root:2019-05-12 09:28:50, Epoch : 1, Step : 5959, Training Loss : 0.07394, Training Acc : 0.978, Run Time : 0.56
INFO:root:2019-05-12 09:28:52, Epoch : 1, Step : 5960, Training Loss : 0.12333, Training Acc : 0.956, Run Time : 1.84
INFO:root:2019-05-12 09:29:14, Epoch : 1, Step : 5961, Training Loss : 0.10568, Training Acc : 0.956, Run Time : 21.85
INFO:root:2019-05-12 09:29:15, Epoch : 1, Step : 5962, Training Loss : 0.17664, Training Acc : 0.933, Run Time : 1.12
INFO:root:2019-05-12 09:29:16, Epoch : 1, Step : 5963, Training Loss : 0.12203, Training Acc : 0.950, Run Time : 0.71
INFO:root:2019-05-12 09:29:32, Epoch : 1, Step : 5964, Training Loss : 0.11712, Training Acc : 0.944, Run Time : 15.95
INFO:root:2019-05-12 09:29:35, Epoch : 1, Step : 5965, Training Loss : 0.08981, Training Acc : 0.956, Run Time : 3.43
INFO:root:2019-05-12 09:29:36, Epoch : 1, Step : 5966, Training Loss : 0.10777, Training Acc : 0.950, Run Time : 0.96
INFO:root:2019-05-12 09:29:47, Epoch : 1, Step : 5967, Training Loss : 0.11583, Training Acc : 0.956, Run Time : 10.46
INFO:root:2019-05-12 09:29:48, Epoch : 1, Step : 5968, Training Loss : 0.07431, Training Acc : 0.989, Run Time : 1.67
INFO:root:2019-05-12 09:30:01, Epoch : 1, Step : 5969, Training Loss : 0.10733, Training Acc : 0.967, Run Time : 13.04
INFO:root:2019-05-12 09:30:02, Epoch : 1, Step : 5970, Training Loss : 0.13984, Training Acc : 0.939, Run Time : 0.52
INFO:root:2019-05-12 09:30:14, Epoch : 1, Step : 5971, Training Loss : 0.13025, Training Acc : 0.944, Run Time : 12.37
INFO:root:2019-05-12 09:30:16, Epoch : 1, Step : 5972, Training Loss : 0.17289, Training Acc : 0.933, Run Time : 1.93
INFO:root:2019-05-12 09:30:17, Epoch : 1, Step : 5973, Training Loss : 0.16204, Training Acc : 0.933, Run Time : 0.58
INFO:root:2019-05-12 09:30:18, Epoch : 1, Step : 5974, Training Loss : 0.15270, Training Acc : 0.917, Run Time : 1.28
INFO:root:2019-05-12 09:30:31, Epoch : 1, Step : 5975, Training Loss : 0.08851, Training Acc : 0.956, Run Time : 13.13
INFO:root:2019-05-12 09:30:32, Epoch : 1, Step : 5976, Training Loss : 0.12696, Training Acc : 0.944, Run Time : 0.92
INFO:root:2019-05-12 09:30:33, Epoch : 1, Step : 5977, Training Loss : 0.07939, Training Acc : 0.972, Run Time : 0.74
INFO:root:2019-05-12 09:30:45, Epoch : 1, Step : 5978, Training Loss : 0.05513, Training Acc : 0.983, Run Time : 12.00
INFO:root:2019-05-12 09:30:45, Epoch : 1, Step : 5979, Training Loss : 0.07094, Training Acc : 0.972, Run Time : 0.61
INFO:root:2019-05-12 09:30:47, Epoch : 1, Step : 5980, Training Loss : 0.07046, Training Acc : 0.978, Run Time : 1.89
INFO:root:2019-05-12 09:30:57, Epoch : 1, Step : 5981, Training Loss : 0.04098, Training Acc : 0.989, Run Time : 9.75
INFO:root:2019-05-12 09:30:57, Epoch : 1, Step : 5982, Training Loss : 0.08083, Training Acc : 0.972, Run Time : 0.47
INFO:root:2019-05-12 09:30:59, Epoch : 1, Step : 5983, Training Loss : 0.07956, Training Acc : 0.972, Run Time : 1.99
INFO:root:2019-05-12 09:31:11, Epoch : 1, Step : 5984, Training Loss : 0.05750, Training Acc : 0.983, Run Time : 11.12
INFO:root:2019-05-12 09:31:11, Epoch : 1, Step : 5985, Training Loss : 0.08612, Training Acc : 0.956, Run Time : 0.58
INFO:root:2019-05-12 09:31:13, Epoch : 1, Step : 5986, Training Loss : 0.08883, Training Acc : 0.961, Run Time : 2.00
INFO:root:2019-05-12 09:31:24, Epoch : 1, Step : 5987, Training Loss : 0.07924, Training Acc : 0.972, Run Time : 10.36
INFO:root:2019-05-12 09:31:24, Epoch : 1, Step : 5988, Training Loss : 0.06723, Training Acc : 0.967, Run Time : 0.84
INFO:root:2019-05-12 09:31:41, Epoch : 1, Step : 5989, Training Loss : 0.07304, Training Acc : 0.972, Run Time : 16.21
INFO:root:2019-05-12 09:31:42, Epoch : 1, Step : 5990, Training Loss : 0.09892, Training Acc : 0.961, Run Time : 1.12
INFO:root:2019-05-12 09:31:43, Epoch : 1, Step : 5991, Training Loss : 0.07372, Training Acc : 0.983, Run Time : 1.54
INFO:root:2019-05-12 09:31:59, Epoch : 1, Step : 5992, Training Loss : 0.06466, Training Acc : 0.972, Run Time : 15.48
INFO:root:2019-05-12 09:31:59, Epoch : 1, Step : 5993, Training Loss : 0.08205, Training Acc : 0.972, Run Time : 0.55
INFO:root:2019-05-12 09:32:00, Epoch : 1, Step : 5994, Training Loss : 0.07916, Training Acc : 0.956, Run Time : 0.59
INFO:root:2019-05-12 09:32:11, Epoch : 1, Step : 5995, Training Loss : 0.07102, Training Acc : 0.967, Run Time : 10.78
INFO:root:2019-05-12 09:32:12, Epoch : 1, Step : 5996, Training Loss : 0.07927, Training Acc : 0.972, Run Time : 1.01
INFO:root:2019-05-12 09:32:12, Epoch : 1, Step : 5997, Training Loss : 0.05374, Training Acc : 0.994, Run Time : 0.57
INFO:root:2019-05-12 09:32:13, Epoch : 1, Step : 5998, Training Loss : 0.09836, Training Acc : 0.961, Run Time : 0.65
INFO:root:2019-05-12 09:32:13, Epoch : 1, Step : 5999, Training Loss : 0.06085, Training Acc : 0.983, Run Time : 0.57
INFO:root:2019-05-12 09:32:32, Epoch : 1, Step : 6000, Training Loss : 0.05972, Training Acc : 0.989, Run Time : 18.68
INFO:root:2019-05-12 09:32:44, Epoch : 1, Step : 6001, Training Loss : 0.57662, Training Acc : 0.800, Run Time : 11.98
INFO:root:2019-05-12 09:32:46, Epoch : 1, Step : 6002, Training Loss : 0.49166, Training Acc : 0.789, Run Time : 1.67
INFO:root:2019-05-12 09:32:46, Epoch : 1, Step : 6003, Training Loss : 0.30061, Training Acc : 0.894, Run Time : 0.53
INFO:root:2019-05-12 09:32:48, Epoch : 1, Step : 6004, Training Loss : 0.63900, Training Acc : 0.783, Run Time : 1.44
INFO:root:2019-05-12 09:33:00, Epoch : 1, Step : 6005, Training Loss : 0.58759, Training Acc : 0.800, Run Time : 12.01
INFO:root:2019-05-12 09:33:11, Epoch : 1, Step : 6006, Training Loss : 0.60810, Training Acc : 0.767, Run Time : 11.52
INFO:root:2019-05-12 09:33:13, Epoch : 1, Step : 6007, Training Loss : 0.35486, Training Acc : 0.872, Run Time : 1.86
INFO:root:2019-05-12 09:33:14, Epoch : 1, Step : 6008, Training Loss : 0.66102, Training Acc : 0.828, Run Time : 1.08
INFO:root:2019-05-12 09:33:23, Epoch : 1, Step : 6009, Training Loss : 0.37165, Training Acc : 0.850, Run Time : 9.13
INFO:root:2019-05-12 09:33:24, Epoch : 1, Step : 6010, Training Loss : 0.55895, Training Acc : 0.828, Run Time : 0.65
INFO:root:2019-05-12 09:33:25, Epoch : 1, Step : 6011, Training Loss : 0.51375, Training Acc : 0.839, Run Time : 0.67
INFO:root:2019-05-12 09:33:39, Epoch : 1, Step : 6012, Training Loss : 0.31360, Training Acc : 0.906, Run Time : 13.88
INFO:root:2019-05-12 09:33:39, Epoch : 1, Step : 6013, Training Loss : 0.25431, Training Acc : 0.883, Run Time : 0.86
INFO:root:2019-05-12 09:33:54, Epoch : 1, Step : 6014, Training Loss : 0.26278, Training Acc : 0.906, Run Time : 14.68
INFO:root:2019-05-12 09:33:55, Epoch : 1, Step : 6015, Training Loss : 0.30394, Training Acc : 0.889, Run Time : 1.33
INFO:root:2019-05-12 09:33:57, Epoch : 1, Step : 6016, Training Loss : 0.37400, Training Acc : 0.883, Run Time : 1.16
INFO:root:2019-05-12 09:34:08, Epoch : 1, Step : 6017, Training Loss : 0.21703, Training Acc : 0.906, Run Time : 10.99
INFO:root:2019-05-12 09:34:09, Epoch : 1, Step : 6018, Training Loss : 0.09332, Training Acc : 0.961, Run Time : 1.25
INFO:root:2019-05-12 09:34:22, Epoch : 1, Step : 6019, Training Loss : 0.18329, Training Acc : 0.939, Run Time : 13.45
INFO:root:2019-05-12 09:34:23, Epoch : 1, Step : 6020, Training Loss : 0.19493, Training Acc : 0.922, Run Time : 0.47
INFO:root:2019-05-12 09:34:23, Epoch : 1, Step : 6021, Training Loss : 0.25680, Training Acc : 0.917, Run Time : 0.42
INFO:root:2019-05-12 09:34:40, Epoch : 1, Step : 6022, Training Loss : 0.23386, Training Acc : 0.922, Run Time : 16.98
INFO:root:2019-05-12 09:34:42, Epoch : 1, Step : 6023, Training Loss : 0.18987, Training Acc : 0.928, Run Time : 1.68
INFO:root:2019-05-12 09:34:55, Epoch : 1, Step : 6024, Training Loss : 0.15990, Training Acc : 0.950, Run Time : 13.08
INFO:root:2019-05-12 09:34:56, Epoch : 1, Step : 6025, Training Loss : 0.07155, Training Acc : 0.983, Run Time : 0.91
INFO:root:2019-05-12 09:34:57, Epoch : 1, Step : 6026, Training Loss : 0.27833, Training Acc : 0.917, Run Time : 0.91
INFO:root:2019-05-12 09:35:06, Epoch : 1, Step : 6027, Training Loss : 0.14040, Training Acc : 0.961, Run Time : 9.47
INFO:root:2019-05-12 09:35:07, Epoch : 1, Step : 6028, Training Loss : 0.10223, Training Acc : 0.978, Run Time : 0.89
INFO:root:2019-05-12 09:35:21, Epoch : 1, Step : 6029, Training Loss : 0.28025, Training Acc : 0.900, Run Time : 14.15
INFO:root:2019-05-12 09:35:22, Epoch : 1, Step : 6030, Training Loss : 0.11451, Training Acc : 0.967, Run Time : 0.52
INFO:root:2019-05-12 09:35:22, Epoch : 1, Step : 6031, Training Loss : 0.23079, Training Acc : 0.906, Run Time : 0.54
INFO:root:2019-05-12 09:35:36, Epoch : 1, Step : 6032, Training Loss : 0.26709, Training Acc : 0.906, Run Time : 13.76
INFO:root:2019-05-12 09:35:38, Epoch : 1, Step : 6033, Training Loss : 0.12844, Training Acc : 0.967, Run Time : 1.49
INFO:root:2019-05-12 09:35:50, Epoch : 1, Step : 6034, Training Loss : 0.08456, Training Acc : 0.978, Run Time : 12.94
INFO:root:2019-05-12 09:35:51, Epoch : 1, Step : 6035, Training Loss : 0.26873, Training Acc : 0.894, Run Time : 0.72
INFO:root:2019-05-12 09:36:05, Epoch : 1, Step : 6036, Training Loss : 0.15930, Training Acc : 0.922, Run Time : 13.95
INFO:root:2019-05-12 09:36:06, Epoch : 1, Step : 6037, Training Loss : 0.16989, Training Acc : 0.950, Run Time : 0.72
INFO:root:2019-05-12 09:36:08, Epoch : 1, Step : 6038, Training Loss : 0.18059, Training Acc : 0.928, Run Time : 1.76
INFO:root:2019-05-12 09:36:19, Epoch : 1, Step : 6039, Training Loss : 0.36588, Training Acc : 0.861, Run Time : 11.58
INFO:root:2019-05-12 09:36:20, Epoch : 1, Step : 6040, Training Loss : 0.22361, Training Acc : 0.906, Run Time : 0.45
INFO:root:2019-05-12 09:36:21, Epoch : 1, Step : 6041, Training Loss : 0.13182, Training Acc : 0.950, Run Time : 0.86
INFO:root:2019-05-12 09:36:21, Epoch : 1, Step : 6042, Training Loss : 0.24259, Training Acc : 0.906, Run Time : 0.63
INFO:root:2019-05-12 09:36:22, Epoch : 1, Step : 6043, Training Loss : 0.31394, Training Acc : 0.889, Run Time : 1.14
INFO:root:2019-05-12 09:36:38, Epoch : 1, Step : 6044, Training Loss : 0.30015, Training Acc : 0.900, Run Time : 15.70
INFO:root:2019-05-12 09:36:40, Epoch : 1, Step : 6045, Training Loss : 0.33483, Training Acc : 0.889, Run Time : 1.73
INFO:root:2019-05-12 09:36:58, Epoch : 1, Step : 6046, Training Loss : 0.15258, Training Acc : 0.933, Run Time : 18.64
INFO:root:2019-05-12 09:37:01, Epoch : 1, Step : 6047, Training Loss : 0.14342, Training Acc : 0.939, Run Time : 2.52
INFO:root:2019-05-12 09:37:02, Epoch : 1, Step : 6048, Training Loss : 0.16083, Training Acc : 0.928, Run Time : 0.62
INFO:root:2019-05-12 09:37:03, Epoch : 1, Step : 6049, Training Loss : 0.11816, Training Acc : 0.972, Run Time : 1.05
INFO:root:2019-05-12 09:37:04, Epoch : 1, Step : 6050, Training Loss : 0.10253, Training Acc : 0.967, Run Time : 1.54
INFO:root:2019-05-12 09:37:05, Epoch : 1, Step : 6051, Training Loss : 0.08419, Training Acc : 0.978, Run Time : 0.63
INFO:root:2019-05-12 09:37:18, Epoch : 1, Step : 6052, Training Loss : 0.16565, Training Acc : 0.933, Run Time : 12.93
INFO:root:2019-05-12 09:37:19, Epoch : 1, Step : 6053, Training Loss : 0.19743, Training Acc : 0.961, Run Time : 0.84
INFO:root:2019-05-12 09:37:33, Epoch : 1, Step : 6054, Training Loss : 0.19092, Training Acc : 0.944, Run Time : 14.37
INFO:root:2019-05-12 09:37:34, Epoch : 1, Step : 6055, Training Loss : 0.21077, Training Acc : 0.922, Run Time : 1.00
INFO:root:2019-05-12 09:37:36, Epoch : 1, Step : 6056, Training Loss : 0.38953, Training Acc : 0.883, Run Time : 1.69
INFO:root:2019-05-12 09:37:47, Epoch : 1, Step : 6057, Training Loss : 0.17785, Training Acc : 0.933, Run Time : 11.01
INFO:root:2019-05-12 09:37:47, Epoch : 1, Step : 6058, Training Loss : 0.27207, Training Acc : 0.906, Run Time : 0.45
INFO:root:2019-05-12 09:37:48, Epoch : 1, Step : 6059, Training Loss : 0.31907, Training Acc : 0.906, Run Time : 0.80
INFO:root:2019-05-12 09:38:02, Epoch : 1, Step : 6060, Training Loss : 0.15808, Training Acc : 0.939, Run Time : 13.95
INFO:root:2019-05-12 09:38:04, Epoch : 1, Step : 6061, Training Loss : 0.11181, Training Acc : 0.944, Run Time : 2.27
INFO:root:2019-05-12 09:38:17, Epoch : 1, Step : 6062, Training Loss : 0.18431, Training Acc : 0.933, Run Time : 13.05
INFO:root:2019-05-12 09:38:19, Epoch : 1, Step : 6063, Training Loss : 0.05889, Training Acc : 0.978, Run Time : 1.63
INFO:root:2019-05-12 09:38:34, Epoch : 1, Step : 6064, Training Loss : 0.14735, Training Acc : 0.944, Run Time : 15.27
INFO:root:2019-05-12 09:38:36, Epoch : 1, Step : 6065, Training Loss : 0.09418, Training Acc : 0.967, Run Time : 2.24
INFO:root:2019-05-12 09:38:39, Epoch : 1, Step : 6066, Training Loss : 0.04175, Training Acc : 0.994, Run Time : 2.99
INFO:root:2019-05-12 09:38:40, Epoch : 1, Step : 6067, Training Loss : 0.25163, Training Acc : 0.917, Run Time : 0.58
INFO:root:2019-05-12 09:38:48, Epoch : 1, Step : 6068, Training Loss : 0.29376, Training Acc : 0.894, Run Time : 8.46
INFO:root:2019-05-12 09:38:49, Epoch : 1, Step : 6069, Training Loss : 0.19191, Training Acc : 0.950, Run Time : 1.07
INFO:root:2019-05-12 09:38:51, Epoch : 1, Step : 6070, Training Loss : 0.14142, Training Acc : 0.950, Run Time : 2.02
INFO:root:2019-05-12 09:39:02, Epoch : 1, Step : 6071, Training Loss : 0.22667, Training Acc : 0.911, Run Time : 10.50
INFO:root:2019-05-12 09:39:03, Epoch : 1, Step : 6072, Training Loss : 0.10693, Training Acc : 0.967, Run Time : 0.97
INFO:root:2019-05-12 09:39:04, Epoch : 1, Step : 6073, Training Loss : 0.14822, Training Acc : 0.939, Run Time : 1.64
INFO:root:2019-05-12 09:39:11, Epoch : 1, Step : 6074, Training Loss : 0.11212, Training Acc : 0.961, Run Time : 6.66
INFO:root:2019-05-12 09:39:12, Epoch : 1, Step : 6075, Training Loss : 0.16313, Training Acc : 0.956, Run Time : 0.56
INFO:root:2019-05-12 09:39:12, Epoch : 1, Step : 6076, Training Loss : 0.21822, Training Acc : 0.933, Run Time : 0.59
INFO:root:2019-05-12 09:39:19, Epoch : 1, Step : 6077, Training Loss : 0.25493, Training Acc : 0.911, Run Time : 6.75
INFO:root:2019-05-12 09:39:20, Epoch : 1, Step : 6078, Training Loss : 0.09260, Training Acc : 0.972, Run Time : 0.86
INFO:root:2019-05-12 09:39:28, Epoch : 1, Step : 6079, Training Loss : 0.37152, Training Acc : 0.889, Run Time : 8.53
INFO:root:2019-05-12 09:39:48, Epoch : 1, Step : 6080, Training Loss : 0.32920, Training Acc : 0.889, Run Time : 19.45
INFO:root:2019-05-12 09:40:03, Epoch : 1, Step : 6081, Training Loss : 0.27942, Training Acc : 0.917, Run Time : 15.02
INFO:root:2019-05-12 09:40:04, Epoch : 1, Step : 6082, Training Loss : 0.18221, Training Acc : 0.944, Run Time : 1.24
INFO:root:2019-05-12 09:40:20, Epoch : 1, Step : 6083, Training Loss : 0.13085, Training Acc : 0.933, Run Time : 15.63
INFO:root:2019-05-12 09:40:21, Epoch : 1, Step : 6084, Training Loss : 0.12592, Training Acc : 0.950, Run Time : 1.04
INFO:root:2019-05-12 09:40:43, Epoch : 1, Step : 6085, Training Loss : 0.15215, Training Acc : 0.950, Run Time : 22.26
INFO:root:2019-05-12 09:40:46, Epoch : 1, Step : 6086, Training Loss : 0.28160, Training Acc : 0.883, Run Time : 2.60
INFO:root:2019-05-12 09:40:51, Epoch : 1, Step : 6087, Training Loss : 0.21711, Training Acc : 0.906, Run Time : 5.62
INFO:root:2019-05-12 09:40:52, Epoch : 1, Step : 6088, Training Loss : 0.18121, Training Acc : 0.933, Run Time : 0.69
INFO:root:2019-05-12 09:40:53, Epoch : 1, Step : 6089, Training Loss : 0.14876, Training Acc : 0.917, Run Time : 0.62
INFO:root:2019-05-12 09:40:53, Epoch : 1, Step : 6090, Training Loss : 0.12496, Training Acc : 0.944, Run Time : 0.79
INFO:root:2019-05-12 09:41:15, Epoch : 1, Step : 6091, Training Loss : 0.16353, Training Acc : 0.944, Run Time : 21.82
INFO:root:2019-05-12 09:41:20, Epoch : 1, Step : 6092, Training Loss : 0.26714, Training Acc : 0.900, Run Time : 4.38
INFO:root:2019-05-12 09:41:20, Epoch : 1, Step : 6093, Training Loss : 0.15805, Training Acc : 0.939, Run Time : 0.55
INFO:root:2019-05-12 09:41:32, Epoch : 1, Step : 6094, Training Loss : 0.26663, Training Acc : 0.872, Run Time : 12.02
INFO:root:2019-05-12 09:41:43, Epoch : 1, Step : 6095, Training Loss : 0.30040, Training Acc : 0.911, Run Time : 11.06
INFO:root:2019-05-12 09:41:49, Epoch : 1, Step : 6096, Training Loss : 0.19896, Training Acc : 0.928, Run Time : 5.85
INFO:root:2019-05-12 09:41:54, Epoch : 1, Step : 6097, Training Loss : 0.30950, Training Acc : 0.856, Run Time : 5.43
INFO:root:2019-05-12 09:41:59, Epoch : 1, Step : 6098, Training Loss : 0.17484, Training Acc : 0.933, Run Time : 4.53
INFO:root:2019-05-12 09:42:09, Epoch : 1, Step : 6099, Training Loss : 0.11979, Training Acc : 0.944, Run Time : 9.99
INFO:root:2019-05-12 09:42:20, Epoch : 1, Step : 6100, Training Loss : 0.13349, Training Acc : 0.956, Run Time : 11.45
INFO:root:2019-05-12 09:42:35, Epoch : 1, Step : 6101, Training Loss : 0.19931, Training Acc : 0.906, Run Time : 14.61
INFO:root:2019-05-12 09:42:36, Epoch : 1, Step : 6102, Training Loss : 0.14875, Training Acc : 0.922, Run Time : 1.11
INFO:root:2019-05-12 09:42:37, Epoch : 1, Step : 6103, Training Loss : 0.21551, Training Acc : 0.939, Run Time : 0.63
INFO:root:2019-05-12 09:42:38, Epoch : 1, Step : 6104, Training Loss : 0.16386, Training Acc : 0.906, Run Time : 1.20
INFO:root:2019-05-12 09:42:52, Epoch : 1, Step : 6105, Training Loss : 0.17276, Training Acc : 0.933, Run Time : 14.19
INFO:root:2019-05-12 09:42:53, Epoch : 1, Step : 6106, Training Loss : 0.18785, Training Acc : 0.917, Run Time : 0.55
INFO:root:2019-05-12 09:42:53, Epoch : 1, Step : 6107, Training Loss : 0.22884, Training Acc : 0.906, Run Time : 0.46
INFO:root:2019-05-12 09:42:54, Epoch : 1, Step : 6108, Training Loss : 0.28744, Training Acc : 0.900, Run Time : 0.80
INFO:root:2019-05-12 09:42:56, Epoch : 1, Step : 6109, Training Loss : 0.16877, Training Acc : 0.939, Run Time : 1.68
INFO:root:2019-05-12 09:43:12, Epoch : 1, Step : 6110, Training Loss : 0.10697, Training Acc : 0.956, Run Time : 16.10
INFO:root:2019-05-12 09:43:13, Epoch : 1, Step : 6111, Training Loss : 0.11202, Training Acc : 0.961, Run Time : 1.03
INFO:root:2019-05-12 09:43:13, Epoch : 1, Step : 6112, Training Loss : 0.18497, Training Acc : 0.939, Run Time : 0.69
INFO:root:2019-05-12 09:43:14, Epoch : 1, Step : 6113, Training Loss : 0.22007, Training Acc : 0.906, Run Time : 0.61
INFO:root:2019-05-12 09:43:15, Epoch : 1, Step : 6114, Training Loss : 0.14735, Training Acc : 0.944, Run Time : 0.65
INFO:root:2019-05-12 09:43:31, Epoch : 1, Step : 6115, Training Loss : 0.25423, Training Acc : 0.911, Run Time : 16.24
INFO:root:2019-05-12 09:43:31, Epoch : 1, Step : 6116, Training Loss : 0.52156, Training Acc : 0.767, Run Time : 0.46
INFO:root:2019-05-12 09:43:33, Epoch : 1, Step : 6117, Training Loss : 0.21602, Training Acc : 0.922, Run Time : 1.73
INFO:root:2019-05-12 09:43:53, Epoch : 1, Step : 6118, Training Loss : 0.23088, Training Acc : 0.922, Run Time : 19.90
INFO:root:2019-05-12 09:43:54, Epoch : 1, Step : 6119, Training Loss : 0.25454, Training Acc : 0.894, Run Time : 1.30
INFO:root:2019-05-12 09:43:55, Epoch : 1, Step : 6120, Training Loss : 0.10665, Training Acc : 0.967, Run Time : 0.82
INFO:root:2019-05-12 09:44:08, Epoch : 1, Step : 6121, Training Loss : 0.17087, Training Acc : 0.939, Run Time : 12.37
INFO:root:2019-05-12 09:44:08, Epoch : 1, Step : 6122, Training Loss : 0.15497, Training Acc : 0.944, Run Time : 0.91
INFO:root:2019-05-12 09:44:09, Epoch : 1, Step : 6123, Training Loss : 0.19987, Training Acc : 0.933, Run Time : 0.65
INFO:root:2019-05-12 09:44:21, Epoch : 1, Step : 6124, Training Loss : 0.15095, Training Acc : 0.944, Run Time : 11.69
INFO:root:2019-05-12 09:44:22, Epoch : 1, Step : 6125, Training Loss : 0.32183, Training Acc : 0.889, Run Time : 1.17
INFO:root:2019-05-12 09:44:32, Epoch : 1, Step : 6126, Training Loss : 0.08257, Training Acc : 0.978, Run Time : 10.30
INFO:root:2019-05-12 09:44:33, Epoch : 1, Step : 6127, Training Loss : 0.22227, Training Acc : 0.939, Run Time : 0.68
INFO:root:2019-05-12 09:44:36, Epoch : 1, Step : 6128, Training Loss : 0.18920, Training Acc : 0.956, Run Time : 2.72
INFO:root:2019-05-12 09:44:42, Epoch : 1, Step : 6129, Training Loss : 0.16561, Training Acc : 0.933, Run Time : 6.81
INFO:root:2019-05-12 09:44:43, Epoch : 1, Step : 6130, Training Loss : 0.23557, Training Acc : 0.906, Run Time : 0.64
INFO:root:2019-05-12 09:44:45, Epoch : 1, Step : 6131, Training Loss : 0.24758, Training Acc : 0.917, Run Time : 1.87
INFO:root:2019-05-12 09:44:58, Epoch : 1, Step : 6132, Training Loss : 0.37472, Training Acc : 0.833, Run Time : 12.64
INFO:root:2019-05-12 09:44:58, Epoch : 1, Step : 6133, Training Loss : 0.13211, Training Acc : 0.939, Run Time : 0.48
INFO:root:2019-05-12 09:44:59, Epoch : 1, Step : 6134, Training Loss : 0.11808, Training Acc : 0.967, Run Time : 0.65
INFO:root:2019-05-12 09:45:11, Epoch : 1, Step : 6135, Training Loss : 0.24784, Training Acc : 0.911, Run Time : 12.00
INFO:root:2019-05-12 09:45:11, Epoch : 1, Step : 6136, Training Loss : 0.06376, Training Acc : 0.983, Run Time : 0.64
INFO:root:2019-05-12 09:45:12, Epoch : 1, Step : 6137, Training Loss : 0.17126, Training Acc : 0.933, Run Time : 0.54
INFO:root:2019-05-12 09:45:29, Epoch : 1, Step : 6138, Training Loss : 0.24364, Training Acc : 0.900, Run Time : 17.52
INFO:root:2019-05-12 09:45:39, Epoch : 1, Step : 6139, Training Loss : 0.13597, Training Acc : 0.944, Run Time : 9.46
INFO:root:2019-05-12 09:45:39, Epoch : 1, Step : 6140, Training Loss : 0.15688, Training Acc : 0.950, Run Time : 0.52
INFO:root:2019-05-12 09:45:48, Epoch : 1, Step : 6141, Training Loss : 0.12231, Training Acc : 0.944, Run Time : 8.64
INFO:root:2019-05-12 09:45:49, Epoch : 1, Step : 6142, Training Loss : 0.15639, Training Acc : 0.939, Run Time : 0.94
INFO:root:2019-05-12 09:45:50, Epoch : 1, Step : 6143, Training Loss : 0.10320, Training Acc : 0.972, Run Time : 1.21
INFO:root:2019-05-12 09:45:58, Epoch : 1, Step : 6144, Training Loss : 0.11878, Training Acc : 0.961, Run Time : 7.69
INFO:root:2019-05-12 09:45:58, Epoch : 1, Step : 6145, Training Loss : 0.15002, Training Acc : 0.939, Run Time : 0.43
INFO:root:2019-05-12 09:45:59, Epoch : 1, Step : 6146, Training Loss : 0.22408, Training Acc : 0.928, Run Time : 0.66
INFO:root:2019-05-12 09:46:09, Epoch : 1, Step : 6147, Training Loss : 0.13736, Training Acc : 0.950, Run Time : 9.58
INFO:root:2019-05-12 09:46:10, Epoch : 1, Step : 6148, Training Loss : 0.16654, Training Acc : 0.944, Run Time : 0.95
INFO:root:2019-05-12 09:46:20, Epoch : 1, Step : 6149, Training Loss : 0.44804, Training Acc : 0.783, Run Time : 10.66
INFO:root:2019-05-12 09:46:21, Epoch : 1, Step : 6150, Training Loss : 0.37081, Training Acc : 0.828, Run Time : 0.71
INFO:root:2019-05-12 09:46:22, Epoch : 1, Step : 6151, Training Loss : 0.27651, Training Acc : 0.889, Run Time : 0.62
INFO:root:2019-05-12 09:46:23, Epoch : 1, Step : 6152, Training Loss : 0.20767, Training Acc : 0.906, Run Time : 1.72
INFO:root:2019-05-12 09:46:45, Epoch : 1, Step : 6153, Training Loss : 0.38501, Training Acc : 0.839, Run Time : 21.50
INFO:root:2019-05-12 09:46:53, Epoch : 1, Step : 6154, Training Loss : 0.12093, Training Acc : 0.939, Run Time : 8.72
INFO:root:2019-05-12 09:46:54, Epoch : 1, Step : 6155, Training Loss : 0.10857, Training Acc : 0.978, Run Time : 0.52
INFO:root:2019-05-12 09:46:55, Epoch : 1, Step : 6156, Training Loss : 0.38625, Training Acc : 0.856, Run Time : 0.65
INFO:root:2019-05-12 09:47:08, Epoch : 1, Step : 6157, Training Loss : 0.18384, Training Acc : 0.956, Run Time : 13.08
INFO:root:2019-05-12 09:47:09, Epoch : 1, Step : 6158, Training Loss : 0.09614, Training Acc : 0.983, Run Time : 0.86
INFO:root:2019-05-12 09:47:18, Epoch : 1, Step : 6159, Training Loss : 0.24964, Training Acc : 0.928, Run Time : 9.80
INFO:root:2019-05-12 09:47:19, Epoch : 1, Step : 6160, Training Loss : 0.28234, Training Acc : 0.933, Run Time : 0.80
INFO:root:2019-05-12 09:47:20, Epoch : 1, Step : 6161, Training Loss : 0.37800, Training Acc : 0.817, Run Time : 0.74
INFO:root:2019-05-12 09:47:32, Epoch : 1, Step : 6162, Training Loss : 0.26442, Training Acc : 0.911, Run Time : 11.61
INFO:root:2019-05-12 09:47:32, Epoch : 1, Step : 6163, Training Loss : 0.31658, Training Acc : 0.872, Run Time : 0.95
INFO:root:2019-05-12 09:47:44, Epoch : 1, Step : 6164, Training Loss : 0.25598, Training Acc : 0.900, Run Time : 11.67
INFO:root:2019-05-12 09:47:45, Epoch : 1, Step : 6165, Training Loss : 0.20700, Training Acc : 0.922, Run Time : 1.14
INFO:root:2019-05-12 09:47:54, Epoch : 1, Step : 6166, Training Loss : 0.39319, Training Acc : 0.889, Run Time : 9.18
INFO:root:2019-05-12 09:47:55, Epoch : 1, Step : 6167, Training Loss : 0.14908, Training Acc : 0.950, Run Time : 0.58
INFO:root:2019-05-12 09:47:56, Epoch : 1, Step : 6168, Training Loss : 0.24911, Training Acc : 0.889, Run Time : 1.06
INFO:root:2019-05-12 09:48:07, Epoch : 1, Step : 6169, Training Loss : 0.18479, Training Acc : 0.928, Run Time : 11.30
INFO:root:2019-05-12 09:48:08, Epoch : 1, Step : 6170, Training Loss : 0.17467, Training Acc : 0.944, Run Time : 0.42
INFO:root:2019-05-12 09:48:08, Epoch : 1, Step : 6171, Training Loss : 0.20477, Training Acc : 0.894, Run Time : 0.61
INFO:root:2019-05-12 09:48:21, Epoch : 1, Step : 6172, Training Loss : 0.13666, Training Acc : 0.950, Run Time : 13.00
INFO:root:2019-05-12 09:48:22, Epoch : 1, Step : 6173, Training Loss : 0.21484, Training Acc : 0.900, Run Time : 0.54
INFO:root:2019-05-12 09:48:23, Epoch : 1, Step : 6174, Training Loss : 0.14301, Training Acc : 0.944, Run Time : 0.64
INFO:root:2019-05-12 09:48:23, Epoch : 1, Step : 6175, Training Loss : 0.15269, Training Acc : 0.944, Run Time : 0.66
INFO:root:2019-05-12 09:48:35, Epoch : 1, Step : 6176, Training Loss : 0.26078, Training Acc : 0.894, Run Time : 11.88
INFO:root:2019-05-12 09:48:37, Epoch : 1, Step : 6177, Training Loss : 0.35648, Training Acc : 0.850, Run Time : 1.58
INFO:root:2019-05-12 09:48:52, Epoch : 1, Step : 6178, Training Loss : 0.22034, Training Acc : 0.906, Run Time : 15.03
INFO:root:2019-05-12 09:49:02, Epoch : 1, Step : 6179, Training Loss : 0.09586, Training Acc : 0.994, Run Time : 10.35
INFO:root:2019-05-12 09:49:03, Epoch : 1, Step : 6180, Training Loss : 0.09170, Training Acc : 0.967, Run Time : 1.33
INFO:root:2019-05-12 09:49:15, Epoch : 1, Step : 6181, Training Loss : 0.19241, Training Acc : 0.928, Run Time : 11.33
INFO:root:2019-05-12 09:49:17, Epoch : 1, Step : 6182, Training Loss : 0.23824, Training Acc : 0.917, Run Time : 2.29
INFO:root:2019-05-12 09:49:18, Epoch : 1, Step : 6183, Training Loss : 0.28350, Training Acc : 0.906, Run Time : 1.19
INFO:root:2019-05-12 09:49:32, Epoch : 1, Step : 6184, Training Loss : 0.55130, Training Acc : 0.761, Run Time : 13.93
INFO:root:2019-05-12 09:49:43, Epoch : 1, Step : 6185, Training Loss : 0.15989, Training Acc : 0.956, Run Time : 11.15
INFO:root:2019-05-12 09:49:46, Epoch : 1, Step : 6186, Training Loss : 0.17209, Training Acc : 0.944, Run Time : 3.02
INFO:root:2019-05-12 09:49:58, Epoch : 1, Step : 6187, Training Loss : 0.21030, Training Acc : 0.894, Run Time : 11.30
INFO:root:2019-05-12 09:49:59, Epoch : 1, Step : 6188, Training Loss : 0.13819, Training Acc : 0.939, Run Time : 0.92
INFO:root:2019-05-12 09:50:12, Epoch : 1, Step : 6189, Training Loss : 0.19810, Training Acc : 0.922, Run Time : 13.68
INFO:root:2019-05-12 09:50:13, Epoch : 1, Step : 6190, Training Loss : 0.22997, Training Acc : 0.917, Run Time : 0.70
INFO:root:2019-05-12 09:50:15, Epoch : 1, Step : 6191, Training Loss : 0.13677, Training Acc : 0.956, Run Time : 1.80
INFO:root:2019-05-12 09:50:28, Epoch : 1, Step : 6192, Training Loss : 0.12606, Training Acc : 0.950, Run Time : 12.97
INFO:root:2019-05-12 09:50:28, Epoch : 1, Step : 6193, Training Loss : 0.13657, Training Acc : 0.944, Run Time : 0.60
INFO:root:2019-05-12 09:50:42, Epoch : 1, Step : 6194, Training Loss : 0.10512, Training Acc : 0.972, Run Time : 13.47
INFO:root:2019-05-12 09:50:42, Epoch : 1, Step : 6195, Training Loss : 0.12572, Training Acc : 0.956, Run Time : 0.51
INFO:root:2019-05-12 09:50:43, Epoch : 1, Step : 6196, Training Loss : 0.11971, Training Acc : 0.944, Run Time : 0.78
INFO:root:2019-05-12 09:50:44, Epoch : 1, Step : 6197, Training Loss : 0.09767, Training Acc : 0.967, Run Time : 1.28
INFO:root:2019-05-12 09:50:55, Epoch : 1, Step : 6198, Training Loss : 0.22238, Training Acc : 0.928, Run Time : 10.31
INFO:root:2019-05-12 09:50:55, Epoch : 1, Step : 6199, Training Loss : 0.40952, Training Acc : 0.861, Run Time : 0.43
INFO:root:2019-05-12 09:50:56, Epoch : 1, Step : 6200, Training Loss : 0.24944, Training Acc : 0.911, Run Time : 0.92
INFO:root:2019-05-12 09:51:04, Epoch : 1, Step : 6201, Training Loss : 0.56587, Training Acc : 0.817, Run Time : 8.00
INFO:root:2019-05-12 09:51:05, Epoch : 1, Step : 6202, Training Loss : 0.76225, Training Acc : 0.678, Run Time : 0.53
INFO:root:2019-05-12 09:51:08, Epoch : 1, Step : 6203, Training Loss : 0.76404, Training Acc : 0.711, Run Time : 3.13
INFO:root:2019-05-12 09:51:09, Epoch : 1, Step : 6204, Training Loss : 0.90627, Training Acc : 0.672, Run Time : 0.87
INFO:root:2019-05-12 09:51:09, Epoch : 1, Step : 6205, Training Loss : 0.87118, Training Acc : 0.672, Run Time : 0.61
INFO:root:2019-05-12 09:51:10, Epoch : 1, Step : 6206, Training Loss : 0.63510, Training Acc : 0.717, Run Time : 0.62
INFO:root:2019-05-12 09:51:19, Epoch : 1, Step : 6207, Training Loss : 0.56248, Training Acc : 0.733, Run Time : 8.72
INFO:root:2019-05-12 09:51:19, Epoch : 1, Step : 6208, Training Loss : 0.41820, Training Acc : 0.744, Run Time : 0.59
INFO:root:2019-05-12 09:51:21, Epoch : 1, Step : 6209, Training Loss : 0.36978, Training Acc : 0.811, Run Time : 1.63
INFO:root:2019-05-12 09:51:32, Epoch : 1, Step : 6210, Training Loss : 0.37501, Training Acc : 0.794, Run Time : 11.08
INFO:root:2019-05-12 09:51:32, Epoch : 1, Step : 6211, Training Loss : 0.31575, Training Acc : 0.867, Run Time : 0.56
INFO:root:2019-05-12 09:51:34, Epoch : 1, Step : 6212, Training Loss : 0.40008, Training Acc : 0.806, Run Time : 1.21
INFO:root:2019-05-12 09:51:51, Epoch : 1, Step : 6213, Training Loss : 0.31559, Training Acc : 0.850, Run Time : 17.06
INFO:root:2019-05-12 09:51:53, Epoch : 1, Step : 6214, Training Loss : 0.26963, Training Acc : 0.867, Run Time : 2.55
INFO:root:2019-05-12 09:52:09, Epoch : 1, Step : 6215, Training Loss : 0.40843, Training Acc : 0.828, Run Time : 15.31
INFO:root:2019-05-12 09:52:10, Epoch : 1, Step : 6216, Training Loss : 0.38858, Training Acc : 0.828, Run Time : 1.34
INFO:root:2019-05-12 09:52:11, Epoch : 1, Step : 6217, Training Loss : 0.45208, Training Acc : 0.806, Run Time : 1.09
INFO:root:2019-05-12 09:52:25, Epoch : 1, Step : 6218, Training Loss : 0.59460, Training Acc : 0.744, Run Time : 14.32
INFO:root:2019-05-12 09:52:26, Epoch : 1, Step : 6219, Training Loss : 0.31631, Training Acc : 0.844, Run Time : 0.52
INFO:root:2019-05-12 09:52:36, Epoch : 1, Step : 6220, Training Loss : 0.53882, Training Acc : 0.733, Run Time : 10.05
INFO:root:2019-05-12 09:52:41, Epoch : 1, Step : 6221, Training Loss : 0.47105, Training Acc : 0.789, Run Time : 4.89
INFO:root:2019-05-12 09:52:41, Epoch : 1, Step : 6222, Training Loss : 0.39406, Training Acc : 0.783, Run Time : 0.70
INFO:root:2019-05-12 09:52:54, Epoch : 1, Step : 6223, Training Loss : 0.40249, Training Acc : 0.861, Run Time : 12.51
INFO:root:2019-05-12 09:52:54, Epoch : 1, Step : 6224, Training Loss : 0.35800, Training Acc : 0.794, Run Time : 0.42
INFO:root:2019-05-12 09:52:56, Epoch : 1, Step : 6225, Training Loss : 0.35674, Training Acc : 0.817, Run Time : 1.76
INFO:root:2019-05-12 09:53:19, Epoch : 1, Step : 6226, Training Loss : 0.27885, Training Acc : 0.867, Run Time : 22.73
INFO:root:2019-05-12 09:53:22, Epoch : 1, Step : 6227, Training Loss : 0.26143, Training Acc : 0.889, Run Time : 3.19
INFO:root:2019-05-12 09:53:23, Epoch : 1, Step : 6228, Training Loss : 0.31110, Training Acc : 0.867, Run Time : 0.58
INFO:root:2019-05-12 09:53:31, Epoch : 1, Step : 6229, Training Loss : 0.27476, Training Acc : 0.883, Run Time : 8.25
INFO:root:2019-05-12 09:53:32, Epoch : 1, Step : 6230, Training Loss : 0.24852, Training Acc : 0.889, Run Time : 0.95
INFO:root:2019-05-12 09:53:33, Epoch : 1, Step : 6231, Training Loss : 0.22270, Training Acc : 0.911, Run Time : 1.06
INFO:root:2019-05-12 09:53:49, Epoch : 1, Step : 6232, Training Loss : 0.15066, Training Acc : 0.950, Run Time : 15.91
INFO:root:2019-05-12 09:53:56, Epoch : 1, Step : 6233, Training Loss : 0.24515, Training Acc : 0.911, Run Time : 6.80
INFO:root:2019-05-12 09:53:59, Epoch : 1, Step : 6234, Training Loss : 0.34042, Training Acc : 0.878, Run Time : 3.19
INFO:root:2019-05-12 09:54:00, Epoch : 1, Step : 6235, Training Loss : 0.22346, Training Acc : 0.911, Run Time : 1.35
INFO:root:2019-05-12 09:54:07, Epoch : 1, Step : 6236, Training Loss : 0.21685, Training Acc : 0.911, Run Time : 6.58
INFO:root:2019-05-12 09:54:07, Epoch : 1, Step : 6237, Training Loss : 0.34107, Training Acc : 0.856, Run Time : 0.62
INFO:root:2019-05-12 09:54:11, Epoch : 1, Step : 6238, Training Loss : 0.19707, Training Acc : 0.944, Run Time : 3.35
INFO:root:2019-05-12 09:54:14, Epoch : 1, Step : 6239, Training Loss : 0.26549, Training Acc : 0.883, Run Time : 3.22
INFO:root:2019-05-12 09:54:14, Epoch : 1, Step : 6240, Training Loss : 0.19108, Training Acc : 0.928, Run Time : 0.58
INFO:root:2019-05-12 09:54:15, Epoch : 1, Step : 6241, Training Loss : 0.23872, Training Acc : 0.900, Run Time : 1.00
INFO:root:2019-05-12 09:54:16, Epoch : 1, Step : 6242, Training Loss : 0.17054, Training Acc : 0.956, Run Time : 0.60
INFO:root:2019-05-12 09:54:17, Epoch : 1, Step : 6243, Training Loss : 0.21940, Training Acc : 0.917, Run Time : 0.88
INFO:root:2019-05-12 09:54:46, Epoch : 1, Step : 6244, Training Loss : 0.28856, Training Acc : 0.878, Run Time : 29.43
INFO:root:2019-05-12 09:54:59, Epoch : 1, Step : 6245, Training Loss : 0.23929, Training Acc : 0.894, Run Time : 12.73
INFO:root:2019-05-12 09:55:01, Epoch : 1, Step : 6246, Training Loss : 0.22380, Training Acc : 0.900, Run Time : 1.45
INFO:root:2019-05-12 09:55:09, Epoch : 1, Step : 6247, Training Loss : 0.25399, Training Acc : 0.922, Run Time : 8.89
INFO:root:2019-05-12 09:55:11, Epoch : 1, Step : 6248, Training Loss : 0.16398, Training Acc : 0.939, Run Time : 1.40
INFO:root:2019-05-12 09:55:25, Epoch : 1, Step : 6249, Training Loss : 0.21941, Training Acc : 0.906, Run Time : 13.78
INFO:root:2019-05-12 09:55:36, Epoch : 1, Step : 6250, Training Loss : 0.22388, Training Acc : 0.928, Run Time : 11.65
INFO:root:2019-05-12 09:55:37, Epoch : 1, Step : 6251, Training Loss : 0.21632, Training Acc : 0.922, Run Time : 1.01
INFO:root:2019-05-12 09:55:38, Epoch : 1, Step : 6252, Training Loss : 0.18448, Training Acc : 0.939, Run Time : 0.88
INFO:root:2019-05-12 09:55:54, Epoch : 1, Step : 6253, Training Loss : 0.15988, Training Acc : 0.944, Run Time : 15.63
INFO:root:2019-05-12 09:55:56, Epoch : 1, Step : 6254, Training Loss : 0.28230, Training Acc : 0.856, Run Time : 2.42
INFO:root:2019-05-12 09:56:04, Epoch : 1, Step : 6255, Training Loss : 0.23624, Training Acc : 0.922, Run Time : 8.09
INFO:root:2019-05-12 09:56:05, Epoch : 1, Step : 6256, Training Loss : 0.20279, Training Acc : 0.917, Run Time : 0.52
INFO:root:2019-05-12 09:56:07, Epoch : 1, Step : 6257, Training Loss : 0.24675, Training Acc : 0.894, Run Time : 2.06
INFO:root:2019-05-12 09:56:20, Epoch : 1, Step : 6258, Training Loss : 0.22770, Training Acc : 0.933, Run Time : 13.56
INFO:root:2019-05-12 09:56:22, Epoch : 1, Step : 6259, Training Loss : 0.20131, Training Acc : 0.928, Run Time : 1.41
INFO:root:2019-05-12 09:56:33, Epoch : 1, Step : 6260, Training Loss : 0.21611, Training Acc : 0.933, Run Time : 11.53
INFO:root:2019-05-12 09:56:34, Epoch : 1, Step : 6261, Training Loss : 0.19957, Training Acc : 0.928, Run Time : 0.48
INFO:root:2019-05-12 09:56:35, Epoch : 1, Step : 6262, Training Loss : 0.20132, Training Acc : 0.911, Run Time : 0.84
INFO:root:2019-05-12 09:56:48, Epoch : 1, Step : 6263, Training Loss : 0.35580, Training Acc : 0.861, Run Time : 12.88
INFO:root:2019-05-12 09:56:49, Epoch : 1, Step : 6264, Training Loss : 1.21418, Training Acc : 0.483, Run Time : 1.00
INFO:root:2019-05-12 09:56:49, Epoch : 1, Step : 6265, Training Loss : 0.48049, Training Acc : 0.761, Run Time : 0.41
INFO:root:2019-05-12 09:57:20, Epoch : 1, Step : 6266, Training Loss : 0.21434, Training Acc : 0.917, Run Time : 31.44
INFO:root:2019-05-12 09:57:38, Epoch : 1, Step : 6267, Training Loss : 0.35906, Training Acc : 0.822, Run Time : 17.37
INFO:root:2019-05-12 09:57:54, Epoch : 1, Step : 6268, Training Loss : 0.24387, Training Acc : 0.889, Run Time : 16.57
INFO:root:2019-05-12 09:57:55, Epoch : 1, Step : 6269, Training Loss : 0.23951, Training Acc : 0.922, Run Time : 0.86
INFO:root:2019-05-12 09:58:09, Epoch : 1, Step : 6270, Training Loss : 0.23531, Training Acc : 0.900, Run Time : 13.86
INFO:root:2019-05-12 09:58:10, Epoch : 1, Step : 6271, Training Loss : 0.24024, Training Acc : 0.906, Run Time : 0.77
INFO:root:2019-05-12 09:58:11, Epoch : 1, Step : 6272, Training Loss : 0.21695, Training Acc : 0.928, Run Time : 1.59
INFO:root:2019-05-12 09:58:23, Epoch : 1, Step : 6273, Training Loss : 0.26981, Training Acc : 0.928, Run Time : 11.46
INFO:root:2019-05-12 09:58:23, Epoch : 1, Step : 6274, Training Loss : 0.18680, Training Acc : 0.922, Run Time : 0.59
INFO:root:2019-05-12 09:58:42, Epoch : 1, Step : 6275, Training Loss : 0.26489, Training Acc : 0.883, Run Time : 18.40
INFO:root:2019-05-12 09:59:04, Epoch : 1, Step : 6276, Training Loss : 0.24888, Training Acc : 0.906, Run Time : 21.93
INFO:root:2019-05-12 09:59:10, Epoch : 1, Step : 6277, Training Loss : 0.19672, Training Acc : 0.917, Run Time : 5.92
INFO:root:2019-05-12 09:59:10, Epoch : 1, Step : 6278, Training Loss : 0.19008, Training Acc : 0.900, Run Time : 0.47
INFO:root:2019-05-12 09:59:23, Epoch : 1, Step : 6279, Training Loss : 0.37066, Training Acc : 0.889, Run Time : 12.52
INFO:root:2019-05-12 09:59:23, Epoch : 1, Step : 6280, Training Loss : 0.24611, Training Acc : 0.900, Run Time : 0.60
INFO:root:2019-05-12 09:59:24, Epoch : 1, Step : 6281, Training Loss : 0.27611, Training Acc : 0.878, Run Time : 1.08
INFO:root:2019-05-12 09:59:40, Epoch : 1, Step : 6282, Training Loss : 0.22746, Training Acc : 0.917, Run Time : 15.40
INFO:root:2019-05-12 09:59:40, Epoch : 1, Step : 6283, Training Loss : 0.25542, Training Acc : 0.883, Run Time : 0.42
INFO:root:2019-05-12 09:59:41, Epoch : 1, Step : 6284, Training Loss : 0.18584, Training Acc : 0.933, Run Time : 0.40
INFO:root:2019-05-12 09:59:41, Epoch : 1, Step : 6285, Training Loss : 0.20231, Training Acc : 0.911, Run Time : 0.40
INFO:root:2019-05-12 09:59:41, Epoch : 1, Step : 6286, Training Loss : 0.20827, Training Acc : 0.906, Run Time : 0.47
INFO:root:2019-05-12 09:59:59, Epoch : 1, Step : 6287, Training Loss : 0.26018, Training Acc : 0.900, Run Time : 17.49
INFO:root:2019-05-12 09:59:59, Epoch : 1, Step : 6288, Training Loss : 0.24448, Training Acc : 0.911, Run Time : 0.50
INFO:root:2019-05-12 10:00:00, Epoch : 1, Step : 6289, Training Loss : 0.46397, Training Acc : 0.783, Run Time : 0.56
INFO:root:2019-05-12 10:00:01, Epoch : 1, Step : 6290, Training Loss : 0.25438, Training Acc : 0.878, Run Time : 1.19
INFO:root:2019-05-12 10:00:12, Epoch : 1, Step : 6291, Training Loss : 0.26763, Training Acc : 0.867, Run Time : 11.24
INFO:root:2019-05-12 10:00:13, Epoch : 1, Step : 6292, Training Loss : 0.20289, Training Acc : 0.922, Run Time : 0.48
INFO:root:2019-05-12 10:00:14, Epoch : 1, Step : 6293, Training Loss : 0.17171, Training Acc : 0.961, Run Time : 0.57
INFO:root:2019-05-12 10:00:26, Epoch : 1, Step : 6294, Training Loss : 0.19293, Training Acc : 0.911, Run Time : 12.51
INFO:root:2019-05-12 10:00:27, Epoch : 1, Step : 6295, Training Loss : 0.25913, Training Acc : 0.889, Run Time : 1.45
INFO:root:2019-05-12 10:00:40, Epoch : 1, Step : 6296, Training Loss : 0.27207, Training Acc : 0.889, Run Time : 12.05
INFO:root:2019-05-12 10:00:40, Epoch : 1, Step : 6297, Training Loss : 0.27508, Training Acc : 0.900, Run Time : 0.78
INFO:root:2019-05-12 10:00:54, Epoch : 1, Step : 6298, Training Loss : 0.27100, Training Acc : 0.872, Run Time : 13.99
INFO:root:2019-05-12 10:00:56, Epoch : 1, Step : 6299, Training Loss : 0.45462, Training Acc : 0.772, Run Time : 1.59
INFO:root:2019-05-12 10:01:03, Epoch : 1, Step : 6300, Training Loss : 0.28148, Training Acc : 0.889, Run Time : 7.31
INFO:root:2019-05-12 10:01:06, Epoch : 1, Step : 6301, Training Loss : 0.25376, Training Acc : 0.911, Run Time : 2.86
INFO:root:2019-05-12 10:01:07, Epoch : 1, Step : 6302, Training Loss : 0.28274, Training Acc : 0.894, Run Time : 0.56
INFO:root:2019-05-12 10:01:13, Epoch : 1, Step : 6303, Training Loss : 0.27223, Training Acc : 0.900, Run Time : 6.60
INFO:root:2019-05-12 10:01:14, Epoch : 1, Step : 6304, Training Loss : 0.24542, Training Acc : 0.906, Run Time : 0.58
INFO:root:2019-05-12 10:01:15, Epoch : 1, Step : 6305, Training Loss : 0.25160, Training Acc : 0.900, Run Time : 0.72
INFO:root:2019-05-12 10:01:16, Epoch : 1, Step : 6306, Training Loss : 0.27032, Training Acc : 0.906, Run Time : 1.54
INFO:root:2019-05-12 10:01:29, Epoch : 1, Step : 6307, Training Loss : 0.30388, Training Acc : 0.861, Run Time : 13.07
INFO:root:2019-05-12 10:01:40, Epoch : 1, Step : 6308, Training Loss : 0.19407, Training Acc : 0.939, Run Time : 11.13
INFO:root:2019-05-12 10:01:42, Epoch : 1, Step : 6309, Training Loss : 0.28700, Training Acc : 0.894, Run Time : 1.65
INFO:root:2019-05-12 10:01:43, Epoch : 1, Step : 6310, Training Loss : 0.24356, Training Acc : 0.928, Run Time : 0.73
INFO:root:2019-05-12 10:01:43, Epoch : 1, Step : 6311, Training Loss : 0.19962, Training Acc : 0.944, Run Time : 0.59
INFO:root:2019-05-12 10:01:44, Epoch : 1, Step : 6312, Training Loss : 0.35952, Training Acc : 0.856, Run Time : 0.60
INFO:root:2019-05-12 10:02:00, Epoch : 1, Step : 6313, Training Loss : 0.22386, Training Acc : 0.933, Run Time : 15.87
INFO:root:2019-05-12 10:02:00, Epoch : 1, Step : 6314, Training Loss : 0.20603, Training Acc : 0.917, Run Time : 0.67
INFO:root:2019-05-12 10:02:03, Epoch : 1, Step : 6315, Training Loss : 0.18496, Training Acc : 0.933, Run Time : 2.43
INFO:root:2019-05-12 10:02:14, Epoch : 1, Step : 6316, Training Loss : 0.24988, Training Acc : 0.906, Run Time : 11.38
INFO:root:2019-05-12 10:02:15, Epoch : 1, Step : 6317, Training Loss : 0.22130, Training Acc : 0.939, Run Time : 0.57
INFO:root:2019-05-12 10:02:16, Epoch : 1, Step : 6318, Training Loss : 0.27471, Training Acc : 0.894, Run Time : 1.07
INFO:root:2019-05-12 10:02:28, Epoch : 1, Step : 6319, Training Loss : 0.25321, Training Acc : 0.911, Run Time : 12.01
INFO:root:2019-05-12 10:02:28, Epoch : 1, Step : 6320, Training Loss : 0.20198, Training Acc : 0.944, Run Time : 0.51
INFO:root:2019-05-12 10:02:29, Epoch : 1, Step : 6321, Training Loss : 0.34473, Training Acc : 0.883, Run Time : 0.46
INFO:root:2019-05-12 10:02:43, Epoch : 1, Step : 6322, Training Loss : 0.23693, Training Acc : 0.911, Run Time : 14.27
INFO:root:2019-05-12 10:02:44, Epoch : 1, Step : 6323, Training Loss : 0.19402, Training Acc : 0.917, Run Time : 0.83
INFO:root:2019-05-12 10:02:46, Epoch : 1, Step : 6324, Training Loss : 0.32075, Training Acc : 0.889, Run Time : 2.13
INFO:root:2019-05-12 10:02:58, Epoch : 1, Step : 6325, Training Loss : 0.18247, Training Acc : 0.939, Run Time : 12.35
INFO:root:2019-05-12 10:02:59, Epoch : 1, Step : 6326, Training Loss : 0.18329, Training Acc : 0.928, Run Time : 0.82
INFO:root:2019-05-12 10:03:00, Epoch : 1, Step : 6327, Training Loss : 0.30980, Training Acc : 0.906, Run Time : 0.61
INFO:root:2019-05-12 10:03:01, Epoch : 1, Step : 6328, Training Loss : 0.24724, Training Acc : 0.911, Run Time : 0.92
INFO:root:2019-05-12 10:03:13, Epoch : 1, Step : 6329, Training Loss : 0.24949, Training Acc : 0.922, Run Time : 11.88
INFO:root:2019-05-12 10:03:13, Epoch : 1, Step : 6330, Training Loss : 0.35715, Training Acc : 0.850, Run Time : 0.51
INFO:root:2019-05-12 10:03:14, Epoch : 1, Step : 6331, Training Loss : 0.21901, Training Acc : 0.911, Run Time : 0.59
INFO:root:2019-05-12 10:03:15, Epoch : 1, Step : 6332, Training Loss : 0.25807, Training Acc : 0.911, Run Time : 1.14
INFO:root:2019-05-12 10:03:25, Epoch : 1, Step : 6333, Training Loss : 0.28328, Training Acc : 0.900, Run Time : 10.37
INFO:root:2019-05-12 10:03:26, Epoch : 1, Step : 6334, Training Loss : 0.27067, Training Acc : 0.911, Run Time : 0.49
INFO:root:2019-05-12 10:03:26, Epoch : 1, Step : 6335, Training Loss : 0.23586, Training Acc : 0.928, Run Time : 0.43
INFO:root:2019-05-12 10:03:40, Epoch : 1, Step : 6336, Training Loss : 0.25365, Training Acc : 0.906, Run Time : 13.72
INFO:root:2019-05-12 10:03:41, Epoch : 1, Step : 6337, Training Loss : 0.28890, Training Acc : 0.856, Run Time : 1.01
INFO:root:2019-05-12 10:03:43, Epoch : 1, Step : 6338, Training Loss : 0.24659, Training Acc : 0.911, Run Time : 1.81
INFO:root:2019-05-12 10:04:00, Epoch : 1, Step : 6339, Training Loss : 0.17638, Training Acc : 0.933, Run Time : 17.11
INFO:root:2019-05-12 10:04:26, Epoch : 1, Step : 6340, Training Loss : 0.29062, Training Acc : 0.889, Run Time : 25.82
INFO:root:2019-05-12 10:04:27, Epoch : 1, Step : 6341, Training Loss : 0.24573, Training Acc : 0.900, Run Time : 1.79
INFO:root:2019-05-12 10:04:28, Epoch : 1, Step : 6342, Training Loss : 0.31111, Training Acc : 0.883, Run Time : 0.65
INFO:root:2019-05-12 10:04:38, Epoch : 1, Step : 6343, Training Loss : 0.22551, Training Acc : 0.906, Run Time : 10.21
INFO:root:2019-05-12 10:04:45, Epoch : 1, Step : 6344, Training Loss : 0.22919, Training Acc : 0.939, Run Time : 7.24
INFO:root:2019-05-12 10:04:58, Epoch : 1, Step : 6345, Training Loss : 0.24871, Training Acc : 0.878, Run Time : 12.92
INFO:root:2019-05-12 10:05:00, Epoch : 1, Step : 6346, Training Loss : 0.21402, Training Acc : 0.917, Run Time : 1.82
INFO:root:2019-05-12 10:05:12, Epoch : 1, Step : 6347, Training Loss : 0.12683, Training Acc : 0.944, Run Time : 11.37
INFO:root:2019-05-12 10:05:16, Epoch : 1, Step : 6348, Training Loss : 0.14306, Training Acc : 0.956, Run Time : 3.98
INFO:root:2019-05-12 10:05:20, Epoch : 1, Step : 6349, Training Loss : 0.19834, Training Acc : 0.922, Run Time : 4.06
INFO:root:2019-05-12 10:05:21, Epoch : 1, Step : 6350, Training Loss : 0.28847, Training Acc : 0.878, Run Time : 0.96
INFO:root:2019-05-12 10:05:28, Epoch : 1, Step : 6351, Training Loss : 0.14615, Training Acc : 0.933, Run Time : 7.33
INFO:root:2019-05-12 10:05:28, Epoch : 1, Step : 6352, Training Loss : 0.13421, Training Acc : 0.956, Run Time : 0.54
INFO:root:2019-05-12 10:05:30, Epoch : 1, Step : 6353, Training Loss : 0.14907, Training Acc : 0.944, Run Time : 1.39
INFO:root:2019-05-12 10:05:40, Epoch : 1, Step : 6354, Training Loss : 0.13763, Training Acc : 0.961, Run Time : 10.52
INFO:root:2019-05-12 10:05:41, Epoch : 1, Step : 6355, Training Loss : 0.14413, Training Acc : 0.956, Run Time : 0.68
INFO:root:2019-05-12 10:05:42, Epoch : 1, Step : 6356, Training Loss : 0.12230, Training Acc : 0.972, Run Time : 1.26
INFO:root:2019-05-12 10:05:53, Epoch : 1, Step : 6357, Training Loss : 0.17529, Training Acc : 0.944, Run Time : 10.53
INFO:root:2019-05-12 10:05:55, Epoch : 1, Step : 6358, Training Loss : 0.10314, Training Acc : 0.967, Run Time : 2.16
INFO:root:2019-05-12 10:06:11, Epoch : 1, Step : 6359, Training Loss : 0.19388, Training Acc : 0.900, Run Time : 15.97
INFO:root:2019-05-12 10:06:22, Epoch : 1, Step : 6360, Training Loss : 0.15988, Training Acc : 0.956, Run Time : 11.02
INFO:root:2019-05-12 10:06:23, Epoch : 1, Step : 6361, Training Loss : 0.17451, Training Acc : 0.939, Run Time : 1.02
INFO:root:2019-05-12 10:06:39, Epoch : 1, Step : 6362, Training Loss : 0.14852, Training Acc : 0.928, Run Time : 16.08
INFO:root:2019-05-12 10:06:40, Epoch : 1, Step : 6363, Training Loss : 0.14663, Training Acc : 0.944, Run Time : 0.48
INFO:root:2019-05-12 10:06:42, Epoch : 1, Step : 6364, Training Loss : 0.24818, Training Acc : 0.883, Run Time : 2.15
INFO:root:2019-05-12 10:07:01, Epoch : 1, Step : 6365, Training Loss : 0.23089, Training Acc : 0.911, Run Time : 19.02
INFO:root:2019-05-12 10:07:12, Epoch : 1, Step : 6366, Training Loss : 0.15385, Training Acc : 0.939, Run Time : 11.26
INFO:root:2019-05-12 10:07:39, Epoch : 1, Step : 6367, Training Loss : 0.18277, Training Acc : 0.911, Run Time : 26.69
INFO:root:2019-05-12 10:07:42, Epoch : 1, Step : 6368, Training Loss : 0.19025, Training Acc : 0.911, Run Time : 2.99
INFO:root:2019-05-12 10:07:43, Epoch : 1, Step : 6369, Training Loss : 0.15747, Training Acc : 0.933, Run Time : 1.05
INFO:root:2019-05-12 10:07:54, Epoch : 1, Step : 6370, Training Loss : 0.18291, Training Acc : 0.917, Run Time : 11.36
INFO:root:2019-05-12 10:08:00, Epoch : 1, Step : 6371, Training Loss : 0.22035, Training Acc : 0.889, Run Time : 6.22
INFO:root:2019-05-12 10:08:01, Epoch : 1, Step : 6372, Training Loss : 0.20044, Training Acc : 0.906, Run Time : 1.09
INFO:root:2019-05-12 10:08:02, Epoch : 1, Step : 6373, Training Loss : 0.12562, Training Acc : 0.978, Run Time : 0.70
INFO:root:2019-05-12 10:08:11, Epoch : 1, Step : 6374, Training Loss : 0.20225, Training Acc : 0.922, Run Time : 9.09
INFO:root:2019-05-12 10:08:12, Epoch : 1, Step : 6375, Training Loss : 0.12417, Training Acc : 0.967, Run Time : 0.65
INFO:root:2019-05-12 10:08:14, Epoch : 1, Step : 6376, Training Loss : 0.11833, Training Acc : 0.961, Run Time : 2.37
INFO:root:2019-05-12 10:08:26, Epoch : 1, Step : 6377, Training Loss : 0.08542, Training Acc : 0.983, Run Time : 11.44
INFO:root:2019-05-12 10:08:26, Epoch : 1, Step : 6378, Training Loss : 0.10290, Training Acc : 0.972, Run Time : 0.58
INFO:root:2019-05-12 10:08:27, Epoch : 1, Step : 6379, Training Loss : 0.16447, Training Acc : 0.922, Run Time : 0.60
INFO:root:2019-05-12 10:08:38, Epoch : 1, Step : 6380, Training Loss : 0.11452, Training Acc : 0.956, Run Time : 11.50
INFO:root:2019-05-12 10:08:39, Epoch : 1, Step : 6381, Training Loss : 0.10505, Training Acc : 0.961, Run Time : 0.68
INFO:root:2019-05-12 10:08:40, Epoch : 1, Step : 6382, Training Loss : 0.11850, Training Acc : 0.944, Run Time : 0.69
INFO:root:2019-05-12 10:08:40, Epoch : 1, Step : 6383, Training Loss : 0.14788, Training Acc : 0.928, Run Time : 0.66
INFO:root:2019-05-12 10:08:41, Epoch : 1, Step : 6384, Training Loss : 0.11198, Training Acc : 0.956, Run Time : 0.54
INFO:root:2019-05-12 10:09:02, Epoch : 1, Step : 6385, Training Loss : 0.11768, Training Acc : 0.950, Run Time : 21.15
INFO:root:2019-05-12 10:09:04, Epoch : 1, Step : 6386, Training Loss : 0.11668, Training Acc : 0.961, Run Time : 2.41
INFO:root:2019-05-12 10:09:05, Epoch : 1, Step : 6387, Training Loss : 0.10613, Training Acc : 0.967, Run Time : 0.57
INFO:root:2019-05-12 10:09:17, Epoch : 1, Step : 6388, Training Loss : 0.13167, Training Acc : 0.967, Run Time : 12.30
INFO:root:2019-05-12 10:09:19, Epoch : 1, Step : 6389, Training Loss : 0.10907, Training Acc : 0.967, Run Time : 1.87
INFO:root:2019-05-12 10:09:20, Epoch : 1, Step : 6390, Training Loss : 0.13436, Training Acc : 0.956, Run Time : 0.59
INFO:root:2019-05-12 10:09:33, Epoch : 1, Step : 6391, Training Loss : 0.12759, Training Acc : 0.967, Run Time : 13.19
INFO:root:2019-05-12 10:09:34, Epoch : 1, Step : 6392, Training Loss : 0.12642, Training Acc : 0.972, Run Time : 0.64
INFO:root:2019-05-12 10:09:36, Epoch : 1, Step : 6393, Training Loss : 0.13695, Training Acc : 0.950, Run Time : 1.88
INFO:root:2019-05-12 10:09:49, Epoch : 1, Step : 6394, Training Loss : 0.17053, Training Acc : 0.928, Run Time : 13.88
INFO:root:2019-05-12 10:09:50, Epoch : 1, Step : 6395, Training Loss : 0.17318, Training Acc : 0.922, Run Time : 0.86
INFO:root:2019-05-12 10:10:04, Epoch : 1, Step : 6396, Training Loss : 0.20624, Training Acc : 0.894, Run Time : 13.66
INFO:root:2019-05-12 10:10:05, Epoch : 1, Step : 6397, Training Loss : 0.18976, Training Acc : 0.922, Run Time : 0.60
INFO:root:2019-05-12 10:10:05, Epoch : 1, Step : 6398, Training Loss : 0.34362, Training Acc : 0.856, Run Time : 0.52
INFO:root:2019-05-12 10:10:17, Epoch : 1, Step : 6399, Training Loss : 0.34492, Training Acc : 0.878, Run Time : 12.00
INFO:root:2019-05-12 10:10:18, Epoch : 1, Step : 6400, Training Loss : 0.41966, Training Acc : 0.867, Run Time : 0.88
INFO:root:2019-05-12 10:10:19, Epoch : 1, Step : 6401, Training Loss : 0.67138, Training Acc : 0.772, Run Time : 1.13
INFO:root:2019-05-12 10:10:20, Epoch : 1, Step : 6402, Training Loss : 0.71040, Training Acc : 0.700, Run Time : 0.65
INFO:root:2019-05-12 10:10:32, Epoch : 1, Step : 6403, Training Loss : 0.62850, Training Acc : 0.728, Run Time : 11.92
INFO:root:2019-05-12 10:10:32, Epoch : 1, Step : 6404, Training Loss : 1.00152, Training Acc : 0.650, Run Time : 0.56
INFO:root:2019-05-12 10:10:33, Epoch : 1, Step : 6405, Training Loss : 1.05216, Training Acc : 0.617, Run Time : 0.60
INFO:root:2019-05-12 10:10:35, Epoch : 1, Step : 6406, Training Loss : 1.01182, Training Acc : 0.678, Run Time : 1.96
INFO:root:2019-05-12 10:10:58, Epoch : 1, Step : 6407, Training Loss : 0.84068, Training Acc : 0.667, Run Time : 23.31
INFO:root:2019-05-12 10:11:07, Epoch : 1, Step : 6408, Training Loss : 0.65584, Training Acc : 0.728, Run Time : 8.65
INFO:root:2019-05-12 10:11:27, Epoch : 1, Step : 6409, Training Loss : 0.50788, Training Acc : 0.772, Run Time : 20.54
INFO:root:2019-05-12 10:11:39, Epoch : 1, Step : 6410, Training Loss : 0.36828, Training Acc : 0.811, Run Time : 12.22
INFO:root:2019-05-12 10:11:56, Epoch : 1, Step : 6411, Training Loss : 0.35144, Training Acc : 0.861, Run Time : 16.61
INFO:root:2019-05-12 10:11:57, Epoch : 1, Step : 6412, Training Loss : 0.28037, Training Acc : 0.894, Run Time : 1.16
INFO:root:2019-05-12 10:11:58, Epoch : 1, Step : 6413, Training Loss : 0.50733, Training Acc : 0.761, Run Time : 0.41
INFO:root:2019-05-12 10:11:59, Epoch : 1, Step : 6414, Training Loss : 0.44572, Training Acc : 0.789, Run Time : 1.04
INFO:root:2019-05-12 10:12:11, Epoch : 1, Step : 6415, Training Loss : 0.30197, Training Acc : 0.906, Run Time : 12.20
INFO:root:2019-05-12 10:12:11, Epoch : 1, Step : 6416, Training Loss : 0.31355, Training Acc : 0.839, Run Time : 0.49
INFO:root:2019-05-12 10:12:12, Epoch : 1, Step : 6417, Training Loss : 0.28716, Training Acc : 0.917, Run Time : 0.42
INFO:root:2019-05-12 10:12:12, Epoch : 1, Step : 6418, Training Loss : 0.34882, Training Acc : 0.828, Run Time : 0.42
INFO:root:2019-05-12 10:12:13, Epoch : 1, Step : 6419, Training Loss : 0.62262, Training Acc : 0.733, Run Time : 0.43
INFO:root:2019-05-12 10:12:37, Epoch : 1, Step : 6420, Training Loss : 0.66195, Training Acc : 0.728, Run Time : 24.29
INFO:root:2019-05-12 10:12:49, Epoch : 1, Step : 6421, Training Loss : 0.46543, Training Acc : 0.817, Run Time : 12.20
INFO:root:2019-05-12 10:12:50, Epoch : 1, Step : 6422, Training Loss : 0.38239, Training Acc : 0.850, Run Time : 0.61
INFO:root:2019-05-12 10:12:50, Epoch : 1, Step : 6423, Training Loss : 0.45614, Training Acc : 0.794, Run Time : 0.38
INFO:root:2019-05-12 10:12:51, Epoch : 1, Step : 6424, Training Loss : 0.46867, Training Acc : 0.822, Run Time : 0.53
INFO:root:2019-05-12 10:13:14, Epoch : 1, Step : 6425, Training Loss : 0.39875, Training Acc : 0.856, Run Time : 23.15
INFO:root:2019-05-12 10:13:23, Epoch : 1, Step : 6426, Training Loss : 0.50940, Training Acc : 0.828, Run Time : 9.00
INFO:root:2019-05-12 10:13:35, Epoch : 1, Step : 6427, Training Loss : 0.70212, Training Acc : 0.794, Run Time : 11.97
INFO:root:2019-05-12 10:13:35, Epoch : 1, Step : 6428, Training Loss : 0.97404, Training Acc : 0.656, Run Time : 0.68
INFO:root:2019-05-12 10:13:36, Epoch : 1, Step : 6429, Training Loss : 0.63526, Training Acc : 0.744, Run Time : 0.49
INFO:root:2019-05-12 10:13:37, Epoch : 1, Step : 6430, Training Loss : 0.41558, Training Acc : 0.839, Run Time : 0.62
INFO:root:2019-05-12 10:13:39, Epoch : 1, Step : 6431, Training Loss : 0.33168, Training Acc : 0.911, Run Time : 2.26
INFO:root:2019-05-12 10:13:55, Epoch : 1, Step : 6432, Training Loss : 0.18993, Training Acc : 0.939, Run Time : 16.63
INFO:root:2019-05-12 10:13:56, Epoch : 1, Step : 6433, Training Loss : 0.27838, Training Acc : 0.894, Run Time : 0.64
INFO:root:2019-05-12 10:13:58, Epoch : 1, Step : 6434, Training Loss : 0.51354, Training Acc : 0.767, Run Time : 1.57
INFO:root:2019-05-12 10:14:13, Epoch : 1, Step : 6435, Training Loss : 0.53168, Training Acc : 0.744, Run Time : 15.69
INFO:root:2019-05-12 10:14:14, Epoch : 1, Step : 6436, Training Loss : 0.47244, Training Acc : 0.844, Run Time : 1.14
INFO:root:2019-05-12 10:14:31, Epoch : 1, Step : 6437, Training Loss : 0.39515, Training Acc : 0.867, Run Time : 16.57
INFO:root:2019-05-12 10:14:46, Epoch : 1, Step : 6438, Training Loss : 0.45983, Training Acc : 0.828, Run Time : 15.09
INFO:root:2019-05-12 10:15:00, Epoch : 1, Step : 6439, Training Loss : 0.38351, Training Acc : 0.839, Run Time : 13.62
INFO:root:2019-05-12 10:15:01, Epoch : 1, Step : 6440, Training Loss : 0.50124, Training Acc : 0.783, Run Time : 1.42
INFO:root:2019-05-12 10:15:02, Epoch : 1, Step : 6441, Training Loss : 0.42415, Training Acc : 0.839, Run Time : 0.91
INFO:root:2019-05-12 10:15:16, Epoch : 1, Step : 6442, Training Loss : 0.22673, Training Acc : 0.917, Run Time : 13.51
INFO:root:2019-05-12 10:15:16, Epoch : 1, Step : 6443, Training Loss : 0.29284, Training Acc : 0.867, Run Time : 0.82
INFO:root:2019-05-12 10:15:31, Epoch : 1, Step : 6444, Training Loss : 0.25775, Training Acc : 0.861, Run Time : 14.86
INFO:root:2019-05-12 10:15:32, Epoch : 1, Step : 6445, Training Loss : 0.50061, Training Acc : 0.778, Run Time : 1.03
INFO:root:2019-05-12 10:15:43, Epoch : 1, Step : 6446, Training Loss : 0.26833, Training Acc : 0.906, Run Time : 10.27
INFO:root:2019-05-12 10:15:43, Epoch : 1, Step : 6447, Training Loss : 0.39890, Training Acc : 0.867, Run Time : 0.62
INFO:root:2019-05-12 10:15:46, Epoch : 1, Step : 6448, Training Loss : 0.33159, Training Acc : 0.850, Run Time : 2.54
INFO:root:2019-05-12 10:16:01, Epoch : 1, Step : 6449, Training Loss : 0.27455, Training Acc : 0.878, Run Time : 15.19
INFO:root:2019-05-12 10:16:03, Epoch : 1, Step : 6450, Training Loss : 0.20277, Training Acc : 0.944, Run Time : 1.95
INFO:root:2019-05-12 10:16:12, Epoch : 1, Step : 6451, Training Loss : 0.18186, Training Acc : 0.944, Run Time : 8.99
INFO:root:2019-05-12 10:16:12, Epoch : 1, Step : 6452, Training Loss : 0.25569, Training Acc : 0.906, Run Time : 0.52
INFO:root:2019-05-12 10:16:14, Epoch : 1, Step : 6453, Training Loss : 0.13988, Training Acc : 0.956, Run Time : 1.67
INFO:root:2019-05-12 10:16:25, Epoch : 1, Step : 6454, Training Loss : 0.35070, Training Acc : 0.867, Run Time : 11.23
INFO:root:2019-05-12 10:16:26, Epoch : 1, Step : 6455, Training Loss : 0.57218, Training Acc : 0.800, Run Time : 0.59
INFO:root:2019-05-12 10:16:28, Epoch : 1, Step : 6456, Training Loss : 0.27640, Training Acc : 0.883, Run Time : 2.23
INFO:root:2019-05-12 10:16:42, Epoch : 1, Step : 6457, Training Loss : 0.41782, Training Acc : 0.817, Run Time : 13.62
INFO:root:2019-05-12 10:16:43, Epoch : 1, Step : 6458, Training Loss : 0.21192, Training Acc : 0.911, Run Time : 0.81
INFO:root:2019-05-12 10:16:43, Epoch : 1, Step : 6459, Training Loss : 0.41972, Training Acc : 0.856, Run Time : 0.49
INFO:root:2019-05-12 10:16:44, Epoch : 1, Step : 6460, Training Loss : 0.18950, Training Acc : 0.928, Run Time : 1.08
INFO:root:2019-05-12 10:16:55, Epoch : 1, Step : 6461, Training Loss : 0.15165, Training Acc : 0.956, Run Time : 11.18
INFO:root:2019-05-12 10:16:56, Epoch : 1, Step : 6462, Training Loss : 0.20899, Training Acc : 0.933, Run Time : 0.85
INFO:root:2019-05-12 10:16:57, Epoch : 1, Step : 6463, Training Loss : 0.26888, Training Acc : 0.900, Run Time : 0.66
INFO:root:2019-05-12 10:17:12, Epoch : 1, Step : 6464, Training Loss : 0.37162, Training Acc : 0.844, Run Time : 14.96
INFO:root:2019-05-12 10:17:12, Epoch : 1, Step : 6465, Training Loss : 0.20763, Training Acc : 0.900, Run Time : 0.55
INFO:root:2019-05-12 10:17:13, Epoch : 1, Step : 6466, Training Loss : 0.22693, Training Acc : 0.922, Run Time : 0.62
INFO:root:2019-05-12 10:17:27, Epoch : 1, Step : 6467, Training Loss : 0.50517, Training Acc : 0.833, Run Time : 13.77
INFO:root:2019-05-12 10:17:28, Epoch : 1, Step : 6468, Training Loss : 0.36339, Training Acc : 0.828, Run Time : 1.08
INFO:root:2019-05-12 10:17:40, Epoch : 1, Step : 6469, Training Loss : 0.36730, Training Acc : 0.856, Run Time : 12.04
INFO:root:2019-05-12 10:17:40, Epoch : 1, Step : 6470, Training Loss : 0.35456, Training Acc : 0.872, Run Time : 0.70
INFO:root:2019-05-12 10:17:52, Epoch : 1, Step : 6471, Training Loss : 0.14623, Training Acc : 0.978, Run Time : 12.01
INFO:root:2019-05-12 10:17:54, Epoch : 1, Step : 6472, Training Loss : 0.18211, Training Acc : 0.939, Run Time : 1.64
INFO:root:2019-05-12 10:17:55, Epoch : 1, Step : 6473, Training Loss : 0.48646, Training Acc : 0.844, Run Time : 0.59
INFO:root:2019-05-12 10:18:04, Epoch : 1, Step : 6474, Training Loss : 0.46126, Training Acc : 0.861, Run Time : 9.47
INFO:root:2019-05-12 10:18:05, Epoch : 1, Step : 6475, Training Loss : 0.41360, Training Acc : 0.872, Run Time : 0.49
INFO:root:2019-05-12 10:18:06, Epoch : 1, Step : 6476, Training Loss : 0.23510, Training Acc : 0.900, Run Time : 1.68
INFO:root:2019-05-12 10:18:23, Epoch : 1, Step : 6477, Training Loss : 0.37133, Training Acc : 0.844, Run Time : 16.45
INFO:root:2019-05-12 10:18:25, Epoch : 1, Step : 6478, Training Loss : 0.24217, Training Acc : 0.911, Run Time : 2.47
INFO:root:2019-05-12 10:18:26, Epoch : 1, Step : 6479, Training Loss : 0.26441, Training Acc : 0.906, Run Time : 0.59
INFO:root:2019-05-12 10:18:40, Epoch : 1, Step : 6480, Training Loss : 0.36698, Training Acc : 0.894, Run Time : 13.85
INFO:root:2019-05-12 10:18:42, Epoch : 1, Step : 6481, Training Loss : 0.21582, Training Acc : 0.906, Run Time : 1.88
INFO:root:2019-05-12 10:18:52, Epoch : 1, Step : 6482, Training Loss : 0.25313, Training Acc : 0.922, Run Time : 10.17
INFO:root:2019-05-12 10:18:53, Epoch : 1, Step : 6483, Training Loss : 0.21795, Training Acc : 0.917, Run Time : 0.96
INFO:root:2019-05-12 10:19:05, Epoch : 1, Step : 6484, Training Loss : 0.14960, Training Acc : 0.967, Run Time : 11.93
INFO:root:2019-05-12 10:19:06, Epoch : 1, Step : 6485, Training Loss : 0.13351, Training Acc : 0.983, Run Time : 1.17
INFO:root:2019-05-12 10:19:06, Epoch : 1, Step : 6486, Training Loss : 0.18316, Training Acc : 0.956, Run Time : 0.59
INFO:root:2019-05-12 10:19:09, Epoch : 1, Step : 6487, Training Loss : 0.18187, Training Acc : 0.939, Run Time : 2.38
INFO:root:2019-05-12 10:19:16, Epoch : 1, Step : 6488, Training Loss : 0.21643, Training Acc : 0.922, Run Time : 7.56
INFO:root:2019-05-12 10:19:17, Epoch : 1, Step : 6489, Training Loss : 0.17094, Training Acc : 0.967, Run Time : 0.81
INFO:root:2019-05-12 10:19:18, Epoch : 1, Step : 6490, Training Loss : 0.20419, Training Acc : 0.917, Run Time : 0.67
INFO:root:2019-05-12 10:19:30, Epoch : 1, Step : 6491, Training Loss : 0.25844, Training Acc : 0.894, Run Time : 12.22
INFO:root:2019-05-12 10:19:31, Epoch : 1, Step : 6492, Training Loss : 0.28748, Training Acc : 0.867, Run Time : 0.72
INFO:root:2019-05-12 10:19:44, Epoch : 1, Step : 6493, Training Loss : 0.40769, Training Acc : 0.850, Run Time : 13.65
INFO:root:2019-05-12 10:19:45, Epoch : 1, Step : 6494, Training Loss : 0.49990, Training Acc : 0.750, Run Time : 0.58
INFO:root:2019-05-12 10:19:46, Epoch : 1, Step : 6495, Training Loss : 0.53839, Training Acc : 0.778, Run Time : 0.57
INFO:root:2019-05-12 10:20:02, Epoch : 1, Step : 6496, Training Loss : 0.41907, Training Acc : 0.806, Run Time : 15.96
INFO:root:2019-05-12 10:20:03, Epoch : 1, Step : 6497, Training Loss : 0.22273, Training Acc : 0.939, Run Time : 1.75
INFO:root:2019-05-12 10:20:04, Epoch : 1, Step : 6498, Training Loss : 0.29846, Training Acc : 0.883, Run Time : 0.70
INFO:root:2019-05-12 10:20:16, Epoch : 1, Step : 6499, Training Loss : 0.20551, Training Acc : 0.911, Run Time : 11.67
INFO:root:2019-05-12 10:20:16, Epoch : 1, Step : 6500, Training Loss : 0.42582, Training Acc : 0.806, Run Time : 0.73
INFO:root:2019-05-12 10:20:27, Epoch : 1, Step : 6501, Training Loss : 0.34020, Training Acc : 0.828, Run Time : 10.76
INFO:root:2019-05-12 10:20:29, Epoch : 1, Step : 6502, Training Loss : 0.45035, Training Acc : 0.800, Run Time : 1.41
INFO:root:2019-05-12 10:20:30, Epoch : 1, Step : 6503, Training Loss : 0.20476, Training Acc : 0.917, Run Time : 1.33
INFO:root:2019-05-12 10:20:46, Epoch : 1, Step : 6504, Training Loss : 0.33096, Training Acc : 0.889, Run Time : 15.66
INFO:root:2019-05-12 10:20:49, Epoch : 1, Step : 6505, Training Loss : 0.22424, Training Acc : 0.933, Run Time : 3.53
INFO:root:2019-05-12 10:20:50, Epoch : 1, Step : 6506, Training Loss : 0.24579, Training Acc : 0.939, Run Time : 0.60
INFO:root:2019-05-12 10:20:50, Epoch : 1, Step : 6507, Training Loss : 0.35116, Training Acc : 0.861, Run Time : 0.61
INFO:root:2019-05-12 10:20:59, Epoch : 1, Step : 6508, Training Loss : 0.49369, Training Acc : 0.772, Run Time : 8.60
INFO:root:2019-05-12 10:21:05, Epoch : 1, Step : 6509, Training Loss : 0.42193, Training Acc : 0.822, Run Time : 6.45
INFO:root:2019-05-12 10:21:06, Epoch : 1, Step : 6510, Training Loss : 0.59059, Training Acc : 0.678, Run Time : 0.57
INFO:root:2019-05-12 10:21:08, Epoch : 1, Step : 6511, Training Loss : 0.46237, Training Acc : 0.794, Run Time : 2.08
INFO:root:2019-05-12 10:21:23, Epoch : 1, Step : 6512, Training Loss : 0.21246, Training Acc : 0.917, Run Time : 14.93
INFO:root:2019-05-12 10:21:24, Epoch : 1, Step : 6513, Training Loss : 0.27167, Training Acc : 0.856, Run Time : 0.91
INFO:root:2019-05-12 10:21:25, Epoch : 1, Step : 6514, Training Loss : 0.41053, Training Acc : 0.800, Run Time : 1.03
INFO:root:2019-05-12 10:21:38, Epoch : 1, Step : 6515, Training Loss : 0.55923, Training Acc : 0.756, Run Time : 12.81
INFO:root:2019-05-12 10:21:39, Epoch : 1, Step : 6516, Training Loss : 0.58589, Training Acc : 0.789, Run Time : 1.14
INFO:root:2019-05-12 10:21:59, Epoch : 1, Step : 6517, Training Loss : 0.48066, Training Acc : 0.794, Run Time : 20.33
INFO:root:2019-05-12 10:22:00, Epoch : 1, Step : 6518, Training Loss : 0.41078, Training Acc : 0.844, Run Time : 0.78
INFO:root:2019-05-12 10:22:16, Epoch : 1, Step : 6519, Training Loss : 0.24337, Training Acc : 0.889, Run Time : 15.61
INFO:root:2019-05-12 10:22:26, Epoch : 1, Step : 6520, Training Loss : 0.21671, Training Acc : 0.917, Run Time : 10.57
INFO:root:2019-05-12 10:22:27, Epoch : 1, Step : 6521, Training Loss : 0.23806, Training Acc : 0.883, Run Time : 0.86
INFO:root:2019-05-12 10:22:28, Epoch : 1, Step : 6522, Training Loss : 0.20110, Training Acc : 0.917, Run Time : 0.67
INFO:root:2019-05-12 10:22:32, Epoch : 1, Step : 6523, Training Loss : 0.19345, Training Acc : 0.917, Run Time : 4.24
INFO:root:2019-05-12 10:22:40, Epoch : 1, Step : 6524, Training Loss : 0.17089, Training Acc : 0.939, Run Time : 7.70
INFO:root:2019-05-12 10:22:40, Epoch : 1, Step : 6525, Training Loss : 0.21167, Training Acc : 0.911, Run Time : 0.46
INFO:root:2019-05-12 10:22:41, Epoch : 1, Step : 6526, Training Loss : 0.16649, Training Acc : 0.933, Run Time : 0.59
INFO:root:2019-05-12 10:22:57, Epoch : 1, Step : 6527, Training Loss : 0.13402, Training Acc : 0.961, Run Time : 15.91
INFO:root:2019-05-12 10:22:57, Epoch : 1, Step : 6528, Training Loss : 0.20108, Training Acc : 0.911, Run Time : 0.67
INFO:root:2019-05-12 10:23:29, Epoch : 1, Step : 6529, Training Loss : 0.24738, Training Acc : 0.906, Run Time : 31.74
INFO:root:2019-05-12 10:23:30, Epoch : 1, Step : 6530, Training Loss : 0.20898, Training Acc : 0.922, Run Time : 1.08
INFO:root:2019-05-12 10:23:47, Epoch : 1, Step : 6531, Training Loss : 0.22135, Training Acc : 0.928, Run Time : 16.77
INFO:root:2019-05-12 10:24:02, Epoch : 1, Step : 6532, Training Loss : 0.18986, Training Acc : 0.917, Run Time : 15.34
INFO:root:2019-05-12 10:24:04, Epoch : 1, Step : 6533, Training Loss : 0.12759, Training Acc : 0.939, Run Time : 1.41
INFO:root:2019-05-12 10:24:04, Epoch : 1, Step : 6534, Training Loss : 0.17567, Training Acc : 0.911, Run Time : 0.64
INFO:root:2019-05-12 10:24:17, Epoch : 1, Step : 6535, Training Loss : 0.12438, Training Acc : 0.961, Run Time : 12.79
INFO:root:2019-05-12 10:24:30, Epoch : 1, Step : 6536, Training Loss : 0.08696, Training Acc : 0.972, Run Time : 13.23
INFO:root:2019-05-12 10:24:32, Epoch : 1, Step : 6537, Training Loss : 0.11523, Training Acc : 0.972, Run Time : 1.48
INFO:root:2019-05-12 10:24:33, Epoch : 1, Step : 6538, Training Loss : 0.07274, Training Acc : 0.994, Run Time : 1.28
INFO:root:2019-05-12 10:24:51, Epoch : 1, Step : 6539, Training Loss : 0.09001, Training Acc : 0.989, Run Time : 18.16
INFO:root:2019-05-12 10:24:53, Epoch : 1, Step : 6540, Training Loss : 0.11040, Training Acc : 0.972, Run Time : 1.60
INFO:root:2019-05-12 10:24:53, Epoch : 1, Step : 6541, Training Loss : 0.12215, Training Acc : 0.967, Run Time : 0.57
INFO:root:2019-05-12 10:24:57, Epoch : 1, Step : 6542, Training Loss : 0.08600, Training Acc : 0.978, Run Time : 3.66
INFO:root:2019-05-12 10:25:01, Epoch : 1, Step : 6543, Training Loss : 0.11888, Training Acc : 0.967, Run Time : 4.29
INFO:root:2019-05-12 10:25:02, Epoch : 1, Step : 6544, Training Loss : 0.15873, Training Acc : 0.939, Run Time : 0.63
INFO:root:2019-05-12 10:25:04, Epoch : 1, Step : 6545, Training Loss : 0.10707, Training Acc : 0.967, Run Time : 1.93
INFO:root:2019-05-12 10:25:24, Epoch : 1, Step : 6546, Training Loss : 0.15581, Training Acc : 0.933, Run Time : 20.12
INFO:root:2019-05-12 10:25:32, Epoch : 1, Step : 6547, Training Loss : 0.07981, Training Acc : 0.989, Run Time : 7.70
INFO:root:2019-05-12 10:25:34, Epoch : 1, Step : 6548, Training Loss : 0.09595, Training Acc : 0.972, Run Time : 2.03
INFO:root:2019-05-12 10:25:45, Epoch : 1, Step : 6549, Training Loss : 0.06471, Training Acc : 0.989, Run Time : 11.63
INFO:root:2019-05-12 10:25:47, Epoch : 1, Step : 6550, Training Loss : 0.08198, Training Acc : 0.972, Run Time : 1.65
INFO:root:2019-05-12 10:25:48, Epoch : 1, Step : 6551, Training Loss : 0.11968, Training Acc : 0.950, Run Time : 1.22
INFO:root:2019-05-12 10:26:02, Epoch : 1, Step : 6552, Training Loss : 0.08060, Training Acc : 0.983, Run Time : 13.40
INFO:root:2019-05-12 10:26:02, Epoch : 1, Step : 6553, Training Loss : 0.05630, Training Acc : 0.994, Run Time : 0.90
INFO:root:2019-05-12 10:26:03, Epoch : 1, Step : 6554, Training Loss : 0.08268, Training Acc : 0.989, Run Time : 0.49
INFO:root:2019-05-12 10:26:19, Epoch : 1, Step : 6555, Training Loss : 0.06789, Training Acc : 0.983, Run Time : 16.10
INFO:root:2019-05-12 10:26:20, Epoch : 1, Step : 6556, Training Loss : 0.08056, Training Acc : 0.967, Run Time : 1.38
INFO:root:2019-05-12 10:26:23, Epoch : 1, Step : 6557, Training Loss : 0.10780, Training Acc : 0.967, Run Time : 2.18
INFO:root:2019-05-12 10:26:36, Epoch : 1, Step : 6558, Training Loss : 0.10718, Training Acc : 0.956, Run Time : 13.12
INFO:root:2019-05-12 10:26:38, Epoch : 1, Step : 6559, Training Loss : 0.07911, Training Acc : 0.978, Run Time : 2.21
INFO:root:2019-05-12 10:27:15, Epoch : 1, Step : 6560, Training Loss : 0.10066, Training Acc : 0.967, Run Time : 36.78
INFO:root:2019-05-12 10:27:19, Epoch : 1, Step : 6561, Training Loss : 0.21488, Training Acc : 0.911, Run Time : 4.62
INFO:root:2019-05-12 10:27:20, Epoch : 1, Step : 6562, Training Loss : 0.20415, Training Acc : 0.939, Run Time : 0.63
INFO:root:2019-05-12 10:27:34, Epoch : 1, Step : 6563, Training Loss : 0.23792, Training Acc : 0.933, Run Time : 13.80
INFO:root:2019-05-12 10:27:34, Epoch : 1, Step : 6564, Training Loss : 0.24894, Training Acc : 0.917, Run Time : 0.65
INFO:root:2019-05-12 10:27:53, Epoch : 1, Step : 6565, Training Loss : 0.63561, Training Acc : 0.822, Run Time : 18.51
INFO:root:2019-05-12 10:27:54, Epoch : 1, Step : 6566, Training Loss : 0.50837, Training Acc : 0.867, Run Time : 1.17
INFO:root:2019-05-12 10:27:55, Epoch : 1, Step : 6567, Training Loss : 0.28238, Training Acc : 0.894, Run Time : 0.41
INFO:root:2019-05-12 10:27:55, Epoch : 1, Step : 6568, Training Loss : 0.28243, Training Acc : 0.883, Run Time : 0.73
INFO:root:2019-05-12 10:27:56, Epoch : 1, Step : 6569, Training Loss : 0.20461, Training Acc : 0.917, Run Time : 0.57
INFO:root:2019-05-12 10:28:18, Epoch : 1, Step : 6570, Training Loss : 0.12068, Training Acc : 0.961, Run Time : 21.85
INFO:root:2019-05-12 10:28:19, Epoch : 1, Step : 6571, Training Loss : 0.10995, Training Acc : 0.972, Run Time : 1.52
INFO:root:2019-05-12 10:28:20, Epoch : 1, Step : 6572, Training Loss : 0.20740, Training Acc : 0.894, Run Time : 0.61
INFO:root:2019-05-12 10:28:20, Epoch : 1, Step : 6573, Training Loss : 0.22267, Training Acc : 0.900, Run Time : 0.62
INFO:root:2019-05-12 10:28:33, Epoch : 1, Step : 6574, Training Loss : 0.17311, Training Acc : 0.911, Run Time : 12.54
INFO:root:2019-05-12 10:28:34, Epoch : 1, Step : 6575, Training Loss : 0.16062, Training Acc : 0.939, Run Time : 1.00
INFO:root:2019-05-12 10:29:01, Epoch : 1, Step : 6576, Training Loss : 0.17714, Training Acc : 0.906, Run Time : 27.35
INFO:root:2019-05-12 10:29:04, Epoch : 1, Step : 6577, Training Loss : 0.14213, Training Acc : 0.956, Run Time : 3.08
INFO:root:2019-05-12 10:29:05, Epoch : 1, Step : 6578, Training Loss : 0.09192, Training Acc : 0.972, Run Time : 0.61
INFO:root:2019-05-12 10:29:22, Epoch : 1, Step : 6579, Training Loss : 0.09230, Training Acc : 0.983, Run Time : 17.19
INFO:root:2019-05-12 10:29:23, Epoch : 1, Step : 6580, Training Loss : 0.14489, Training Acc : 0.928, Run Time : 0.58
INFO:root:2019-05-12 10:29:36, Epoch : 1, Step : 6581, Training Loss : 0.14796, Training Acc : 0.944, Run Time : 13.07
INFO:root:2019-05-12 10:29:37, Epoch : 1, Step : 6582, Training Loss : 0.28785, Training Acc : 0.878, Run Time : 1.00
INFO:root:2019-05-12 10:29:38, Epoch : 1, Step : 6583, Training Loss : 0.22945, Training Acc : 0.900, Run Time : 0.76
INFO:root:2019-05-12 10:29:58, Epoch : 1, Step : 6584, Training Loss : 0.15269, Training Acc : 0.933, Run Time : 20.79
INFO:root:2019-05-12 10:30:10, Epoch : 1, Step : 6585, Training Loss : 0.19219, Training Acc : 0.906, Run Time : 11.59
INFO:root:2019-05-12 10:30:24, Epoch : 1, Step : 6586, Training Loss : 0.11764, Training Acc : 0.956, Run Time : 13.73
INFO:root:2019-05-12 10:30:26, Epoch : 1, Step : 6587, Training Loss : 0.16508, Training Acc : 0.944, Run Time : 1.95
INFO:root:2019-05-12 10:30:26, Epoch : 1, Step : 6588, Training Loss : 0.09583, Training Acc : 0.961, Run Time : 0.66
INFO:root:2019-05-12 10:30:45, Epoch : 1, Step : 6589, Training Loss : 0.26261, Training Acc : 0.900, Run Time : 18.94
INFO:root:2019-05-12 10:30:55, Epoch : 1, Step : 6590, Training Loss : 0.23986, Training Acc : 0.917, Run Time : 9.72
INFO:root:2019-05-12 10:31:06, Epoch : 1, Step : 6591, Training Loss : 0.22571, Training Acc : 0.911, Run Time : 11.46
INFO:root:2019-05-12 10:31:07, Epoch : 1, Step : 6592, Training Loss : 0.17850, Training Acc : 0.928, Run Time : 0.92
INFO:root:2019-05-12 10:31:25, Epoch : 1, Step : 6593, Training Loss : 0.32886, Training Acc : 0.856, Run Time : 17.91
INFO:root:2019-05-12 10:31:27, Epoch : 1, Step : 6594, Training Loss : 0.39065, Training Acc : 0.856, Run Time : 1.77
INFO:root:2019-05-12 10:31:41, Epoch : 1, Step : 6595, Training Loss : 0.20640, Training Acc : 0.933, Run Time : 13.99
INFO:root:2019-05-12 10:31:41, Epoch : 1, Step : 6596, Training Loss : 0.20715, Training Acc : 0.939, Run Time : 0.42
INFO:root:2019-05-12 10:31:43, Epoch : 1, Step : 6597, Training Loss : 0.06370, Training Acc : 0.989, Run Time : 1.18
INFO:root:2019-05-12 10:31:57, Epoch : 1, Step : 6598, Training Loss : 0.10771, Training Acc : 0.961, Run Time : 14.38
INFO:root:2019-05-12 10:31:57, Epoch : 1, Step : 6599, Training Loss : 0.18181, Training Acc : 0.911, Run Time : 0.42
INFO:root:2019-05-12 10:31:59, Epoch : 1, Step : 6600, Training Loss : 0.29623, Training Acc : 0.850, Run Time : 2.04
INFO:root:2019-05-12 10:32:15, Epoch : 1, Step : 6601, Training Loss : 1.22546, Training Acc : 0.561, Run Time : 15.70
INFO:root:2019-05-12 10:32:16, Epoch : 1, Step : 6602, Training Loss : 1.49934, Training Acc : 0.489, Run Time : 0.82
INFO:root:2019-05-12 10:32:30, Epoch : 1, Step : 6603, Training Loss : 1.50187, Training Acc : 0.439, Run Time : 14.51
INFO:root:2019-05-12 10:32:32, Epoch : 1, Step : 6604, Training Loss : 0.88357, Training Acc : 0.556, Run Time : 1.10
INFO:root:2019-05-12 10:32:32, Epoch : 1, Step : 6605, Training Loss : 1.29636, Training Acc : 0.394, Run Time : 0.68
INFO:root:2019-05-12 10:32:44, Epoch : 1, Step : 6606, Training Loss : 0.89077, Training Acc : 0.506, Run Time : 12.03
INFO:root:2019-05-12 10:32:45, Epoch : 1, Step : 6607, Training Loss : 0.71721, Training Acc : 0.628, Run Time : 0.52
INFO:root:2019-05-12 10:32:46, Epoch : 1, Step : 6608, Training Loss : 0.82523, Training Acc : 0.600, Run Time : 0.73
INFO:root:2019-05-12 10:32:46, Epoch : 1, Step : 6609, Training Loss : 0.74930, Training Acc : 0.711, Run Time : 0.64
INFO:root:2019-05-12 10:32:47, Epoch : 1, Step : 6610, Training Loss : 0.83926, Training Acc : 0.656, Run Time : 0.61
INFO:root:2019-05-12 10:33:15, Epoch : 1, Step : 6611, Training Loss : 1.06015, Training Acc : 0.461, Run Time : 28.58
INFO:root:2019-05-12 10:33:18, Epoch : 1, Step : 6612, Training Loss : 0.79796, Training Acc : 0.622, Run Time : 2.40
INFO:root:2019-05-12 10:33:28, Epoch : 1, Step : 6613, Training Loss : 0.89539, Training Acc : 0.544, Run Time : 9.90
INFO:root:2019-05-12 10:33:29, Epoch : 1, Step : 6614, Training Loss : 0.87953, Training Acc : 0.528, Run Time : 1.08
INFO:root:2019-05-12 10:33:29, Epoch : 1, Step : 6615, Training Loss : 0.60322, Training Acc : 0.650, Run Time : 0.73
INFO:root:2019-05-12 10:33:31, Epoch : 1, Step : 6616, Training Loss : 0.63923, Training Acc : 0.661, Run Time : 1.52
INFO:root:2019-05-12 10:33:43, Epoch : 1, Step : 6617, Training Loss : 0.61323, Training Acc : 0.683, Run Time : 11.84
INFO:root:2019-05-12 10:33:43, Epoch : 1, Step : 6618, Training Loss : 0.51329, Training Acc : 0.772, Run Time : 0.58
INFO:root:2019-05-12 10:33:45, Epoch : 1, Step : 6619, Training Loss : 0.47359, Training Acc : 0.783, Run Time : 1.60
INFO:root:2019-05-12 10:34:00, Epoch : 1, Step : 6620, Training Loss : 0.55202, Training Acc : 0.700, Run Time : 15.32
INFO:root:2019-05-12 10:34:02, Epoch : 1, Step : 6621, Training Loss : 0.53253, Training Acc : 0.783, Run Time : 1.55
INFO:root:2019-05-12 10:34:04, Epoch : 1, Step : 6622, Training Loss : 0.46785, Training Acc : 0.839, Run Time : 1.81
INFO:root:2019-05-12 10:34:16, Epoch : 1, Step : 6623, Training Loss : 0.52626, Training Acc : 0.722, Run Time : 12.24
INFO:root:2019-05-12 10:34:17, Epoch : 1, Step : 6624, Training Loss : 0.62050, Training Acc : 0.683, Run Time : 1.02
INFO:root:2019-05-12 10:34:33, Epoch : 1, Step : 6625, Training Loss : 0.51912, Training Acc : 0.722, Run Time : 16.52
INFO:root:2019-05-12 10:34:34, Epoch : 1, Step : 6626, Training Loss : 0.47949, Training Acc : 0.794, Run Time : 0.78
INFO:root:2019-05-12 10:34:45, Epoch : 1, Step : 6627, Training Loss : 0.45494, Training Acc : 0.794, Run Time : 10.93
INFO:root:2019-05-12 10:35:07, Epoch : 1, Step : 6628, Training Loss : 0.39559, Training Acc : 0.833, Run Time : 21.48
INFO:root:2019-05-12 10:35:11, Epoch : 1, Step : 6629, Training Loss : 0.52024, Training Acc : 0.711, Run Time : 3.95
INFO:root:2019-05-12 10:35:11, Epoch : 1, Step : 6630, Training Loss : 0.37582, Training Acc : 0.844, Run Time : 0.46
INFO:root:2019-05-12 10:35:12, Epoch : 1, Step : 6631, Training Loss : 0.37359, Training Acc : 0.856, Run Time : 1.22
INFO:root:2019-05-12 10:35:22, Epoch : 1, Step : 6632, Training Loss : 0.39723, Training Acc : 0.828, Run Time : 9.88
INFO:root:2019-05-12 10:35:23, Epoch : 1, Step : 6633, Training Loss : 0.46529, Training Acc : 0.872, Run Time : 0.56
INFO:root:2019-05-12 10:35:23, Epoch : 1, Step : 6634, Training Loss : 0.38677, Training Acc : 0.850, Run Time : 0.57
INFO:root:2019-05-12 10:35:46, Epoch : 1, Step : 6635, Training Loss : 0.40648, Training Acc : 0.833, Run Time : 22.99
INFO:root:2019-05-12 10:35:53, Epoch : 1, Step : 6636, Training Loss : 0.27395, Training Acc : 0.978, Run Time : 6.46
INFO:root:2019-05-12 10:35:53, Epoch : 1, Step : 6637, Training Loss : 0.42582, Training Acc : 0.778, Run Time : 0.51
INFO:root:2019-05-12 10:35:56, Epoch : 1, Step : 6638, Training Loss : 0.50511, Training Acc : 0.739, Run Time : 2.28
INFO:root:2019-05-12 10:36:07, Epoch : 1, Step : 6639, Training Loss : 0.40544, Training Acc : 0.828, Run Time : 11.73
INFO:root:2019-05-12 10:36:08, Epoch : 1, Step : 6640, Training Loss : 0.23674, Training Acc : 0.972, Run Time : 0.55
INFO:root:2019-05-12 10:36:09, Epoch : 1, Step : 6641, Training Loss : 0.46923, Training Acc : 0.856, Run Time : 0.67
INFO:root:2019-05-12 10:36:20, Epoch : 1, Step : 6642, Training Loss : 0.32951, Training Acc : 0.900, Run Time : 11.71
INFO:root:2019-05-12 10:36:21, Epoch : 1, Step : 6643, Training Loss : 0.35196, Training Acc : 0.850, Run Time : 0.79
INFO:root:2019-05-12 10:36:29, Epoch : 1, Step : 6644, Training Loss : 0.37663, Training Acc : 0.872, Run Time : 7.92
INFO:root:2019-05-12 10:36:34, Epoch : 1, Step : 6645, Training Loss : 0.53552, Training Acc : 0.689, Run Time : 5.15
INFO:root:2019-05-12 10:36:35, Epoch : 1, Step : 6646, Training Loss : 0.44474, Training Acc : 0.828, Run Time : 0.62
INFO:root:2019-05-12 10:36:48, Epoch : 1, Step : 6647, Training Loss : 0.52226, Training Acc : 0.756, Run Time : 13.56
INFO:root:2019-05-12 10:36:49, Epoch : 1, Step : 6648, Training Loss : 0.37619, Training Acc : 0.817, Run Time : 0.48
INFO:root:2019-05-12 10:36:49, Epoch : 1, Step : 6649, Training Loss : 0.38484, Training Acc : 0.822, Run Time : 0.38
INFO:root:2019-05-12 10:36:52, Epoch : 1, Step : 6650, Training Loss : 0.31774, Training Acc : 0.900, Run Time : 2.74
INFO:root:2019-05-12 10:37:04, Epoch : 1, Step : 6651, Training Loss : 0.49347, Training Acc : 0.728, Run Time : 11.93
INFO:root:2019-05-12 10:37:05, Epoch : 1, Step : 6652, Training Loss : 0.43953, Training Acc : 0.767, Run Time : 1.26
INFO:root:2019-05-12 10:37:15, Epoch : 1, Step : 6653, Training Loss : 0.25765, Training Acc : 0.928, Run Time : 9.62
INFO:root:2019-05-12 10:37:15, Epoch : 1, Step : 6654, Training Loss : 0.40444, Training Acc : 0.811, Run Time : 0.77
INFO:root:2019-05-12 10:37:16, Epoch : 1, Step : 6655, Training Loss : 0.38887, Training Acc : 0.822, Run Time : 0.68
INFO:root:2019-05-12 10:37:27, Epoch : 1, Step : 6656, Training Loss : 0.32811, Training Acc : 0.861, Run Time : 10.99
INFO:root:2019-05-12 10:37:28, Epoch : 1, Step : 6657, Training Loss : 0.40571, Training Acc : 0.839, Run Time : 0.55
INFO:root:2019-05-12 10:37:28, Epoch : 1, Step : 6658, Training Loss : 0.25793, Training Acc : 0.939, Run Time : 0.69
INFO:root:2019-05-12 10:37:30, Epoch : 1, Step : 6659, Training Loss : 0.38466, Training Acc : 0.822, Run Time : 1.87
INFO:root:2019-05-12 10:37:43, Epoch : 1, Step : 6660, Training Loss : 0.35550, Training Acc : 0.833, Run Time : 12.30
INFO:root:2019-05-12 10:37:44, Epoch : 1, Step : 6661, Training Loss : 0.40469, Training Acc : 0.822, Run Time : 1.03
INFO:root:2019-05-12 10:37:44, Epoch : 1, Step : 6662, Training Loss : 0.41046, Training Acc : 0.839, Run Time : 0.65
INFO:root:2019-05-12 10:38:00, Epoch : 1, Step : 6663, Training Loss : 0.36624, Training Acc : 0.839, Run Time : 16.07
INFO:root:2019-05-12 10:38:14, Epoch : 1, Step : 6664, Training Loss : 0.34728, Training Acc : 0.872, Run Time : 13.40
INFO:root:2019-05-12 10:38:15, Epoch : 1, Step : 6665, Training Loss : 0.41257, Training Acc : 0.800, Run Time : 1.31
INFO:root:2019-05-12 10:38:16, Epoch : 1, Step : 6666, Training Loss : 0.31253, Training Acc : 0.872, Run Time : 0.58
INFO:root:2019-05-12 10:38:16, Epoch : 1, Step : 6667, Training Loss : 0.32069, Training Acc : 0.861, Run Time : 0.62
INFO:root:2019-05-12 10:38:32, Epoch : 1, Step : 6668, Training Loss : 0.68017, Training Acc : 0.683, Run Time : 15.43
INFO:root:2019-05-12 10:38:45, Epoch : 1, Step : 6669, Training Loss : 0.40289, Training Acc : 0.806, Run Time : 13.07
INFO:root:2019-05-12 10:38:51, Epoch : 1, Step : 6670, Training Loss : 0.34930, Training Acc : 0.833, Run Time : 6.03
INFO:root:2019-05-12 10:38:51, Epoch : 1, Step : 6671, Training Loss : 0.43506, Training Acc : 0.839, Run Time : 0.49
INFO:root:2019-05-12 10:38:52, Epoch : 1, Step : 6672, Training Loss : 0.42461, Training Acc : 0.817, Run Time : 0.67
INFO:root:2019-05-12 10:39:04, Epoch : 1, Step : 6673, Training Loss : 0.25217, Training Acc : 0.961, Run Time : 12.22
INFO:root:2019-05-12 10:39:05, Epoch : 1, Step : 6674, Training Loss : 0.25311, Training Acc : 0.933, Run Time : 0.82
INFO:root:2019-05-12 10:39:27, Epoch : 1, Step : 6675, Training Loss : 0.32443, Training Acc : 0.878, Run Time : 22.03
INFO:root:2019-05-12 10:39:40, Epoch : 1, Step : 6676, Training Loss : 0.37507, Training Acc : 0.850, Run Time : 12.93
INFO:root:2019-05-12 10:39:47, Epoch : 1, Step : 6677, Training Loss : 0.32990, Training Acc : 0.867, Run Time : 6.61
INFO:root:2019-05-12 10:39:47, Epoch : 1, Step : 6678, Training Loss : 0.35004, Training Acc : 0.883, Run Time : 0.81
INFO:root:2019-05-12 10:40:02, Epoch : 1, Step : 6679, Training Loss : 0.34557, Training Acc : 0.872, Run Time : 14.59
INFO:root:2019-05-12 10:40:03, Epoch : 1, Step : 6680, Training Loss : 0.18190, Training Acc : 0.989, Run Time : 0.85
INFO:root:2019-05-12 10:40:03, Epoch : 1, Step : 6681, Training Loss : 0.24716, Training Acc : 0.950, Run Time : 0.56
INFO:root:2019-05-12 10:40:04, Epoch : 1, Step : 6682, Training Loss : 0.20499, Training Acc : 0.922, Run Time : 0.63
INFO:root:2019-05-12 10:40:15, Epoch : 1, Step : 6683, Training Loss : 0.43819, Training Acc : 0.783, Run Time : 11.20
INFO:root:2019-05-12 10:40:16, Epoch : 1, Step : 6684, Training Loss : 0.37269, Training Acc : 0.856, Run Time : 0.46
INFO:root:2019-05-12 10:40:16, Epoch : 1, Step : 6685, Training Loss : 0.22961, Training Acc : 0.939, Run Time : 0.77
INFO:root:2019-05-12 10:40:30, Epoch : 1, Step : 6686, Training Loss : 0.29213, Training Acc : 0.900, Run Time : 13.33
INFO:root:2019-05-12 10:40:31, Epoch : 1, Step : 6687, Training Loss : 0.33571, Training Acc : 0.856, Run Time : 1.08
INFO:root:2019-05-12 10:40:32, Epoch : 1, Step : 6688, Training Loss : 0.23975, Training Acc : 0.939, Run Time : 1.11
INFO:root:2019-05-12 10:40:46, Epoch : 1, Step : 6689, Training Loss : 0.20158, Training Acc : 0.944, Run Time : 13.81
INFO:root:2019-05-12 10:41:00, Epoch : 1, Step : 6690, Training Loss : 0.49642, Training Acc : 0.739, Run Time : 14.47
INFO:root:2019-05-12 10:41:01, Epoch : 1, Step : 6691, Training Loss : 0.26855, Training Acc : 0.922, Run Time : 1.23
INFO:root:2019-05-12 10:41:02, Epoch : 1, Step : 6692, Training Loss : 0.21620, Training Acc : 0.939, Run Time : 0.61
INFO:root:2019-05-12 10:41:13, Epoch : 1, Step : 6693, Training Loss : 0.34702, Training Acc : 0.839, Run Time : 10.92
INFO:root:2019-05-12 10:41:14, Epoch : 1, Step : 6694, Training Loss : 0.18736, Training Acc : 0.933, Run Time : 0.66
INFO:root:2019-05-12 10:41:15, Epoch : 1, Step : 6695, Training Loss : 0.44177, Training Acc : 0.850, Run Time : 1.64
INFO:root:2019-05-12 10:41:34, Epoch : 1, Step : 6696, Training Loss : 0.24044, Training Acc : 0.894, Run Time : 18.88
INFO:root:2019-05-12 10:41:36, Epoch : 1, Step : 6697, Training Loss : 0.36795, Training Acc : 0.850, Run Time : 1.40
INFO:root:2019-05-12 10:41:47, Epoch : 1, Step : 6698, Training Loss : 0.26626, Training Acc : 0.906, Run Time : 11.94
INFO:root:2019-05-12 10:41:48, Epoch : 1, Step : 6699, Training Loss : 0.21062, Training Acc : 0.950, Run Time : 0.95
INFO:root:2019-05-12 10:41:52, Epoch : 1, Step : 6700, Training Loss : 0.33753, Training Acc : 0.872, Run Time : 3.61
INFO:root:2019-05-12 10:42:02, Epoch : 1, Step : 6701, Training Loss : 0.27005, Training Acc : 0.894, Run Time : 10.36
INFO:root:2019-05-12 10:42:03, Epoch : 1, Step : 6702, Training Loss : 0.19154, Training Acc : 0.944, Run Time : 0.44
INFO:root:2019-05-12 10:42:04, Epoch : 1, Step : 6703, Training Loss : 0.23173, Training Acc : 0.917, Run Time : 1.65
INFO:root:2019-05-12 10:42:16, Epoch : 1, Step : 6704, Training Loss : 0.30113, Training Acc : 0.883, Run Time : 11.67
INFO:root:2019-05-12 10:42:17, Epoch : 1, Step : 6705, Training Loss : 0.37398, Training Acc : 0.850, Run Time : 0.83
INFO:root:2019-05-12 10:42:30, Epoch : 1, Step : 6706, Training Loss : 0.14856, Training Acc : 0.961, Run Time : 12.88
INFO:root:2019-05-12 10:42:32, Epoch : 1, Step : 6707, Training Loss : 0.22133, Training Acc : 0.911, Run Time : 1.67
INFO:root:2019-05-12 10:42:32, Epoch : 1, Step : 6708, Training Loss : 0.31909, Training Acc : 0.878, Run Time : 0.97
INFO:root:2019-05-12 10:42:45, Epoch : 1, Step : 6709, Training Loss : 0.29000, Training Acc : 0.889, Run Time : 12.61
INFO:root:2019-05-12 10:42:46, Epoch : 1, Step : 6710, Training Loss : 0.24302, Training Acc : 0.933, Run Time : 0.97
INFO:root:2019-05-12 10:42:57, Epoch : 1, Step : 6711, Training Loss : 0.30535, Training Acc : 0.889, Run Time : 11.06
INFO:root:2019-05-12 10:42:58, Epoch : 1, Step : 6712, Training Loss : 0.39399, Training Acc : 0.850, Run Time : 1.35
INFO:root:2019-05-12 10:43:00, Epoch : 1, Step : 6713, Training Loss : 0.20278, Training Acc : 0.950, Run Time : 1.42
INFO:root:2019-05-12 10:43:10, Epoch : 1, Step : 6714, Training Loss : 0.33352, Training Acc : 0.833, Run Time : 10.58
INFO:root:2019-05-12 10:43:11, Epoch : 1, Step : 6715, Training Loss : 0.16526, Training Acc : 0.956, Run Time : 0.44
INFO:root:2019-05-12 10:43:12, Epoch : 1, Step : 6716, Training Loss : 0.43569, Training Acc : 0.789, Run Time : 0.70
INFO:root:2019-05-12 10:43:13, Epoch : 1, Step : 6717, Training Loss : 0.39585, Training Acc : 0.844, Run Time : 0.98
INFO:root:2019-05-12 10:43:23, Epoch : 1, Step : 6718, Training Loss : 0.31942, Training Acc : 0.833, Run Time : 10.50
INFO:root:2019-05-12 10:43:24, Epoch : 1, Step : 6719, Training Loss : 0.28398, Training Acc : 0.878, Run Time : 1.25
INFO:root:2019-05-12 10:43:38, Epoch : 1, Step : 6720, Training Loss : 0.24774, Training Acc : 0.883, Run Time : 13.75
INFO:root:2019-05-12 10:43:39, Epoch : 1, Step : 6721, Training Loss : 0.48862, Training Acc : 0.772, Run Time : 0.42
INFO:root:2019-05-12 10:43:39, Epoch : 1, Step : 6722, Training Loss : 0.35602, Training Acc : 0.844, Run Time : 0.58
INFO:root:2019-05-12 10:43:41, Epoch : 1, Step : 6723, Training Loss : 0.18603, Training Acc : 0.956, Run Time : 1.79
INFO:root:2019-05-12 10:43:58, Epoch : 1, Step : 6724, Training Loss : 0.20238, Training Acc : 0.944, Run Time : 17.55
INFO:root:2019-05-12 10:44:15, Epoch : 1, Step : 6725, Training Loss : 0.18939, Training Acc : 0.922, Run Time : 16.09
INFO:root:2019-05-12 10:44:16, Epoch : 1, Step : 6726, Training Loss : 0.21189, Training Acc : 0.922, Run Time : 1.14
INFO:root:2019-05-12 10:44:16, Epoch : 1, Step : 6727, Training Loss : 0.28210, Training Acc : 0.872, Run Time : 0.69
INFO:root:2019-05-12 10:44:17, Epoch : 1, Step : 6728, Training Loss : 0.27736, Training Acc : 0.889, Run Time : 1.01
INFO:root:2019-05-12 10:44:29, Epoch : 1, Step : 6729, Training Loss : 0.15364, Training Acc : 0.950, Run Time : 11.15
INFO:root:2019-05-12 10:44:30, Epoch : 1, Step : 6730, Training Loss : 0.23067, Training Acc : 0.922, Run Time : 1.06
INFO:root:2019-05-12 10:44:31, Epoch : 1, Step : 6731, Training Loss : 0.23302, Training Acc : 0.894, Run Time : 1.13
INFO:root:2019-05-12 10:44:49, Epoch : 1, Step : 6732, Training Loss : 0.21605, Training Acc : 0.894, Run Time : 18.74
INFO:root:2019-05-12 10:45:04, Epoch : 1, Step : 6733, Training Loss : 0.21581, Training Acc : 0.906, Run Time : 14.19
INFO:root:2019-05-12 10:45:08, Epoch : 1, Step : 6734, Training Loss : 0.16919, Training Acc : 0.950, Run Time : 3.95
INFO:root:2019-05-12 10:45:08, Epoch : 1, Step : 6735, Training Loss : 0.26832, Training Acc : 0.906, Run Time : 0.42
INFO:root:2019-05-12 10:45:08, Epoch : 1, Step : 6736, Training Loss : 0.31506, Training Acc : 0.833, Run Time : 0.40
INFO:root:2019-05-12 10:45:11, Epoch : 1, Step : 6737, Training Loss : 0.19577, Training Acc : 0.894, Run Time : 2.21
INFO:root:2019-05-12 10:45:23, Epoch : 1, Step : 6738, Training Loss : 0.36822, Training Acc : 0.822, Run Time : 12.00
INFO:root:2019-05-12 10:45:24, Epoch : 1, Step : 6739, Training Loss : 0.13545, Training Acc : 0.967, Run Time : 1.37
INFO:root:2019-05-12 10:45:55, Epoch : 1, Step : 6740, Training Loss : 0.19936, Training Acc : 0.939, Run Time : 30.66
INFO:root:2019-05-12 10:45:56, Epoch : 1, Step : 6741, Training Loss : 0.36367, Training Acc : 0.850, Run Time : 1.53
INFO:root:2019-05-12 10:45:57, Epoch : 1, Step : 6742, Training Loss : 0.40788, Training Acc : 0.772, Run Time : 0.81
INFO:root:2019-05-12 10:46:11, Epoch : 1, Step : 6743, Training Loss : 0.22263, Training Acc : 0.928, Run Time : 13.62
INFO:root:2019-05-12 10:46:12, Epoch : 1, Step : 6744, Training Loss : 0.30243, Training Acc : 0.856, Run Time : 1.81
INFO:root:2019-05-12 10:46:13, Epoch : 1, Step : 6745, Training Loss : 0.30157, Training Acc : 0.872, Run Time : 0.46
INFO:root:2019-05-12 10:46:28, Epoch : 1, Step : 6746, Training Loss : 0.34752, Training Acc : 0.817, Run Time : 15.54
INFO:root:2019-05-12 10:46:29, Epoch : 1, Step : 6747, Training Loss : 0.12623, Training Acc : 0.967, Run Time : 1.00
INFO:root:2019-05-12 10:46:43, Epoch : 1, Step : 6748, Training Loss : 0.36926, Training Acc : 0.844, Run Time : 13.70
INFO:root:2019-05-12 10:46:44, Epoch : 1, Step : 6749, Training Loss : 0.23880, Training Acc : 0.889, Run Time : 0.89
INFO:root:2019-05-12 10:46:45, Epoch : 1, Step : 6750, Training Loss : 0.13839, Training Acc : 0.950, Run Time : 0.62
INFO:root:2019-05-12 10:46:46, Epoch : 1, Step : 6751, Training Loss : 0.24778, Training Acc : 0.917, Run Time : 1.07
INFO:root:2019-05-12 10:46:57, Epoch : 1, Step : 6752, Training Loss : 0.21343, Training Acc : 0.928, Run Time : 10.96
INFO:root:2019-05-12 10:46:58, Epoch : 1, Step : 6753, Training Loss : 0.32899, Training Acc : 0.817, Run Time : 1.51
INFO:root:2019-05-12 10:47:10, Epoch : 1, Step : 6754, Training Loss : 0.27015, Training Acc : 0.900, Run Time : 11.58
INFO:root:2019-05-12 10:47:11, Epoch : 1, Step : 6755, Training Loss : 0.55037, Training Acc : 0.744, Run Time : 1.20
INFO:root:2019-05-12 10:47:12, Epoch : 1, Step : 6756, Training Loss : 0.27898, Training Acc : 0.878, Run Time : 0.58
INFO:root:2019-05-12 10:47:12, Epoch : 1, Step : 6757, Training Loss : 0.26933, Training Acc : 0.878, Run Time : 0.60
INFO:root:2019-05-12 10:47:13, Epoch : 1, Step : 6758, Training Loss : 0.22901, Training Acc : 0.894, Run Time : 0.58
INFO:root:2019-05-12 10:47:17, Epoch : 1, Step : 6759, Training Loss : 0.37339, Training Acc : 0.789, Run Time : 4.70
INFO:root:2019-05-12 10:47:35, Epoch : 1, Step : 6760, Training Loss : 0.25070, Training Acc : 0.900, Run Time : 17.11
INFO:root:2019-05-12 10:47:36, Epoch : 1, Step : 6761, Training Loss : 0.14850, Training Acc : 0.944, Run Time : 1.48
INFO:root:2019-05-12 10:48:09, Epoch : 1, Step : 6762, Training Loss : 0.24252, Training Acc : 0.911, Run Time : 33.44
INFO:root:2019-05-12 10:48:21, Epoch : 1, Step : 6763, Training Loss : 0.24805, Training Acc : 0.911, Run Time : 11.70
INFO:root:2019-05-12 10:48:43, Epoch : 1, Step : 6764, Training Loss : 0.38127, Training Acc : 0.856, Run Time : 21.79
INFO:root:2019-05-12 10:48:57, Epoch : 1, Step : 6765, Training Loss : 0.25975, Training Acc : 0.889, Run Time : 13.76
INFO:root:2019-05-12 10:48:59, Epoch : 1, Step : 6766, Training Loss : 0.32520, Training Acc : 0.861, Run Time : 1.94
INFO:root:2019-05-12 10:49:08, Epoch : 1, Step : 6767, Training Loss : 0.36409, Training Acc : 0.833, Run Time : 9.46
INFO:root:2019-05-12 10:49:09, Epoch : 1, Step : 6768, Training Loss : 0.22372, Training Acc : 0.900, Run Time : 0.43
INFO:root:2019-05-12 10:49:09, Epoch : 1, Step : 6769, Training Loss : 0.19056, Training Acc : 0.917, Run Time : 0.39
INFO:root:2019-05-12 10:49:10, Epoch : 1, Step : 6770, Training Loss : 0.37558, Training Acc : 0.839, Run Time : 0.72
INFO:root:2019-05-12 10:49:10, Epoch : 1, Step : 6771, Training Loss : 0.31541, Training Acc : 0.839, Run Time : 0.64
INFO:root:2019-05-12 10:49:26, Epoch : 1, Step : 6772, Training Loss : 0.16637, Training Acc : 0.939, Run Time : 15.99
INFO:root:2019-05-12 10:49:27, Epoch : 1, Step : 6773, Training Loss : 0.22137, Training Acc : 0.894, Run Time : 0.83
INFO:root:2019-05-12 10:49:42, Epoch : 1, Step : 6774, Training Loss : 0.19515, Training Acc : 0.900, Run Time : 15.20
INFO:root:2019-05-12 10:49:43, Epoch : 1, Step : 6775, Training Loss : 0.25506, Training Acc : 0.894, Run Time : 0.95
INFO:root:2019-05-12 10:50:11, Epoch : 1, Step : 6776, Training Loss : 0.21375, Training Acc : 0.917, Run Time : 27.31
INFO:root:2019-05-12 10:50:15, Epoch : 1, Step : 6777, Training Loss : 0.17087, Training Acc : 0.939, Run Time : 4.30
INFO:root:2019-05-12 10:50:15, Epoch : 1, Step : 6778, Training Loss : 0.16683, Training Acc : 0.922, Run Time : 0.47
INFO:root:2019-05-12 10:50:26, Epoch : 1, Step : 6779, Training Loss : 0.17025, Training Acc : 0.933, Run Time : 11.06
INFO:root:2019-05-12 10:50:28, Epoch : 1, Step : 6780, Training Loss : 0.27497, Training Acc : 0.878, Run Time : 1.43
INFO:root:2019-05-12 10:50:42, Epoch : 1, Step : 6781, Training Loss : 0.21085, Training Acc : 0.911, Run Time : 14.37
INFO:root:2019-05-12 10:50:43, Epoch : 1, Step : 6782, Training Loss : 0.10742, Training Acc : 0.961, Run Time : 0.45
INFO:root:2019-05-12 10:50:43, Epoch : 1, Step : 6783, Training Loss : 0.21780, Training Acc : 0.911, Run Time : 0.63
INFO:root:2019-05-12 10:50:45, Epoch : 1, Step : 6784, Training Loss : 0.28350, Training Acc : 0.850, Run Time : 2.05
INFO:root:2019-05-12 10:51:05, Epoch : 1, Step : 6785, Training Loss : 0.28605, Training Acc : 0.867, Run Time : 19.54
INFO:root:2019-05-12 10:51:08, Epoch : 1, Step : 6786, Training Loss : 0.48859, Training Acc : 0.839, Run Time : 2.90
INFO:root:2019-05-12 10:51:09, Epoch : 1, Step : 6787, Training Loss : 1.02325, Training Acc : 0.567, Run Time : 1.68
INFO:root:2019-05-12 10:51:18, Epoch : 1, Step : 6788, Training Loss : 0.88608, Training Acc : 0.639, Run Time : 8.33
INFO:root:2019-05-12 10:51:19, Epoch : 1, Step : 6789, Training Loss : 0.67134, Training Acc : 0.728, Run Time : 0.80
INFO:root:2019-05-12 10:51:20, Epoch : 1, Step : 6790, Training Loss : 0.67289, Training Acc : 0.706, Run Time : 0.99
INFO:root:2019-05-12 10:51:40, Epoch : 1, Step : 6791, Training Loss : 0.48762, Training Acc : 0.767, Run Time : 20.53
INFO:root:2019-05-12 10:51:42, Epoch : 1, Step : 6792, Training Loss : 0.37503, Training Acc : 0.850, Run Time : 1.58
INFO:root:2019-05-12 10:51:43, Epoch : 1, Step : 6793, Training Loss : 0.28208, Training Acc : 0.867, Run Time : 0.82
INFO:root:2019-05-12 10:51:54, Epoch : 1, Step : 6794, Training Loss : 0.22819, Training Acc : 0.900, Run Time : 11.45
INFO:root:2019-05-12 10:51:55, Epoch : 1, Step : 6795, Training Loss : 0.34309, Training Acc : 0.839, Run Time : 0.54
INFO:root:2019-05-12 10:52:09, Epoch : 1, Step : 6796, Training Loss : 0.48203, Training Acc : 0.800, Run Time : 14.12
INFO:root:2019-05-12 10:52:09, Epoch : 1, Step : 6797, Training Loss : 0.45376, Training Acc : 0.789, Run Time : 0.43
INFO:root:2019-05-12 10:52:10, Epoch : 1, Step : 6798, Training Loss : 0.25826, Training Acc : 0.878, Run Time : 0.63
INFO:root:2019-05-12 10:52:10, Epoch : 1, Step : 6799, Training Loss : 0.60350, Training Acc : 0.772, Run Time : 0.62
INFO:root:2019-05-12 10:52:11, Epoch : 1, Step : 6800, Training Loss : 0.57802, Training Acc : 0.678, Run Time : 0.49
INFO:root:2019-05-12 10:52:26, Epoch : 1, Step : 6801, Training Loss : 1.45983, Training Acc : 0.461, Run Time : 15.40
INFO:root:2019-05-12 10:52:27, Epoch : 1, Step : 6802, Training Loss : 0.98426, Training Acc : 0.572, Run Time : 0.78
INFO:root:2019-05-12 10:52:27, Epoch : 1, Step : 6803, Training Loss : 0.84748, Training Acc : 0.622, Run Time : 0.46
INFO:root:2019-05-12 10:52:28, Epoch : 1, Step : 6804, Training Loss : 0.71963, Training Acc : 0.706, Run Time : 0.38
INFO:root:2019-05-12 10:52:38, Epoch : 1, Step : 6805, Training Loss : 0.48446, Training Acc : 0.761, Run Time : 10.46
INFO:root:2019-05-12 10:52:47, Epoch : 1, Step : 6806, Training Loss : 0.37119, Training Acc : 0.867, Run Time : 8.74
INFO:root:2019-05-12 10:52:48, Epoch : 1, Step : 6807, Training Loss : 0.36118, Training Acc : 0.856, Run Time : 0.50
INFO:root:2019-05-12 10:52:50, Epoch : 1, Step : 6808, Training Loss : 0.26031, Training Acc : 0.906, Run Time : 2.08
INFO:root:2019-05-12 10:53:03, Epoch : 1, Step : 6809, Training Loss : 0.21663, Training Acc : 0.944, Run Time : 13.33
INFO:root:2019-05-12 10:53:03, Epoch : 1, Step : 6810, Training Loss : 0.17269, Training Acc : 0.950, Run Time : 0.41
INFO:root:2019-05-12 10:53:05, Epoch : 1, Step : 6811, Training Loss : 0.20628, Training Acc : 0.922, Run Time : 1.30
INFO:root:2019-05-12 10:53:16, Epoch : 1, Step : 6812, Training Loss : 0.24524, Training Acc : 0.894, Run Time : 11.60
INFO:root:2019-05-12 10:53:30, Epoch : 1, Step : 6813, Training Loss : 0.22115, Training Acc : 0.911, Run Time : 13.61
INFO:root:2019-05-12 10:53:31, Epoch : 1, Step : 6814, Training Loss : 0.21037, Training Acc : 0.928, Run Time : 0.67
INFO:root:2019-05-12 10:53:32, Epoch : 1, Step : 6815, Training Loss : 0.16839, Training Acc : 0.928, Run Time : 1.01
INFO:root:2019-05-12 10:53:44, Epoch : 1, Step : 6816, Training Loss : 0.16487, Training Acc : 0.922, Run Time : 12.02
INFO:root:2019-05-12 10:53:44, Epoch : 1, Step : 6817, Training Loss : 0.23629, Training Acc : 0.917, Run Time : 0.56
INFO:root:2019-05-12 10:53:45, Epoch : 1, Step : 6818, Training Loss : 0.15859, Training Acc : 0.933, Run Time : 1.01
INFO:root:2019-05-12 10:53:59, Epoch : 1, Step : 6819, Training Loss : 0.18351, Training Acc : 0.922, Run Time : 13.89
INFO:root:2019-05-12 10:54:00, Epoch : 1, Step : 6820, Training Loss : 0.13125, Training Acc : 0.944, Run Time : 1.13
INFO:root:2019-05-12 10:54:15, Epoch : 1, Step : 6821, Training Loss : 0.17202, Training Acc : 0.939, Run Time : 14.67
INFO:root:2019-05-12 10:54:15, Epoch : 1, Step : 6822, Training Loss : 0.16157, Training Acc : 0.922, Run Time : 0.45
INFO:root:2019-05-12 10:54:16, Epoch : 1, Step : 6823, Training Loss : 0.19891, Training Acc : 0.922, Run Time : 0.94
INFO:root:2019-05-12 10:54:17, Epoch : 1, Step : 6824, Training Loss : 0.15288, Training Acc : 0.944, Run Time : 1.24
INFO:root:2019-05-12 10:54:30, Epoch : 1, Step : 6825, Training Loss : 0.14289, Training Acc : 0.928, Run Time : 12.13
INFO:root:2019-05-12 10:54:30, Epoch : 1, Step : 6826, Training Loss : 0.15331, Training Acc : 0.933, Run Time : 0.48
INFO:root:2019-05-12 10:54:31, Epoch : 1, Step : 6827, Training Loss : 0.12590, Training Acc : 0.933, Run Time : 0.57
INFO:root:2019-05-12 10:54:32, Epoch : 1, Step : 6828, Training Loss : 0.14574, Training Acc : 0.928, Run Time : 1.40
INFO:root:2019-05-12 10:54:43, Epoch : 1, Step : 6829, Training Loss : 0.14129, Training Acc : 0.944, Run Time : 10.86
INFO:root:2019-05-12 10:54:44, Epoch : 1, Step : 6830, Training Loss : 0.12261, Training Acc : 0.944, Run Time : 1.28
INFO:root:2019-05-12 10:54:56, Epoch : 1, Step : 6831, Training Loss : 0.15512, Training Acc : 0.939, Run Time : 11.98
INFO:root:2019-05-12 10:54:57, Epoch : 1, Step : 6832, Training Loss : 0.12468, Training Acc : 0.950, Run Time : 0.80
INFO:root:2019-05-12 10:55:07, Epoch : 1, Step : 6833, Training Loss : 0.17048, Training Acc : 0.922, Run Time : 9.71
INFO:root:2019-05-12 10:55:11, Epoch : 1, Step : 6834, Training Loss : 0.29343, Training Acc : 0.861, Run Time : 4.71
INFO:root:2019-05-12 10:55:12, Epoch : 1, Step : 6835, Training Loss : 0.17642, Training Acc : 0.922, Run Time : 0.64
INFO:root:2019-05-12 10:55:29, Epoch : 1, Step : 6836, Training Loss : 0.13559, Training Acc : 0.939, Run Time : 16.66
INFO:root:2019-05-12 10:55:36, Epoch : 1, Step : 6837, Training Loss : 0.10178, Training Acc : 0.956, Run Time : 7.71
INFO:root:2019-05-12 10:55:37, Epoch : 1, Step : 6838, Training Loss : 0.12416, Training Acc : 0.939, Run Time : 1.02
INFO:root:2019-05-12 10:55:39, Epoch : 1, Step : 6839, Training Loss : 0.12940, Training Acc : 0.939, Run Time : 1.36
INFO:root:2019-05-12 10:55:54, Epoch : 1, Step : 6840, Training Loss : 0.10477, Training Acc : 0.967, Run Time : 15.34
INFO:root:2019-05-12 10:56:02, Epoch : 1, Step : 6841, Training Loss : 0.11116, Training Acc : 0.956, Run Time : 8.19
INFO:root:2019-05-12 10:56:09, Epoch : 1, Step : 6842, Training Loss : 0.11190, Training Acc : 0.950, Run Time : 6.65
INFO:root:2019-05-12 10:56:10, Epoch : 1, Step : 6843, Training Loss : 0.12202, Training Acc : 0.956, Run Time : 0.98
INFO:root:2019-05-12 10:56:22, Epoch : 1, Step : 6844, Training Loss : 0.11232, Training Acc : 0.961, Run Time : 12.13
INFO:root:2019-05-12 10:56:24, Epoch : 1, Step : 6845, Training Loss : 0.10726, Training Acc : 0.972, Run Time : 1.56
INFO:root:2019-05-12 10:56:26, Epoch : 1, Step : 6846, Training Loss : 0.08411, Training Acc : 0.972, Run Time : 2.20
INFO:root:2019-05-12 10:56:40, Epoch : 1, Step : 6847, Training Loss : 0.11408, Training Acc : 0.956, Run Time : 14.39
INFO:root:2019-05-12 10:56:41, Epoch : 1, Step : 6848, Training Loss : 0.09819, Training Acc : 0.967, Run Time : 0.84
INFO:root:2019-05-12 10:56:42, Epoch : 1, Step : 6849, Training Loss : 0.12150, Training Acc : 0.956, Run Time : 0.67
INFO:root:2019-05-12 10:56:44, Epoch : 1, Step : 6850, Training Loss : 0.10435, Training Acc : 0.961, Run Time : 2.10
INFO:root:2019-05-12 10:56:55, Epoch : 1, Step : 6851, Training Loss : 0.11817, Training Acc : 0.972, Run Time : 11.10
INFO:root:2019-05-12 10:56:56, Epoch : 1, Step : 6852, Training Loss : 0.12438, Training Acc : 0.950, Run Time : 0.91
INFO:root:2019-05-12 10:56:57, Epoch : 1, Step : 6853, Training Loss : 0.10875, Training Acc : 0.978, Run Time : 1.66
INFO:root:2019-05-12 10:57:10, Epoch : 1, Step : 6854, Training Loss : 0.10954, Training Acc : 0.983, Run Time : 12.07
INFO:root:2019-05-12 10:57:10, Epoch : 1, Step : 6855, Training Loss : 0.14078, Training Acc : 0.928, Run Time : 0.57
INFO:root:2019-05-12 10:57:11, Epoch : 1, Step : 6856, Training Loss : 0.10258, Training Acc : 0.978, Run Time : 0.57
INFO:root:2019-05-12 10:57:22, Epoch : 1, Step : 6857, Training Loss : 0.12269, Training Acc : 0.950, Run Time : 10.95
INFO:root:2019-05-12 10:57:23, Epoch : 1, Step : 6858, Training Loss : 0.11096, Training Acc : 0.967, Run Time : 1.37
INFO:root:2019-05-12 10:57:36, Epoch : 1, Step : 6859, Training Loss : 0.08727, Training Acc : 0.967, Run Time : 13.08
INFO:root:2019-05-12 10:57:37, Epoch : 1, Step : 6860, Training Loss : 0.09736, Training Acc : 0.972, Run Time : 0.75
INFO:root:2019-05-12 10:57:38, Epoch : 1, Step : 6861, Training Loss : 0.10977, Training Acc : 0.956, Run Time : 1.04
INFO:root:2019-05-12 10:57:52, Epoch : 1, Step : 6862, Training Loss : 0.10090, Training Acc : 0.961, Run Time : 13.73
INFO:root:2019-05-12 10:58:00, Epoch : 1, Step : 6863, Training Loss : 0.08545, Training Acc : 0.978, Run Time : 8.45
INFO:root:2019-05-12 10:58:02, Epoch : 1, Step : 6864, Training Loss : 0.11184, Training Acc : 0.978, Run Time : 1.95
INFO:root:2019-05-12 10:58:15, Epoch : 1, Step : 6865, Training Loss : 0.11031, Training Acc : 0.950, Run Time : 13.49
INFO:root:2019-05-12 10:58:16, Epoch : 1, Step : 6866, Training Loss : 0.10352, Training Acc : 0.950, Run Time : 0.89
INFO:root:2019-05-12 10:58:18, Epoch : 1, Step : 6867, Training Loss : 0.11645, Training Acc : 0.950, Run Time : 1.34
INFO:root:2019-05-12 10:58:31, Epoch : 1, Step : 6868, Training Loss : 0.10419, Training Acc : 0.956, Run Time : 13.25
INFO:root:2019-05-12 10:58:32, Epoch : 1, Step : 6869, Training Loss : 0.10368, Training Acc : 0.956, Run Time : 1.21
INFO:root:2019-05-12 10:58:48, Epoch : 1, Step : 6870, Training Loss : 0.12252, Training Acc : 0.944, Run Time : 15.64
INFO:root:2019-05-12 10:58:49, Epoch : 1, Step : 6871, Training Loss : 0.10809, Training Acc : 0.950, Run Time : 1.14
INFO:root:2019-05-12 10:59:02, Epoch : 1, Step : 6872, Training Loss : 0.12640, Training Acc : 0.944, Run Time : 12.86
INFO:root:2019-05-12 10:59:19, Epoch : 1, Step : 6873, Training Loss : 0.11708, Training Acc : 0.933, Run Time : 17.61
INFO:root:2019-05-12 10:59:20, Epoch : 1, Step : 6874, Training Loss : 0.11457, Training Acc : 0.939, Run Time : 0.95
INFO:root:2019-05-12 10:59:21, Epoch : 1, Step : 6875, Training Loss : 0.11956, Training Acc : 0.939, Run Time : 0.75
INFO:root:2019-05-12 10:59:22, Epoch : 1, Step : 6876, Training Loss : 0.13318, Training Acc : 0.933, Run Time : 0.87
INFO:root:2019-05-12 10:59:36, Epoch : 1, Step : 6877, Training Loss : 0.07824, Training Acc : 0.972, Run Time : 13.68
INFO:root:2019-05-12 10:59:36, Epoch : 1, Step : 6878, Training Loss : 0.10411, Training Acc : 0.961, Run Time : 0.49
INFO:root:2019-05-12 10:59:37, Epoch : 1, Step : 6879, Training Loss : 0.09693, Training Acc : 0.950, Run Time : 0.62
INFO:root:2019-05-12 10:59:49, Epoch : 1, Step : 6880, Training Loss : 0.10045, Training Acc : 0.978, Run Time : 12.23
INFO:root:2019-05-12 10:59:50, Epoch : 1, Step : 6881, Training Loss : 0.10962, Training Acc : 0.944, Run Time : 1.02
INFO:root:2019-05-12 11:00:04, Epoch : 1, Step : 6882, Training Loss : 0.09640, Training Acc : 0.944, Run Time : 14.45
INFO:root:2019-05-12 11:00:11, Epoch : 1, Step : 6883, Training Loss : 0.10761, Training Acc : 0.939, Run Time : 7.04
INFO:root:2019-05-12 11:00:12, Epoch : 1, Step : 6884, Training Loss : 0.08236, Training Acc : 0.972, Run Time : 0.78
INFO:root:2019-05-12 11:00:13, Epoch : 1, Step : 6885, Training Loss : 0.09327, Training Acc : 0.967, Run Time : 0.55
INFO:root:2019-05-12 11:00:14, Epoch : 1, Step : 6886, Training Loss : 0.09204, Training Acc : 0.967, Run Time : 0.82
INFO:root:2019-05-12 11:00:27, Epoch : 1, Step : 6887, Training Loss : 0.08577, Training Acc : 0.961, Run Time : 13.57
INFO:root:2019-05-12 11:00:37, Epoch : 1, Step : 6888, Training Loss : 0.09264, Training Acc : 0.950, Run Time : 9.65
INFO:root:2019-05-12 11:00:39, Epoch : 1, Step : 6889, Training Loss : 0.09964, Training Acc : 0.961, Run Time : 1.71
INFO:root:2019-05-12 11:00:44, Epoch : 1, Step : 6890, Training Loss : 0.12743, Training Acc : 0.961, Run Time : 4.93
INFO:root:2019-05-12 11:00:58, Epoch : 1, Step : 6891, Training Loss : 0.27891, Training Acc : 0.933, Run Time : 14.67
INFO:root:2019-05-12 11:01:01, Epoch : 1, Step : 6892, Training Loss : 0.35008, Training Acc : 0.917, Run Time : 3.27
INFO:root:2019-05-12 11:01:02, Epoch : 1, Step : 6893, Training Loss : 0.20848, Training Acc : 0.933, Run Time : 0.72
INFO:root:2019-05-12 11:01:09, Epoch : 1, Step : 6894, Training Loss : 0.13291, Training Acc : 0.950, Run Time : 7.31
INFO:root:2019-05-12 11:01:11, Epoch : 1, Step : 6895, Training Loss : 0.27291, Training Acc : 0.900, Run Time : 1.44
INFO:root:2019-05-12 11:01:22, Epoch : 1, Step : 6896, Training Loss : 0.20577, Training Acc : 0.911, Run Time : 11.59
INFO:root:2019-05-12 11:01:23, Epoch : 1, Step : 6897, Training Loss : 0.23452, Training Acc : 0.894, Run Time : 0.51
INFO:root:2019-05-12 11:01:24, Epoch : 1, Step : 6898, Training Loss : 0.25527, Training Acc : 0.900, Run Time : 1.02
INFO:root:2019-05-12 11:01:37, Epoch : 1, Step : 6899, Training Loss : 0.14277, Training Acc : 0.928, Run Time : 13.11
INFO:root:2019-05-12 11:01:38, Epoch : 1, Step : 6900, Training Loss : 0.16300, Training Acc : 0.933, Run Time : 1.04
INFO:root:2019-05-12 11:01:57, Epoch : 1, Step : 6901, Training Loss : 0.14125, Training Acc : 0.928, Run Time : 18.47
INFO:root:2019-05-12 11:01:58, Epoch : 1, Step : 6902, Training Loss : 0.10664, Training Acc : 0.956, Run Time : 1.45
INFO:root:2019-05-12 11:02:14, Epoch : 1, Step : 6903, Training Loss : 0.12777, Training Acc : 0.944, Run Time : 15.43
INFO:root:2019-05-12 11:02:15, Epoch : 1, Step : 6904, Training Loss : 0.12855, Training Acc : 0.933, Run Time : 1.08
INFO:root:2019-05-12 11:02:28, Epoch : 1, Step : 6905, Training Loss : 0.18098, Training Acc : 0.917, Run Time : 12.96
INFO:root:2019-05-12 11:02:28, Epoch : 1, Step : 6906, Training Loss : 0.53620, Training Acc : 0.867, Run Time : 0.69
INFO:root:2019-05-12 11:02:29, Epoch : 1, Step : 6907, Training Loss : 0.63020, Training Acc : 0.811, Run Time : 0.54
INFO:root:2019-05-12 11:02:50, Epoch : 1, Step : 6908, Training Loss : 0.56251, Training Acc : 0.794, Run Time : 20.74
INFO:root:2019-05-12 11:02:51, Epoch : 1, Step : 6909, Training Loss : 0.58128, Training Acc : 0.839, Run Time : 1.52
INFO:root:2019-05-12 11:02:52, Epoch : 1, Step : 6910, Training Loss : 0.59007, Training Acc : 0.817, Run Time : 0.59
INFO:root:2019-05-12 11:02:53, Epoch : 1, Step : 6911, Training Loss : 0.40653, Training Acc : 0.850, Run Time : 1.50
INFO:root:2019-05-12 11:03:08, Epoch : 1, Step : 6912, Training Loss : 0.36414, Training Acc : 0.867, Run Time : 14.49
INFO:root:2019-05-12 11:03:09, Epoch : 1, Step : 6913, Training Loss : 0.46014, Training Acc : 0.789, Run Time : 1.29
INFO:root:2019-05-12 11:03:20, Epoch : 1, Step : 6914, Training Loss : 0.45071, Training Acc : 0.789, Run Time : 11.46
INFO:root:2019-05-12 11:03:21, Epoch : 1, Step : 6915, Training Loss : 0.32984, Training Acc : 0.850, Run Time : 0.87
INFO:root:2019-05-12 11:03:22, Epoch : 1, Step : 6916, Training Loss : 0.35549, Training Acc : 0.850, Run Time : 0.56
INFO:root:2019-05-12 11:03:31, Epoch : 1, Step : 6917, Training Loss : 0.25799, Training Acc : 0.900, Run Time : 9.01
INFO:root:2019-05-12 11:03:32, Epoch : 1, Step : 6918, Training Loss : 0.30688, Training Acc : 0.872, Run Time : 1.01
INFO:root:2019-05-12 11:03:32, Epoch : 1, Step : 6919, Training Loss : 0.33801, Training Acc : 0.883, Run Time : 0.63
INFO:root:2019-05-12 11:03:42, Epoch : 1, Step : 6920, Training Loss : 0.30215, Training Acc : 0.867, Run Time : 9.69
INFO:root:2019-05-12 11:03:43, Epoch : 1, Step : 6921, Training Loss : 0.44478, Training Acc : 0.767, Run Time : 0.74
INFO:root:2019-05-12 11:03:44, Epoch : 1, Step : 6922, Training Loss : 0.44812, Training Acc : 0.867, Run Time : 1.03
INFO:root:2019-05-12 11:03:55, Epoch : 1, Step : 6923, Training Loss : 0.46067, Training Acc : 0.800, Run Time : 11.23
INFO:root:2019-05-12 11:03:56, Epoch : 1, Step : 6924, Training Loss : 0.40664, Training Acc : 0.794, Run Time : 1.09
INFO:root:2019-05-12 11:04:17, Epoch : 1, Step : 6925, Training Loss : 0.39628, Training Acc : 0.839, Run Time : 21.05
INFO:root:2019-05-12 11:04:28, Epoch : 1, Step : 6926, Training Loss : 0.38116, Training Acc : 0.828, Run Time : 11.11
INFO:root:2019-05-12 11:04:29, Epoch : 1, Step : 6927, Training Loss : 0.34676, Training Acc : 0.806, Run Time : 0.41
INFO:root:2019-05-12 11:04:29, Epoch : 1, Step : 6928, Training Loss : 0.30280, Training Acc : 0.844, Run Time : 0.45
INFO:root:2019-05-12 11:04:57, Epoch : 1, Step : 6929, Training Loss : 0.33745, Training Acc : 0.844, Run Time : 27.72
INFO:root:2019-05-12 11:05:01, Epoch : 1, Step : 6930, Training Loss : 0.22856, Training Acc : 0.906, Run Time : 3.81
INFO:root:2019-05-12 11:05:13, Epoch : 1, Step : 6931, Training Loss : 0.28141, Training Acc : 0.883, Run Time : 12.62
INFO:root:2019-05-12 11:05:17, Epoch : 1, Step : 6932, Training Loss : 0.30480, Training Acc : 0.867, Run Time : 3.16
INFO:root:2019-05-12 11:05:17, Epoch : 1, Step : 6933, Training Loss : 0.25273, Training Acc : 0.883, Run Time : 0.90
INFO:root:2019-05-12 11:05:24, Epoch : 1, Step : 6934, Training Loss : 0.46142, Training Acc : 0.794, Run Time : 6.88
INFO:root:2019-05-12 11:05:25, Epoch : 1, Step : 6935, Training Loss : 0.41251, Training Acc : 0.822, Run Time : 0.42
INFO:root:2019-05-12 11:05:26, Epoch : 1, Step : 6936, Training Loss : 0.30358, Training Acc : 0.856, Run Time : 1.55
INFO:root:2019-05-12 11:05:40, Epoch : 1, Step : 6937, Training Loss : 0.40430, Training Acc : 0.789, Run Time : 13.81
INFO:root:2019-05-12 11:05:41, Epoch : 1, Step : 6938, Training Loss : 0.34752, Training Acc : 0.839, Run Time : 0.41
INFO:root:2019-05-12 11:05:41, Epoch : 1, Step : 6939, Training Loss : 0.25052, Training Acc : 0.906, Run Time : 0.64
INFO:root:2019-05-12 11:05:42, Epoch : 1, Step : 6940, Training Loss : 0.43215, Training Acc : 0.756, Run Time : 0.59
INFO:root:2019-05-12 11:05:43, Epoch : 1, Step : 6941, Training Loss : 0.28884, Training Acc : 0.850, Run Time : 0.80
INFO:root:2019-05-12 11:06:01, Epoch : 1, Step : 6942, Training Loss : 0.35928, Training Acc : 0.856, Run Time : 18.28
INFO:root:2019-05-12 11:06:02, Epoch : 1, Step : 6943, Training Loss : 0.33868, Training Acc : 0.856, Run Time : 0.64
INFO:root:2019-05-12 11:06:03, Epoch : 1, Step : 6944, Training Loss : 0.26424, Training Acc : 0.850, Run Time : 1.43
INFO:root:2019-05-12 11:06:14, Epoch : 1, Step : 6945, Training Loss : 0.31553, Training Acc : 0.850, Run Time : 11.34
INFO:root:2019-05-12 11:06:15, Epoch : 1, Step : 6946, Training Loss : 0.43828, Training Acc : 0.744, Run Time : 0.86
INFO:root:2019-05-12 11:06:26, Epoch : 1, Step : 6947, Training Loss : 0.36885, Training Acc : 0.800, Run Time : 11.34
INFO:root:2019-05-12 11:06:27, Epoch : 1, Step : 6948, Training Loss : 0.32333, Training Acc : 0.833, Run Time : 0.53
INFO:root:2019-05-12 11:06:28, Epoch : 1, Step : 6949, Training Loss : 0.39308, Training Acc : 0.778, Run Time : 0.61
INFO:root:2019-05-12 11:06:41, Epoch : 1, Step : 6950, Training Loss : 0.41146, Training Acc : 0.800, Run Time : 12.93
INFO:root:2019-05-12 11:06:41, Epoch : 1, Step : 6951, Training Loss : 0.33149, Training Acc : 0.822, Run Time : 0.43
INFO:root:2019-05-12 11:06:42, Epoch : 1, Step : 6952, Training Loss : 0.26696, Training Acc : 0.872, Run Time : 0.60
INFO:root:2019-05-12 11:06:57, Epoch : 1, Step : 6953, Training Loss : 0.34322, Training Acc : 0.833, Run Time : 15.60
INFO:root:2019-05-12 11:06:59, Epoch : 1, Step : 6954, Training Loss : 0.36938, Training Acc : 0.800, Run Time : 1.53
INFO:root:2019-05-12 11:07:30, Epoch : 1, Step : 6955, Training Loss : 0.28614, Training Acc : 0.872, Run Time : 30.94
INFO:root:2019-05-12 11:07:35, Epoch : 1, Step : 6956, Training Loss : 0.31301, Training Acc : 0.822, Run Time : 5.12
INFO:root:2019-05-12 11:07:35, Epoch : 1, Step : 6957, Training Loss : 0.37278, Training Acc : 0.828, Run Time : 0.68
INFO:root:2019-05-12 11:07:45, Epoch : 1, Step : 6958, Training Loss : 0.32805, Training Acc : 0.850, Run Time : 9.94
INFO:root:2019-05-12 11:07:46, Epoch : 1, Step : 6959, Training Loss : 0.38123, Training Acc : 0.778, Run Time : 0.84
INFO:root:2019-05-12 11:07:48, Epoch : 1, Step : 6960, Training Loss : 0.38997, Training Acc : 0.794, Run Time : 1.44
INFO:root:2019-05-12 11:08:03, Epoch : 1, Step : 6961, Training Loss : 0.24925, Training Acc : 0.906, Run Time : 15.00
INFO:root:2019-05-12 11:08:04, Epoch : 1, Step : 6962, Training Loss : 0.31641, Training Acc : 0.833, Run Time : 1.66
INFO:root:2019-05-12 11:08:18, Epoch : 1, Step : 6963, Training Loss : 0.43613, Training Acc : 0.794, Run Time : 13.91
INFO:root:2019-05-12 11:08:26, Epoch : 1, Step : 6964, Training Loss : 0.33994, Training Acc : 0.828, Run Time : 8.24
INFO:root:2019-05-12 11:08:27, Epoch : 1, Step : 6965, Training Loss : 0.23118, Training Acc : 0.883, Run Time : 0.66
INFO:root:2019-05-12 11:08:28, Epoch : 1, Step : 6966, Training Loss : 0.41602, Training Acc : 0.761, Run Time : 0.63
INFO:root:2019-05-12 11:08:44, Epoch : 1, Step : 6967, Training Loss : 0.40747, Training Acc : 0.817, Run Time : 16.21
INFO:root:2019-05-12 11:08:53, Epoch : 1, Step : 6968, Training Loss : 0.32076, Training Acc : 0.850, Run Time : 9.34
INFO:root:2019-05-12 11:09:00, Epoch : 1, Step : 6969, Training Loss : 0.49559, Training Acc : 0.722, Run Time : 6.78
INFO:root:2019-05-12 11:09:03, Epoch : 1, Step : 6970, Training Loss : 0.27597, Training Acc : 0.889, Run Time : 3.36
INFO:root:2019-05-12 11:09:04, Epoch : 1, Step : 6971, Training Loss : 0.45025, Training Acc : 0.739, Run Time : 0.68
INFO:root:2019-05-12 11:09:14, Epoch : 1, Step : 6972, Training Loss : 0.28634, Training Acc : 0.867, Run Time : 10.02
INFO:root:2019-05-12 11:09:15, Epoch : 1, Step : 6973, Training Loss : 0.28456, Training Acc : 0.839, Run Time : 0.86
INFO:root:2019-05-12 11:09:24, Epoch : 1, Step : 6974, Training Loss : 0.48140, Training Acc : 0.733, Run Time : 9.08
INFO:root:2019-05-12 11:09:25, Epoch : 1, Step : 6975, Training Loss : 0.42949, Training Acc : 0.794, Run Time : 0.76
INFO:root:2019-05-12 11:09:33, Epoch : 1, Step : 6976, Training Loss : 0.31574, Training Acc : 0.844, Run Time : 7.73
INFO:root:2019-05-12 11:09:34, Epoch : 1, Step : 6977, Training Loss : 0.34481, Training Acc : 0.811, Run Time : 1.78
INFO:root:2019-05-12 11:09:35, Epoch : 1, Step : 6978, Training Loss : 0.34716, Training Acc : 0.817, Run Time : 0.65
INFO:root:2019-05-12 11:09:43, Epoch : 1, Step : 6979, Training Loss : 0.47220, Training Acc : 0.722, Run Time : 8.21
INFO:root:2019-05-12 11:09:44, Epoch : 1, Step : 6980, Training Loss : 0.23324, Training Acc : 0.894, Run Time : 0.52
INFO:root:2019-05-12 11:09:45, Epoch : 1, Step : 6981, Training Loss : 0.40138, Training Acc : 0.767, Run Time : 1.26
INFO:root:2019-05-12 11:09:57, Epoch : 1, Step : 6982, Training Loss : 0.25393, Training Acc : 0.878, Run Time : 11.91
INFO:root:2019-05-12 11:09:58, Epoch : 1, Step : 6983, Training Loss : 0.38725, Training Acc : 0.778, Run Time : 0.94
INFO:root:2019-05-12 11:10:07, Epoch : 1, Step : 6984, Training Loss : 0.29167, Training Acc : 0.878, Run Time : 8.76
INFO:root:2019-05-12 11:10:07, Epoch : 1, Step : 6985, Training Loss : 0.29584, Training Acc : 0.867, Run Time : 0.53
INFO:root:2019-05-12 11:10:08, Epoch : 1, Step : 6986, Training Loss : 0.25498, Training Acc : 0.872, Run Time : 0.70
INFO:root:2019-05-12 11:10:21, Epoch : 1, Step : 6987, Training Loss : 0.28446, Training Acc : 0.894, Run Time : 12.97
INFO:root:2019-05-12 11:10:22, Epoch : 1, Step : 6988, Training Loss : 0.25739, Training Acc : 0.883, Run Time : 0.78
INFO:root:2019-05-12 11:10:33, Epoch : 1, Step : 6989, Training Loss : 0.38332, Training Acc : 0.772, Run Time : 11.13
INFO:root:2019-05-12 11:10:34, Epoch : 1, Step : 6990, Training Loss : 0.29866, Training Acc : 0.828, Run Time : 1.31
INFO:root:2019-05-12 11:10:45, Epoch : 1, Step : 6991, Training Loss : 0.24097, Training Acc : 0.894, Run Time : 10.57
INFO:root:2019-05-12 11:10:46, Epoch : 1, Step : 6992, Training Loss : 0.21706, Training Acc : 0.911, Run Time : 0.91
INFO:root:2019-05-12 11:10:46, Epoch : 1, Step : 6993, Training Loss : 0.30186, Training Acc : 0.867, Run Time : 0.56
INFO:root:2019-05-12 11:10:58, Epoch : 1, Step : 6994, Training Loss : 0.23423, Training Acc : 0.906, Run Time : 11.57
INFO:root:2019-05-12 11:10:58, Epoch : 1, Step : 6995, Training Loss : 0.25435, Training Acc : 0.889, Run Time : 0.76
INFO:root:2019-05-12 11:11:07, Epoch : 1, Step : 6996, Training Loss : 0.24339, Training Acc : 0.911, Run Time : 8.94
INFO:root:2019-05-12 11:11:09, Epoch : 1, Step : 6997, Training Loss : 0.32000, Training Acc : 0.833, Run Time : 1.55
INFO:root:2019-05-12 11:11:10, Epoch : 1, Step : 6998, Training Loss : 0.27396, Training Acc : 0.872, Run Time : 0.84
INFO:root:2019-05-12 11:11:19, Epoch : 1, Step : 6999, Training Loss : 0.39340, Training Acc : 0.811, Run Time : 8.96
INFO:root:2019-05-12 11:11:20, Epoch : 1, Step : 7000, Training Loss : 0.34673, Training Acc : 0.844, Run Time : 0.84
INFO:root:2019-05-12 11:11:28, Epoch : 1, Step : 7001, Training Loss : 0.94688, Training Acc : 0.650, Run Time : 8.58
INFO:root:2019-05-12 11:11:29, Epoch : 1, Step : 7002, Training Loss : 0.95353, Training Acc : 0.717, Run Time : 0.51
INFO:root:2019-05-12 11:11:30, Epoch : 1, Step : 7003, Training Loss : 0.76881, Training Acc : 0.667, Run Time : 0.94
INFO:root:2019-05-12 11:11:42, Epoch : 1, Step : 7004, Training Loss : 0.90013, Training Acc : 0.628, Run Time : 12.30
INFO:root:2019-05-12 11:11:43, Epoch : 1, Step : 7005, Training Loss : 0.54795, Training Acc : 0.744, Run Time : 0.99
INFO:root:2019-05-12 11:11:52, Epoch : 1, Step : 7006, Training Loss : 0.39090, Training Acc : 0.806, Run Time : 9.48
INFO:root:2019-05-12 11:11:53, Epoch : 1, Step : 7007, Training Loss : 0.36367, Training Acc : 0.850, Run Time : 0.48
INFO:root:2019-05-12 11:11:53, Epoch : 1, Step : 7008, Training Loss : 0.34722, Training Acc : 0.844, Run Time : 0.58
INFO:root:2019-05-12 11:11:54, Epoch : 1, Step : 7009, Training Loss : 0.50760, Training Acc : 0.733, Run Time : 0.63
INFO:root:2019-05-12 11:11:55, Epoch : 1, Step : 7010, Training Loss : 0.36029, Training Acc : 0.817, Run Time : 0.60
INFO:root:2019-05-12 11:12:09, Epoch : 1, Step : 7011, Training Loss : 0.29051, Training Acc : 0.889, Run Time : 14.10
INFO:root:2019-05-12 11:12:09, Epoch : 1, Step : 7012, Training Loss : 0.34761, Training Acc : 0.861, Run Time : 0.54
INFO:root:2019-05-12 11:12:10, Epoch : 1, Step : 7013, Training Loss : 0.51484, Training Acc : 0.806, Run Time : 0.63
INFO:root:2019-05-12 11:12:11, Epoch : 1, Step : 7014, Training Loss : 0.31537, Training Acc : 0.878, Run Time : 0.63
INFO:root:2019-05-12 11:12:11, Epoch : 1, Step : 7015, Training Loss : 0.46894, Training Acc : 0.833, Run Time : 0.62
INFO:root:2019-05-12 11:12:25, Epoch : 1, Step : 7016, Training Loss : 0.41979, Training Acc : 0.833, Run Time : 13.97
INFO:root:2019-05-12 11:12:27, Epoch : 1, Step : 7017, Training Loss : 0.47459, Training Acc : 0.811, Run Time : 1.43
INFO:root:2019-05-12 11:12:39, Epoch : 1, Step : 7018, Training Loss : 0.54221, Training Acc : 0.789, Run Time : 12.17
INFO:root:2019-05-12 11:12:40, Epoch : 1, Step : 7019, Training Loss : 0.45133, Training Acc : 0.794, Run Time : 1.62
INFO:root:2019-05-12 11:12:50, Epoch : 1, Step : 7020, Training Loss : 0.36768, Training Acc : 0.856, Run Time : 9.16
INFO:root:2019-05-12 11:12:51, Epoch : 1, Step : 7021, Training Loss : 0.41857, Training Acc : 0.783, Run Time : 1.09
INFO:root:2019-05-12 11:12:51, Epoch : 1, Step : 7022, Training Loss : 0.41146, Training Acc : 0.778, Run Time : 0.62
INFO:root:2019-05-12 11:12:53, Epoch : 1, Step : 7023, Training Loss : 0.32900, Training Acc : 0.850, Run Time : 1.55
INFO:root:2019-05-12 11:13:04, Epoch : 1, Step : 7024, Training Loss : 0.34008, Training Acc : 0.844, Run Time : 11.51
INFO:root:2019-05-12 11:13:05, Epoch : 1, Step : 7025, Training Loss : 0.49988, Training Acc : 0.728, Run Time : 1.14
INFO:root:2019-05-12 11:13:07, Epoch : 1, Step : 7026, Training Loss : 0.36495, Training Acc : 0.844, Run Time : 1.60
INFO:root:2019-05-12 11:13:17, Epoch : 1, Step : 7027, Training Loss : 0.50593, Training Acc : 0.767, Run Time : 9.75
INFO:root:2019-05-12 11:13:27, Epoch : 1, Step : 7028, Training Loss : 0.33440, Training Acc : 0.872, Run Time : 10.67
INFO:root:2019-05-12 11:13:29, Epoch : 1, Step : 7029, Training Loss : 0.34509, Training Acc : 0.833, Run Time : 1.15
INFO:root:2019-05-12 11:13:30, Epoch : 1, Step : 7030, Training Loss : 0.30062, Training Acc : 0.861, Run Time : 1.68
INFO:root:2019-05-12 11:13:42, Epoch : 1, Step : 7031, Training Loss : 0.52392, Training Acc : 0.750, Run Time : 11.65
INFO:root:2019-05-12 11:13:42, Epoch : 1, Step : 7032, Training Loss : 0.57187, Training Acc : 0.767, Run Time : 0.49
INFO:root:2019-05-12 11:13:43, Epoch : 1, Step : 7033, Training Loss : 0.47986, Training Acc : 0.767, Run Time : 0.63
INFO:root:2019-05-12 11:14:02, Epoch : 1, Step : 7034, Training Loss : 0.42620, Training Acc : 0.778, Run Time : 18.90
INFO:root:2019-05-12 11:14:04, Epoch : 1, Step : 7035, Training Loss : 0.33045, Training Acc : 0.822, Run Time : 1.93
INFO:root:2019-05-12 11:14:06, Epoch : 1, Step : 7036, Training Loss : 0.55941, Training Acc : 0.717, Run Time : 1.82
INFO:root:2019-05-12 11:14:19, Epoch : 1, Step : 7037, Training Loss : 0.36221, Training Acc : 0.822, Run Time : 13.24
INFO:root:2019-05-12 11:14:19, Epoch : 1, Step : 7038, Training Loss : 0.45752, Training Acc : 0.772, Run Time : 0.53
INFO:root:2019-05-12 11:14:21, Epoch : 1, Step : 7039, Training Loss : 0.40957, Training Acc : 0.839, Run Time : 1.40
INFO:root:2019-05-12 11:14:22, Epoch : 1, Step : 7040, Training Loss : 0.21178, Training Acc : 0.922, Run Time : 0.70
INFO:root:2019-05-12 11:14:34, Epoch : 1, Step : 7041, Training Loss : 0.34238, Training Acc : 0.817, Run Time : 12.22
INFO:root:2019-05-12 11:14:35, Epoch : 1, Step : 7042, Training Loss : 0.14884, Training Acc : 0.950, Run Time : 1.53
INFO:root:2019-05-12 11:14:37, Epoch : 1, Step : 7043, Training Loss : 0.30444, Training Acc : 0.867, Run Time : 1.29
INFO:root:2019-05-12 11:14:47, Epoch : 1, Step : 7044, Training Loss : 0.31855, Training Acc : 0.883, Run Time : 10.45
INFO:root:2019-05-12 11:14:48, Epoch : 1, Step : 7045, Training Loss : 0.44214, Training Acc : 0.783, Run Time : 0.75
INFO:root:2019-05-12 11:14:49, Epoch : 1, Step : 7046, Training Loss : 0.43298, Training Acc : 0.772, Run Time : 1.49
INFO:root:2019-05-12 11:15:03, Epoch : 1, Step : 7047, Training Loss : 0.38515, Training Acc : 0.789, Run Time : 13.57
INFO:root:2019-05-12 11:15:04, Epoch : 1, Step : 7048, Training Loss : 0.34352, Training Acc : 0.856, Run Time : 0.69
INFO:root:2019-05-12 11:15:04, Epoch : 1, Step : 7049, Training Loss : 0.28885, Training Acc : 0.878, Run Time : 0.45
INFO:root:2019-05-12 11:15:05, Epoch : 1, Step : 7050, Training Loss : 0.23397, Training Acc : 0.889, Run Time : 1.30
INFO:root:2019-05-12 11:15:16, Epoch : 1, Step : 7051, Training Loss : 0.29889, Training Acc : 0.861, Run Time : 10.86
INFO:root:2019-05-12 11:15:17, Epoch : 1, Step : 7052, Training Loss : 0.23926, Training Acc : 0.889, Run Time : 0.62
INFO:root:2019-05-12 11:15:17, Epoch : 1, Step : 7053, Training Loss : 0.40922, Training Acc : 0.839, Run Time : 0.64
INFO:root:2019-05-12 11:15:20, Epoch : 1, Step : 7054, Training Loss : 0.30639, Training Acc : 0.900, Run Time : 2.25
INFO:root:2019-05-12 11:15:32, Epoch : 1, Step : 7055, Training Loss : 0.37662, Training Acc : 0.822, Run Time : 12.38
INFO:root:2019-05-12 11:15:34, Epoch : 1, Step : 7056, Training Loss : 0.52225, Training Acc : 0.767, Run Time : 1.78
INFO:root:2019-05-12 11:15:45, Epoch : 1, Step : 7057, Training Loss : 0.41977, Training Acc : 0.806, Run Time : 10.92
INFO:root:2019-05-12 11:15:53, Epoch : 1, Step : 7058, Training Loss : 0.18356, Training Acc : 0.956, Run Time : 8.08
INFO:root:2019-05-12 11:15:56, Epoch : 1, Step : 7059, Training Loss : 0.22811, Training Acc : 0.889, Run Time : 2.74
INFO:root:2019-05-12 11:15:57, Epoch : 1, Step : 7060, Training Loss : 0.30080, Training Acc : 0.900, Run Time : 0.99
INFO:root:2019-05-12 11:16:05, Epoch : 1, Step : 7061, Training Loss : 0.25593, Training Acc : 0.900, Run Time : 8.01
INFO:root:2019-05-12 11:16:05, Epoch : 1, Step : 7062, Training Loss : 0.26834, Training Acc : 0.889, Run Time : 0.68
INFO:root:2019-05-12 11:16:07, Epoch : 1, Step : 7063, Training Loss : 0.41998, Training Acc : 0.878, Run Time : 1.27
INFO:root:2019-05-12 11:16:16, Epoch : 1, Step : 7064, Training Loss : 0.28134, Training Acc : 0.894, Run Time : 9.61
INFO:root:2019-05-12 11:16:17, Epoch : 1, Step : 7065, Training Loss : 0.26776, Training Acc : 0.911, Run Time : 0.62
INFO:root:2019-05-12 11:16:17, Epoch : 1, Step : 7066, Training Loss : 0.17069, Training Acc : 0.944, Run Time : 0.56
INFO:root:2019-05-12 11:16:19, Epoch : 1, Step : 7067, Training Loss : 0.30435, Training Acc : 0.872, Run Time : 1.67
INFO:root:2019-05-12 11:16:29, Epoch : 1, Step : 7068, Training Loss : 0.26825, Training Acc : 0.928, Run Time : 10.36
INFO:root:2019-05-12 11:16:30, Epoch : 1, Step : 7069, Training Loss : 0.18866, Training Acc : 0.933, Run Time : 0.53
INFO:root:2019-05-12 11:16:32, Epoch : 1, Step : 7070, Training Loss : 0.10329, Training Acc : 0.978, Run Time : 2.38
INFO:root:2019-05-12 11:16:44, Epoch : 1, Step : 7071, Training Loss : 0.15474, Training Acc : 0.972, Run Time : 11.89
INFO:root:2019-05-12 11:16:59, Epoch : 1, Step : 7072, Training Loss : 0.13095, Training Acc : 0.967, Run Time : 14.57
INFO:root:2019-05-12 11:17:01, Epoch : 1, Step : 7073, Training Loss : 0.21748, Training Acc : 0.928, Run Time : 2.01
INFO:root:2019-05-12 11:17:12, Epoch : 1, Step : 7074, Training Loss : 0.19934, Training Acc : 0.939, Run Time : 11.07
INFO:root:2019-05-12 11:17:13, Epoch : 1, Step : 7075, Training Loss : 0.12537, Training Acc : 0.972, Run Time : 1.43
INFO:root:2019-05-12 11:17:24, Epoch : 1, Step : 7076, Training Loss : 0.15655, Training Acc : 0.956, Run Time : 10.55
INFO:root:2019-05-12 11:17:25, Epoch : 1, Step : 7077, Training Loss : 0.14456, Training Acc : 0.956, Run Time : 0.75
INFO:root:2019-05-12 11:17:25, Epoch : 1, Step : 7078, Training Loss : 0.17011, Training Acc : 0.939, Run Time : 0.59
INFO:root:2019-05-12 11:17:37, Epoch : 1, Step : 7079, Training Loss : 0.22349, Training Acc : 0.889, Run Time : 11.90
INFO:root:2019-05-12 11:17:38, Epoch : 1, Step : 7080, Training Loss : 0.23950, Training Acc : 0.911, Run Time : 0.49
INFO:root:2019-05-12 11:17:40, Epoch : 1, Step : 7081, Training Loss : 0.26252, Training Acc : 0.900, Run Time : 2.12
INFO:root:2019-05-12 11:17:52, Epoch : 1, Step : 7082, Training Loss : 0.14523, Training Acc : 0.950, Run Time : 12.54
INFO:root:2019-05-12 11:17:53, Epoch : 1, Step : 7083, Training Loss : 0.16924, Training Acc : 0.933, Run Time : 0.65
INFO:root:2019-05-12 11:17:53, Epoch : 1, Step : 7084, Training Loss : 0.24630, Training Acc : 0.878, Run Time : 0.63
INFO:root:2019-05-12 11:17:54, Epoch : 1, Step : 7085, Training Loss : 0.19951, Training Acc : 0.933, Run Time : 0.62
INFO:root:2019-05-12 11:17:55, Epoch : 1, Step : 7086, Training Loss : 0.41560, Training Acc : 0.856, Run Time : 0.62
INFO:root:2019-05-12 11:18:10, Epoch : 1, Step : 7087, Training Loss : 0.36456, Training Acc : 0.844, Run Time : 14.85
INFO:root:2019-05-12 11:18:10, Epoch : 1, Step : 7088, Training Loss : 0.29525, Training Acc : 0.911, Run Time : 0.90
INFO:root:2019-05-12 11:18:13, Epoch : 1, Step : 7089, Training Loss : 0.20519, Training Acc : 0.939, Run Time : 2.37
INFO:root:2019-05-12 11:18:23, Epoch : 1, Step : 7090, Training Loss : 0.15918, Training Acc : 0.956, Run Time : 10.47
INFO:root:2019-05-12 11:18:24, Epoch : 1, Step : 7091, Training Loss : 0.35635, Training Acc : 0.861, Run Time : 0.47
INFO:root:2019-05-12 11:18:26, Epoch : 1, Step : 7092, Training Loss : 0.22053, Training Acc : 0.906, Run Time : 1.80
INFO:root:2019-05-12 11:18:37, Epoch : 1, Step : 7093, Training Loss : 0.21012, Training Acc : 0.922, Run Time : 11.46
INFO:root:2019-05-12 11:18:38, Epoch : 1, Step : 7094, Training Loss : 0.17492, Training Acc : 0.922, Run Time : 0.58
INFO:root:2019-05-12 11:18:53, Epoch : 1, Step : 7095, Training Loss : 0.23845, Training Acc : 0.911, Run Time : 15.30
INFO:root:2019-05-12 11:18:59, Epoch : 1, Step : 7096, Training Loss : 0.17853, Training Acc : 0.928, Run Time : 5.84
INFO:root:2019-05-12 11:18:59, Epoch : 1, Step : 7097, Training Loss : 0.21781, Training Acc : 0.900, Run Time : 0.53
INFO:root:2019-05-12 11:19:01, Epoch : 1, Step : 7098, Training Loss : 0.21427, Training Acc : 0.900, Run Time : 1.29
INFO:root:2019-05-12 11:19:10, Epoch : 1, Step : 7099, Training Loss : 0.21300, Training Acc : 0.906, Run Time : 9.26
INFO:root:2019-05-12 11:19:10, Epoch : 1, Step : 7100, Training Loss : 0.32664, Training Acc : 0.828, Run Time : 0.47
INFO:root:2019-05-12 11:19:24, Epoch : 1, Step : 7101, Training Loss : 0.18779, Training Acc : 0.922, Run Time : 13.92
INFO:root:2019-05-12 11:19:25, Epoch : 1, Step : 7102, Training Loss : 0.36019, Training Acc : 0.856, Run Time : 0.60
INFO:root:2019-05-12 11:19:25, Epoch : 1, Step : 7103, Training Loss : 0.31993, Training Acc : 0.833, Run Time : 0.63
INFO:root:2019-05-12 11:19:26, Epoch : 1, Step : 7104, Training Loss : 0.48361, Training Acc : 0.783, Run Time : 0.59
INFO:root:2019-05-12 11:19:27, Epoch : 1, Step : 7105, Training Loss : 0.38277, Training Acc : 0.822, Run Time : 0.57
INFO:root:2019-05-12 11:19:48, Epoch : 1, Step : 7106, Training Loss : 0.38578, Training Acc : 0.839, Run Time : 21.20
INFO:root:2019-05-12 11:19:49, Epoch : 1, Step : 7107, Training Loss : 0.37646, Training Acc : 0.844, Run Time : 0.83
INFO:root:2019-05-12 11:19:49, Epoch : 1, Step : 7108, Training Loss : 0.28023, Training Acc : 0.878, Run Time : 0.59
INFO:root:2019-05-12 11:19:50, Epoch : 1, Step : 7109, Training Loss : 0.42182, Training Acc : 0.822, Run Time : 0.58
INFO:root:2019-05-12 11:19:51, Epoch : 1, Step : 7110, Training Loss : 0.62025, Training Acc : 0.694, Run Time : 1.16
INFO:root:2019-05-12 11:20:08, Epoch : 1, Step : 7111, Training Loss : 0.60727, Training Acc : 0.678, Run Time : 17.16
INFO:root:2019-05-12 11:20:09, Epoch : 1, Step : 7112, Training Loss : 0.29626, Training Acc : 0.856, Run Time : 1.06
INFO:root:2019-05-12 11:20:23, Epoch : 1, Step : 7113, Training Loss : 0.28903, Training Acc : 0.839, Run Time : 14.30
INFO:root:2019-05-12 11:20:25, Epoch : 1, Step : 7114, Training Loss : 0.42752, Training Acc : 0.772, Run Time : 1.80
INFO:root:2019-05-12 11:20:26, Epoch : 1, Step : 7115, Training Loss : 0.34230, Training Acc : 0.844, Run Time : 1.02
INFO:root:2019-05-12 11:20:40, Epoch : 1, Step : 7116, Training Loss : 0.35217, Training Acc : 0.850, Run Time : 13.92
INFO:root:2019-05-12 11:20:41, Epoch : 1, Step : 7117, Training Loss : 0.27876, Training Acc : 0.861, Run Time : 0.96
INFO:root:2019-05-12 11:20:54, Epoch : 1, Step : 7118, Training Loss : 0.20681, Training Acc : 0.922, Run Time : 12.63
INFO:root:2019-05-12 11:20:54, Epoch : 1, Step : 7119, Training Loss : 0.20921, Training Acc : 0.928, Run Time : 0.71
INFO:root:2019-05-12 11:21:04, Epoch : 1, Step : 7120, Training Loss : 0.27459, Training Acc : 0.894, Run Time : 9.68
INFO:root:2019-05-12 11:21:05, Epoch : 1, Step : 7121, Training Loss : 0.26399, Training Acc : 0.883, Run Time : 0.98
INFO:root:2019-05-12 11:21:15, Epoch : 1, Step : 7122, Training Loss : 0.27540, Training Acc : 0.900, Run Time : 9.53
INFO:root:2019-05-12 11:21:16, Epoch : 1, Step : 7123, Training Loss : 0.34965, Training Acc : 0.856, Run Time : 1.40
INFO:root:2019-05-12 11:21:17, Epoch : 1, Step : 7124, Training Loss : 0.18538, Training Acc : 0.928, Run Time : 0.88
INFO:root:2019-05-12 11:21:30, Epoch : 1, Step : 7125, Training Loss : 0.12930, Training Acc : 0.956, Run Time : 12.74
INFO:root:2019-05-12 11:21:31, Epoch : 1, Step : 7126, Training Loss : 0.18392, Training Acc : 0.928, Run Time : 0.87
INFO:root:2019-05-12 11:21:43, Epoch : 1, Step : 7127, Training Loss : 0.22705, Training Acc : 0.928, Run Time : 12.14
INFO:root:2019-05-12 11:21:44, Epoch : 1, Step : 7128, Training Loss : 0.39582, Training Acc : 0.822, Run Time : 1.31
INFO:root:2019-05-12 11:21:45, Epoch : 1, Step : 7129, Training Loss : 0.41641, Training Acc : 0.806, Run Time : 0.61
INFO:root:2019-05-12 11:21:46, Epoch : 1, Step : 7130, Training Loss : 0.38870, Training Acc : 0.794, Run Time : 1.62
INFO:root:2019-05-12 11:21:57, Epoch : 1, Step : 7131, Training Loss : 0.26796, Training Acc : 0.861, Run Time : 10.97
INFO:root:2019-05-12 11:22:10, Epoch : 1, Step : 7132, Training Loss : 0.27109, Training Acc : 0.900, Run Time : 12.59
INFO:root:2019-05-12 11:22:10, Epoch : 1, Step : 7133, Training Loss : 0.28149, Training Acc : 0.850, Run Time : 0.57
INFO:root:2019-05-12 11:22:12, Epoch : 1, Step : 7134, Training Loss : 0.22838, Training Acc : 0.867, Run Time : 1.18
INFO:root:2019-05-12 11:22:13, Epoch : 1, Step : 7135, Training Loss : 0.25725, Training Acc : 0.861, Run Time : 1.75
INFO:root:2019-05-12 11:22:26, Epoch : 1, Step : 7136, Training Loss : 0.21734, Training Acc : 0.894, Run Time : 13.06
INFO:root:2019-05-12 11:22:44, Epoch : 1, Step : 7137, Training Loss : 0.19596, Training Acc : 0.917, Run Time : 17.96
INFO:root:2019-05-12 11:22:47, Epoch : 1, Step : 7138, Training Loss : 0.25415, Training Acc : 0.883, Run Time : 2.80
INFO:root:2019-05-12 11:22:48, Epoch : 1, Step : 7139, Training Loss : 0.28288, Training Acc : 0.872, Run Time : 0.67
INFO:root:2019-05-12 11:22:57, Epoch : 1, Step : 7140, Training Loss : 0.50037, Training Acc : 0.778, Run Time : 9.59
INFO:root:2019-05-12 11:22:58, Epoch : 1, Step : 7141, Training Loss : 0.46079, Training Acc : 0.761, Run Time : 0.50
INFO:root:2019-05-12 11:22:59, Epoch : 1, Step : 7142, Training Loss : 0.22570, Training Acc : 0.906, Run Time : 0.64
INFO:root:2019-05-12 11:23:13, Epoch : 1, Step : 7143, Training Loss : 0.20177, Training Acc : 0.922, Run Time : 14.00
INFO:root:2019-05-12 11:23:14, Epoch : 1, Step : 7144, Training Loss : 0.12067, Training Acc : 0.956, Run Time : 1.00
INFO:root:2019-05-12 11:23:25, Epoch : 1, Step : 7145, Training Loss : 0.21769, Training Acc : 0.900, Run Time : 11.96
INFO:root:2019-05-12 11:23:26, Epoch : 1, Step : 7146, Training Loss : 0.27162, Training Acc : 0.878, Run Time : 0.99
INFO:root:2019-05-12 11:23:36, Epoch : 1, Step : 7147, Training Loss : 0.16315, Training Acc : 0.961, Run Time : 9.03
INFO:root:2019-05-12 11:23:39, Epoch : 1, Step : 7148, Training Loss : 0.32381, Training Acc : 0.861, Run Time : 3.00
INFO:root:2019-05-12 11:23:39, Epoch : 1, Step : 7149, Training Loss : 0.59052, Training Acc : 0.756, Run Time : 0.68
INFO:root:2019-05-12 11:23:40, Epoch : 1, Step : 7150, Training Loss : 0.42693, Training Acc : 0.817, Run Time : 1.28
INFO:root:2019-05-12 11:23:47, Epoch : 1, Step : 7151, Training Loss : 0.34689, Training Acc : 0.844, Run Time : 6.08
INFO:root:2019-05-12 11:23:47, Epoch : 1, Step : 7152, Training Loss : 0.45523, Training Acc : 0.756, Run Time : 0.83
INFO:root:2019-05-12 11:23:59, Epoch : 1, Step : 7153, Training Loss : 0.44584, Training Acc : 0.800, Run Time : 11.28
INFO:root:2019-05-12 11:24:00, Epoch : 1, Step : 7154, Training Loss : 0.47819, Training Acc : 0.761, Run Time : 1.02
INFO:root:2019-05-12 11:24:10, Epoch : 1, Step : 7155, Training Loss : 0.37496, Training Acc : 0.856, Run Time : 10.60
INFO:root:2019-05-12 11:24:11, Epoch : 1, Step : 7156, Training Loss : 0.73093, Training Acc : 0.722, Run Time : 0.52
INFO:root:2019-05-12 11:24:11, Epoch : 1, Step : 7157, Training Loss : 0.56154, Training Acc : 0.722, Run Time : 0.63
INFO:root:2019-05-12 11:24:13, Epoch : 1, Step : 7158, Training Loss : 0.40403, Training Acc : 0.817, Run Time : 1.86
INFO:root:2019-05-12 11:24:25, Epoch : 1, Step : 7159, Training Loss : 0.34739, Training Acc : 0.867, Run Time : 11.56
INFO:root:2019-05-12 11:24:26, Epoch : 1, Step : 7160, Training Loss : 0.33232, Training Acc : 0.867, Run Time : 0.94
INFO:root:2019-05-12 11:24:26, Epoch : 1, Step : 7161, Training Loss : 0.15611, Training Acc : 0.933, Run Time : 0.63
INFO:root:2019-05-12 11:24:28, Epoch : 1, Step : 7162, Training Loss : 0.35436, Training Acc : 0.844, Run Time : 1.82
INFO:root:2019-05-12 11:24:38, Epoch : 1, Step : 7163, Training Loss : 0.41718, Training Acc : 0.844, Run Time : 9.70
INFO:root:2019-05-12 11:24:39, Epoch : 1, Step : 7164, Training Loss : 0.23090, Training Acc : 0.933, Run Time : 1.05
INFO:root:2019-05-12 11:24:40, Epoch : 1, Step : 7165, Training Loss : 0.19028, Training Acc : 0.928, Run Time : 1.24
INFO:root:2019-05-12 11:24:58, Epoch : 1, Step : 7166, Training Loss : 0.15126, Training Acc : 0.961, Run Time : 18.22
INFO:root:2019-05-12 11:25:01, Epoch : 1, Step : 7167, Training Loss : 0.23370, Training Acc : 0.917, Run Time : 2.08
INFO:root:2019-05-12 11:25:01, Epoch : 1, Step : 7168, Training Loss : 0.19769, Training Acc : 0.928, Run Time : 0.64
INFO:root:2019-05-12 11:25:15, Epoch : 1, Step : 7169, Training Loss : 0.21014, Training Acc : 0.906, Run Time : 13.91
INFO:root:2019-05-12 11:25:16, Epoch : 1, Step : 7170, Training Loss : 0.16969, Training Acc : 0.950, Run Time : 0.51
INFO:root:2019-05-12 11:25:28, Epoch : 1, Step : 7171, Training Loss : 0.20559, Training Acc : 0.928, Run Time : 12.16
INFO:root:2019-05-12 11:25:29, Epoch : 1, Step : 7172, Training Loss : 0.15118, Training Acc : 0.950, Run Time : 1.42
INFO:root:2019-05-12 11:25:30, Epoch : 1, Step : 7173, Training Loss : 0.16365, Training Acc : 0.933, Run Time : 0.66
INFO:root:2019-05-12 11:25:42, Epoch : 1, Step : 7174, Training Loss : 0.13069, Training Acc : 0.961, Run Time : 12.25
INFO:root:2019-05-12 11:25:44, Epoch : 1, Step : 7175, Training Loss : 0.20724, Training Acc : 0.906, Run Time : 2.03
INFO:root:2019-05-12 11:25:53, Epoch : 1, Step : 7176, Training Loss : 0.24115, Training Acc : 0.861, Run Time : 8.54
INFO:root:2019-05-12 11:25:53, Epoch : 1, Step : 7177, Training Loss : 0.14764, Training Acc : 0.933, Run Time : 0.78
INFO:root:2019-05-12 11:26:06, Epoch : 1, Step : 7178, Training Loss : 0.11675, Training Acc : 0.956, Run Time : 12.42
INFO:root:2019-05-12 11:26:07, Epoch : 1, Step : 7179, Training Loss : 0.21506, Training Acc : 0.928, Run Time : 1.39
INFO:root:2019-05-12 11:26:09, Epoch : 1, Step : 7180, Training Loss : 0.16736, Training Acc : 0.950, Run Time : 1.66
INFO:root:2019-05-12 11:26:20, Epoch : 1, Step : 7181, Training Loss : 0.20872, Training Acc : 0.889, Run Time : 10.87
INFO:root:2019-05-12 11:26:20, Epoch : 1, Step : 7182, Training Loss : 0.14408, Training Acc : 0.956, Run Time : 0.56
INFO:root:2019-05-12 11:26:21, Epoch : 1, Step : 7183, Training Loss : 0.15166, Training Acc : 0.950, Run Time : 0.59
INFO:root:2019-05-12 11:26:32, Epoch : 1, Step : 7184, Training Loss : 0.15405, Training Acc : 0.944, Run Time : 10.64
INFO:root:2019-05-12 11:26:33, Epoch : 1, Step : 7185, Training Loss : 0.10878, Training Acc : 0.956, Run Time : 1.15
INFO:root:2019-05-12 11:26:34, Epoch : 1, Step : 7186, Training Loss : 0.14576, Training Acc : 0.944, Run Time : 1.04
INFO:root:2019-05-12 11:26:46, Epoch : 1, Step : 7187, Training Loss : 0.14622, Training Acc : 0.956, Run Time : 11.79
INFO:root:2019-05-12 11:26:46, Epoch : 1, Step : 7188, Training Loss : 0.13461, Training Acc : 0.978, Run Time : 0.50
INFO:root:2019-05-12 11:26:47, Epoch : 1, Step : 7189, Training Loss : 0.30602, Training Acc : 0.861, Run Time : 0.61
INFO:root:2019-05-12 11:26:59, Epoch : 1, Step : 7190, Training Loss : 0.36357, Training Acc : 0.844, Run Time : 11.96
INFO:root:2019-05-12 11:26:59, Epoch : 1, Step : 7191, Training Loss : 0.27346, Training Acc : 0.872, Run Time : 0.67
INFO:root:2019-05-12 11:27:00, Epoch : 1, Step : 7192, Training Loss : 0.68647, Training Acc : 0.656, Run Time : 0.43
INFO:root:2019-05-12 11:27:11, Epoch : 1, Step : 7193, Training Loss : 0.39105, Training Acc : 0.811, Run Time : 11.13
INFO:root:2019-05-12 11:27:12, Epoch : 1, Step : 7194, Training Loss : 0.42105, Training Acc : 0.811, Run Time : 0.67
INFO:root:2019-05-12 11:27:14, Epoch : 1, Step : 7195, Training Loss : 0.29583, Training Acc : 0.861, Run Time : 2.38
INFO:root:2019-05-12 11:27:31, Epoch : 1, Step : 7196, Training Loss : 0.30427, Training Acc : 0.878, Run Time : 16.82
INFO:root:2019-05-12 11:27:37, Epoch : 1, Step : 7197, Training Loss : 0.22511, Training Acc : 0.911, Run Time : 5.80
INFO:root:2019-05-12 11:27:37, Epoch : 1, Step : 7198, Training Loss : 0.41853, Training Acc : 0.806, Run Time : 0.50
INFO:root:2019-05-12 11:27:45, Epoch : 1, Step : 7199, Training Loss : 0.35410, Training Acc : 0.844, Run Time : 8.07
INFO:root:2019-05-12 11:27:48, Epoch : 1, Step : 7200, Training Loss : 0.43512, Training Acc : 0.828, Run Time : 2.92
INFO:root:2019-05-12 11:27:50, Epoch : 1, Step : 7201, Training Loss : 0.71630, Training Acc : 0.656, Run Time : 1.69
INFO:root:2019-05-12 11:27:56, Epoch : 1, Step : 7202, Training Loss : 0.67355, Training Acc : 0.706, Run Time : 6.09
INFO:root:2019-05-12 11:27:56, Epoch : 1, Step : 7203, Training Loss : 0.40090, Training Acc : 0.806, Run Time : 0.51
INFO:root:2019-05-12 11:27:57, Epoch : 1, Step : 7204, Training Loss : 0.62516, Training Acc : 0.717, Run Time : 0.64
INFO:root:2019-05-12 11:27:58, Epoch : 1, Step : 7205, Training Loss : 0.51578, Training Acc : 0.800, Run Time : 0.56
INFO:root:2019-05-12 11:27:59, Epoch : 1, Step : 7206, Training Loss : 0.44179, Training Acc : 0.822, Run Time : 1.58
INFO:root:2019-05-12 11:28:11, Epoch : 1, Step : 7207, Training Loss : 0.84647, Training Acc : 0.639, Run Time : 11.66
INFO:root:2019-05-12 11:28:11, Epoch : 1, Step : 7208, Training Loss : 0.60098, Training Acc : 0.778, Run Time : 0.55
INFO:root:2019-05-12 11:28:20, Epoch : 1, Step : 7209, Training Loss : 0.44611, Training Acc : 0.778, Run Time : 8.99
INFO:root:2019-05-12 11:28:24, Epoch : 1, Step : 7210, Training Loss : 0.26762, Training Acc : 0.883, Run Time : 3.27
INFO:root:2019-05-12 11:28:25, Epoch : 1, Step : 7211, Training Loss : 0.25484, Training Acc : 0.906, Run Time : 0.97
INFO:root:2019-05-12 11:28:34, Epoch : 1, Step : 7212, Training Loss : 0.19081, Training Acc : 0.939, Run Time : 9.80
INFO:root:2019-05-12 11:28:36, Epoch : 1, Step : 7213, Training Loss : 0.30734, Training Acc : 0.861, Run Time : 1.42
INFO:root:2019-05-12 11:28:46, Epoch : 1, Step : 7214, Training Loss : 0.29165, Training Acc : 0.911, Run Time : 10.43
INFO:root:2019-05-12 11:28:47, Epoch : 1, Step : 7215, Training Loss : 0.36516, Training Acc : 0.878, Run Time : 0.61
INFO:root:2019-05-12 11:28:55, Epoch : 1, Step : 7216, Training Loss : 0.38366, Training Acc : 0.850, Run Time : 8.23
INFO:root:2019-05-12 11:28:58, Epoch : 1, Step : 7217, Training Loss : 0.17267, Training Acc : 0.939, Run Time : 3.09
INFO:root:2019-05-12 11:28:59, Epoch : 1, Step : 7218, Training Loss : 0.14418, Training Acc : 0.967, Run Time : 0.57
INFO:root:2019-05-12 11:29:08, Epoch : 1, Step : 7219, Training Loss : 0.21725, Training Acc : 0.911, Run Time : 9.79
INFO:root:2019-05-12 11:29:18, Epoch : 1, Step : 7220, Training Loss : 0.23083, Training Acc : 0.906, Run Time : 10.00
INFO:root:2019-05-12 11:29:20, Epoch : 1, Step : 7221, Training Loss : 0.19302, Training Acc : 0.933, Run Time : 1.63
INFO:root:2019-05-12 11:29:36, Epoch : 1, Step : 7222, Training Loss : 0.17445, Training Acc : 0.933, Run Time : 15.70
INFO:root:2019-05-12 11:29:37, Epoch : 1, Step : 7223, Training Loss : 0.24026, Training Acc : 0.911, Run Time : 1.25
INFO:root:2019-05-12 11:29:39, Epoch : 1, Step : 7224, Training Loss : 0.34300, Training Acc : 0.856, Run Time : 1.83
INFO:root:2019-05-12 11:29:49, Epoch : 1, Step : 7225, Training Loss : 0.34464, Training Acc : 0.856, Run Time : 9.70
INFO:root:2019-05-12 11:29:49, Epoch : 1, Step : 7226, Training Loss : 0.38901, Training Acc : 0.794, Run Time : 0.96
INFO:root:2019-05-12 11:30:00, Epoch : 1, Step : 7227, Training Loss : 0.29800, Training Acc : 0.867, Run Time : 10.64
INFO:root:2019-05-12 11:30:01, Epoch : 1, Step : 7228, Training Loss : 0.29569, Training Acc : 0.867, Run Time : 0.70
INFO:root:2019-05-12 11:30:01, Epoch : 1, Step : 7229, Training Loss : 0.19825, Training Acc : 0.911, Run Time : 0.58
INFO:root:2019-05-12 11:30:13, Epoch : 1, Step : 7230, Training Loss : 0.27679, Training Acc : 0.867, Run Time : 12.06
INFO:root:2019-05-12 11:30:14, Epoch : 1, Step : 7231, Training Loss : 0.18186, Training Acc : 0.906, Run Time : 0.55
INFO:root:2019-05-12 11:30:17, Epoch : 1, Step : 7232, Training Loss : 0.22560, Training Acc : 0.878, Run Time : 2.75
INFO:root:2019-05-12 11:30:31, Epoch : 1, Step : 7233, Training Loss : 0.25960, Training Acc : 0.883, Run Time : 14.47
INFO:root:2019-05-12 11:30:32, Epoch : 1, Step : 7234, Training Loss : 0.26632, Training Acc : 0.906, Run Time : 1.21
INFO:root:2019-05-12 11:30:43, Epoch : 1, Step : 7235, Training Loss : 0.20689, Training Acc : 0.933, Run Time : 10.78
INFO:root:2019-05-12 11:30:45, Epoch : 1, Step : 7236, Training Loss : 0.20721, Training Acc : 0.906, Run Time : 1.89
INFO:root:2019-05-12 11:30:56, Epoch : 1, Step : 7237, Training Loss : 0.15751, Training Acc : 0.939, Run Time : 11.08
INFO:root:2019-05-12 11:30:57, Epoch : 1, Step : 7238, Training Loss : 0.19845, Training Acc : 0.928, Run Time : 0.44
INFO:root:2019-05-12 11:30:57, Epoch : 1, Step : 7239, Training Loss : 0.19318, Training Acc : 0.922, Run Time : 0.60
INFO:root:2019-05-12 11:31:15, Epoch : 1, Step : 7240, Training Loss : 0.11442, Training Acc : 0.950, Run Time : 17.28
INFO:root:2019-05-12 11:31:19, Epoch : 1, Step : 7241, Training Loss : 0.14995, Training Acc : 0.950, Run Time : 4.23
INFO:root:2019-05-12 11:31:20, Epoch : 1, Step : 7242, Training Loss : 0.10465, Training Acc : 0.967, Run Time : 0.80
INFO:root:2019-05-12 11:31:31, Epoch : 1, Step : 7243, Training Loss : 0.15804, Training Acc : 0.922, Run Time : 11.81
INFO:root:2019-05-12 11:31:33, Epoch : 1, Step : 7244, Training Loss : 0.15288, Training Acc : 0.939, Run Time : 1.76
INFO:root:2019-05-12 11:31:35, Epoch : 1, Step : 7245, Training Loss : 0.14647, Training Acc : 0.950, Run Time : 1.61
INFO:root:2019-05-12 11:31:55, Epoch : 1, Step : 7246, Training Loss : 0.18596, Training Acc : 0.922, Run Time : 20.22
INFO:root:2019-05-12 11:31:58, Epoch : 1, Step : 7247, Training Loss : 0.32908, Training Acc : 0.883, Run Time : 3.36
INFO:root:2019-05-12 11:32:07, Epoch : 1, Step : 7248, Training Loss : 0.31448, Training Acc : 0.844, Run Time : 8.62
INFO:root:2019-05-12 11:32:08, Epoch : 1, Step : 7249, Training Loss : 0.38043, Training Acc : 0.850, Run Time : 0.59
INFO:root:2019-05-12 11:32:09, Epoch : 1, Step : 7250, Training Loss : 0.17740, Training Acc : 0.911, Run Time : 1.62
INFO:root:2019-05-12 11:32:20, Epoch : 1, Step : 7251, Training Loss : 0.16079, Training Acc : 0.950, Run Time : 10.42
INFO:root:2019-05-12 11:32:20, Epoch : 1, Step : 7252, Training Loss : 0.15891, Training Acc : 0.922, Run Time : 0.46
INFO:root:2019-05-12 11:32:22, Epoch : 1, Step : 7253, Training Loss : 0.15847, Training Acc : 0.944, Run Time : 2.08
INFO:root:2019-05-12 11:32:32, Epoch : 1, Step : 7254, Training Loss : 0.13207, Training Acc : 0.956, Run Time : 10.33
INFO:root:2019-05-12 11:32:35, Epoch : 1, Step : 7255, Training Loss : 0.08261, Training Acc : 0.983, Run Time : 2.67
INFO:root:2019-05-12 11:32:50, Epoch : 1, Step : 7256, Training Loss : 0.10697, Training Acc : 0.950, Run Time : 14.76
INFO:root:2019-05-12 11:32:51, Epoch : 1, Step : 7257, Training Loss : 0.12665, Training Acc : 0.944, Run Time : 1.34
INFO:root:2019-05-12 11:33:00, Epoch : 1, Step : 7258, Training Loss : 0.10714, Training Acc : 0.972, Run Time : 8.90
INFO:root:2019-05-12 11:33:01, Epoch : 1, Step : 7259, Training Loss : 0.09570, Training Acc : 0.978, Run Time : 1.22
INFO:root:2019-05-12 11:33:11, Epoch : 1, Step : 7260, Training Loss : 0.10032, Training Acc : 0.978, Run Time : 9.57
INFO:root:2019-05-12 11:33:12, Epoch : 1, Step : 7261, Training Loss : 0.16165, Training Acc : 0.950, Run Time : 0.65
INFO:root:2019-05-12 11:33:13, Epoch : 1, Step : 7262, Training Loss : 0.20965, Training Acc : 0.906, Run Time : 1.83
INFO:root:2019-05-12 11:33:23, Epoch : 1, Step : 7263, Training Loss : 0.12635, Training Acc : 0.950, Run Time : 9.66
INFO:root:2019-05-12 11:33:24, Epoch : 1, Step : 7264, Training Loss : 0.09208, Training Acc : 0.978, Run Time : 0.72
INFO:root:2019-05-12 11:33:34, Epoch : 1, Step : 7265, Training Loss : 0.11184, Training Acc : 0.950, Run Time : 9.75
INFO:root:2019-05-12 11:33:35, Epoch : 1, Step : 7266, Training Loss : 0.16170, Training Acc : 0.944, Run Time : 1.13
INFO:root:2019-05-12 11:33:46, Epoch : 1, Step : 7267, Training Loss : 0.14740, Training Acc : 0.939, Run Time : 11.00
INFO:root:2019-05-12 11:33:46, Epoch : 1, Step : 7268, Training Loss : 0.06256, Training Acc : 0.983, Run Time : 0.83
INFO:root:2019-05-12 11:33:50, Epoch : 1, Step : 7269, Training Loss : 0.13600, Training Acc : 0.956, Run Time : 3.03
INFO:root:2019-05-12 11:34:01, Epoch : 1, Step : 7270, Training Loss : 0.05588, Training Acc : 0.989, Run Time : 11.10
INFO:root:2019-05-12 11:34:01, Epoch : 1, Step : 7271, Training Loss : 0.16294, Training Acc : 0.922, Run Time : 0.66
INFO:root:2019-05-12 11:34:02, Epoch : 1, Step : 7272, Training Loss : 0.15392, Training Acc : 0.928, Run Time : 0.58
INFO:root:2019-05-12 11:34:15, Epoch : 1, Step : 7273, Training Loss : 0.15244, Training Acc : 0.956, Run Time : 13.43
INFO:root:2019-05-12 11:34:16, Epoch : 1, Step : 7274, Training Loss : 0.11022, Training Acc : 0.978, Run Time : 0.77
INFO:root:2019-05-12 11:34:28, Epoch : 1, Step : 7275, Training Loss : 0.11854, Training Acc : 0.950, Run Time : 11.73
INFO:root:2019-05-12 11:34:30, Epoch : 1, Step : 7276, Training Loss : 0.08843, Training Acc : 0.978, Run Time : 1.98
INFO:root:2019-05-12 11:34:42, Epoch : 1, Step : 7277, Training Loss : 0.18569, Training Acc : 0.917, Run Time : 12.03
INFO:root:2019-05-12 11:34:52, Epoch : 1, Step : 7278, Training Loss : 0.13520, Training Acc : 0.939, Run Time : 9.71
INFO:root:2019-05-12 11:35:04, Epoch : 1, Step : 7279, Training Loss : 0.16530, Training Acc : 0.939, Run Time : 12.77
INFO:root:2019-05-12 11:35:15, Epoch : 1, Step : 7280, Training Loss : 0.21795, Training Acc : 0.917, Run Time : 10.39
INFO:root:2019-05-12 11:35:15, Epoch : 1, Step : 7281, Training Loss : 0.20614, Training Acc : 0.933, Run Time : 0.82
INFO:root:2019-05-12 11:35:26, Epoch : 1, Step : 7282, Training Loss : 0.20013, Training Acc : 0.928, Run Time : 10.90
INFO:root:2019-05-12 11:35:27, Epoch : 1, Step : 7283, Training Loss : 0.17074, Training Acc : 0.961, Run Time : 0.58
INFO:root:2019-05-12 11:35:29, Epoch : 1, Step : 7284, Training Loss : 0.13281, Training Acc : 0.972, Run Time : 1.68
INFO:root:2019-05-12 11:35:37, Epoch : 1, Step : 7285, Training Loss : 0.25740, Training Acc : 0.911, Run Time : 8.76
INFO:root:2019-05-12 11:35:38, Epoch : 1, Step : 7286, Training Loss : 0.32545, Training Acc : 0.894, Run Time : 0.45
INFO:root:2019-05-12 11:35:40, Epoch : 1, Step : 7287, Training Loss : 0.27733, Training Acc : 0.911, Run Time : 1.81
INFO:root:2019-05-12 11:35:54, Epoch : 1, Step : 7288, Training Loss : 0.21648, Training Acc : 0.922, Run Time : 13.97
INFO:root:2019-05-12 11:35:56, Epoch : 1, Step : 7289, Training Loss : 0.16764, Training Acc : 0.939, Run Time : 2.03
INFO:root:2019-05-12 11:36:08, Epoch : 1, Step : 7290, Training Loss : 0.17686, Training Acc : 0.933, Run Time : 11.92
INFO:root:2019-05-12 11:36:09, Epoch : 1, Step : 7291, Training Loss : 0.14769, Training Acc : 0.956, Run Time : 1.38
INFO:root:2019-05-12 11:36:22, Epoch : 1, Step : 7292, Training Loss : 0.19683, Training Acc : 0.917, Run Time : 12.91
INFO:root:2019-05-12 11:36:35, Epoch : 1, Step : 7293, Training Loss : 0.13981, Training Acc : 0.944, Run Time : 12.71
INFO:root:2019-05-12 11:36:36, Epoch : 1, Step : 7294, Training Loss : 0.15395, Training Acc : 0.933, Run Time : 1.75
INFO:root:2019-05-12 11:36:38, Epoch : 1, Step : 7295, Training Loss : 0.19020, Training Acc : 0.922, Run Time : 1.57
INFO:root:2019-05-12 11:36:50, Epoch : 1, Step : 7296, Training Loss : 0.26191, Training Acc : 0.900, Run Time : 12.20
INFO:root:2019-05-12 11:36:52, Epoch : 1, Step : 7297, Training Loss : 0.20581, Training Acc : 0.917, Run Time : 1.54
INFO:root:2019-05-12 11:37:03, Epoch : 1, Step : 7298, Training Loss : 0.14438, Training Acc : 0.956, Run Time : 10.97
INFO:root:2019-05-12 11:37:03, Epoch : 1, Step : 7299, Training Loss : 0.36519, Training Acc : 0.850, Run Time : 0.46
INFO:root:2019-05-12 11:37:04, Epoch : 1, Step : 7300, Training Loss : 0.70467, Training Acc : 0.739, Run Time : 0.61
INFO:root:2019-05-12 11:37:18, Epoch : 1, Step : 7301, Training Loss : 0.66615, Training Acc : 0.728, Run Time : 13.94
INFO:root:2019-05-12 11:37:18, Epoch : 1, Step : 7302, Training Loss : 0.56458, Training Acc : 0.778, Run Time : 0.44
INFO:root:2019-05-12 11:37:29, Epoch : 1, Step : 7303, Training Loss : 0.64026, Training Acc : 0.706, Run Time : 11.26
INFO:root:2019-05-12 11:37:30, Epoch : 1, Step : 7304, Training Loss : 0.49924, Training Acc : 0.800, Run Time : 0.57
INFO:root:2019-05-12 11:37:31, Epoch : 1, Step : 7305, Training Loss : 0.30357, Training Acc : 0.872, Run Time : 0.78
INFO:root:2019-05-12 11:37:33, Epoch : 1, Step : 7306, Training Loss : 0.39494, Training Acc : 0.833, Run Time : 2.12
INFO:root:2019-05-12 11:37:44, Epoch : 1, Step : 7307, Training Loss : 0.15346, Training Acc : 0.961, Run Time : 11.42
INFO:root:2019-05-12 11:37:45, Epoch : 1, Step : 7308, Training Loss : 0.34062, Training Acc : 0.867, Run Time : 0.51
INFO:root:2019-05-12 11:37:47, Epoch : 1, Step : 7309, Training Loss : 0.31612, Training Acc : 0.900, Run Time : 1.85
INFO:root:2019-05-12 11:37:58, Epoch : 1, Step : 7310, Training Loss : 0.42239, Training Acc : 0.828, Run Time : 11.40
INFO:root:2019-05-12 11:37:58, Epoch : 1, Step : 7311, Training Loss : 0.40080, Training Acc : 0.839, Run Time : 0.48
INFO:root:2019-05-12 11:38:00, Epoch : 1, Step : 7312, Training Loss : 0.32010, Training Acc : 0.878, Run Time : 1.44
INFO:root:2019-05-12 11:38:11, Epoch : 1, Step : 7313, Training Loss : 0.18959, Training Acc : 0.928, Run Time : 11.51
INFO:root:2019-05-12 11:38:12, Epoch : 1, Step : 7314, Training Loss : 0.33483, Training Acc : 0.883, Run Time : 0.57
INFO:root:2019-05-12 11:38:13, Epoch : 1, Step : 7315, Training Loss : 0.41468, Training Acc : 0.878, Run Time : 0.60
INFO:root:2019-05-12 11:38:13, Epoch : 1, Step : 7316, Training Loss : 0.43355, Training Acc : 0.817, Run Time : 0.67
INFO:root:2019-05-12 11:38:16, Epoch : 1, Step : 7317, Training Loss : 0.32629, Training Acc : 0.900, Run Time : 2.72
INFO:root:2019-05-12 11:38:27, Epoch : 1, Step : 7318, Training Loss : 0.35504, Training Acc : 0.883, Run Time : 11.44
INFO:root:2019-05-12 11:38:28, Epoch : 1, Step : 7319, Training Loss : 0.57997, Training Acc : 0.761, Run Time : 0.86
INFO:root:2019-05-12 11:38:37, Epoch : 1, Step : 7320, Training Loss : 0.33969, Training Acc : 0.872, Run Time : 8.45
INFO:root:2019-05-12 11:38:40, Epoch : 1, Step : 7321, Training Loss : 0.20204, Training Acc : 0.944, Run Time : 3.55
INFO:root:2019-05-12 11:38:41, Epoch : 1, Step : 7322, Training Loss : 0.17758, Training Acc : 0.933, Run Time : 0.56
INFO:root:2019-05-12 11:38:41, Epoch : 1, Step : 7323, Training Loss : 0.17247, Training Acc : 0.950, Run Time : 0.60
INFO:root:2019-05-12 11:38:42, Epoch : 1, Step : 7324, Training Loss : 0.32748, Training Acc : 0.828, Run Time : 0.61
INFO:root:2019-05-12 11:38:43, Epoch : 1, Step : 7325, Training Loss : 0.21103, Training Acc : 0.900, Run Time : 1.13
INFO:root:2019-05-12 11:38:46, Epoch : 1, Step : 7326, Training Loss : 0.24425, Training Acc : 0.878, Run Time : 3.05
INFO:root:2019-05-12 11:38:47, Epoch : 1, Step : 7327, Training Loss : 0.19220, Training Acc : 0.939, Run Time : 1.00
INFO:root:2019-05-12 11:39:06, Epoch : 1, Step : 7328, Training Loss : 0.16513, Training Acc : 0.950, Run Time : 18.36
INFO:root:2019-05-12 11:39:06, Epoch : 1, Step : 7329, Training Loss : 0.21851, Training Acc : 0.906, Run Time : 0.51
INFO:root:2019-05-12 11:39:07, Epoch : 1, Step : 7330, Training Loss : 0.27946, Training Acc : 0.856, Run Time : 0.61
INFO:root:2019-05-12 11:39:18, Epoch : 1, Step : 7331, Training Loss : 0.19975, Training Acc : 0.944, Run Time : 11.63
INFO:root:2019-05-12 11:39:19, Epoch : 1, Step : 7332, Training Loss : 0.30505, Training Acc : 0.856, Run Time : 0.90
INFO:root:2019-05-12 11:39:20, Epoch : 1, Step : 7333, Training Loss : 0.18896, Training Acc : 0.939, Run Time : 0.62
INFO:root:2019-05-12 11:39:21, Epoch : 1, Step : 7334, Training Loss : 0.20150, Training Acc : 0.906, Run Time : 1.61
INFO:root:2019-05-12 11:39:34, Epoch : 1, Step : 7335, Training Loss : 0.19132, Training Acc : 0.906, Run Time : 12.77
INFO:root:2019-05-12 11:39:35, Epoch : 1, Step : 7336, Training Loss : 0.17604, Training Acc : 0.944, Run Time : 0.52
INFO:root:2019-05-12 11:39:36, Epoch : 1, Step : 7337, Training Loss : 0.12588, Training Acc : 0.950, Run Time : 0.84
INFO:root:2019-05-12 11:39:36, Epoch : 1, Step : 7338, Training Loss : 0.18729, Training Acc : 0.939, Run Time : 0.59
INFO:root:2019-05-12 11:39:37, Epoch : 1, Step : 7339, Training Loss : 0.22058, Training Acc : 0.906, Run Time : 0.56
INFO:root:2019-05-12 11:39:51, Epoch : 1, Step : 7340, Training Loss : 0.31904, Training Acc : 0.861, Run Time : 14.15
INFO:root:2019-05-12 11:39:51, Epoch : 1, Step : 7341, Training Loss : 0.43692, Training Acc : 0.806, Run Time : 0.53
INFO:root:2019-05-12 11:39:52, Epoch : 1, Step : 7342, Training Loss : 0.19274, Training Acc : 0.906, Run Time : 0.59
INFO:root:2019-05-12 11:39:54, Epoch : 1, Step : 7343, Training Loss : 0.12782, Training Acc : 0.967, Run Time : 1.57
INFO:root:2019-05-12 11:40:02, Epoch : 1, Step : 7344, Training Loss : 0.12676, Training Acc : 0.983, Run Time : 8.44
INFO:root:2019-05-12 11:40:03, Epoch : 1, Step : 7345, Training Loss : 0.16636, Training Acc : 0.944, Run Time : 0.53
INFO:root:2019-05-12 11:40:03, Epoch : 1, Step : 7346, Training Loss : 0.23017, Training Acc : 0.900, Run Time : 0.59
INFO:root:2019-05-12 11:40:04, Epoch : 1, Step : 7347, Training Loss : 0.24657, Training Acc : 0.894, Run Time : 0.57
INFO:root:2019-05-12 11:40:04, Epoch : 1, Step : 7348, Training Loss : 0.33538, Training Acc : 0.872, Run Time : 0.63
INFO:root:2019-05-12 11:40:20, Epoch : 1, Step : 7349, Training Loss : 0.24079, Training Acc : 0.878, Run Time : 15.85
INFO:root:2019-05-12 11:40:21, Epoch : 1, Step : 7350, Training Loss : 0.22330, Training Acc : 0.894, Run Time : 1.04
INFO:root:2019-05-12 11:40:23, Epoch : 1, Step : 7351, Training Loss : 0.38518, Training Acc : 0.850, Run Time : 1.39
INFO:root:2019-05-12 11:40:35, Epoch : 1, Step : 7352, Training Loss : 0.24308, Training Acc : 0.894, Run Time : 12.12
INFO:root:2019-05-12 11:40:36, Epoch : 1, Step : 7353, Training Loss : 0.40837, Training Acc : 0.833, Run Time : 1.09
INFO:root:2019-05-12 11:40:46, Epoch : 1, Step : 7354, Training Loss : 0.33517, Training Acc : 0.867, Run Time : 9.75
INFO:root:2019-05-12 11:40:46, Epoch : 1, Step : 7355, Training Loss : 0.39558, Training Acc : 0.828, Run Time : 0.61
INFO:root:2019-05-12 11:40:48, Epoch : 1, Step : 7356, Training Loss : 0.40345, Training Acc : 0.794, Run Time : 2.00
INFO:root:2019-05-12 11:41:00, Epoch : 1, Step : 7357, Training Loss : 0.52337, Training Acc : 0.739, Run Time : 11.89
INFO:root:2019-05-12 11:41:01, Epoch : 1, Step : 7358, Training Loss : 0.36349, Training Acc : 0.833, Run Time : 0.54
INFO:root:2019-05-12 11:41:02, Epoch : 1, Step : 7359, Training Loss : 0.37984, Training Acc : 0.844, Run Time : 1.68
INFO:root:2019-05-12 11:41:13, Epoch : 1, Step : 7360, Training Loss : 0.44898, Training Acc : 0.789, Run Time : 10.51
INFO:root:2019-05-12 11:41:13, Epoch : 1, Step : 7361, Training Loss : 0.36389, Training Acc : 0.817, Run Time : 0.62
INFO:root:2019-05-12 11:41:14, Epoch : 1, Step : 7362, Training Loss : 0.30797, Training Acc : 0.856, Run Time : 1.00
INFO:root:2019-05-12 11:41:26, Epoch : 1, Step : 7363, Training Loss : 0.39160, Training Acc : 0.844, Run Time : 11.48
INFO:root:2019-05-12 11:41:26, Epoch : 1, Step : 7364, Training Loss : 0.43114, Training Acc : 0.811, Run Time : 0.55
INFO:root:2019-05-12 11:41:27, Epoch : 1, Step : 7365, Training Loss : 0.40768, Training Acc : 0.817, Run Time : 0.74
INFO:root:2019-05-12 11:41:40, Epoch : 1, Step : 7366, Training Loss : 0.33619, Training Acc : 0.872, Run Time : 12.55
INFO:root:2019-05-12 11:41:40, Epoch : 1, Step : 7367, Training Loss : 0.31415, Training Acc : 0.850, Run Time : 0.47
INFO:root:2019-05-12 11:41:41, Epoch : 1, Step : 7368, Training Loss : 0.35038, Training Acc : 0.828, Run Time : 0.61
INFO:root:2019-05-12 11:41:53, Epoch : 1, Step : 7369, Training Loss : 0.48133, Training Acc : 0.750, Run Time : 11.72
INFO:root:2019-05-12 11:41:53, Epoch : 1, Step : 7370, Training Loss : 0.34065, Training Acc : 0.861, Run Time : 0.80
INFO:root:2019-05-12 11:41:54, Epoch : 1, Step : 7371, Training Loss : 0.30068, Training Acc : 0.878, Run Time : 0.63
INFO:root:2019-05-12 11:42:08, Epoch : 1, Step : 7372, Training Loss : 0.46127, Training Acc : 0.756, Run Time : 14.46
INFO:root:2019-05-12 11:42:10, Epoch : 1, Step : 7373, Training Loss : 0.50807, Training Acc : 0.789, Run Time : 1.16
INFO:root:2019-05-12 11:42:10, Epoch : 1, Step : 7374, Training Loss : 0.30631, Training Acc : 0.861, Run Time : 0.67
INFO:root:2019-05-12 11:42:23, Epoch : 1, Step : 7375, Training Loss : 0.32856, Training Acc : 0.844, Run Time : 12.77
INFO:root:2019-05-12 11:42:24, Epoch : 1, Step : 7376, Training Loss : 0.36650, Training Acc : 0.844, Run Time : 0.98
INFO:root:2019-05-12 11:42:27, Epoch : 1, Step : 7377, Training Loss : 0.40464, Training Acc : 0.794, Run Time : 2.86
INFO:root:2019-05-12 11:42:38, Epoch : 1, Step : 7378, Training Loss : 0.34407, Training Acc : 0.800, Run Time : 11.56
INFO:root:2019-05-12 11:42:40, Epoch : 1, Step : 7379, Training Loss : 0.26581, Training Acc : 0.867, Run Time : 1.33
INFO:root:2019-05-12 11:42:55, Epoch : 1, Step : 7380, Training Loss : 0.36671, Training Acc : 0.828, Run Time : 15.41
INFO:root:2019-05-12 11:43:03, Epoch : 1, Step : 7381, Training Loss : 0.24172, Training Acc : 0.917, Run Time : 8.01
INFO:root:2019-05-12 11:43:05, Epoch : 1, Step : 7382, Training Loss : 0.25704, Training Acc : 0.911, Run Time : 1.33
INFO:root:2019-05-12 11:43:14, Epoch : 1, Step : 7383, Training Loss : 0.26798, Training Acc : 0.906, Run Time : 9.76
INFO:root:2019-05-12 11:43:15, Epoch : 1, Step : 7384, Training Loss : 0.18740, Training Acc : 0.939, Run Time : 1.11
INFO:root:2019-05-12 11:43:26, Epoch : 1, Step : 7385, Training Loss : 0.20546, Training Acc : 0.917, Run Time : 10.34
INFO:root:2019-05-12 11:43:27, Epoch : 1, Step : 7386, Training Loss : 0.23481, Training Acc : 0.878, Run Time : 1.58
INFO:root:2019-05-12 11:43:29, Epoch : 1, Step : 7387, Training Loss : 0.17494, Training Acc : 0.950, Run Time : 1.94
INFO:root:2019-05-12 11:43:40, Epoch : 1, Step : 7388, Training Loss : 0.14826, Training Acc : 0.944, Run Time : 11.21
INFO:root:2019-05-12 11:43:41, Epoch : 1, Step : 7389, Training Loss : 0.24530, Training Acc : 0.906, Run Time : 0.44
INFO:root:2019-05-12 11:43:41, Epoch : 1, Step : 7390, Training Loss : 0.12096, Training Acc : 0.967, Run Time : 0.47
INFO:root:2019-05-12 11:43:43, Epoch : 1, Step : 7391, Training Loss : 0.28609, Training Acc : 0.889, Run Time : 1.22
INFO:root:2019-05-12 11:43:55, Epoch : 1, Step : 7392, Training Loss : 0.27633, Training Acc : 0.883, Run Time : 12.35
INFO:root:2019-05-12 11:43:56, Epoch : 1, Step : 7393, Training Loss : 0.34316, Training Acc : 0.839, Run Time : 0.91
INFO:root:2019-05-12 11:43:56, Epoch : 1, Step : 7394, Training Loss : 0.28517, Training Acc : 0.889, Run Time : 0.56
INFO:root:2019-05-12 11:44:11, Epoch : 1, Step : 7395, Training Loss : 0.35665, Training Acc : 0.850, Run Time : 14.33
INFO:root:2019-05-12 11:44:17, Epoch : 1, Step : 7396, Training Loss : 0.23358, Training Acc : 0.894, Run Time : 6.37
INFO:root:2019-05-12 11:44:26, Epoch : 1, Step : 7397, Training Loss : 0.18854, Training Acc : 0.911, Run Time : 9.12
INFO:root:2019-05-12 11:44:27, Epoch : 1, Step : 7398, Training Loss : 0.24573, Training Acc : 0.900, Run Time : 0.55
INFO:root:2019-05-12 11:44:27, Epoch : 1, Step : 7399, Training Loss : 0.21798, Training Acc : 0.906, Run Time : 0.68
INFO:root:2019-05-12 11:44:29, Epoch : 1, Step : 7400, Training Loss : 0.16153, Training Acc : 0.933, Run Time : 1.76
INFO:root:2019-05-12 11:44:39, Epoch : 1, Step : 7401, Training Loss : 0.16826, Training Acc : 0.917, Run Time : 9.86
INFO:root:2019-05-12 11:44:40, Epoch : 1, Step : 7402, Training Loss : 0.12163, Training Acc : 0.956, Run Time : 0.54
INFO:root:2019-05-12 11:44:41, Epoch : 1, Step : 7403, Training Loss : 0.17743, Training Acc : 0.928, Run Time : 0.98
INFO:root:2019-05-12 11:44:52, Epoch : 1, Step : 7404, Training Loss : 0.19246, Training Acc : 0.922, Run Time : 11.59
INFO:root:2019-05-12 11:44:53, Epoch : 1, Step : 7405, Training Loss : 0.31451, Training Acc : 0.856, Run Time : 0.70
INFO:root:2019-05-12 11:44:54, Epoch : 1, Step : 7406, Training Loss : 0.24119, Training Acc : 0.889, Run Time : 1.18
INFO:root:2019-05-12 11:45:04, Epoch : 1, Step : 7407, Training Loss : 0.27563, Training Acc : 0.872, Run Time : 10.17
INFO:root:2019-05-12 11:45:05, Epoch : 1, Step : 7408, Training Loss : 0.37417, Training Acc : 0.861, Run Time : 1.14
INFO:root:2019-05-12 11:45:06, Epoch : 1, Step : 7409, Training Loss : 0.35304, Training Acc : 0.867, Run Time : 0.59
INFO:root:2019-05-12 11:45:08, Epoch : 1, Step : 7410, Training Loss : 0.32389, Training Acc : 0.878, Run Time : 1.55
INFO:root:2019-05-12 11:45:19, Epoch : 1, Step : 7411, Training Loss : 0.44799, Training Acc : 0.856, Run Time : 11.11
INFO:root:2019-05-12 11:45:19, Epoch : 1, Step : 7412, Training Loss : 0.60632, Training Acc : 0.767, Run Time : 0.46
INFO:root:2019-05-12 11:45:20, Epoch : 1, Step : 7413, Training Loss : 0.70001, Training Acc : 0.739, Run Time : 0.77
INFO:root:2019-05-12 11:45:20, Epoch : 1, Step : 7414, Training Loss : 0.30975, Training Acc : 0.872, Run Time : 0.64
INFO:root:2019-05-12 11:45:21, Epoch : 1, Step : 7415, Training Loss : 0.23332, Training Acc : 0.917, Run Time : 0.82
INFO:root:2019-05-12 11:45:36, Epoch : 1, Step : 7416, Training Loss : 0.18607, Training Acc : 0.956, Run Time : 14.38
INFO:root:2019-05-12 11:45:37, Epoch : 1, Step : 7417, Training Loss : 0.17014, Training Acc : 0.944, Run Time : 0.97
INFO:root:2019-05-12 11:45:37, Epoch : 1, Step : 7418, Training Loss : 0.20196, Training Acc : 0.933, Run Time : 0.64
INFO:root:2019-05-12 11:45:39, Epoch : 1, Step : 7419, Training Loss : 0.30833, Training Acc : 0.861, Run Time : 2.10
INFO:root:2019-05-12 11:45:50, Epoch : 1, Step : 7420, Training Loss : 0.32447, Training Acc : 0.894, Run Time : 10.52
INFO:root:2019-05-12 11:45:50, Epoch : 1, Step : 7421, Training Loss : 0.26749, Training Acc : 0.883, Run Time : 0.47
INFO:root:2019-05-12 11:45:52, Epoch : 1, Step : 7422, Training Loss : 0.23644, Training Acc : 0.906, Run Time : 1.71
INFO:root:2019-05-12 11:46:02, Epoch : 1, Step : 7423, Training Loss : 0.24151, Training Acc : 0.906, Run Time : 9.98
INFO:root:2019-05-12 11:46:03, Epoch : 1, Step : 7424, Training Loss : 0.16855, Training Acc : 0.944, Run Time : 0.97
INFO:root:2019-05-12 11:46:15, Epoch : 1, Step : 7425, Training Loss : 0.30245, Training Acc : 0.867, Run Time : 12.11
INFO:root:2019-05-12 11:46:16, Epoch : 1, Step : 7426, Training Loss : 0.23105, Training Acc : 0.906, Run Time : 1.09
INFO:root:2019-05-12 11:46:28, Epoch : 1, Step : 7427, Training Loss : 0.27831, Training Acc : 0.861, Run Time : 12.01
INFO:root:2019-05-12 11:46:29, Epoch : 1, Step : 7428, Training Loss : 0.28018, Training Acc : 0.883, Run Time : 1.04
INFO:root:2019-05-12 11:46:30, Epoch : 1, Step : 7429, Training Loss : 0.19774, Training Acc : 0.933, Run Time : 0.58
INFO:root:2019-05-12 11:46:36, Epoch : 1, Step : 7430, Training Loss : 0.16641, Training Acc : 0.922, Run Time : 5.70
INFO:root:2019-05-12 11:46:36, Epoch : 1, Step : 7431, Training Loss : 0.15918, Training Acc : 0.956, Run Time : 0.85
INFO:root:2019-05-12 11:46:38, Epoch : 1, Step : 7432, Training Loss : 0.20383, Training Acc : 0.928, Run Time : 1.78
INFO:root:2019-05-12 11:46:50, Epoch : 1, Step : 7433, Training Loss : 0.13920, Training Acc : 0.972, Run Time : 11.47
INFO:root:2019-05-12 11:46:51, Epoch : 1, Step : 7434, Training Loss : 0.38182, Training Acc : 0.828, Run Time : 1.34
INFO:root:2019-05-12 11:47:02, Epoch : 1, Step : 7435, Training Loss : 0.17320, Training Acc : 0.933, Run Time : 10.58
INFO:root:2019-05-12 11:47:02, Epoch : 1, Step : 7436, Training Loss : 0.19251, Training Acc : 0.933, Run Time : 0.78
INFO:root:2019-05-12 11:47:11, Epoch : 1, Step : 7437, Training Loss : 0.12910, Training Acc : 0.956, Run Time : 9.05
INFO:root:2019-05-12 11:47:12, Epoch : 1, Step : 7438, Training Loss : 0.19712, Training Acc : 0.933, Run Time : 0.61
INFO:root:2019-05-12 11:47:13, Epoch : 1, Step : 7439, Training Loss : 0.11436, Training Acc : 0.967, Run Time : 1.16
INFO:root:2019-05-12 11:47:23, Epoch : 1, Step : 7440, Training Loss : 0.11400, Training Acc : 0.956, Run Time : 10.22
INFO:root:2019-05-12 11:47:24, Epoch : 1, Step : 7441, Training Loss : 0.23710, Training Acc : 0.883, Run Time : 0.52
INFO:root:2019-05-12 11:47:26, Epoch : 1, Step : 7442, Training Loss : 0.13123, Training Acc : 0.956, Run Time : 2.15
INFO:root:2019-05-12 11:47:36, Epoch : 1, Step : 7443, Training Loss : 0.05227, Training Acc : 0.989, Run Time : 9.91
INFO:root:2019-05-12 11:47:36, Epoch : 1, Step : 7444, Training Loss : 0.10800, Training Acc : 0.967, Run Time : 0.42
INFO:root:2019-05-12 11:47:38, Epoch : 1, Step : 7445, Training Loss : 0.19393, Training Acc : 0.922, Run Time : 1.55
INFO:root:2019-05-12 11:47:48, Epoch : 1, Step : 7446, Training Loss : 0.17995, Training Acc : 0.944, Run Time : 10.04
INFO:root:2019-05-12 11:47:48, Epoch : 1, Step : 7447, Training Loss : 0.10500, Training Acc : 0.983, Run Time : 0.42
INFO:root:2019-05-12 11:47:49, Epoch : 1, Step : 7448, Training Loss : 0.12621, Training Acc : 0.956, Run Time : 0.86
INFO:root:2019-05-12 11:47:51, Epoch : 1, Step : 7449, Training Loss : 0.26248, Training Acc : 0.889, Run Time : 1.48
INFO:root:2019-05-12 11:48:01, Epoch : 1, Step : 7450, Training Loss : 0.12633, Training Acc : 0.967, Run Time : 10.63
INFO:root:2019-05-12 11:48:02, Epoch : 1, Step : 7451, Training Loss : 0.13625, Training Acc : 0.939, Run Time : 0.53
INFO:root:2019-05-12 11:48:03, Epoch : 1, Step : 7452, Training Loss : 0.08114, Training Acc : 0.972, Run Time : 1.26
INFO:root:2019-05-12 11:48:15, Epoch : 1, Step : 7453, Training Loss : 0.03597, Training Acc : 0.994, Run Time : 11.41
INFO:root:2019-05-12 11:48:15, Epoch : 1, Step : 7454, Training Loss : 0.11441, Training Acc : 0.950, Run Time : 0.45
INFO:root:2019-05-12 11:48:16, Epoch : 1, Step : 7455, Training Loss : 0.08248, Training Acc : 0.972, Run Time : 0.59
INFO:root:2019-05-12 11:48:16, Epoch : 1, Step : 7456, Training Loss : 0.13764, Training Acc : 0.933, Run Time : 0.64
INFO:root:2019-05-12 11:48:29, Epoch : 1, Step : 7457, Training Loss : 0.12302, Training Acc : 0.956, Run Time : 12.73
INFO:root:2019-05-12 11:48:30, Epoch : 1, Step : 7458, Training Loss : 0.08613, Training Acc : 0.967, Run Time : 0.55
INFO:root:2019-05-12 11:48:31, Epoch : 1, Step : 7459, Training Loss : 0.11804, Training Acc : 0.956, Run Time : 1.23
INFO:root:2019-05-12 11:48:43, Epoch : 1, Step : 7460, Training Loss : 0.26443, Training Acc : 0.867, Run Time : 11.77
INFO:root:2019-05-12 11:48:44, Epoch : 1, Step : 7461, Training Loss : 0.08404, Training Acc : 0.961, Run Time : 0.94
INFO:root:2019-05-12 11:48:45, Epoch : 1, Step : 7462, Training Loss : 0.08688, Training Acc : 0.967, Run Time : 1.66
INFO:root:2019-05-12 11:48:57, Epoch : 1, Step : 7463, Training Loss : 0.39410, Training Acc : 0.900, Run Time : 11.74
INFO:root:2019-05-12 11:48:59, Epoch : 1, Step : 7464, Training Loss : 0.08550, Training Acc : 0.983, Run Time : 1.64
INFO:root:2019-05-12 11:49:09, Epoch : 1, Step : 7465, Training Loss : 0.22505, Training Acc : 0.928, Run Time : 10.28
INFO:root:2019-05-12 11:49:12, Epoch : 1, Step : 7466, Training Loss : 0.28622, Training Acc : 0.911, Run Time : 3.17
INFO:root:2019-05-12 11:49:26, Epoch : 1, Step : 7467, Training Loss : 0.43697, Training Acc : 0.839, Run Time : 13.78
INFO:root:2019-05-12 11:49:32, Epoch : 1, Step : 7468, Training Loss : 0.29052, Training Acc : 0.900, Run Time : 6.66
INFO:root:2019-05-12 11:49:35, Epoch : 1, Step : 7469, Training Loss : 0.20270, Training Acc : 0.928, Run Time : 2.83
INFO:root:2019-05-12 11:49:41, Epoch : 1, Step : 7470, Training Loss : 0.17017, Training Acc : 0.944, Run Time : 5.35
INFO:root:2019-05-12 11:49:41, Epoch : 1, Step : 7471, Training Loss : 0.33868, Training Acc : 0.889, Run Time : 0.79
INFO:root:2019-05-12 11:49:50, Epoch : 1, Step : 7472, Training Loss : 0.33949, Training Acc : 0.883, Run Time : 8.59
INFO:root:2019-05-12 11:49:51, Epoch : 1, Step : 7473, Training Loss : 0.27371, Training Acc : 0.894, Run Time : 0.82
INFO:root:2019-05-12 11:49:51, Epoch : 1, Step : 7474, Training Loss : 0.22384, Training Acc : 0.900, Run Time : 0.62
INFO:root:2019-05-12 11:50:03, Epoch : 1, Step : 7475, Training Loss : 0.23707, Training Acc : 0.894, Run Time : 11.22
INFO:root:2019-05-12 11:50:03, Epoch : 1, Step : 7476, Training Loss : 0.17618, Training Acc : 0.917, Run Time : 0.56
INFO:root:2019-05-12 11:50:05, Epoch : 1, Step : 7477, Training Loss : 0.20237, Training Acc : 0.939, Run Time : 1.85
INFO:root:2019-05-12 11:50:13, Epoch : 1, Step : 7478, Training Loss : 0.21064, Training Acc : 0.911, Run Time : 8.22
INFO:root:2019-05-12 11:50:14, Epoch : 1, Step : 7479, Training Loss : 0.34442, Training Acc : 0.833, Run Time : 0.42
INFO:root:2019-05-12 11:50:14, Epoch : 1, Step : 7480, Training Loss : 0.33292, Training Acc : 0.861, Run Time : 0.55
INFO:root:2019-05-12 11:50:24, Epoch : 1, Step : 7481, Training Loss : 0.23854, Training Acc : 0.878, Run Time : 9.97
INFO:root:2019-05-12 11:50:25, Epoch : 1, Step : 7482, Training Loss : 0.20362, Training Acc : 0.911, Run Time : 0.62
INFO:root:2019-05-12 11:50:26, Epoch : 1, Step : 7483, Training Loss : 0.22133, Training Acc : 0.906, Run Time : 0.87
INFO:root:2019-05-12 11:50:37, Epoch : 1, Step : 7484, Training Loss : 0.26014, Training Acc : 0.900, Run Time : 11.07
INFO:root:2019-05-12 11:50:38, Epoch : 1, Step : 7485, Training Loss : 0.21870, Training Acc : 0.906, Run Time : 1.07
INFO:root:2019-05-12 11:50:38, Epoch : 1, Step : 7486, Training Loss : 0.15799, Training Acc : 0.944, Run Time : 0.61
INFO:root:2019-05-12 11:50:46, Epoch : 1, Step : 7487, Training Loss : 0.32774, Training Acc : 0.828, Run Time : 7.69
INFO:root:2019-05-12 11:50:47, Epoch : 1, Step : 7488, Training Loss : 0.20103, Training Acc : 0.933, Run Time : 1.29
INFO:root:2019-05-12 11:50:49, Epoch : 1, Step : 7489, Training Loss : 0.16521, Training Acc : 0.917, Run Time : 1.97
INFO:root:2019-05-12 11:51:00, Epoch : 1, Step : 7490, Training Loss : 0.26950, Training Acc : 0.878, Run Time : 10.46
INFO:root:2019-05-12 11:51:00, Epoch : 1, Step : 7491, Training Loss : 0.25834, Training Acc : 0.889, Run Time : 0.40
INFO:root:2019-05-12 11:51:02, Epoch : 1, Step : 7492, Training Loss : 0.20406, Training Acc : 0.928, Run Time : 1.63
INFO:root:2019-05-12 11:51:14, Epoch : 1, Step : 7493, Training Loss : 0.14664, Training Acc : 0.950, Run Time : 12.00
INFO:root:2019-05-12 11:51:14, Epoch : 1, Step : 7494, Training Loss : 0.25114, Training Acc : 0.883, Run Time : 0.53
INFO:root:2019-05-12 11:51:22, Epoch : 1, Step : 7495, Training Loss : 0.14065, Training Acc : 0.950, Run Time : 7.62
INFO:root:2019-05-12 11:51:25, Epoch : 1, Step : 7496, Training Loss : 0.13383, Training Acc : 0.961, Run Time : 2.69
INFO:root:2019-05-12 11:51:25, Epoch : 1, Step : 7497, Training Loss : 0.09942, Training Acc : 0.972, Run Time : 0.54
INFO:root:2019-05-12 11:51:31, Epoch : 1, Step : 7498, Training Loss : 0.16166, Training Acc : 0.939, Run Time : 5.66
INFO:root:2019-05-12 11:51:32, Epoch : 1, Step : 7499, Training Loss : 0.20284, Training Acc : 0.917, Run Time : 0.90
INFO:root:2019-05-12 11:51:32, Epoch : 1, Step : 7500, Training Loss : 0.13842, Training Acc : 0.928, Run Time : 0.59
INFO:root:2019-05-12 11:51:43, Epoch : 1, Step : 7501, Training Loss : 0.16725, Training Acc : 0.950, Run Time : 10.34
INFO:root:2019-05-12 11:51:43, Epoch : 1, Step : 7502, Training Loss : 0.20000, Training Acc : 0.928, Run Time : 0.43
INFO:root:2019-05-12 11:51:44, Epoch : 1, Step : 7503, Training Loss : 0.17442, Training Acc : 0.944, Run Time : 0.71
INFO:root:2019-05-12 11:51:55, Epoch : 1, Step : 7504, Training Loss : 0.27545, Training Acc : 0.889, Run Time : 11.15
INFO:root:2019-05-12 11:51:56, Epoch : 1, Step : 7505, Training Loss : 0.28387, Training Acc : 0.894, Run Time : 1.09
INFO:root:2019-05-12 11:52:07, Epoch : 1, Step : 7506, Training Loss : 0.18982, Training Acc : 0.928, Run Time : 11.17
INFO:root:2019-05-12 11:52:08, Epoch : 1, Step : 7507, Training Loss : 0.16770, Training Acc : 0.928, Run Time : 0.96
INFO:root:2019-05-12 11:52:09, Epoch : 1, Step : 7508, Training Loss : 0.06273, Training Acc : 0.983, Run Time : 0.58
INFO:root:2019-05-12 11:52:10, Epoch : 1, Step : 7509, Training Loss : 0.10002, Training Acc : 0.961, Run Time : 0.76
INFO:root:2019-05-12 11:52:19, Epoch : 1, Step : 7510, Training Loss : 0.26850, Training Acc : 0.894, Run Time : 9.84
INFO:root:2019-05-12 11:52:20, Epoch : 1, Step : 7511, Training Loss : 0.19821, Training Acc : 0.928, Run Time : 0.61
INFO:root:2019-05-12 11:52:21, Epoch : 1, Step : 7512, Training Loss : 0.28464, Training Acc : 0.894, Run Time : 0.81
INFO:root:2019-05-12 11:52:32, Epoch : 1, Step : 7513, Training Loss : 0.20032, Training Acc : 0.906, Run Time : 11.46
INFO:root:2019-05-12 11:52:33, Epoch : 1, Step : 7514, Training Loss : 0.16500, Training Acc : 0.933, Run Time : 0.61
INFO:root:2019-05-12 11:52:34, Epoch : 1, Step : 7515, Training Loss : 0.23372, Training Acc : 0.911, Run Time : 0.84
INFO:root:2019-05-12 11:52:44, Epoch : 1, Step : 7516, Training Loss : 0.16411, Training Acc : 0.950, Run Time : 9.91
INFO:root:2019-05-12 11:52:45, Epoch : 1, Step : 7517, Training Loss : 0.25326, Training Acc : 0.906, Run Time : 1.44
INFO:root:2019-05-12 11:52:47, Epoch : 1, Step : 7518, Training Loss : 0.35054, Training Acc : 0.850, Run Time : 1.46
INFO:root:2019-05-12 11:52:56, Epoch : 1, Step : 7519, Training Loss : 0.21309, Training Acc : 0.906, Run Time : 8.92
INFO:root:2019-05-12 11:52:56, Epoch : 1, Step : 7520, Training Loss : 0.26016, Training Acc : 0.911, Run Time : 0.74
INFO:root:2019-05-12 11:52:57, Epoch : 1, Step : 7521, Training Loss : 0.45218, Training Acc : 0.822, Run Time : 0.58
INFO:root:2019-05-12 11:53:11, Epoch : 1, Step : 7522, Training Loss : 0.20009, Training Acc : 0.928, Run Time : 14.10
INFO:root:2019-05-12 11:53:19, Epoch : 1, Step : 7523, Training Loss : 0.15346, Training Acc : 0.944, Run Time : 7.71
INFO:root:2019-05-12 11:53:19, Epoch : 1, Step : 7524, Training Loss : 0.10537, Training Acc : 0.967, Run Time : 0.69
INFO:root:2019-05-12 11:53:20, Epoch : 1, Step : 7525, Training Loss : 0.17116, Training Acc : 0.939, Run Time : 0.55
INFO:root:2019-05-12 11:53:32, Epoch : 1, Step : 7526, Training Loss : 0.06655, Training Acc : 0.989, Run Time : 11.75
INFO:root:2019-05-12 11:53:32, Epoch : 1, Step : 7527, Training Loss : 0.14027, Training Acc : 0.933, Run Time : 0.80
INFO:root:2019-05-12 11:53:44, Epoch : 1, Step : 7528, Training Loss : 0.25258, Training Acc : 0.889, Run Time : 11.93
INFO:root:2019-05-12 11:53:45, Epoch : 1, Step : 7529, Training Loss : 0.21007, Training Acc : 0.900, Run Time : 0.92
INFO:root:2019-05-12 11:53:56, Epoch : 1, Step : 7530, Training Loss : 0.20216, Training Acc : 0.911, Run Time : 10.43
INFO:root:2019-05-12 11:53:56, Epoch : 1, Step : 7531, Training Loss : 0.26410, Training Acc : 0.894, Run Time : 0.76
INFO:root:2019-05-12 11:53:59, Epoch : 1, Step : 7532, Training Loss : 0.40439, Training Acc : 0.839, Run Time : 2.08
INFO:root:2019-05-12 11:54:08, Epoch : 1, Step : 7533, Training Loss : 0.28296, Training Acc : 0.872, Run Time : 9.01
INFO:root:2019-05-12 11:54:08, Epoch : 1, Step : 7534, Training Loss : 0.24468, Training Acc : 0.889, Run Time : 0.67
INFO:root:2019-05-12 11:54:09, Epoch : 1, Step : 7535, Training Loss : 0.35626, Training Acc : 0.856, Run Time : 0.64
INFO:root:2019-05-12 11:54:09, Epoch : 1, Step : 7536, Training Loss : 0.20423, Training Acc : 0.900, Run Time : 0.59
INFO:root:2019-05-12 11:54:18, Epoch : 1, Step : 7537, Training Loss : 0.26457, Training Acc : 0.878, Run Time : 8.13
INFO:root:2019-05-12 11:54:18, Epoch : 1, Step : 7538, Training Loss : 0.11635, Training Acc : 0.967, Run Time : 0.84
INFO:root:2019-05-12 11:54:28, Epoch : 1, Step : 7539, Training Loss : 0.24424, Training Acc : 0.900, Run Time : 9.68
INFO:root:2019-05-12 11:54:29, Epoch : 1, Step : 7540, Training Loss : 0.12458, Training Acc : 0.950, Run Time : 0.94
INFO:root:2019-05-12 11:54:41, Epoch : 1, Step : 7541, Training Loss : 0.18914, Training Acc : 0.917, Run Time : 11.70
INFO:root:2019-05-12 11:54:41, Epoch : 1, Step : 7542, Training Loss : 0.35361, Training Acc : 0.856, Run Time : 0.62
INFO:root:2019-05-12 11:54:42, Epoch : 1, Step : 7543, Training Loss : 0.51090, Training Acc : 0.783, Run Time : 0.55
INFO:root:2019-05-12 11:54:43, Epoch : 1, Step : 7544, Training Loss : 0.31652, Training Acc : 0.889, Run Time : 1.46
INFO:root:2019-05-12 11:54:53, Epoch : 1, Step : 7545, Training Loss : 0.41847, Training Acc : 0.800, Run Time : 9.36
INFO:root:2019-05-12 11:54:53, Epoch : 1, Step : 7546, Training Loss : 0.68561, Training Acc : 0.761, Run Time : 0.60
INFO:root:2019-05-12 11:54:55, Epoch : 1, Step : 7547, Training Loss : 0.71888, Training Acc : 0.717, Run Time : 1.65
INFO:root:2019-05-12 11:55:03, Epoch : 1, Step : 7548, Training Loss : 0.17539, Training Acc : 0.911, Run Time : 7.92
INFO:root:2019-05-12 11:55:03, Epoch : 1, Step : 7549, Training Loss : 0.26817, Training Acc : 0.872, Run Time : 0.48
INFO:root:2019-05-12 11:55:05, Epoch : 1, Step : 7550, Training Loss : 0.28187, Training Acc : 0.889, Run Time : 1.82
INFO:root:2019-05-12 11:55:16, Epoch : 1, Step : 7551, Training Loss : 0.32630, Training Acc : 0.861, Run Time : 10.57
INFO:root:2019-05-12 11:55:17, Epoch : 1, Step : 7552, Training Loss : 0.25211, Training Acc : 0.883, Run Time : 0.78
INFO:root:2019-05-12 11:55:18, Epoch : 1, Step : 7553, Training Loss : 0.38377, Training Acc : 0.822, Run Time : 1.49
INFO:root:2019-05-12 11:55:28, Epoch : 1, Step : 7554, Training Loss : 0.53266, Training Acc : 0.811, Run Time : 10.39
INFO:root:2019-05-12 11:55:29, Epoch : 1, Step : 7555, Training Loss : 0.17261, Training Acc : 0.939, Run Time : 0.47
INFO:root:2019-05-12 11:55:30, Epoch : 1, Step : 7556, Training Loss : 0.22383, Training Acc : 0.894, Run Time : 0.57
INFO:root:2019-05-12 11:55:31, Epoch : 1, Step : 7557, Training Loss : 0.07025, Training Acc : 0.983, Run Time : 1.28
INFO:root:2019-05-12 11:55:43, Epoch : 1, Step : 7558, Training Loss : 0.17683, Training Acc : 0.933, Run Time : 12.12
INFO:root:2019-05-12 11:55:44, Epoch : 1, Step : 7559, Training Loss : 0.14353, Training Acc : 0.961, Run Time : 1.18
INFO:root:2019-05-12 11:55:55, Epoch : 1, Step : 7560, Training Loss : 0.18621, Training Acc : 0.911, Run Time : 11.21
INFO:root:2019-05-12 11:55:56, Epoch : 1, Step : 7561, Training Loss : 0.18302, Training Acc : 0.933, Run Time : 1.00
INFO:root:2019-05-12 11:55:57, Epoch : 1, Step : 7562, Training Loss : 0.05768, Training Acc : 0.978, Run Time : 0.61
INFO:root:2019-05-12 11:56:09, Epoch : 1, Step : 7563, Training Loss : 0.27109, Training Acc : 0.928, Run Time : 11.63
INFO:root:2019-05-12 11:56:09, Epoch : 1, Step : 7564, Training Loss : 0.06934, Training Acc : 0.978, Run Time : 0.53
INFO:root:2019-05-12 11:56:12, Epoch : 1, Step : 7565, Training Loss : 0.08901, Training Acc : 0.967, Run Time : 3.15
INFO:root:2019-05-12 11:56:22, Epoch : 1, Step : 7566, Training Loss : 0.15511, Training Acc : 0.922, Run Time : 9.81
INFO:root:2019-05-12 11:56:22, Epoch : 1, Step : 7567, Training Loss : 0.20263, Training Acc : 0.933, Run Time : 0.44
INFO:root:2019-05-12 11:56:33, Epoch : 1, Step : 7568, Training Loss : 0.14270, Training Acc : 0.933, Run Time : 10.44
INFO:root:2019-05-12 11:56:34, Epoch : 1, Step : 7569, Training Loss : 0.24071, Training Acc : 0.900, Run Time : 0.81
INFO:root:2019-05-12 11:56:34, Epoch : 1, Step : 7570, Training Loss : 0.57157, Training Acc : 0.783, Run Time : 0.62
INFO:root:2019-05-12 11:56:44, Epoch : 1, Step : 7571, Training Loss : 0.55158, Training Acc : 0.811, Run Time : 9.68
INFO:root:2019-05-12 11:56:45, Epoch : 1, Step : 7572, Training Loss : 0.41049, Training Acc : 0.856, Run Time : 0.68
INFO:root:2019-05-12 11:56:47, Epoch : 1, Step : 7573, Training Loss : 0.13954, Training Acc : 0.950, Run Time : 1.99
INFO:root:2019-05-12 11:56:55, Epoch : 1, Step : 7574, Training Loss : 0.13714, Training Acc : 0.944, Run Time : 8.73
INFO:root:2019-05-12 11:56:56, Epoch : 1, Step : 7575, Training Loss : 0.16655, Training Acc : 0.933, Run Time : 0.68
INFO:root:2019-05-12 11:56:57, Epoch : 1, Step : 7576, Training Loss : 0.16989, Training Acc : 0.939, Run Time : 0.66
INFO:root:2019-05-12 11:57:10, Epoch : 1, Step : 7577, Training Loss : 0.14267, Training Acc : 0.944, Run Time : 13.48
INFO:root:2019-05-12 11:57:11, Epoch : 1, Step : 7578, Training Loss : 0.21300, Training Acc : 0.894, Run Time : 1.08
INFO:root:2019-05-12 11:57:23, Epoch : 1, Step : 7579, Training Loss : 0.26851, Training Acc : 0.894, Run Time : 11.54
INFO:root:2019-05-12 11:57:24, Epoch : 1, Step : 7580, Training Loss : 0.16521, Training Acc : 0.944, Run Time : 1.55
INFO:root:2019-05-12 11:57:34, Epoch : 1, Step : 7581, Training Loss : 0.22312, Training Acc : 0.878, Run Time : 9.35
INFO:root:2019-05-12 11:57:35, Epoch : 1, Step : 7582, Training Loss : 0.13704, Training Acc : 0.939, Run Time : 0.80
INFO:root:2019-05-12 11:57:47, Epoch : 1, Step : 7583, Training Loss : 0.18029, Training Acc : 0.928, Run Time : 12.20
INFO:root:2019-05-12 11:57:47, Epoch : 1, Step : 7584, Training Loss : 0.12358, Training Acc : 0.950, Run Time : 0.62
INFO:root:2019-05-12 11:57:49, Epoch : 1, Step : 7585, Training Loss : 0.12473, Training Acc : 0.961, Run Time : 1.51
INFO:root:2019-05-12 11:57:58, Epoch : 1, Step : 7586, Training Loss : 0.13767, Training Acc : 0.939, Run Time : 8.92
INFO:root:2019-05-12 11:57:58, Epoch : 1, Step : 7587, Training Loss : 0.15529, Training Acc : 0.933, Run Time : 0.49
INFO:root:2019-05-12 11:58:00, Epoch : 1, Step : 7588, Training Loss : 0.36378, Training Acc : 0.856, Run Time : 1.91
INFO:root:2019-05-12 11:58:10, Epoch : 1, Step : 7589, Training Loss : 0.16646, Training Acc : 0.922, Run Time : 10.10
INFO:root:2019-05-12 11:58:12, Epoch : 1, Step : 7590, Training Loss : 0.28416, Training Acc : 0.889, Run Time : 2.04
INFO:root:2019-05-12 11:58:25, Epoch : 1, Step : 7591, Training Loss : 0.32440, Training Acc : 0.872, Run Time : 12.56
INFO:root:2019-05-12 11:58:26, Epoch : 1, Step : 7592, Training Loss : 0.58485, Training Acc : 0.728, Run Time : 0.58
INFO:root:2019-05-12 11:58:27, Epoch : 1, Step : 7593, Training Loss : 0.32252, Training Acc : 0.844, Run Time : 1.78
INFO:root:2019-05-12 11:58:38, Epoch : 1, Step : 7594, Training Loss : 0.33328, Training Acc : 0.867, Run Time : 10.79
INFO:root:2019-05-12 11:58:39, Epoch : 1, Step : 7595, Training Loss : 0.56276, Training Acc : 0.783, Run Time : 0.41
INFO:root:2019-05-12 11:58:40, Epoch : 1, Step : 7596, Training Loss : 0.27505, Training Acc : 0.867, Run Time : 1.72
INFO:root:2019-05-12 11:58:50, Epoch : 1, Step : 7597, Training Loss : 0.29977, Training Acc : 0.844, Run Time : 9.57
INFO:root:2019-05-12 11:58:50, Epoch : 1, Step : 7598, Training Loss : 0.25954, Training Acc : 0.906, Run Time : 0.69
INFO:root:2019-05-12 11:58:52, Epoch : 1, Step : 7599, Training Loss : 0.48648, Training Acc : 0.783, Run Time : 1.27
INFO:root:2019-05-12 11:59:03, Epoch : 1, Step : 7600, Training Loss : 0.62342, Training Acc : 0.717, Run Time : 10.81
INFO:root:2019-05-12 11:59:06, Epoch : 1, Step : 7601, Training Loss : 0.34241, Training Acc : 0.822, Run Time : 2.94
INFO:root:2019-05-12 11:59:15, Epoch : 1, Step : 7602, Training Loss : 0.32519, Training Acc : 0.850, Run Time : 9.74
INFO:root:2019-05-12 11:59:16, Epoch : 1, Step : 7603, Training Loss : 0.44325, Training Acc : 0.806, Run Time : 0.42
INFO:root:2019-05-12 11:59:16, Epoch : 1, Step : 7604, Training Loss : 0.32081, Training Acc : 0.872, Run Time : 0.42
INFO:root:2019-05-12 11:59:17, Epoch : 1, Step : 7605, Training Loss : 0.64638, Training Acc : 0.767, Run Time : 1.30
INFO:root:2019-05-12 11:59:28, Epoch : 1, Step : 7606, Training Loss : 0.53711, Training Acc : 0.794, Run Time : 10.70
INFO:root:2019-05-12 11:59:29, Epoch : 1, Step : 7607, Training Loss : 0.33069, Training Acc : 0.844, Run Time : 0.83
INFO:root:2019-05-12 11:59:32, Epoch : 1, Step : 7608, Training Loss : 0.22215, Training Acc : 0.900, Run Time : 2.89
INFO:root:2019-05-12 11:59:44, Epoch : 1, Step : 7609, Training Loss : 0.18394, Training Acc : 0.933, Run Time : 12.10
INFO:root:2019-05-12 11:59:45, Epoch : 1, Step : 7610, Training Loss : 0.22525, Training Acc : 0.900, Run Time : 1.30
INFO:root:2019-05-12 11:59:51, Epoch : 1, Step : 7611, Training Loss : 0.40225, Training Acc : 0.850, Run Time : 5.29
INFO:root:2019-05-12 11:59:51, Epoch : 1, Step : 7612, Training Loss : 0.46331, Training Acc : 0.806, Run Time : 0.75
INFO:root:2019-05-12 11:59:52, Epoch : 1, Step : 7613, Training Loss : 0.44772, Training Acc : 0.772, Run Time : 0.67
INFO:root:2019-05-12 11:59:56, Epoch : 1, Step : 7614, Training Loss : 0.35288, Training Acc : 0.839, Run Time : 4.06
INFO:root:2019-05-12 11:59:58, Epoch : 1, Step : 7615, Training Loss : 0.24117, Training Acc : 0.906, Run Time : 2.28
INFO:root:2019-05-12 11:59:59, Epoch : 1, Step : 7616, Training Loss : 0.18108, Training Acc : 0.928, Run Time : 0.59
INFO:root:2019-05-12 11:59:59, Epoch : 1, Step : 7617, Training Loss : 0.16494, Training Acc : 0.956, Run Time : 0.59
INFO:root:2019-05-12 12:00:10, Epoch : 1, Step : 7618, Training Loss : 0.25698, Training Acc : 0.917, Run Time : 10.66
INFO:root:2019-05-12 12:00:11, Epoch : 1, Step : 7619, Training Loss : 0.27103, Training Acc : 0.867, Run Time : 0.74
INFO:root:2019-05-12 12:00:21, Epoch : 1, Step : 7620, Training Loss : 0.35922, Training Acc : 0.850, Run Time : 10.53
INFO:root:2019-05-12 12:00:22, Epoch : 1, Step : 7621, Training Loss : 0.50821, Training Acc : 0.783, Run Time : 0.85
INFO:root:2019-05-12 12:00:23, Epoch : 1, Step : 7622, Training Loss : 0.37938, Training Acc : 0.822, Run Time : 1.13
INFO:root:2019-05-12 12:00:24, Epoch : 1, Step : 7623, Training Loss : 0.33103, Training Acc : 0.856, Run Time : 0.73
INFO:root:2019-05-12 12:00:34, Epoch : 1, Step : 7624, Training Loss : 0.24592, Training Acc : 0.906, Run Time : 10.32
INFO:root:2019-05-12 12:00:35, Epoch : 1, Step : 7625, Training Loss : 0.20943, Training Acc : 0.911, Run Time : 0.49
INFO:root:2019-05-12 12:00:36, Epoch : 1, Step : 7626, Training Loss : 0.23883, Training Acc : 0.894, Run Time : 0.72
INFO:root:2019-05-12 12:00:45, Epoch : 1, Step : 7627, Training Loss : 0.17082, Training Acc : 0.933, Run Time : 9.19
INFO:root:2019-05-12 12:00:46, Epoch : 1, Step : 7628, Training Loss : 0.24825, Training Acc : 0.894, Run Time : 1.34
INFO:root:2019-05-12 12:00:47, Epoch : 1, Step : 7629, Training Loss : 0.17516, Training Acc : 0.933, Run Time : 1.14
INFO:root:2019-05-12 12:00:56, Epoch : 1, Step : 7630, Training Loss : 0.23277, Training Acc : 0.917, Run Time : 8.73
INFO:root:2019-05-12 12:00:57, Epoch : 1, Step : 7631, Training Loss : 0.19435, Training Acc : 0.939, Run Time : 1.19
INFO:root:2019-05-12 12:01:07, Epoch : 1, Step : 7632, Training Loss : 0.32068, Training Acc : 0.844, Run Time : 9.86
INFO:root:2019-05-12 12:01:08, Epoch : 1, Step : 7633, Training Loss : 0.33179, Training Acc : 0.856, Run Time : 0.96
INFO:root:2019-05-12 12:01:09, Epoch : 1, Step : 7634, Training Loss : 0.35515, Training Acc : 0.811, Run Time : 0.97
INFO:root:2019-05-12 12:01:19, Epoch : 1, Step : 7635, Training Loss : 0.40020, Training Acc : 0.850, Run Time : 9.60
INFO:root:2019-05-12 12:01:19, Epoch : 1, Step : 7636, Training Loss : 0.53423, Training Acc : 0.828, Run Time : 0.45
INFO:root:2019-05-12 12:01:20, Epoch : 1, Step : 7637, Training Loss : 0.17154, Training Acc : 0.961, Run Time : 0.62
INFO:root:2019-05-12 12:01:29, Epoch : 1, Step : 7638, Training Loss : 0.14064, Training Acc : 0.956, Run Time : 9.53
INFO:root:2019-05-12 12:01:30, Epoch : 1, Step : 7639, Training Loss : 0.17572, Training Acc : 0.928, Run Time : 0.90
INFO:root:2019-05-12 12:01:31, Epoch : 1, Step : 7640, Training Loss : 0.13083, Training Acc : 0.972, Run Time : 0.64
INFO:root:2019-05-12 12:01:41, Epoch : 1, Step : 7641, Training Loss : 0.27684, Training Acc : 0.889, Run Time : 10.63
INFO:root:2019-05-12 12:01:42, Epoch : 1, Step : 7642, Training Loss : 0.31478, Training Acc : 0.889, Run Time : 0.75
INFO:root:2019-05-12 12:01:44, Epoch : 1, Step : 7643, Training Loss : 0.27818, Training Acc : 0.878, Run Time : 1.41
INFO:root:2019-05-12 12:01:54, Epoch : 1, Step : 7644, Training Loss : 0.30710, Training Acc : 0.900, Run Time : 10.52
INFO:root:2019-05-12 12:01:55, Epoch : 1, Step : 7645, Training Loss : 0.06249, Training Acc : 0.989, Run Time : 0.92
INFO:root:2019-05-12 12:02:05, Epoch : 1, Step : 7646, Training Loss : 0.15963, Training Acc : 0.939, Run Time : 10.31
INFO:root:2019-05-12 12:02:06, Epoch : 1, Step : 7647, Training Loss : 0.11603, Training Acc : 0.961, Run Time : 0.59
INFO:root:2019-05-12 12:02:06, Epoch : 1, Step : 7648, Training Loss : 0.41409, Training Acc : 0.839, Run Time : 0.41
INFO:root:2019-05-12 12:02:17, Epoch : 1, Step : 7649, Training Loss : 0.24833, Training Acc : 0.894, Run Time : 10.99
INFO:root:2019-05-12 12:02:18, Epoch : 1, Step : 7650, Training Loss : 0.47892, Training Acc : 0.817, Run Time : 0.65
INFO:root:2019-05-12 12:02:19, Epoch : 1, Step : 7651, Training Loss : 0.20631, Training Acc : 0.928, Run Time : 1.48
INFO:root:2019-05-12 12:02:28, Epoch : 1, Step : 7652, Training Loss : 0.48857, Training Acc : 0.811, Run Time : 8.59
INFO:root:2019-05-12 12:02:28, Epoch : 1, Step : 7653, Training Loss : 0.31740, Training Acc : 0.856, Run Time : 0.44
INFO:root:2019-05-12 12:02:30, Epoch : 1, Step : 7654, Training Loss : 0.37429, Training Acc : 0.844, Run Time : 1.24
INFO:root:2019-05-12 12:02:39, Epoch : 1, Step : 7655, Training Loss : 0.37915, Training Acc : 0.833, Run Time : 9.11
INFO:root:2019-05-12 12:02:39, Epoch : 1, Step : 7656, Training Loss : 0.30459, Training Acc : 0.883, Run Time : 0.47
INFO:root:2019-05-12 12:02:40, Epoch : 1, Step : 7657, Training Loss : 0.20954, Training Acc : 0.900, Run Time : 0.72
INFO:root:2019-05-12 12:02:57, Epoch : 1, Step : 7658, Training Loss : 0.19815, Training Acc : 0.906, Run Time : 16.95
INFO:root:2019-05-12 12:03:03, Epoch : 1, Step : 7659, Training Loss : 0.46495, Training Acc : 0.844, Run Time : 5.83
INFO:root:2019-05-12 12:03:03, Epoch : 1, Step : 7660, Training Loss : 0.54204, Training Acc : 0.856, Run Time : 0.66
INFO:root:2019-05-12 12:03:05, Epoch : 1, Step : 7661, Training Loss : 0.74923, Training Acc : 0.767, Run Time : 1.51
INFO:root:2019-05-12 12:03:11, Epoch : 1, Step : 7662, Training Loss : 0.87596, Training Acc : 0.722, Run Time : 6.12
INFO:root:2019-05-12 12:03:20, Epoch : 1, Step : 7663, Training Loss : 0.89119, Training Acc : 0.744, Run Time : 9.43
INFO:root:2019-05-12 12:03:21, Epoch : 1, Step : 7664, Training Loss : 0.70673, Training Acc : 0.817, Run Time : 0.81
INFO:root:2019-05-12 12:03:23, Epoch : 1, Step : 7665, Training Loss : 0.58523, Training Acc : 0.833, Run Time : 1.79
INFO:root:2019-05-12 12:03:33, Epoch : 1, Step : 7666, Training Loss : 0.19272, Training Acc : 0.944, Run Time : 9.94
INFO:root:2019-05-12 12:03:34, Epoch : 1, Step : 7667, Training Loss : 0.29534, Training Acc : 0.850, Run Time : 0.59
INFO:root:2019-05-12 12:03:35, Epoch : 1, Step : 7668, Training Loss : 0.21327, Training Acc : 0.911, Run Time : 0.97
INFO:root:2019-05-12 12:03:44, Epoch : 1, Step : 7669, Training Loss : 0.23055, Training Acc : 0.922, Run Time : 9.34
INFO:root:2019-05-12 12:03:45, Epoch : 1, Step : 7670, Training Loss : 0.25254, Training Acc : 0.900, Run Time : 0.97
INFO:root:2019-05-12 12:03:46, Epoch : 1, Step : 7671, Training Loss : 0.25117, Training Acc : 0.883, Run Time : 1.33
INFO:root:2019-05-12 12:03:57, Epoch : 1, Step : 7672, Training Loss : 0.23302, Training Acc : 0.906, Run Time : 10.34
INFO:root:2019-05-12 12:04:08, Epoch : 1, Step : 7673, Training Loss : 0.40411, Training Acc : 0.789, Run Time : 11.64
INFO:root:2019-05-12 12:04:13, Epoch : 1, Step : 7674, Training Loss : 0.22106, Training Acc : 0.900, Run Time : 4.67
INFO:root:2019-05-12 12:04:21, Epoch : 1, Step : 7675, Training Loss : 0.29584, Training Acc : 0.844, Run Time : 8.10
INFO:root:2019-05-12 12:04:22, Epoch : 1, Step : 7676, Training Loss : 0.38963, Training Acc : 0.822, Run Time : 1.44
INFO:root:2019-05-12 12:04:23, Epoch : 1, Step : 7677, Training Loss : 0.25312, Training Acc : 0.917, Run Time : 0.75
INFO:root:2019-05-12 12:04:33, Epoch : 1, Step : 7678, Training Loss : 0.30805, Training Acc : 0.861, Run Time : 9.90
INFO:root:2019-05-12 12:04:33, Epoch : 1, Step : 7679, Training Loss : 0.24222, Training Acc : 0.917, Run Time : 0.49
INFO:root:2019-05-12 12:04:42, Epoch : 1, Step : 7680, Training Loss : 0.23286, Training Acc : 0.906, Run Time : 8.87
INFO:root:2019-05-12 12:04:44, Epoch : 1, Step : 7681, Training Loss : 0.14935, Training Acc : 0.944, Run Time : 1.19
INFO:root:2019-05-12 12:04:44, Epoch : 1, Step : 7682, Training Loss : 0.14242, Training Acc : 0.956, Run Time : 0.59
INFO:root:2019-05-12 12:04:56, Epoch : 1, Step : 7683, Training Loss : 0.17738, Training Acc : 0.939, Run Time : 12.10
INFO:root:2019-05-12 12:04:57, Epoch : 1, Step : 7684, Training Loss : 0.19854, Training Acc : 0.917, Run Time : 1.02
INFO:root:2019-05-12 12:05:12, Epoch : 1, Step : 7685, Training Loss : 0.14392, Training Acc : 0.956, Run Time : 14.23
INFO:root:2019-05-12 12:05:13, Epoch : 1, Step : 7686, Training Loss : 0.26952, Training Acc : 0.917, Run Time : 1.43
INFO:root:2019-05-12 12:05:22, Epoch : 1, Step : 7687, Training Loss : 0.24320, Training Acc : 0.878, Run Time : 9.48
INFO:root:2019-05-12 12:05:24, Epoch : 1, Step : 7688, Training Loss : 0.18140, Training Acc : 0.933, Run Time : 1.20
INFO:root:2019-05-12 12:05:25, Epoch : 1, Step : 7689, Training Loss : 0.13679, Training Acc : 0.950, Run Time : 1.29
INFO:root:2019-05-12 12:05:34, Epoch : 1, Step : 7690, Training Loss : 0.20742, Training Acc : 0.933, Run Time : 8.96
INFO:root:2019-05-12 12:05:34, Epoch : 1, Step : 7691, Training Loss : 0.37221, Training Acc : 0.833, Run Time : 0.52
INFO:root:2019-05-12 12:05:35, Epoch : 1, Step : 7692, Training Loss : 0.29374, Training Acc : 0.850, Run Time : 0.64
INFO:root:2019-05-12 12:05:52, Epoch : 1, Step : 7693, Training Loss : 0.25156, Training Acc : 0.906, Run Time : 16.89
INFO:root:2019-05-12 12:06:00, Epoch : 1, Step : 7694, Training Loss : 0.24261, Training Acc : 0.878, Run Time : 8.33
INFO:root:2019-05-12 12:06:02, Epoch : 1, Step : 7695, Training Loss : 0.43810, Training Acc : 0.839, Run Time : 1.61
INFO:root:2019-05-12 12:06:04, Epoch : 1, Step : 7696, Training Loss : 0.26008, Training Acc : 0.883, Run Time : 2.15
INFO:root:2019-05-12 12:06:05, Epoch : 1, Step : 7697, Training Loss : 0.24071, Training Acc : 0.917, Run Time : 1.00
INFO:root:2019-05-12 12:06:06, Epoch : 1, Step : 7698, Training Loss : 0.10224, Training Acc : 0.972, Run Time : 0.61
INFO:root:2019-05-12 12:06:17, Epoch : 1, Step : 7699, Training Loss : 0.20838, Training Acc : 0.933, Run Time : 11.19
INFO:root:2019-05-12 12:06:27, Epoch : 1, Step : 7700, Training Loss : 0.27494, Training Acc : 0.894, Run Time : 10.43
INFO:root:2019-05-12 12:06:29, Epoch : 1, Step : 7701, Training Loss : 0.16045, Training Acc : 0.956, Run Time : 1.61
INFO:root:2019-05-12 12:06:29, Epoch : 1, Step : 7702, Training Loss : 0.33628, Training Acc : 0.844, Run Time : 0.59
INFO:root:2019-05-12 12:06:39, Epoch : 1, Step : 7703, Training Loss : 0.12621, Training Acc : 0.944, Run Time : 9.57
INFO:root:2019-05-12 12:06:40, Epoch : 1, Step : 7704, Training Loss : 0.12972, Training Acc : 0.950, Run Time : 0.84
INFO:root:2019-05-12 12:06:40, Epoch : 1, Step : 7705, Training Loss : 0.08587, Training Acc : 0.967, Run Time : 0.57
INFO:root:2019-05-12 12:06:52, Epoch : 1, Step : 7706, Training Loss : 0.12778, Training Acc : 0.950, Run Time : 12.04
INFO:root:2019-05-12 12:06:53, Epoch : 1, Step : 7707, Training Loss : 0.06265, Training Acc : 0.994, Run Time : 0.82
INFO:root:2019-05-12 12:06:54, Epoch : 1, Step : 7708, Training Loss : 0.11053, Training Acc : 0.956, Run Time : 0.59
INFO:root:2019-05-12 12:06:55, Epoch : 1, Step : 7709, Training Loss : 0.08831, Training Acc : 0.978, Run Time : 1.16
INFO:root:2019-05-12 12:07:03, Epoch : 1, Step : 7710, Training Loss : 0.13622, Training Acc : 0.950, Run Time : 7.70
INFO:root:2019-05-12 12:07:04, Epoch : 1, Step : 7711, Training Loss : 0.18069, Training Acc : 0.922, Run Time : 1.17
INFO:root:2019-05-12 12:07:14, Epoch : 1, Step : 7712, Training Loss : 0.57226, Training Acc : 0.772, Run Time : 10.08
INFO:root:2019-05-12 12:07:15, Epoch : 1, Step : 7713, Training Loss : 0.81537, Training Acc : 0.661, Run Time : 1.05
INFO:root:2019-05-12 12:07:16, Epoch : 1, Step : 7714, Training Loss : 0.71146, Training Acc : 0.672, Run Time : 0.81
INFO:root:2019-05-12 12:07:26, Epoch : 1, Step : 7715, Training Loss : 0.21181, Training Acc : 0.911, Run Time : 10.69
INFO:root:2019-05-12 12:07:28, Epoch : 1, Step : 7716, Training Loss : 0.16212, Training Acc : 0.956, Run Time : 1.65
INFO:root:2019-05-12 12:07:36, Epoch : 1, Step : 7717, Training Loss : 0.31067, Training Acc : 0.850, Run Time : 7.74
INFO:root:2019-05-12 12:07:41, Epoch : 1, Step : 7718, Training Loss : 0.36270, Training Acc : 0.844, Run Time : 4.95
INFO:root:2019-05-12 12:07:41, Epoch : 1, Step : 7719, Training Loss : 0.33625, Training Acc : 0.839, Run Time : 0.54
INFO:root:2019-05-12 12:07:51, Epoch : 1, Step : 7720, Training Loss : 0.39528, Training Acc : 0.811, Run Time : 9.46
INFO:root:2019-05-12 12:07:51, Epoch : 1, Step : 7721, Training Loss : 0.22634, Training Acc : 0.894, Run Time : 0.49
INFO:root:2019-05-12 12:07:52, Epoch : 1, Step : 7722, Training Loss : 0.17549, Training Acc : 0.933, Run Time : 0.54
INFO:root:2019-05-12 12:08:03, Epoch : 1, Step : 7723, Training Loss : 0.18223, Training Acc : 0.917, Run Time : 11.10
INFO:root:2019-05-12 12:08:03, Epoch : 1, Step : 7724, Training Loss : 0.22291, Training Acc : 0.906, Run Time : 0.49
INFO:root:2019-05-12 12:08:04, Epoch : 1, Step : 7725, Training Loss : 0.15245, Training Acc : 0.939, Run Time : 0.61
INFO:root:2019-05-12 12:08:16, Epoch : 1, Step : 7726, Training Loss : 0.15680, Training Acc : 0.950, Run Time : 11.80
INFO:root:2019-05-12 12:08:17, Epoch : 1, Step : 7727, Training Loss : 0.15826, Training Acc : 0.939, Run Time : 1.35
INFO:root:2019-05-12 12:08:28, Epoch : 1, Step : 7728, Training Loss : 0.13971, Training Acc : 0.939, Run Time : 11.28
INFO:root:2019-05-12 12:08:41, Epoch : 1, Step : 7729, Training Loss : 0.18483, Training Acc : 0.900, Run Time : 12.16
INFO:root:2019-05-12 12:08:43, Epoch : 1, Step : 7730, Training Loss : 0.18677, Training Acc : 0.911, Run Time : 1.90
INFO:root:2019-05-12 12:08:52, Epoch : 1, Step : 7731, Training Loss : 0.12828, Training Acc : 0.972, Run Time : 9.87
INFO:root:2019-05-12 12:08:55, Epoch : 1, Step : 7732, Training Loss : 0.12758, Training Acc : 0.950, Run Time : 2.22
INFO:root:2019-05-12 12:09:04, Epoch : 1, Step : 7733, Training Loss : 0.07198, Training Acc : 0.989, Run Time : 9.40
INFO:root:2019-05-12 12:09:06, Epoch : 1, Step : 7734, Training Loss : 0.11388, Training Acc : 0.961, Run Time : 1.62
INFO:root:2019-05-12 12:09:06, Epoch : 1, Step : 7735, Training Loss : 0.13125, Training Acc : 0.944, Run Time : 0.62
INFO:root:2019-05-12 12:09:07, Epoch : 1, Step : 7736, Training Loss : 0.11034, Training Acc : 0.972, Run Time : 0.82
INFO:root:2019-05-12 12:09:21, Epoch : 1, Step : 7737, Training Loss : 0.09722, Training Acc : 0.972, Run Time : 13.95
INFO:root:2019-05-12 12:09:22, Epoch : 1, Step : 7738, Training Loss : 0.17711, Training Acc : 0.933, Run Time : 0.82
INFO:root:2019-05-12 12:09:22, Epoch : 1, Step : 7739, Training Loss : 0.09701, Training Acc : 0.978, Run Time : 0.51
INFO:root:2019-05-12 12:09:23, Epoch : 1, Step : 7740, Training Loss : 0.08292, Training Acc : 0.978, Run Time : 0.81
INFO:root:2019-05-12 12:09:32, Epoch : 1, Step : 7741, Training Loss : 0.08615, Training Acc : 0.983, Run Time : 8.37
INFO:root:2019-05-12 12:09:36, Epoch : 1, Step : 7742, Training Loss : 0.13456, Training Acc : 0.950, Run Time : 4.88
INFO:root:2019-05-12 12:09:37, Epoch : 1, Step : 7743, Training Loss : 0.05005, Training Acc : 1.000, Run Time : 0.45
INFO:root:2019-05-12 12:09:38, Epoch : 1, Step : 7744, Training Loss : 0.04580, Training Acc : 0.989, Run Time : 1.24
INFO:root:2019-05-12 12:09:51, Epoch : 1, Step : 7745, Training Loss : 0.10332, Training Acc : 0.950, Run Time : 12.85
INFO:root:2019-05-12 12:09:52, Epoch : 1, Step : 7746, Training Loss : 0.09735, Training Acc : 0.972, Run Time : 0.79
INFO:root:2019-05-12 12:09:52, Epoch : 1, Step : 7747, Training Loss : 0.06079, Training Acc : 0.983, Run Time : 0.62
INFO:root:2019-05-12 12:10:04, Epoch : 1, Step : 7748, Training Loss : 0.04398, Training Acc : 1.000, Run Time : 11.86
INFO:root:2019-05-12 12:10:06, Epoch : 1, Step : 7749, Training Loss : 0.03880, Training Acc : 1.000, Run Time : 1.59
INFO:root:2019-05-12 12:10:19, Epoch : 1, Step : 7750, Training Loss : 0.16616, Training Acc : 0.933, Run Time : 12.91
INFO:root:2019-05-12 12:10:25, Epoch : 1, Step : 7751, Training Loss : 0.18031, Training Acc : 0.939, Run Time : 6.38
INFO:root:2019-05-12 12:10:34, Epoch : 1, Step : 7752, Training Loss : 0.09648, Training Acc : 0.978, Run Time : 8.66
INFO:root:2019-05-12 12:10:35, Epoch : 1, Step : 7753, Training Loss : 0.17379, Training Acc : 0.939, Run Time : 0.75
INFO:root:2019-05-12 12:10:44, Epoch : 1, Step : 7754, Training Loss : 0.09144, Training Acc : 0.967, Run Time : 9.65
INFO:root:2019-05-12 12:10:45, Epoch : 1, Step : 7755, Training Loss : 0.08835, Training Acc : 0.983, Run Time : 0.57
INFO:root:2019-05-12 12:10:45, Epoch : 1, Step : 7756, Training Loss : 0.05211, Training Acc : 0.989, Run Time : 0.55
INFO:root:2019-05-12 12:10:59, Epoch : 1, Step : 7757, Training Loss : 0.05787, Training Acc : 0.994, Run Time : 13.49
INFO:root:2019-05-12 12:10:59, Epoch : 1, Step : 7758, Training Loss : 0.04528, Training Acc : 1.000, Run Time : 0.46
INFO:root:2019-05-12 12:11:00, Epoch : 1, Step : 7759, Training Loss : 0.04977, Training Acc : 0.994, Run Time : 0.60
INFO:root:2019-05-12 12:11:01, Epoch : 1, Step : 7760, Training Loss : 0.05232, Training Acc : 0.989, Run Time : 1.29
INFO:root:2019-05-12 12:11:10, Epoch : 1, Step : 7761, Training Loss : 0.06990, Training Acc : 0.983, Run Time : 9.16
INFO:root:2019-05-12 12:11:11, Epoch : 1, Step : 7762, Training Loss : 0.08215, Training Acc : 0.978, Run Time : 0.51
INFO:root:2019-05-12 12:11:11, Epoch : 1, Step : 7763, Training Loss : 0.07578, Training Acc : 0.983, Run Time : 0.59
INFO:root:2019-05-12 12:11:22, Epoch : 1, Step : 7764, Training Loss : 0.09363, Training Acc : 0.978, Run Time : 10.85
INFO:root:2019-05-12 12:11:23, Epoch : 1, Step : 7765, Training Loss : 0.11591, Training Acc : 0.972, Run Time : 0.71
INFO:root:2019-05-12 12:11:24, Epoch : 1, Step : 7766, Training Loss : 0.08487, Training Acc : 0.978, Run Time : 1.36
INFO:root:2019-05-12 12:11:32, Epoch : 1, Step : 7767, Training Loss : 0.10020, Training Acc : 0.967, Run Time : 7.87
INFO:root:2019-05-12 12:11:33, Epoch : 1, Step : 7768, Training Loss : 0.16745, Training Acc : 0.944, Run Time : 0.74
INFO:root:2019-05-12 12:11:40, Epoch : 1, Step : 7769, Training Loss : 0.05650, Training Acc : 1.000, Run Time : 7.16
INFO:root:2019-05-12 12:11:41, Epoch : 1, Step : 7770, Training Loss : 0.07462, Training Acc : 0.978, Run Time : 0.75
INFO:root:2019-05-12 12:11:41, Epoch : 1, Step : 7771, Training Loss : 0.21703, Training Acc : 0.906, Run Time : 0.62
INFO:root:2019-05-12 12:11:51, Epoch : 1, Step : 7772, Training Loss : 0.19123, Training Acc : 0.922, Run Time : 9.21
INFO:root:2019-05-12 12:11:51, Epoch : 1, Step : 7773, Training Loss : 0.20680, Training Acc : 0.911, Run Time : 0.74
INFO:root:2019-05-12 12:11:53, Epoch : 1, Step : 7774, Training Loss : 0.15631, Training Acc : 0.933, Run Time : 1.75
INFO:root:2019-05-12 12:12:06, Epoch : 1, Step : 7775, Training Loss : 0.12697, Training Acc : 0.950, Run Time : 12.74
INFO:root:2019-05-12 12:12:07, Epoch : 1, Step : 7776, Training Loss : 0.22921, Training Acc : 0.922, Run Time : 0.78
INFO:root:2019-05-12 12:12:07, Epoch : 1, Step : 7777, Training Loss : 0.17689, Training Acc : 0.939, Run Time : 0.62
INFO:root:2019-05-12 12:12:18, Epoch : 1, Step : 7778, Training Loss : 0.15460, Training Acc : 0.933, Run Time : 10.47
INFO:root:2019-05-12 12:12:18, Epoch : 1, Step : 7779, Training Loss : 0.14842, Training Acc : 0.939, Run Time : 0.59
INFO:root:2019-05-12 12:12:21, Epoch : 1, Step : 7780, Training Loss : 0.16327, Training Acc : 0.939, Run Time : 2.18
INFO:root:2019-05-12 12:12:31, Epoch : 1, Step : 7781, Training Loss : 0.25693, Training Acc : 0.872, Run Time : 10.05
INFO:root:2019-05-12 12:12:31, Epoch : 1, Step : 7782, Training Loss : 0.17020, Training Acc : 0.933, Run Time : 0.83
INFO:root:2019-05-12 12:12:32, Epoch : 1, Step : 7783, Training Loss : 0.06434, Training Acc : 0.978, Run Time : 0.60
INFO:root:2019-05-12 12:12:44, Epoch : 1, Step : 7784, Training Loss : 0.14573, Training Acc : 0.933, Run Time : 11.85
INFO:root:2019-05-12 12:12:45, Epoch : 1, Step : 7785, Training Loss : 0.32201, Training Acc : 0.839, Run Time : 0.64
INFO:root:2019-05-12 12:12:45, Epoch : 1, Step : 7786, Training Loss : 0.33385, Training Acc : 0.856, Run Time : 0.61
INFO:root:2019-05-12 12:12:56, Epoch : 1, Step : 7787, Training Loss : 0.38561, Training Acc : 0.889, Run Time : 10.41
INFO:root:2019-05-12 12:12:56, Epoch : 1, Step : 7788, Training Loss : 0.34133, Training Acc : 0.900, Run Time : 0.82
INFO:root:2019-05-12 12:12:57, Epoch : 1, Step : 7789, Training Loss : 0.29622, Training Acc : 0.906, Run Time : 0.60
INFO:root:2019-05-12 12:13:07, Epoch : 1, Step : 7790, Training Loss : 0.16109, Training Acc : 0.933, Run Time : 9.92
INFO:root:2019-05-12 12:13:07, Epoch : 1, Step : 7791, Training Loss : 0.17603, Training Acc : 0.928, Run Time : 0.46
INFO:root:2019-05-12 12:13:09, Epoch : 1, Step : 7792, Training Loss : 0.21402, Training Acc : 0.922, Run Time : 1.78
INFO:root:2019-05-12 12:13:21, Epoch : 1, Step : 7793, Training Loss : 0.20500, Training Acc : 0.917, Run Time : 11.88
INFO:root:2019-05-12 12:13:32, Epoch : 1, Step : 7794, Training Loss : 0.24733, Training Acc : 0.922, Run Time : 11.15
INFO:root:2019-05-12 12:13:34, Epoch : 1, Step : 7795, Training Loss : 0.15211, Training Acc : 0.950, Run Time : 1.85
INFO:root:2019-05-12 12:13:35, Epoch : 1, Step : 7796, Training Loss : 0.10169, Training Acc : 0.978, Run Time : 0.62
INFO:root:2019-05-12 12:13:45, Epoch : 1, Step : 7797, Training Loss : 0.14169, Training Acc : 0.922, Run Time : 10.28
INFO:root:2019-05-12 12:13:45, Epoch : 1, Step : 7798, Training Loss : 0.23633, Training Acc : 0.872, Run Time : 0.46
INFO:root:2019-05-12 12:13:46, Epoch : 1, Step : 7799, Training Loss : 0.29275, Training Acc : 0.867, Run Time : 0.67
INFO:root:2019-05-12 12:13:56, Epoch : 1, Step : 7800, Training Loss : 0.38067, Training Acc : 0.794, Run Time : 9.67
INFO:root:2019-05-12 12:13:56, Epoch : 1, Step : 7801, Training Loss : 1.47003, Training Acc : 0.483, Run Time : 0.81
INFO:root:2019-05-12 12:14:06, Epoch : 1, Step : 7802, Training Loss : 1.38715, Training Acc : 0.556, Run Time : 9.61
INFO:root:2019-05-12 12:14:07, Epoch : 1, Step : 7803, Training Loss : 1.01158, Training Acc : 0.633, Run Time : 0.46
INFO:root:2019-05-12 12:14:08, Epoch : 1, Step : 7804, Training Loss : 0.76750, Training Acc : 0.678, Run Time : 1.05
INFO:root:2019-05-12 12:14:18, Epoch : 1, Step : 7805, Training Loss : 0.73394, Training Acc : 0.700, Run Time : 9.99
INFO:root:2019-05-12 12:14:18, Epoch : 1, Step : 7806, Training Loss : 0.55400, Training Acc : 0.739, Run Time : 0.53
INFO:root:2019-05-12 12:14:19, Epoch : 1, Step : 7807, Training Loss : 0.62233, Training Acc : 0.728, Run Time : 0.57
INFO:root:2019-05-12 12:14:29, Epoch : 1, Step : 7808, Training Loss : 0.29036, Training Acc : 0.878, Run Time : 9.96
INFO:root:2019-05-12 12:14:30, Epoch : 1, Step : 7809, Training Loss : 0.29118, Training Acc : 0.878, Run Time : 0.94
INFO:root:2019-05-12 12:14:30, Epoch : 1, Step : 7810, Training Loss : 0.21118, Training Acc : 0.917, Run Time : 0.73
INFO:root:2019-05-12 12:14:43, Epoch : 1, Step : 7811, Training Loss : 0.11243, Training Acc : 0.944, Run Time : 12.21
INFO:root:2019-05-12 12:14:44, Epoch : 1, Step : 7812, Training Loss : 0.19927, Training Acc : 0.917, Run Time : 1.18
INFO:root:2019-05-12 12:14:55, Epoch : 1, Step : 7813, Training Loss : 0.14671, Training Acc : 0.944, Run Time : 11.42
INFO:root:2019-05-12 12:14:56, Epoch : 1, Step : 7814, Training Loss : 0.21328, Training Acc : 0.911, Run Time : 1.06
INFO:root:2019-05-12 12:14:57, Epoch : 1, Step : 7815, Training Loss : 0.19386, Training Acc : 0.922, Run Time : 0.74
INFO:root:2019-05-12 12:14:59, Epoch : 1, Step : 7816, Training Loss : 0.13192, Training Acc : 0.967, Run Time : 1.56
INFO:root:2019-05-12 12:15:07, Epoch : 1, Step : 7817, Training Loss : 0.20172, Training Acc : 0.917, Run Time : 8.63
INFO:root:2019-05-12 12:15:08, Epoch : 1, Step : 7818, Training Loss : 0.13813, Training Acc : 0.950, Run Time : 0.70
INFO:root:2019-05-12 12:15:08, Epoch : 1, Step : 7819, Training Loss : 0.13248, Training Acc : 0.950, Run Time : 0.57
INFO:root:2019-05-12 12:15:09, Epoch : 1, Step : 7820, Training Loss : 0.26433, Training Acc : 0.944, Run Time : 0.97
INFO:root:2019-05-12 12:15:10, Epoch : 1, Step : 7821, Training Loss : 0.30041, Training Acc : 0.917, Run Time : 0.71
INFO:root:2019-05-12 12:15:26, Epoch : 1, Step : 7822, Training Loss : 0.16177, Training Acc : 0.922, Run Time : 15.88
INFO:root:2019-05-12 12:15:26, Epoch : 1, Step : 7823, Training Loss : 0.21178, Training Acc : 0.933, Run Time : 0.42
INFO:root:2019-05-12 12:15:28, Epoch : 1, Step : 7824, Training Loss : 0.32089, Training Acc : 0.917, Run Time : 1.24
INFO:root:2019-05-12 12:15:40, Epoch : 1, Step : 7825, Training Loss : 0.11620, Training Acc : 0.944, Run Time : 12.37
INFO:root:2019-05-12 12:15:40, Epoch : 1, Step : 7826, Training Loss : 0.28126, Training Acc : 0.928, Run Time : 0.43
INFO:root:2019-05-12 12:15:42, Epoch : 1, Step : 7827, Training Loss : 0.17874, Training Acc : 0.944, Run Time : 1.44
INFO:root:2019-05-12 12:15:52, Epoch : 1, Step : 7828, Training Loss : 0.27105, Training Acc : 0.922, Run Time : 10.44
INFO:root:2019-05-12 12:15:53, Epoch : 1, Step : 7829, Training Loss : 0.21722, Training Acc : 0.906, Run Time : 0.50
INFO:root:2019-05-12 12:15:54, Epoch : 1, Step : 7830, Training Loss : 0.69817, Training Acc : 0.828, Run Time : 0.76
INFO:root:2019-05-12 12:15:55, Epoch : 1, Step : 7831, Training Loss : 0.24855, Training Acc : 0.922, Run Time : 1.11
INFO:root:2019-05-12 12:15:55, Epoch : 1, Step : 7832, Training Loss : 0.26567, Training Acc : 0.894, Run Time : 0.55
INFO:root:2019-05-12 12:16:09, Epoch : 1, Step : 7833, Training Loss : 0.42785, Training Acc : 0.872, Run Time : 13.34
INFO:root:2019-05-12 12:16:10, Epoch : 1, Step : 7834, Training Loss : 0.51347, Training Acc : 0.906, Run Time : 0.94
INFO:root:2019-05-12 12:16:10, Epoch : 1, Step : 7835, Training Loss : 1.10260, Training Acc : 0.772, Run Time : 0.94
INFO:root:2019-05-12 12:16:21, Epoch : 1, Step : 7836, Training Loss : 0.59472, Training Acc : 0.833, Run Time : 10.52
INFO:root:2019-05-12 12:16:22, Epoch : 1, Step : 7837, Training Loss : 0.55952, Training Acc : 0.828, Run Time : 1.08
INFO:root:2019-05-12 12:16:23, Epoch : 1, Step : 7838, Training Loss : 0.50020, Training Acc : 0.794, Run Time : 0.61
INFO:root:2019-05-12 12:16:32, Epoch : 1, Step : 7839, Training Loss : 0.22359, Training Acc : 0.906, Run Time : 9.49
INFO:root:2019-05-12 12:16:33, Epoch : 1, Step : 7840, Training Loss : 0.24591, Training Acc : 0.911, Run Time : 1.17
INFO:root:2019-05-12 12:16:34, Epoch : 1, Step : 7841, Training Loss : 0.33647, Training Acc : 0.867, Run Time : 0.62
INFO:root:2019-05-12 12:16:35, Epoch : 1, Step : 7842, Training Loss : 0.32981, Training Acc : 0.856, Run Time : 0.74
INFO:root:2019-05-12 12:16:36, Epoch : 1, Step : 7843, Training Loss : 0.23305, Training Acc : 0.878, Run Time : 0.96
INFO:root:2019-05-12 12:16:50, Epoch : 1, Step : 7844, Training Loss : 0.41411, Training Acc : 0.856, Run Time : 13.91
INFO:root:2019-05-12 12:16:51, Epoch : 1, Step : 7845, Training Loss : 0.42033, Training Acc : 0.844, Run Time : 1.51
INFO:root:2019-05-12 12:16:57, Epoch : 1, Step : 7846, Training Loss : 0.31115, Training Acc : 0.878, Run Time : 5.91
INFO:root:2019-05-12 12:16:58, Epoch : 1, Step : 7847, Training Loss : 0.36636, Training Acc : 0.811, Run Time : 0.55
INFO:root:2019-05-12 12:16:59, Epoch : 1, Step : 7848, Training Loss : 0.33799, Training Acc : 0.867, Run Time : 1.04
INFO:root:2019-05-12 12:17:10, Epoch : 1, Step : 7849, Training Loss : 0.37763, Training Acc : 0.844, Run Time : 11.30
INFO:root:2019-05-12 12:17:10, Epoch : 1, Step : 7850, Training Loss : 0.34355, Training Acc : 0.856, Run Time : 0.42
INFO:root:2019-05-12 12:17:11, Epoch : 1, Step : 7851, Training Loss : 0.40761, Training Acc : 0.828, Run Time : 0.55
INFO:root:2019-05-12 12:17:22, Epoch : 1, Step : 7852, Training Loss : 0.54089, Training Acc : 0.772, Run Time : 11.20
INFO:root:2019-05-12 12:17:22, Epoch : 1, Step : 7853, Training Loss : 0.40097, Training Acc : 0.817, Run Time : 0.45
INFO:root:2019-05-12 12:17:23, Epoch : 1, Step : 7854, Training Loss : 0.38109, Training Acc : 0.828, Run Time : 0.56
INFO:root:2019-05-12 12:17:24, Epoch : 1, Step : 7855, Training Loss : 0.39263, Training Acc : 0.844, Run Time : 1.26
INFO:root:2019-05-12 12:17:25, Epoch : 1, Step : 7856, Training Loss : 0.41528, Training Acc : 0.828, Run Time : 0.62
INFO:root:2019-05-12 12:17:39, Epoch : 1, Step : 7857, Training Loss : 0.34696, Training Acc : 0.861, Run Time : 14.06
INFO:root:2019-05-12 12:17:39, Epoch : 1, Step : 7858, Training Loss : 0.48948, Training Acc : 0.811, Run Time : 0.44
INFO:root:2019-05-12 12:17:41, Epoch : 1, Step : 7859, Training Loss : 0.53447, Training Acc : 0.767, Run Time : 1.23
INFO:root:2019-05-12 12:17:50, Epoch : 1, Step : 7860, Training Loss : 0.48886, Training Acc : 0.800, Run Time : 8.99
INFO:root:2019-05-12 12:17:51, Epoch : 1, Step : 7861, Training Loss : 0.58344, Training Acc : 0.761, Run Time : 1.11
INFO:root:2019-05-12 12:17:51, Epoch : 1, Step : 7862, Training Loss : 0.50120, Training Acc : 0.811, Run Time : 0.62
INFO:root:2019-05-12 12:17:52, Epoch : 1, Step : 7863, Training Loss : 0.41547, Training Acc : 0.828, Run Time : 0.63
INFO:root:2019-05-12 12:17:53, Epoch : 1, Step : 7864, Training Loss : 0.34113, Training Acc : 0.839, Run Time : 0.61
INFO:root:2019-05-12 12:18:05, Epoch : 1, Step : 7865, Training Loss : 0.34233, Training Acc : 0.861, Run Time : 12.31
INFO:root:2019-05-12 12:18:05, Epoch : 1, Step : 7866, Training Loss : 0.38622, Training Acc : 0.811, Run Time : 0.44
INFO:root:2019-05-12 12:18:06, Epoch : 1, Step : 7867, Training Loss : 0.34728, Training Acc : 0.878, Run Time : 0.63
INFO:root:2019-05-12 12:18:17, Epoch : 1, Step : 7868, Training Loss : 0.29775, Training Acc : 0.894, Run Time : 10.87
INFO:root:2019-05-12 12:18:17, Epoch : 1, Step : 7869, Training Loss : 0.32616, Training Acc : 0.861, Run Time : 0.61
INFO:root:2019-05-12 12:18:19, Epoch : 1, Step : 7870, Training Loss : 0.35193, Training Acc : 0.889, Run Time : 1.80
INFO:root:2019-05-12 12:18:29, Epoch : 1, Step : 7871, Training Loss : 0.35006, Training Acc : 0.839, Run Time : 9.44
INFO:root:2019-05-12 12:18:29, Epoch : 1, Step : 7872, Training Loss : 0.27289, Training Acc : 0.900, Run Time : 0.47
INFO:root:2019-05-12 12:18:30, Epoch : 1, Step : 7873, Training Loss : 0.27233, Training Acc : 0.906, Run Time : 0.58
INFO:root:2019-05-12 12:18:39, Epoch : 1, Step : 7874, Training Loss : 0.18200, Training Acc : 0.922, Run Time : 9.42
INFO:root:2019-05-12 12:18:40, Epoch : 1, Step : 7875, Training Loss : 0.23746, Training Acc : 0.894, Run Time : 0.62
INFO:root:2019-05-12 12:18:40, Epoch : 1, Step : 7876, Training Loss : 0.28622, Training Acc : 0.900, Run Time : 0.62
INFO:root:2019-05-12 12:18:41, Epoch : 1, Step : 7877, Training Loss : 0.19443, Training Acc : 0.933, Run Time : 0.61
INFO:root:2019-05-12 12:18:42, Epoch : 1, Step : 7878, Training Loss : 0.28547, Training Acc : 0.911, Run Time : 0.89
INFO:root:2019-05-12 12:18:55, Epoch : 1, Step : 7879, Training Loss : 0.32671, Training Acc : 0.856, Run Time : 13.56
INFO:root:2019-05-12 12:18:56, Epoch : 1, Step : 7880, Training Loss : 0.26403, Training Acc : 0.883, Run Time : 0.51
INFO:root:2019-05-12 12:18:57, Epoch : 1, Step : 7881, Training Loss : 0.21979, Training Acc : 0.906, Run Time : 1.37
INFO:root:2019-05-12 12:19:05, Epoch : 1, Step : 7882, Training Loss : 0.17201, Training Acc : 0.950, Run Time : 7.65
INFO:root:2019-05-12 12:19:06, Epoch : 1, Step : 7883, Training Loss : 0.23397, Training Acc : 0.906, Run Time : 0.50
INFO:root:2019-05-12 12:19:15, Epoch : 1, Step : 7884, Training Loss : 0.13702, Training Acc : 0.939, Run Time : 9.86
INFO:root:2019-05-12 12:19:16, Epoch : 1, Step : 7885, Training Loss : 0.22016, Training Acc : 0.900, Run Time : 0.58
INFO:root:2019-05-12 12:19:17, Epoch : 1, Step : 7886, Training Loss : 0.12478, Training Acc : 0.944, Run Time : 0.86
INFO:root:2019-05-12 12:19:17, Epoch : 1, Step : 7887, Training Loss : 0.16524, Training Acc : 0.939, Run Time : 0.59
INFO:root:2019-05-12 12:19:19, Epoch : 1, Step : 7888, Training Loss : 0.20949, Training Acc : 0.933, Run Time : 1.59
INFO:root:2019-05-12 12:19:31, Epoch : 1, Step : 7889, Training Loss : 0.14908, Training Acc : 0.961, Run Time : 11.80
INFO:root:2019-05-12 12:19:31, Epoch : 1, Step : 7890, Training Loss : 0.17129, Training Acc : 0.928, Run Time : 0.54
INFO:root:2019-05-12 12:19:32, Epoch : 1, Step : 7891, Training Loss : 0.19456, Training Acc : 0.939, Run Time : 0.60
INFO:root:2019-05-12 12:19:33, Epoch : 1, Step : 7892, Training Loss : 0.12886, Training Acc : 0.956, Run Time : 0.62
INFO:root:2019-05-12 12:19:33, Epoch : 1, Step : 7893, Training Loss : 0.17811, Training Acc : 0.950, Run Time : 0.61
INFO:root:2019-05-12 12:19:47, Epoch : 1, Step : 7894, Training Loss : 0.11818, Training Acc : 0.967, Run Time : 14.08
INFO:root:2019-05-12 12:19:48, Epoch : 1, Step : 7895, Training Loss : 0.15005, Training Acc : 0.956, Run Time : 1.08
INFO:root:2019-05-12 12:20:00, Epoch : 1, Step : 7896, Training Loss : 0.17292, Training Acc : 0.944, Run Time : 11.86
INFO:root:2019-05-12 12:20:01, Epoch : 1, Step : 7897, Training Loss : 0.13755, Training Acc : 0.950, Run Time : 0.46
INFO:root:2019-05-12 12:20:01, Epoch : 1, Step : 7898, Training Loss : 0.11481, Training Acc : 0.956, Run Time : 0.62
INFO:root:2019-05-12 12:20:12, Epoch : 1, Step : 7899, Training Loss : 0.15881, Training Acc : 0.928, Run Time : 11.06
INFO:root:2019-05-12 12:20:13, Epoch : 1, Step : 7900, Training Loss : 0.14233, Training Acc : 0.944, Run Time : 0.76
INFO:root:2019-05-12 12:20:19, Epoch : 1, Step : 7901, Training Loss : 0.19484, Training Acc : 0.917, Run Time : 5.89
INFO:root:2019-05-12 12:20:22, Epoch : 1, Step : 7902, Training Loss : 0.14012, Training Acc : 0.939, Run Time : 3.45
INFO:root:2019-05-12 12:20:23, Epoch : 1, Step : 7903, Training Loss : 0.15154, Training Acc : 0.944, Run Time : 0.47
INFO:root:2019-05-12 12:20:33, Epoch : 1, Step : 7904, Training Loss : 0.13259, Training Acc : 0.933, Run Time : 9.86
INFO:root:2019-05-12 12:20:34, Epoch : 1, Step : 7905, Training Loss : 0.20404, Training Acc : 0.911, Run Time : 0.80
INFO:root:2019-05-12 12:20:44, Epoch : 1, Step : 7906, Training Loss : 0.12643, Training Acc : 0.944, Run Time : 10.43
INFO:root:2019-05-12 12:20:45, Epoch : 1, Step : 7907, Training Loss : 0.17348, Training Acc : 0.928, Run Time : 0.83
INFO:root:2019-05-12 12:20:45, Epoch : 1, Step : 7908, Training Loss : 0.14619, Training Acc : 0.956, Run Time : 0.63
INFO:root:2019-05-12 12:20:56, Epoch : 1, Step : 7909, Training Loss : 0.27516, Training Acc : 0.894, Run Time : 10.29
INFO:root:2019-05-12 12:20:56, Epoch : 1, Step : 7910, Training Loss : 0.19079, Training Acc : 0.906, Run Time : 0.52
INFO:root:2019-05-12 12:20:58, Epoch : 1, Step : 7911, Training Loss : 0.18605, Training Acc : 0.939, Run Time : 1.31
INFO:root:2019-05-12 12:21:07, Epoch : 1, Step : 7912, Training Loss : 0.21673, Training Acc : 0.906, Run Time : 9.73
INFO:root:2019-05-12 12:21:08, Epoch : 1, Step : 7913, Training Loss : 0.18268, Training Acc : 0.933, Run Time : 0.99
INFO:root:2019-05-12 12:21:09, Epoch : 1, Step : 7914, Training Loss : 0.18555, Training Acc : 0.922, Run Time : 0.62
INFO:root:2019-05-12 12:21:20, Epoch : 1, Step : 7915, Training Loss : 0.18605, Training Acc : 0.906, Run Time : 11.09
INFO:root:2019-05-12 12:21:21, Epoch : 1, Step : 7916, Training Loss : 0.18063, Training Acc : 0.933, Run Time : 0.86
INFO:root:2019-05-12 12:21:22, Epoch : 1, Step : 7917, Training Loss : 0.13839, Training Acc : 0.956, Run Time : 1.03
INFO:root:2019-05-12 12:21:31, Epoch : 1, Step : 7918, Training Loss : 0.22317, Training Acc : 0.894, Run Time : 9.17
INFO:root:2019-05-12 12:21:32, Epoch : 1, Step : 7919, Training Loss : 0.19103, Training Acc : 0.944, Run Time : 0.72
INFO:root:2019-05-12 12:21:32, Epoch : 1, Step : 7920, Training Loss : 0.19724, Training Acc : 0.922, Run Time : 0.63
INFO:root:2019-05-12 12:21:33, Epoch : 1, Step : 7921, Training Loss : 0.22292, Training Acc : 0.883, Run Time : 0.49
INFO:root:2019-05-12 12:21:45, Epoch : 1, Step : 7922, Training Loss : 0.22589, Training Acc : 0.872, Run Time : 12.04
INFO:root:2019-05-12 12:21:46, Epoch : 1, Step : 7923, Training Loss : 0.17816, Training Acc : 0.933, Run Time : 0.77
INFO:root:2019-05-12 12:21:57, Epoch : 1, Step : 7924, Training Loss : 0.20522, Training Acc : 0.911, Run Time : 11.16
INFO:root:2019-05-12 12:21:58, Epoch : 1, Step : 7925, Training Loss : 0.14492, Training Acc : 0.939, Run Time : 1.59
INFO:root:2019-05-12 12:22:01, Epoch : 1, Step : 7926, Training Loss : 0.14572, Training Acc : 0.956, Run Time : 2.93
INFO:root:2019-05-12 12:22:06, Epoch : 1, Step : 7927, Training Loss : 0.13147, Training Acc : 0.939, Run Time : 4.35
INFO:root:2019-05-12 12:22:07, Epoch : 1, Step : 7928, Training Loss : 0.19813, Training Acc : 0.911, Run Time : 0.82
INFO:root:2019-05-12 12:22:07, Epoch : 1, Step : 7929, Training Loss : 0.13251, Training Acc : 0.950, Run Time : 0.57
INFO:root:2019-05-12 12:22:08, Epoch : 1, Step : 7930, Training Loss : 0.13932, Training Acc : 0.933, Run Time : 0.61
INFO:root:2019-05-12 12:22:18, Epoch : 1, Step : 7931, Training Loss : 0.09942, Training Acc : 0.972, Run Time : 10.61
INFO:root:2019-05-12 12:22:19, Epoch : 1, Step : 7932, Training Loss : 0.13077, Training Acc : 0.950, Run Time : 0.45
INFO:root:2019-05-12 12:22:19, Epoch : 1, Step : 7933, Training Loss : 0.15929, Training Acc : 0.922, Run Time : 0.61
INFO:root:2019-05-12 12:22:20, Epoch : 1, Step : 7934, Training Loss : 0.13194, Training Acc : 0.956, Run Time : 0.68
INFO:root:2019-05-12 12:22:22, Epoch : 1, Step : 7935, Training Loss : 0.15688, Training Acc : 0.950, Run Time : 1.70
INFO:root:2019-05-12 12:22:34, Epoch : 1, Step : 7936, Training Loss : 0.12183, Training Acc : 0.950, Run Time : 12.01
INFO:root:2019-05-12 12:22:35, Epoch : 1, Step : 7937, Training Loss : 0.10413, Training Acc : 0.967, Run Time : 0.80
INFO:root:2019-05-12 12:22:35, Epoch : 1, Step : 7938, Training Loss : 0.25735, Training Acc : 0.911, Run Time : 0.89
INFO:root:2019-05-12 12:22:37, Epoch : 1, Step : 7939, Training Loss : 0.13200, Training Acc : 0.944, Run Time : 1.56
INFO:root:2019-05-12 12:22:39, Epoch : 1, Step : 7940, Training Loss : 0.17132, Training Acc : 0.944, Run Time : 1.58
INFO:root:2019-05-12 12:22:49, Epoch : 1, Step : 7941, Training Loss : 0.08705, Training Acc : 0.967, Run Time : 9.95
INFO:root:2019-05-12 12:22:50, Epoch : 1, Step : 7942, Training Loss : 0.05438, Training Acc : 0.989, Run Time : 1.52
INFO:root:2019-05-12 12:23:01, Epoch : 1, Step : 7943, Training Loss : 0.06319, Training Acc : 0.978, Run Time : 10.84
INFO:root:2019-05-12 12:23:01, Epoch : 1, Step : 7944, Training Loss : 0.05049, Training Acc : 0.989, Run Time : 0.45
INFO:root:2019-05-12 12:23:03, Epoch : 1, Step : 7945, Training Loss : 0.04047, Training Acc : 0.994, Run Time : 1.33
INFO:root:2019-05-12 12:23:13, Epoch : 1, Step : 7946, Training Loss : 0.04746, Training Acc : 0.994, Run Time : 10.10
INFO:root:2019-05-12 12:23:13, Epoch : 1, Step : 7947, Training Loss : 0.07728, Training Acc : 0.972, Run Time : 0.49
INFO:root:2019-05-12 12:23:14, Epoch : 1, Step : 7948, Training Loss : 0.06187, Training Acc : 0.978, Run Time : 0.62
INFO:root:2019-05-12 12:23:15, Epoch : 1, Step : 7949, Training Loss : 0.06985, Training Acc : 0.967, Run Time : 0.60
INFO:root:2019-05-12 12:23:15, Epoch : 1, Step : 7950, Training Loss : 0.07481, Training Acc : 0.978, Run Time : 0.79
INFO:root:2019-05-12 12:23:30, Epoch : 1, Step : 7951, Training Loss : 0.06984, Training Acc : 0.978, Run Time : 14.58
INFO:root:2019-05-12 12:23:31, Epoch : 1, Step : 7952, Training Loss : 0.09596, Training Acc : 0.950, Run Time : 0.83
INFO:root:2019-05-12 12:23:31, Epoch : 1, Step : 7953, Training Loss : 0.08495, Training Acc : 0.956, Run Time : 0.65
INFO:root:2019-05-12 12:23:32, Epoch : 1, Step : 7954, Training Loss : 0.36807, Training Acc : 0.917, Run Time : 0.60
INFO:root:2019-05-12 12:23:33, Epoch : 1, Step : 7955, Training Loss : 0.29969, Training Acc : 0.889, Run Time : 0.60
INFO:root:2019-05-12 12:23:46, Epoch : 1, Step : 7956, Training Loss : 0.30372, Training Acc : 0.872, Run Time : 13.66
INFO:root:2019-05-12 12:23:47, Epoch : 1, Step : 7957, Training Loss : 0.21565, Training Acc : 0.922, Run Time : 0.64
INFO:root:2019-05-12 12:23:48, Epoch : 1, Step : 7958, Training Loss : 0.20749, Training Acc : 0.911, Run Time : 0.79
INFO:root:2019-05-12 12:23:59, Epoch : 1, Step : 7959, Training Loss : 0.11605, Training Acc : 0.944, Run Time : 11.81
INFO:root:2019-05-12 12:24:00, Epoch : 1, Step : 7960, Training Loss : 0.09145, Training Acc : 0.967, Run Time : 0.62
INFO:root:2019-05-12 12:24:01, Epoch : 1, Step : 7961, Training Loss : 0.11686, Training Acc : 0.950, Run Time : 0.64
INFO:root:2019-05-12 12:24:09, Epoch : 1, Step : 7962, Training Loss : 0.12537, Training Acc : 0.956, Run Time : 8.25
INFO:root:2019-05-12 12:24:10, Epoch : 1, Step : 7963, Training Loss : 0.11900, Training Acc : 0.939, Run Time : 1.23
INFO:root:2019-05-12 12:24:11, Epoch : 1, Step : 7964, Training Loss : 0.06068, Training Acc : 0.972, Run Time : 0.61
INFO:root:2019-05-12 12:24:11, Epoch : 1, Step : 7965, Training Loss : 0.09737, Training Acc : 0.956, Run Time : 0.56
INFO:root:2019-05-12 12:24:12, Epoch : 1, Step : 7966, Training Loss : 0.09550, Training Acc : 0.950, Run Time : 0.56
INFO:root:2019-05-12 12:24:27, Epoch : 1, Step : 7967, Training Loss : 0.13237, Training Acc : 0.944, Run Time : 14.58
INFO:root:2019-05-12 12:24:27, Epoch : 1, Step : 7968, Training Loss : 0.17352, Training Acc : 0.950, Run Time : 0.94
INFO:root:2019-05-12 12:24:37, Epoch : 1, Step : 7969, Training Loss : 0.19038, Training Acc : 0.939, Run Time : 10.02
INFO:root:2019-05-12 12:24:39, Epoch : 1, Step : 7970, Training Loss : 0.19995, Training Acc : 0.950, Run Time : 1.27
INFO:root:2019-05-12 12:24:41, Epoch : 1, Step : 7971, Training Loss : 0.22283, Training Acc : 0.928, Run Time : 2.24
INFO:root:2019-05-12 12:24:52, Epoch : 1, Step : 7972, Training Loss : 0.11631, Training Acc : 0.950, Run Time : 11.10
INFO:root:2019-05-12 12:24:54, Epoch : 1, Step : 7973, Training Loss : 0.17790, Training Acc : 0.939, Run Time : 2.19
INFO:root:2019-05-12 12:25:03, Epoch : 1, Step : 7974, Training Loss : 0.18463, Training Acc : 0.939, Run Time : 9.11
INFO:root:2019-05-12 12:25:05, Epoch : 1, Step : 7975, Training Loss : 0.11363, Training Acc : 0.950, Run Time : 1.60
INFO:root:2019-05-12 12:25:13, Epoch : 1, Step : 7976, Training Loss : 0.12162, Training Acc : 0.950, Run Time : 7.68
INFO:root:2019-05-12 12:25:13, Epoch : 1, Step : 7977, Training Loss : 0.15227, Training Acc : 0.933, Run Time : 0.83
INFO:root:2019-05-12 12:25:15, Epoch : 1, Step : 7978, Training Loss : 0.13543, Training Acc : 0.939, Run Time : 1.41
INFO:root:2019-05-12 12:25:29, Epoch : 1, Step : 7979, Training Loss : 0.06851, Training Acc : 0.972, Run Time : 13.79
INFO:root:2019-05-12 12:25:29, Epoch : 1, Step : 7980, Training Loss : 0.15487, Training Acc : 0.939, Run Time : 0.45
INFO:root:2019-05-12 12:25:31, Epoch : 1, Step : 7981, Training Loss : 0.12097, Training Acc : 0.950, Run Time : 1.55
INFO:root:2019-05-12 12:25:45, Epoch : 1, Step : 7982, Training Loss : 0.10845, Training Acc : 0.944, Run Time : 13.86
INFO:root:2019-05-12 12:25:45, Epoch : 1, Step : 7983, Training Loss : 0.19039, Training Acc : 0.900, Run Time : 0.46
INFO:root:2019-05-12 12:25:46, Epoch : 1, Step : 7984, Training Loss : 0.12320, Training Acc : 0.956, Run Time : 0.58
INFO:root:2019-05-12 12:25:47, Epoch : 1, Step : 7985, Training Loss : 0.16000, Training Acc : 0.933, Run Time : 1.52
INFO:root:2019-05-12 12:25:59, Epoch : 1, Step : 7986, Training Loss : 0.10085, Training Acc : 0.956, Run Time : 11.52
INFO:root:2019-05-12 12:25:59, Epoch : 1, Step : 7987, Training Loss : 0.15462, Training Acc : 0.933, Run Time : 0.49
INFO:root:2019-05-12 12:26:00, Epoch : 1, Step : 7988, Training Loss : 0.11862, Training Acc : 0.944, Run Time : 0.57
INFO:root:2019-05-12 12:26:05, Epoch : 1, Step : 7989, Training Loss : 0.12705, Training Acc : 0.956, Run Time : 4.93
INFO:root:2019-05-12 12:26:05, Epoch : 1, Step : 7990, Training Loss : 0.17141, Training Acc : 0.928, Run Time : 0.65
INFO:root:2019-05-12 12:26:06, Epoch : 1, Step : 7991, Training Loss : 0.17174, Training Acc : 0.906, Run Time : 0.70
INFO:root:2019-05-12 12:26:07, Epoch : 1, Step : 7992, Training Loss : 0.20830, Training Acc : 0.900, Run Time : 0.62
INFO:root:2019-05-12 12:26:20, Epoch : 1, Step : 7993, Training Loss : 0.15871, Training Acc : 0.961, Run Time : 13.77
INFO:root:2019-05-12 12:26:22, Epoch : 1, Step : 7994, Training Loss : 0.11557, Training Acc : 0.950, Run Time : 1.69
INFO:root:2019-05-12 12:26:23, Epoch : 1, Step : 7995, Training Loss : 0.10181, Training Acc : 0.967, Run Time : 1.31
INFO:root:2019-05-12 12:26:25, Epoch : 1, Step : 7996, Training Loss : 0.13398, Training Acc : 0.933, Run Time : 1.17
INFO:root:2019-05-12 12:26:34, Epoch : 1, Step : 7997, Training Loss : 0.14023, Training Acc : 0.928, Run Time : 9.96
INFO:root:2019-05-12 12:26:36, Epoch : 1, Step : 7998, Training Loss : 0.12862, Training Acc : 0.956, Run Time : 1.06
INFO:root:2019-05-12 12:26:36, Epoch : 1, Step : 7999, Training Loss : 0.13114, Training Acc : 0.944, Run Time : 0.58
INFO:root:2019-05-12 12:26:46, Epoch : 1, Step : 8000, Training Loss : 0.17004, Training Acc : 0.922, Run Time : 9.81
INFO:root:2019-05-12 12:26:47, Epoch : 1, Step : 8001, Training Loss : 0.25932, Training Acc : 0.883, Run Time : 1.51
INFO:root:2019-05-12 12:26:58, Epoch : 1, Step : 8002, Training Loss : 0.22843, Training Acc : 0.922, Run Time : 10.72
INFO:root:2019-05-12 12:26:59, Epoch : 1, Step : 8003, Training Loss : 0.17498, Training Acc : 0.928, Run Time : 0.60
INFO:root:2019-05-12 12:26:59, Epoch : 1, Step : 8004, Training Loss : 0.22389, Training Acc : 0.933, Run Time : 0.57
INFO:root:2019-05-12 12:27:00, Epoch : 1, Step : 8005, Training Loss : 0.17494, Training Acc : 0.939, Run Time : 0.85
INFO:root:2019-05-12 12:27:12, Epoch : 1, Step : 8006, Training Loss : 0.25068, Training Acc : 0.900, Run Time : 12.02
INFO:root:2019-05-12 12:27:13, Epoch : 1, Step : 8007, Training Loss : 0.19624, Training Acc : 0.911, Run Time : 0.45
INFO:root:2019-05-12 12:27:13, Epoch : 1, Step : 8008, Training Loss : 0.17312, Training Acc : 0.928, Run Time : 0.59
INFO:root:2019-05-12 12:27:25, Epoch : 1, Step : 8009, Training Loss : 0.21618, Training Acc : 0.894, Run Time : 11.29
INFO:root:2019-05-12 12:27:25, Epoch : 1, Step : 8010, Training Loss : 0.18129, Training Acc : 0.917, Run Time : 0.82
INFO:root:2019-05-12 12:27:26, Epoch : 1, Step : 8011, Training Loss : 0.21627, Training Acc : 0.900, Run Time : 0.80
INFO:root:2019-05-12 12:27:27, Epoch : 1, Step : 8012, Training Loss : 0.16507, Training Acc : 0.928, Run Time : 0.65
INFO:root:2019-05-12 12:27:29, Epoch : 1, Step : 8013, Training Loss : 0.20541, Training Acc : 0.917, Run Time : 1.74
INFO:root:2019-05-12 12:27:40, Epoch : 1, Step : 8014, Training Loss : 0.26196, Training Acc : 0.889, Run Time : 11.82
INFO:root:2019-05-12 12:27:41, Epoch : 1, Step : 8015, Training Loss : 0.20379, Training Acc : 0.933, Run Time : 0.44
INFO:root:2019-05-12 12:27:43, Epoch : 1, Step : 8016, Training Loss : 0.16150, Training Acc : 0.950, Run Time : 2.40
INFO:root:2019-05-12 12:27:52, Epoch : 1, Step : 8017, Training Loss : 0.18334, Training Acc : 0.939, Run Time : 8.36
INFO:root:2019-05-12 12:27:52, Epoch : 1, Step : 8018, Training Loss : 0.16309, Training Acc : 0.933, Run Time : 0.49
INFO:root:2019-05-12 12:27:54, Epoch : 1, Step : 8019, Training Loss : 0.15562, Training Acc : 0.928, Run Time : 1.78
INFO:root:2019-05-12 12:28:06, Epoch : 1, Step : 8020, Training Loss : 0.20834, Training Acc : 0.906, Run Time : 12.21
INFO:root:2019-05-12 12:28:07, Epoch : 1, Step : 8021, Training Loss : 0.14915, Training Acc : 0.933, Run Time : 0.62
INFO:root:2019-05-12 12:28:07, Epoch : 1, Step : 8022, Training Loss : 0.19500, Training Acc : 0.922, Run Time : 0.65
INFO:root:2019-05-12 12:28:10, Epoch : 1, Step : 8023, Training Loss : 0.25238, Training Acc : 0.861, Run Time : 2.40
INFO:root:2019-05-12 12:28:19, Epoch : 1, Step : 8024, Training Loss : 0.21075, Training Acc : 0.917, Run Time : 9.01
INFO:root:2019-05-12 12:28:19, Epoch : 1, Step : 8025, Training Loss : 0.22579, Training Acc : 0.894, Run Time : 0.45
INFO:root:2019-05-12 12:28:20, Epoch : 1, Step : 8026, Training Loss : 0.24295, Training Acc : 0.883, Run Time : 0.57
INFO:root:2019-05-12 12:28:20, Epoch : 1, Step : 8027, Training Loss : 0.25401, Training Acc : 0.889, Run Time : 0.60
INFO:root:2019-05-12 12:28:30, Epoch : 1, Step : 8028, Training Loss : 0.23109, Training Acc : 0.911, Run Time : 9.48
INFO:root:2019-05-12 12:28:32, Epoch : 1, Step : 8029, Training Loss : 0.13379, Training Acc : 0.944, Run Time : 2.30
INFO:root:2019-05-12 12:28:33, Epoch : 1, Step : 8030, Training Loss : 0.12011, Training Acc : 0.967, Run Time : 0.99
INFO:root:2019-05-12 12:28:34, Epoch : 1, Step : 8031, Training Loss : 0.22411, Training Acc : 0.911, Run Time : 0.63
INFO:root:2019-05-12 12:28:34, Epoch : 1, Step : 8032, Training Loss : 0.21932, Training Acc : 0.906, Run Time : 0.42
INFO:root:2019-05-12 12:28:35, Epoch : 1, Step : 8033, Training Loss : 0.14909, Training Acc : 0.928, Run Time : 0.61
INFO:root:2019-05-12 12:28:48, Epoch : 1, Step : 8034, Training Loss : 0.19085, Training Acc : 0.928, Run Time : 12.99
INFO:root:2019-05-12 12:28:48, Epoch : 1, Step : 8035, Training Loss : 0.20733, Training Acc : 0.906, Run Time : 0.56
INFO:root:2019-05-12 12:28:49, Epoch : 1, Step : 8036, Training Loss : 0.15289, Training Acc : 0.939, Run Time : 0.56
INFO:root:2019-05-12 12:28:59, Epoch : 1, Step : 8037, Training Loss : 0.23102, Training Acc : 0.917, Run Time : 10.41
INFO:root:2019-05-12 12:29:00, Epoch : 1, Step : 8038, Training Loss : 0.22911, Training Acc : 0.889, Run Time : 0.66
INFO:root:2019-05-12 12:29:01, Epoch : 1, Step : 8039, Training Loss : 0.27922, Training Acc : 0.889, Run Time : 0.63
INFO:root:2019-05-12 12:29:13, Epoch : 1, Step : 8040, Training Loss : 0.25086, Training Acc : 0.911, Run Time : 12.37
INFO:root:2019-05-12 12:29:14, Epoch : 1, Step : 8041, Training Loss : 0.24777, Training Acc : 0.911, Run Time : 0.71
INFO:root:2019-05-12 12:29:14, Epoch : 1, Step : 8042, Training Loss : 0.26151, Training Acc : 0.894, Run Time : 0.62
INFO:root:2019-05-12 12:29:27, Epoch : 1, Step : 8043, Training Loss : 0.35189, Training Acc : 0.850, Run Time : 12.74
INFO:root:2019-05-12 12:29:28, Epoch : 1, Step : 8044, Training Loss : 0.29889, Training Acc : 0.878, Run Time : 1.34
INFO:root:2019-05-12 12:29:37, Epoch : 1, Step : 8045, Training Loss : 0.32797, Training Acc : 0.850, Run Time : 8.34
INFO:root:2019-05-12 12:29:39, Epoch : 1, Step : 8046, Training Loss : 0.30086, Training Acc : 0.872, Run Time : 1.77
INFO:root:2019-05-12 12:29:39, Epoch : 1, Step : 8047, Training Loss : 0.31578, Training Acc : 0.894, Run Time : 0.58
INFO:root:2019-05-12 12:29:40, Epoch : 1, Step : 8048, Training Loss : 0.20512, Training Acc : 0.911, Run Time : 0.60
INFO:root:2019-05-12 12:29:40, Epoch : 1, Step : 8049, Training Loss : 0.20597, Training Acc : 0.917, Run Time : 0.58
INFO:root:2019-05-12 12:29:53, Epoch : 1, Step : 8050, Training Loss : 0.23472, Training Acc : 0.889, Run Time : 12.59
INFO:root:2019-05-12 12:29:53, Epoch : 1, Step : 8051, Training Loss : 0.24786, Training Acc : 0.906, Run Time : 0.57
INFO:root:2019-05-12 12:29:55, Epoch : 1, Step : 8052, Training Loss : 0.27638, Training Acc : 0.883, Run Time : 1.44
INFO:root:2019-05-12 12:30:03, Epoch : 1, Step : 8053, Training Loss : 0.19572, Training Acc : 0.911, Run Time : 8.35
INFO:root:2019-05-12 12:30:04, Epoch : 1, Step : 8054, Training Loss : 0.21309, Training Acc : 0.911, Run Time : 0.73
INFO:root:2019-05-12 12:30:05, Epoch : 1, Step : 8055, Training Loss : 0.28925, Training Acc : 0.889, Run Time : 0.61
INFO:root:2019-05-12 12:30:05, Epoch : 1, Step : 8056, Training Loss : 0.24736, Training Acc : 0.906, Run Time : 0.62
INFO:root:2019-05-12 12:30:06, Epoch : 1, Step : 8057, Training Loss : 0.29913, Training Acc : 0.861, Run Time : 0.58
INFO:root:2019-05-12 12:30:19, Epoch : 1, Step : 8058, Training Loss : 0.25400, Training Acc : 0.883, Run Time : 12.76
INFO:root:2019-05-12 12:30:19, Epoch : 1, Step : 8059, Training Loss : 0.22744, Training Acc : 0.911, Run Time : 0.68
INFO:root:2019-05-12 12:30:20, Epoch : 1, Step : 8060, Training Loss : 0.22735, Training Acc : 0.900, Run Time : 0.74
INFO:root:2019-05-12 12:30:21, Epoch : 1, Step : 8061, Training Loss : 0.21107, Training Acc : 0.906, Run Time : 0.83
INFO:root:2019-05-12 12:30:21, Epoch : 1, Step : 8062, Training Loss : 0.21907, Training Acc : 0.917, Run Time : 0.66
INFO:root:2019-05-12 12:30:36, Epoch : 1, Step : 8063, Training Loss : 0.28025, Training Acc : 0.850, Run Time : 14.70
INFO:root:2019-05-12 12:30:37, Epoch : 1, Step : 8064, Training Loss : 0.26443, Training Acc : 0.878, Run Time : 0.44
INFO:root:2019-05-12 12:30:47, Epoch : 1, Step : 8065, Training Loss : 0.24681, Training Acc : 0.894, Run Time : 10.48
INFO:root:2019-05-12 12:30:48, Epoch : 1, Step : 8066, Training Loss : 0.24897, Training Acc : 0.883, Run Time : 0.74
INFO:root:2019-05-12 12:30:48, Epoch : 1, Step : 8067, Training Loss : 0.25574, Training Acc : 0.883, Run Time : 0.62
INFO:root:2019-05-12 12:31:00, Epoch : 1, Step : 8068, Training Loss : 0.25806, Training Acc : 0.900, Run Time : 11.36
INFO:root:2019-05-12 12:31:00, Epoch : 1, Step : 8069, Training Loss : 0.20528, Training Acc : 0.933, Run Time : 0.47
INFO:root:2019-05-12 12:31:01, Epoch : 1, Step : 8070, Training Loss : 0.18212, Training Acc : 0.922, Run Time : 0.58
INFO:root:2019-05-12 12:31:03, Epoch : 1, Step : 8071, Training Loss : 0.16156, Training Acc : 0.939, Run Time : 2.12
INFO:root:2019-05-12 12:31:11, Epoch : 1, Step : 8072, Training Loss : 0.27749, Training Acc : 0.850, Run Time : 7.83
INFO:root:2019-05-12 12:31:12, Epoch : 1, Step : 8073, Training Loss : 0.21014, Training Acc : 0.939, Run Time : 1.02
INFO:root:2019-05-12 12:31:13, Epoch : 1, Step : 8074, Training Loss : 0.17863, Training Acc : 0.933, Run Time : 1.60
INFO:root:2019-05-12 12:31:25, Epoch : 1, Step : 8075, Training Loss : 0.23656, Training Acc : 0.906, Run Time : 12.07
INFO:root:2019-05-12 12:31:40, Epoch : 1, Step : 8076, Training Loss : 0.21976, Training Acc : 0.894, Run Time : 14.91
INFO:root:2019-05-12 12:31:42, Epoch : 1, Step : 8077, Training Loss : 0.26896, Training Acc : 0.872, Run Time : 1.76
INFO:root:2019-05-12 12:31:43, Epoch : 1, Step : 8078, Training Loss : 0.24969, Training Acc : 0.872, Run Time : 0.61
INFO:root:2019-05-12 12:32:05, Epoch : 1, Step : 8079, Training Loss : 0.18359, Training Acc : 0.939, Run Time : 22.74
INFO:root:2019-05-12 12:32:08, Epoch : 1, Step : 8080, Training Loss : 0.18878, Training Acc : 0.922, Run Time : 2.87
INFO:root:2019-05-12 12:32:09, Epoch : 1, Step : 8081, Training Loss : 0.30153, Training Acc : 0.894, Run Time : 0.74
INFO:root:2019-05-12 12:32:18, Epoch : 1, Step : 8082, Training Loss : 0.20054, Training Acc : 0.928, Run Time : 8.50
INFO:root:2019-05-12 12:32:18, Epoch : 1, Step : 8083, Training Loss : 0.19758, Training Acc : 0.911, Run Time : 0.86
INFO:root:2019-05-12 12:32:26, Epoch : 1, Step : 8084, Training Loss : 0.21682, Training Acc : 0.917, Run Time : 7.05
INFO:root:2019-05-12 12:32:31, Epoch : 1, Step : 8085, Training Loss : 0.21581, Training Acc : 0.900, Run Time : 5.40
INFO:root:2019-05-12 12:32:32, Epoch : 1, Step : 8086, Training Loss : 0.24221, Training Acc : 0.906, Run Time : 0.71
INFO:root:2019-05-12 12:32:32, Epoch : 1, Step : 8087, Training Loss : 0.22407, Training Acc : 0.900, Run Time : 0.86
INFO:root:2019-05-12 12:32:33, Epoch : 1, Step : 8088, Training Loss : 0.29320, Training Acc : 0.889, Run Time : 0.42
INFO:root:2019-05-12 12:32:34, Epoch : 1, Step : 8089, Training Loss : 0.22439, Training Acc : 0.917, Run Time : 0.69
INFO:root:2019-05-12 12:32:48, Epoch : 1, Step : 8090, Training Loss : 0.46068, Training Acc : 0.800, Run Time : 14.43
INFO:root:2019-05-12 12:32:49, Epoch : 1, Step : 8091, Training Loss : 0.46830, Training Acc : 0.794, Run Time : 0.76
INFO:root:2019-05-12 12:32:49, Epoch : 1, Step : 8092, Training Loss : 0.30459, Training Acc : 0.872, Run Time : 0.62
INFO:root:2019-05-12 12:33:01, Epoch : 1, Step : 8093, Training Loss : 0.29249, Training Acc : 0.861, Run Time : 11.39
INFO:root:2019-05-12 12:33:02, Epoch : 1, Step : 8094, Training Loss : 0.21174, Training Acc : 0.922, Run Time : 1.59
INFO:root:2019-05-12 12:33:15, Epoch : 1, Step : 8095, Training Loss : 0.22300, Training Acc : 0.944, Run Time : 12.95
INFO:root:2019-05-12 12:33:17, Epoch : 1, Step : 8096, Training Loss : 0.32495, Training Acc : 0.850, Run Time : 1.65
INFO:root:2019-05-12 12:33:18, Epoch : 1, Step : 8097, Training Loss : 0.25830, Training Acc : 0.889, Run Time : 0.62
INFO:root:2019-05-12 12:33:30, Epoch : 1, Step : 8098, Training Loss : 0.29867, Training Acc : 0.861, Run Time : 12.05
INFO:root:2019-05-12 12:33:30, Epoch : 1, Step : 8099, Training Loss : 0.16427, Training Acc : 0.944, Run Time : 0.79
INFO:root:2019-05-12 12:33:31, Epoch : 1, Step : 8100, Training Loss : 0.25034, Training Acc : 0.917, Run Time : 0.66
INFO:root:2019-05-12 12:33:42, Epoch : 1, Step : 8101, Training Loss : 0.29002, Training Acc : 0.889, Run Time : 10.85
INFO:root:2019-05-12 12:33:43, Epoch : 1, Step : 8102, Training Loss : 0.19728, Training Acc : 0.939, Run Time : 0.60
INFO:root:2019-05-12 12:33:43, Epoch : 1, Step : 8103, Training Loss : 0.19116, Training Acc : 0.917, Run Time : 0.40
INFO:root:2019-05-12 12:33:44, Epoch : 1, Step : 8104, Training Loss : 0.31734, Training Acc : 0.878, Run Time : 0.57
INFO:root:2019-05-12 12:33:57, Epoch : 1, Step : 8105, Training Loss : 0.28943, Training Acc : 0.911, Run Time : 13.85
INFO:root:2019-05-12 12:33:58, Epoch : 1, Step : 8106, Training Loss : 0.25124, Training Acc : 0.894, Run Time : 0.75
INFO:root:2019-05-12 12:34:00, Epoch : 1, Step : 8107, Training Loss : 0.28223, Training Acc : 0.883, Run Time : 1.41
INFO:root:2019-05-12 12:34:13, Epoch : 1, Step : 8108, Training Loss : 0.22169, Training Acc : 0.900, Run Time : 13.56
INFO:root:2019-05-12 12:34:15, Epoch : 1, Step : 8109, Training Loss : 0.14982, Training Acc : 0.950, Run Time : 1.51
INFO:root:2019-05-12 12:34:16, Epoch : 1, Step : 8110, Training Loss : 0.19447, Training Acc : 0.900, Run Time : 1.06
INFO:root:2019-05-12 12:34:27, Epoch : 1, Step : 8111, Training Loss : 0.20873, Training Acc : 0.911, Run Time : 11.14
INFO:root:2019-05-12 12:34:27, Epoch : 1, Step : 8112, Training Loss : 0.23726, Training Acc : 0.889, Run Time : 0.46
INFO:root:2019-05-12 12:34:28, Epoch : 1, Step : 8113, Training Loss : 0.25566, Training Acc : 0.861, Run Time : 0.58
INFO:root:2019-05-12 12:34:43, Epoch : 1, Step : 8114, Training Loss : 0.20286, Training Acc : 0.917, Run Time : 15.55
INFO:root:2019-05-12 12:34:45, Epoch : 1, Step : 8115, Training Loss : 0.16715, Training Acc : 0.939, Run Time : 1.53
INFO:root:2019-05-12 12:34:46, Epoch : 1, Step : 8116, Training Loss : 0.18696, Training Acc : 0.928, Run Time : 0.63
INFO:root:2019-05-12 12:34:47, Epoch : 1, Step : 8117, Training Loss : 0.21522, Training Acc : 0.900, Run Time : 1.10
INFO:root:2019-05-12 12:34:58, Epoch : 1, Step : 8118, Training Loss : 0.16728, Training Acc : 0.917, Run Time : 11.83
INFO:root:2019-05-12 12:34:59, Epoch : 1, Step : 8119, Training Loss : 0.13834, Training Acc : 0.928, Run Time : 0.48
INFO:root:2019-05-12 12:35:00, Epoch : 1, Step : 8120, Training Loss : 0.17042, Training Acc : 0.933, Run Time : 0.60
INFO:root:2019-05-12 12:35:01, Epoch : 1, Step : 8121, Training Loss : 0.23489, Training Acc : 0.889, Run Time : 1.93
INFO:root:2019-05-12 12:35:11, Epoch : 1, Step : 8122, Training Loss : 0.23190, Training Acc : 0.889, Run Time : 9.22
INFO:root:2019-05-12 12:35:12, Epoch : 1, Step : 8123, Training Loss : 0.22909, Training Acc : 0.928, Run Time : 0.94
INFO:root:2019-05-12 12:35:13, Epoch : 1, Step : 8124, Training Loss : 0.31522, Training Acc : 0.850, Run Time : 1.38
INFO:root:2019-05-12 12:35:26, Epoch : 1, Step : 8125, Training Loss : 0.20836, Training Acc : 0.878, Run Time : 12.85
INFO:root:2019-05-12 12:35:27, Epoch : 1, Step : 8126, Training Loss : 0.24015, Training Acc : 0.894, Run Time : 1.12
INFO:root:2019-05-12 12:35:28, Epoch : 1, Step : 8127, Training Loss : 0.23095, Training Acc : 0.889, Run Time : 0.59
INFO:root:2019-05-12 12:35:38, Epoch : 1, Step : 8128, Training Loss : 0.31654, Training Acc : 0.844, Run Time : 10.83
INFO:root:2019-05-12 12:35:39, Epoch : 1, Step : 8129, Training Loss : 0.20738, Training Acc : 0.889, Run Time : 0.62
INFO:root:2019-05-12 12:35:40, Epoch : 1, Step : 8130, Training Loss : 0.22388, Training Acc : 0.894, Run Time : 1.14
INFO:root:2019-05-12 12:35:50, Epoch : 1, Step : 8131, Training Loss : 0.24284, Training Acc : 0.850, Run Time : 10.23
INFO:root:2019-05-12 12:35:51, Epoch : 1, Step : 8132, Training Loss : 0.28488, Training Acc : 0.872, Run Time : 0.77
INFO:root:2019-05-12 12:35:52, Epoch : 1, Step : 8133, Training Loss : 0.18205, Training Acc : 0.917, Run Time : 1.13
INFO:root:2019-05-12 12:36:03, Epoch : 1, Step : 8134, Training Loss : 0.22511, Training Acc : 0.878, Run Time : 10.22
INFO:root:2019-05-12 12:36:03, Epoch : 1, Step : 8135, Training Loss : 0.34794, Training Acc : 0.794, Run Time : 0.46
INFO:root:2019-05-12 12:36:04, Epoch : 1, Step : 8136, Training Loss : 0.39579, Training Acc : 0.789, Run Time : 0.61
INFO:root:2019-05-12 12:36:13, Epoch : 1, Step : 8137, Training Loss : 0.33116, Training Acc : 0.833, Run Time : 9.38
INFO:root:2019-05-12 12:36:14, Epoch : 1, Step : 8138, Training Loss : 0.29616, Training Acc : 0.833, Run Time : 0.69
INFO:root:2019-05-12 12:36:14, Epoch : 1, Step : 8139, Training Loss : 0.18749, Training Acc : 0.911, Run Time : 0.58
INFO:root:2019-05-12 12:36:26, Epoch : 1, Step : 8140, Training Loss : 0.17812, Training Acc : 0.906, Run Time : 11.90
INFO:root:2019-05-12 12:36:27, Epoch : 1, Step : 8141, Training Loss : 0.22476, Training Acc : 0.900, Run Time : 0.84
INFO:root:2019-05-12 12:36:28, Epoch : 1, Step : 8142, Training Loss : 0.15753, Training Acc : 0.939, Run Time : 1.32
INFO:root:2019-05-12 12:36:42, Epoch : 1, Step : 8143, Training Loss : 0.17570, Training Acc : 0.917, Run Time : 13.52
INFO:root:2019-05-12 12:36:43, Epoch : 1, Step : 8144, Training Loss : 0.19689, Training Acc : 0.933, Run Time : 1.51
INFO:root:2019-05-12 12:36:44, Epoch : 1, Step : 8145, Training Loss : 0.29024, Training Acc : 0.856, Run Time : 1.09
INFO:root:2019-05-12 12:36:55, Epoch : 1, Step : 8146, Training Loss : 0.24835, Training Acc : 0.900, Run Time : 10.73
INFO:root:2019-05-12 12:36:56, Epoch : 1, Step : 8147, Training Loss : 0.17729, Training Acc : 0.944, Run Time : 1.02
INFO:root:2019-05-12 12:37:00, Epoch : 1, Step : 8148, Training Loss : 0.13249, Training Acc : 0.972, Run Time : 4.26
INFO:root:2019-05-12 12:37:02, Epoch : 1, Step : 8149, Training Loss : 0.21652, Training Acc : 0.900, Run Time : 1.72
INFO:root:2019-05-12 12:37:03, Epoch : 1, Step : 8150, Training Loss : 0.21516, Training Acc : 0.894, Run Time : 0.59
INFO:root:2019-05-12 12:37:04, Epoch : 1, Step : 8151, Training Loss : 0.20321, Training Acc : 0.917, Run Time : 1.51
INFO:root:2019-05-12 12:37:05, Epoch : 1, Step : 8152, Training Loss : 0.26663, Training Acc : 0.867, Run Time : 0.86
INFO:root:2019-05-12 12:37:18, Epoch : 1, Step : 8153, Training Loss : 0.19766, Training Acc : 0.922, Run Time : 12.62
INFO:root:2019-05-12 12:37:19, Epoch : 1, Step : 8154, Training Loss : 0.27817, Training Acc : 0.861, Run Time : 0.95
INFO:root:2019-05-12 12:37:28, Epoch : 1, Step : 8155, Training Loss : 0.21252, Training Acc : 0.894, Run Time : 8.95
INFO:root:2019-05-12 12:37:37, Epoch : 1, Step : 8156, Training Loss : 0.27058, Training Acc : 0.867, Run Time : 8.84
INFO:root:2019-05-12 12:37:38, Epoch : 1, Step : 8157, Training Loss : 0.21908, Training Acc : 0.911, Run Time : 1.82
INFO:root:2019-05-12 12:37:39, Epoch : 1, Step : 8158, Training Loss : 0.25457, Training Acc : 0.889, Run Time : 0.58
INFO:root:2019-05-12 12:37:40, Epoch : 1, Step : 8159, Training Loss : 0.25552, Training Acc : 0.867, Run Time : 0.67
INFO:root:2019-05-12 12:37:47, Epoch : 1, Step : 8160, Training Loss : 0.18804, Training Acc : 0.933, Run Time : 7.80
INFO:root:2019-05-12 12:37:48, Epoch : 1, Step : 8161, Training Loss : 0.12365, Training Acc : 0.961, Run Time : 0.56
INFO:root:2019-05-12 12:37:49, Epoch : 1, Step : 8162, Training Loss : 0.10887, Training Acc : 0.972, Run Time : 0.99
INFO:root:2019-05-12 12:37:58, Epoch : 1, Step : 8163, Training Loss : 0.13680, Training Acc : 0.961, Run Time : 8.59
INFO:root:2019-05-12 12:37:59, Epoch : 1, Step : 8164, Training Loss : 0.07858, Training Acc : 0.983, Run Time : 1.72
INFO:root:2019-05-12 12:38:09, Epoch : 1, Step : 8165, Training Loss : 0.10771, Training Acc : 0.967, Run Time : 9.75
INFO:root:2019-05-12 12:38:09, Epoch : 1, Step : 8166, Training Loss : 0.06714, Training Acc : 0.989, Run Time : 0.46
INFO:root:2019-05-12 12:38:10, Epoch : 1, Step : 8167, Training Loss : 0.19216, Training Acc : 0.928, Run Time : 0.66
INFO:root:2019-05-12 12:38:23, Epoch : 1, Step : 8168, Training Loss : 0.09842, Training Acc : 0.961, Run Time : 12.59
INFO:root:2019-05-12 12:38:23, Epoch : 1, Step : 8169, Training Loss : 0.16730, Training Acc : 0.933, Run Time : 0.67
INFO:root:2019-05-12 12:38:24, Epoch : 1, Step : 8170, Training Loss : 0.16199, Training Acc : 0.917, Run Time : 0.61
INFO:root:2019-05-12 12:38:25, Epoch : 1, Step : 8171, Training Loss : 0.11303, Training Acc : 0.961, Run Time : 1.27
INFO:root:2019-05-12 12:38:27, Epoch : 1, Step : 8172, Training Loss : 0.13497, Training Acc : 0.956, Run Time : 1.42
INFO:root:2019-05-12 12:38:36, Epoch : 1, Step : 8173, Training Loss : 0.16455, Training Acc : 0.939, Run Time : 8.90
INFO:root:2019-05-12 12:38:36, Epoch : 1, Step : 8174, Training Loss : 0.20933, Training Acc : 0.894, Run Time : 0.71
INFO:root:2019-05-12 12:38:47, Epoch : 1, Step : 8175, Training Loss : 0.10154, Training Acc : 0.967, Run Time : 10.52
INFO:root:2019-05-12 12:38:47, Epoch : 1, Step : 8176, Training Loss : 0.17487, Training Acc : 0.928, Run Time : 0.56
INFO:root:2019-05-12 12:38:49, Epoch : 1, Step : 8177, Training Loss : 0.08280, Training Acc : 0.983, Run Time : 1.88
INFO:root:2019-05-12 12:38:58, Epoch : 1, Step : 8178, Training Loss : 0.13231, Training Acc : 0.917, Run Time : 9.02
INFO:root:2019-05-12 12:38:59, Epoch : 1, Step : 8179, Training Loss : 0.13998, Training Acc : 0.917, Run Time : 0.41
INFO:root:2019-05-12 12:38:59, Epoch : 1, Step : 8180, Training Loss : 0.16391, Training Acc : 0.939, Run Time : 0.61
INFO:root:2019-05-12 12:39:00, Epoch : 1, Step : 8181, Training Loss : 0.13211, Training Acc : 0.939, Run Time : 0.60
INFO:root:2019-05-12 12:39:10, Epoch : 1, Step : 8182, Training Loss : 0.13950, Training Acc : 0.939, Run Time : 9.79
INFO:root:2019-05-12 12:39:10, Epoch : 1, Step : 8183, Training Loss : 0.09724, Training Acc : 0.978, Run Time : 0.66
INFO:root:2019-05-12 12:39:11, Epoch : 1, Step : 8184, Training Loss : 0.09753, Training Acc : 0.961, Run Time : 1.01
INFO:root:2019-05-12 12:39:24, Epoch : 1, Step : 8185, Training Loss : 0.09677, Training Acc : 0.967, Run Time : 12.61
INFO:root:2019-05-12 12:39:27, Epoch : 1, Step : 8186, Training Loss : 0.09720, Training Acc : 0.961, Run Time : 3.51
INFO:root:2019-05-12 12:39:29, Epoch : 1, Step : 8187, Training Loss : 0.08144, Training Acc : 0.967, Run Time : 1.25
INFO:root:2019-05-12 12:39:36, Epoch : 1, Step : 8188, Training Loss : 0.10538, Training Acc : 0.956, Run Time : 7.34
INFO:root:2019-05-12 12:39:37, Epoch : 1, Step : 8189, Training Loss : 0.17164, Training Acc : 0.922, Run Time : 0.63
INFO:root:2019-05-12 12:39:37, Epoch : 1, Step : 8190, Training Loss : 0.15259, Training Acc : 0.928, Run Time : 0.47
INFO:root:2019-05-12 12:39:49, Epoch : 1, Step : 8191, Training Loss : 0.18388, Training Acc : 0.922, Run Time : 11.68
INFO:root:2019-05-12 12:39:49, Epoch : 1, Step : 8192, Training Loss : 0.21696, Training Acc : 0.889, Run Time : 0.64
INFO:root:2019-05-12 12:39:51, Epoch : 1, Step : 8193, Training Loss : 0.45693, Training Acc : 0.867, Run Time : 2.00
INFO:root:2019-05-12 12:40:01, Epoch : 1, Step : 8194, Training Loss : 0.50632, Training Acc : 0.844, Run Time : 9.86
INFO:root:2019-05-12 12:40:02, Epoch : 1, Step : 8195, Training Loss : 0.42261, Training Acc : 0.906, Run Time : 0.82
INFO:root:2019-05-12 12:40:04, Epoch : 1, Step : 8196, Training Loss : 0.23929, Training Acc : 0.928, Run Time : 1.53
INFO:root:2019-05-12 12:40:16, Epoch : 1, Step : 8197, Training Loss : 0.29197, Training Acc : 0.911, Run Time : 12.18
INFO:root:2019-05-12 12:40:17, Epoch : 1, Step : 8198, Training Loss : 0.40879, Training Acc : 0.878, Run Time : 0.87
INFO:root:2019-05-12 12:40:19, Epoch : 1, Step : 8199, Training Loss : 0.13064, Training Acc : 0.939, Run Time : 2.11
INFO:root:2019-05-12 12:40:31, Epoch : 1, Step : 8200, Training Loss : 0.20731, Training Acc : 0.906, Run Time : 12.00
INFO:root:2019-05-12 12:40:32, Epoch : 1, Step : 8201, Training Loss : 0.36200, Training Acc : 0.872, Run Time : 0.76
INFO:root:2019-05-12 12:40:43, Epoch : 1, Step : 8202, Training Loss : 0.67702, Training Acc : 0.772, Run Time : 11.79
INFO:root:2019-05-12 12:40:44, Epoch : 1, Step : 8203, Training Loss : 0.72103, Training Acc : 0.744, Run Time : 0.41
INFO:root:2019-05-12 12:40:44, Epoch : 1, Step : 8204, Training Loss : 0.68726, Training Acc : 0.767, Run Time : 0.63
INFO:root:2019-05-12 12:40:56, Epoch : 1, Step : 8205, Training Loss : 0.51718, Training Acc : 0.761, Run Time : 11.19
INFO:root:2019-05-12 12:40:57, Epoch : 1, Step : 8206, Training Loss : 0.48127, Training Acc : 0.811, Run Time : 0.91
INFO:root:2019-05-12 12:40:59, Epoch : 1, Step : 8207, Training Loss : 0.65465, Training Acc : 0.744, Run Time : 2.03
INFO:root:2019-05-12 12:41:11, Epoch : 1, Step : 8208, Training Loss : 0.26452, Training Acc : 0.894, Run Time : 12.18
INFO:root:2019-05-12 12:41:11, Epoch : 1, Step : 8209, Training Loss : 0.38052, Training Acc : 0.850, Run Time : 0.51
INFO:root:2019-05-12 12:41:13, Epoch : 1, Step : 8210, Training Loss : 0.22266, Training Acc : 0.917, Run Time : 1.59
INFO:root:2019-05-12 12:41:23, Epoch : 1, Step : 8211, Training Loss : 0.25214, Training Acc : 0.867, Run Time : 10.26
INFO:root:2019-05-12 12:41:24, Epoch : 1, Step : 8212, Training Loss : 0.28728, Training Acc : 0.883, Run Time : 0.61
INFO:root:2019-05-12 12:41:33, Epoch : 1, Step : 8213, Training Loss : 0.24708, Training Acc : 0.894, Run Time : 9.69
INFO:root:2019-05-12 12:41:37, Epoch : 1, Step : 8214, Training Loss : 0.27389, Training Acc : 0.844, Run Time : 3.92
INFO:root:2019-05-12 12:41:38, Epoch : 1, Step : 8215, Training Loss : 0.17698, Training Acc : 0.922, Run Time : 0.50
INFO:root:2019-05-12 12:41:50, Epoch : 1, Step : 8216, Training Loss : 0.25693, Training Acc : 0.906, Run Time : 11.96
INFO:root:2019-05-12 12:41:50, Epoch : 1, Step : 8217, Training Loss : 0.16282, Training Acc : 0.944, Run Time : 0.43
INFO:root:2019-05-12 12:41:51, Epoch : 1, Step : 8218, Training Loss : 0.20266, Training Acc : 0.917, Run Time : 0.47
INFO:root:2019-05-12 12:42:01, Epoch : 1, Step : 8219, Training Loss : 0.22960, Training Acc : 0.939, Run Time : 9.91
INFO:root:2019-05-12 12:42:01, Epoch : 1, Step : 8220, Training Loss : 0.28996, Training Acc : 0.894, Run Time : 0.66
INFO:root:2019-05-12 12:42:03, Epoch : 1, Step : 8221, Training Loss : 0.30058, Training Acc : 0.872, Run Time : 1.40
INFO:root:2019-05-12 12:42:13, Epoch : 1, Step : 8222, Training Loss : 0.36947, Training Acc : 0.850, Run Time : 9.95
INFO:root:2019-05-12 12:42:14, Epoch : 1, Step : 8223, Training Loss : 0.18325, Training Acc : 0.950, Run Time : 1.29
INFO:root:2019-05-12 12:42:20, Epoch : 1, Step : 8224, Training Loss : 0.16578, Training Acc : 0.944, Run Time : 6.07
INFO:root:2019-05-12 12:42:28, Epoch : 1, Step : 8225, Training Loss : 0.13327, Training Acc : 0.967, Run Time : 7.80
INFO:root:2019-05-12 12:42:30, Epoch : 1, Step : 8226, Training Loss : 0.33486, Training Acc : 0.889, Run Time : 2.58
INFO:root:2019-05-12 12:42:31, Epoch : 1, Step : 8227, Training Loss : 0.09064, Training Acc : 0.972, Run Time : 0.56
INFO:root:2019-05-12 12:42:32, Epoch : 1, Step : 8228, Training Loss : 0.12178, Training Acc : 0.978, Run Time : 0.61
INFO:root:2019-05-12 12:42:44, Epoch : 1, Step : 8229, Training Loss : 0.10087, Training Acc : 0.972, Run Time : 12.17
INFO:root:2019-05-12 12:42:44, Epoch : 1, Step : 8230, Training Loss : 0.08852, Training Acc : 0.978, Run Time : 0.59
INFO:root:2019-05-12 12:42:45, Epoch : 1, Step : 8231, Training Loss : 0.09065, Training Acc : 0.972, Run Time : 0.61
INFO:root:2019-05-12 12:42:55, Epoch : 1, Step : 8232, Training Loss : 0.05187, Training Acc : 0.989, Run Time : 9.91
INFO:root:2019-05-12 12:42:56, Epoch : 1, Step : 8233, Training Loss : 0.10082, Training Acc : 0.972, Run Time : 0.94
INFO:root:2019-05-12 12:42:56, Epoch : 1, Step : 8234, Training Loss : 0.07537, Training Acc : 0.989, Run Time : 0.60
INFO:root:2019-05-12 12:43:10, Epoch : 1, Step : 8235, Training Loss : 0.06908, Training Acc : 0.983, Run Time : 13.86
INFO:root:2019-05-12 12:43:11, Epoch : 1, Step : 8236, Training Loss : 0.05900, Training Acc : 0.989, Run Time : 1.14
INFO:root:2019-05-12 12:43:12, Epoch : 1, Step : 8237, Training Loss : 0.06399, Training Acc : 0.994, Run Time : 0.97
INFO:root:2019-05-12 12:43:31, Epoch : 1, Step : 8238, Training Loss : 0.05222, Training Acc : 0.989, Run Time : 18.98
INFO:root:2019-05-12 12:43:37, Epoch : 1, Step : 8239, Training Loss : 0.08833, Training Acc : 0.967, Run Time : 6.15
INFO:root:2019-05-12 12:43:38, Epoch : 1, Step : 8240, Training Loss : 0.08138, Training Acc : 0.983, Run Time : 0.60
INFO:root:2019-05-12 12:43:51, Epoch : 1, Step : 8241, Training Loss : 0.10714, Training Acc : 0.956, Run Time : 12.49
INFO:root:2019-05-12 12:43:51, Epoch : 1, Step : 8242, Training Loss : 0.17906, Training Acc : 0.917, Run Time : 0.84
INFO:root:2019-05-12 12:44:05, Epoch : 1, Step : 8243, Training Loss : 0.06107, Training Acc : 0.972, Run Time : 13.56
INFO:root:2019-05-12 12:44:06, Epoch : 1, Step : 8244, Training Loss : 0.05230, Training Acc : 0.989, Run Time : 0.73
INFO:root:2019-05-12 12:44:18, Epoch : 1, Step : 8245, Training Loss : 0.13443, Training Acc : 0.944, Run Time : 12.02
INFO:root:2019-05-12 12:44:23, Epoch : 1, Step : 8246, Training Loss : 0.06012, Training Acc : 0.989, Run Time : 5.15
INFO:root:2019-05-12 12:44:23, Epoch : 1, Step : 8247, Training Loss : 0.03834, Training Acc : 0.994, Run Time : 0.63
INFO:root:2019-05-12 12:44:25, Epoch : 1, Step : 8248, Training Loss : 0.06144, Training Acc : 0.983, Run Time : 1.26
INFO:root:2019-05-12 12:44:35, Epoch : 1, Step : 8249, Training Loss : 0.19111, Training Acc : 0.922, Run Time : 10.02
INFO:root:2019-05-12 12:44:35, Epoch : 1, Step : 8250, Training Loss : 0.12369, Training Acc : 0.944, Run Time : 0.48
INFO:root:2019-05-12 12:44:36, Epoch : 1, Step : 8251, Training Loss : 0.07037, Training Acc : 0.978, Run Time : 1.20
INFO:root:2019-05-12 12:44:46, Epoch : 1, Step : 8252, Training Loss : 0.12166, Training Acc : 0.978, Run Time : 9.55
INFO:root:2019-05-12 12:44:47, Epoch : 1, Step : 8253, Training Loss : 0.07385, Training Acc : 0.972, Run Time : 0.57
INFO:root:2019-05-12 12:44:47, Epoch : 1, Step : 8254, Training Loss : 0.05365, Training Acc : 0.994, Run Time : 0.62
INFO:root:2019-05-12 12:44:48, Epoch : 1, Step : 8255, Training Loss : 0.13772, Training Acc : 0.950, Run Time : 0.60
INFO:root:2019-05-12 12:44:57, Epoch : 1, Step : 8256, Training Loss : 0.08889, Training Acc : 0.972, Run Time : 9.43
INFO:root:2019-05-12 12:44:58, Epoch : 1, Step : 8257, Training Loss : 0.09611, Training Acc : 0.967, Run Time : 0.78
INFO:root:2019-05-12 12:44:59, Epoch : 1, Step : 8258, Training Loss : 0.06989, Training Acc : 0.978, Run Time : 1.40
INFO:root:2019-05-12 12:45:09, Epoch : 1, Step : 8259, Training Loss : 0.08215, Training Acc : 0.978, Run Time : 9.35
INFO:root:2019-05-12 12:45:10, Epoch : 1, Step : 8260, Training Loss : 0.06874, Training Acc : 0.978, Run Time : 0.97
INFO:root:2019-05-12 12:45:20, Epoch : 1, Step : 8261, Training Loss : 0.12589, Training Acc : 0.950, Run Time : 10.67
INFO:root:2019-05-12 12:45:21, Epoch : 1, Step : 8262, Training Loss : 0.11857, Training Acc : 0.961, Run Time : 0.68
INFO:root:2019-05-12 12:45:22, Epoch : 1, Step : 8263, Training Loss : 0.09646, Training Acc : 0.961, Run Time : 1.30
INFO:root:2019-05-12 12:45:35, Epoch : 1, Step : 8264, Training Loss : 0.13102, Training Acc : 0.939, Run Time : 12.40
INFO:root:2019-05-12 12:45:35, Epoch : 1, Step : 8265, Training Loss : 0.11734, Training Acc : 0.956, Run Time : 0.48
INFO:root:2019-05-12 12:45:37, Epoch : 1, Step : 8266, Training Loss : 0.11616, Training Acc : 0.950, Run Time : 1.84
INFO:root:2019-05-12 12:45:52, Epoch : 1, Step : 8267, Training Loss : 0.04069, Training Acc : 0.994, Run Time : 15.10
INFO:root:2019-05-12 12:46:03, Epoch : 1, Step : 8268, Training Loss : 0.05277, Training Acc : 0.983, Run Time : 10.58
INFO:root:2019-05-12 12:46:15, Epoch : 1, Step : 8269, Training Loss : 0.08676, Training Acc : 0.961, Run Time : 11.77
INFO:root:2019-05-12 12:46:15, Epoch : 1, Step : 8270, Training Loss : 0.12874, Training Acc : 0.956, Run Time : 0.55
INFO:root:2019-05-12 12:46:16, Epoch : 1, Step : 8271, Training Loss : 0.08326, Training Acc : 0.978, Run Time : 0.57
INFO:root:2019-05-12 12:46:28, Epoch : 1, Step : 8272, Training Loss : 0.04959, Training Acc : 1.000, Run Time : 11.89
INFO:root:2019-05-12 12:46:28, Epoch : 1, Step : 8273, Training Loss : 0.11969, Training Acc : 0.950, Run Time : 0.89
INFO:root:2019-05-12 12:46:40, Epoch : 1, Step : 8274, Training Loss : 0.09418, Training Acc : 0.972, Run Time : 11.53
INFO:root:2019-05-12 12:46:41, Epoch : 1, Step : 8275, Training Loss : 0.08905, Training Acc : 0.967, Run Time : 0.73
INFO:root:2019-05-12 12:46:41, Epoch : 1, Step : 8276, Training Loss : 0.13722, Training Acc : 0.950, Run Time : 0.66
INFO:root:2019-05-12 12:46:52, Epoch : 1, Step : 8277, Training Loss : 0.14039, Training Acc : 0.950, Run Time : 10.95
INFO:root:2019-05-12 12:46:53, Epoch : 1, Step : 8278, Training Loss : 0.08156, Training Acc : 0.978, Run Time : 0.57
INFO:root:2019-05-12 12:46:55, Epoch : 1, Step : 8279, Training Loss : 0.14388, Training Acc : 0.933, Run Time : 1.85
INFO:root:2019-05-12 12:47:05, Epoch : 1, Step : 8280, Training Loss : 0.07321, Training Acc : 0.972, Run Time : 10.19
INFO:root:2019-05-12 12:47:05, Epoch : 1, Step : 8281, Training Loss : 0.09258, Training Acc : 0.978, Run Time : 0.52
INFO:root:2019-05-12 12:47:07, Epoch : 1, Step : 8282, Training Loss : 0.12783, Training Acc : 0.944, Run Time : 1.69
INFO:root:2019-05-12 12:47:17, Epoch : 1, Step : 8283, Training Loss : 0.21884, Training Acc : 0.906, Run Time : 9.96
INFO:root:2019-05-12 12:47:19, Epoch : 1, Step : 8284, Training Loss : 0.16357, Training Acc : 0.939, Run Time : 1.97
INFO:root:2019-05-12 12:47:28, Epoch : 1, Step : 8285, Training Loss : 0.15789, Training Acc : 0.933, Run Time : 8.97
INFO:root:2019-05-12 12:47:29, Epoch : 1, Step : 8286, Training Loss : 0.16081, Training Acc : 0.950, Run Time : 0.45
INFO:root:2019-05-12 12:47:29, Epoch : 1, Step : 8287, Training Loss : 0.15261, Training Acc : 0.928, Run Time : 0.65
INFO:root:2019-05-12 12:47:42, Epoch : 1, Step : 8288, Training Loss : 0.12505, Training Acc : 0.950, Run Time : 12.99
INFO:root:2019-05-12 12:47:51, Epoch : 1, Step : 8289, Training Loss : 0.12172, Training Acc : 0.967, Run Time : 9.15
INFO:root:2019-05-12 12:48:04, Epoch : 1, Step : 8290, Training Loss : 0.16087, Training Acc : 0.939, Run Time : 13.08
INFO:root:2019-05-12 12:48:05, Epoch : 1, Step : 8291, Training Loss : 0.15549, Training Acc : 0.944, Run Time : 0.97
INFO:root:2019-05-12 12:48:13, Epoch : 1, Step : 8292, Training Loss : 0.14986, Training Acc : 0.939, Run Time : 7.70
INFO:root:2019-05-12 12:48:21, Epoch : 1, Step : 8293, Training Loss : 0.14775, Training Acc : 0.933, Run Time : 8.07
INFO:root:2019-05-12 12:48:22, Epoch : 1, Step : 8294, Training Loss : 0.15363, Training Acc : 0.922, Run Time : 0.87
INFO:root:2019-05-12 12:48:34, Epoch : 1, Step : 8295, Training Loss : 0.21515, Training Acc : 0.906, Run Time : 11.70
INFO:root:2019-05-12 12:48:34, Epoch : 1, Step : 8296, Training Loss : 0.13927, Training Acc : 0.950, Run Time : 0.42
INFO:root:2019-05-12 12:48:36, Epoch : 1, Step : 8297, Training Loss : 0.05971, Training Acc : 0.983, Run Time : 2.33
INFO:root:2019-05-12 12:48:49, Epoch : 1, Step : 8298, Training Loss : 0.10251, Training Acc : 0.961, Run Time : 12.21
INFO:root:2019-05-12 12:48:50, Epoch : 1, Step : 8299, Training Loss : 0.13491, Training Acc : 0.944, Run Time : 0.89
INFO:root:2019-05-12 12:49:01, Epoch : 1, Step : 8300, Training Loss : 0.08837, Training Acc : 0.967, Run Time : 11.07
INFO:root:2019-05-12 12:49:01, Epoch : 1, Step : 8301, Training Loss : 0.19856, Training Acc : 0.944, Run Time : 0.82
INFO:root:2019-05-12 12:49:04, Epoch : 1, Step : 8302, Training Loss : 0.24375, Training Acc : 0.889, Run Time : 2.62
INFO:root:2019-05-12 12:49:18, Epoch : 1, Step : 8303, Training Loss : 0.08052, Training Acc : 0.978, Run Time : 14.11
INFO:root:2019-05-12 12:49:20, Epoch : 1, Step : 8304, Training Loss : 0.10008, Training Acc : 0.956, Run Time : 1.44
INFO:root:2019-05-12 12:49:28, Epoch : 1, Step : 8305, Training Loss : 0.14684, Training Acc : 0.950, Run Time : 8.64
INFO:root:2019-05-12 12:49:29, Epoch : 1, Step : 8306, Training Loss : 0.12040, Training Acc : 0.956, Run Time : 0.71
INFO:root:2019-05-12 12:49:30, Epoch : 1, Step : 8307, Training Loss : 0.03707, Training Acc : 1.000, Run Time : 0.97
INFO:root:2019-05-12 12:49:41, Epoch : 1, Step : 8308, Training Loss : 0.17491, Training Acc : 0.906, Run Time : 11.43
INFO:root:2019-05-12 12:49:42, Epoch : 1, Step : 8309, Training Loss : 0.07610, Training Acc : 0.983, Run Time : 1.00
INFO:root:2019-05-12 12:49:53, Epoch : 1, Step : 8310, Training Loss : 0.20388, Training Acc : 0.950, Run Time : 10.90
INFO:root:2019-05-12 12:49:54, Epoch : 1, Step : 8311, Training Loss : 0.22761, Training Acc : 0.950, Run Time : 0.52
INFO:root:2019-05-12 12:49:54, Epoch : 1, Step : 8312, Training Loss : 0.04332, Training Acc : 0.994, Run Time : 0.70
INFO:root:2019-05-12 12:50:05, Epoch : 1, Step : 8313, Training Loss : 0.08216, Training Acc : 0.967, Run Time : 10.63
INFO:root:2019-05-12 12:50:06, Epoch : 1, Step : 8314, Training Loss : 0.06705, Training Acc : 0.967, Run Time : 0.63
INFO:root:2019-05-12 12:50:06, Epoch : 1, Step : 8315, Training Loss : 0.07410, Training Acc : 0.967, Run Time : 0.74
INFO:root:2019-05-12 12:50:16, Epoch : 1, Step : 8316, Training Loss : 0.06803, Training Acc : 0.972, Run Time : 9.92
INFO:root:2019-05-12 12:50:17, Epoch : 1, Step : 8317, Training Loss : 0.14294, Training Acc : 0.928, Run Time : 0.52
INFO:root:2019-05-12 12:50:17, Epoch : 1, Step : 8318, Training Loss : 0.09120, Training Acc : 0.972, Run Time : 0.53
INFO:root:2019-05-12 12:50:30, Epoch : 1, Step : 8319, Training Loss : 0.04225, Training Acc : 1.000, Run Time : 12.21
INFO:root:2019-05-12 12:50:30, Epoch : 1, Step : 8320, Training Loss : 0.07584, Training Acc : 0.989, Run Time : 0.44
INFO:root:2019-05-12 12:50:32, Epoch : 1, Step : 8321, Training Loss : 0.04988, Training Acc : 0.989, Run Time : 1.59
INFO:root:2019-05-12 12:50:42, Epoch : 1, Step : 8322, Training Loss : 0.06137, Training Acc : 0.978, Run Time : 10.83
INFO:root:2019-05-12 12:50:43, Epoch : 1, Step : 8323, Training Loss : 0.04379, Training Acc : 0.983, Run Time : 0.85
INFO:root:2019-05-12 12:50:46, Epoch : 1, Step : 8324, Training Loss : 0.03505, Training Acc : 0.989, Run Time : 2.25
INFO:root:2019-05-12 12:50:59, Epoch : 1, Step : 8325, Training Loss : 0.02806, Training Acc : 1.000, Run Time : 13.22
INFO:root:2019-05-12 12:51:09, Epoch : 1, Step : 8326, Training Loss : 0.02913, Training Acc : 1.000, Run Time : 10.08
INFO:root:2019-05-12 12:51:19, Epoch : 1, Step : 8327, Training Loss : 0.02086, Training Acc : 1.000, Run Time : 10.45
INFO:root:2019-05-12 12:51:21, Epoch : 1, Step : 8328, Training Loss : 0.16240, Training Acc : 0.928, Run Time : 2.07
INFO:root:2019-05-12 12:51:30, Epoch : 1, Step : 8329, Training Loss : 0.04131, Training Acc : 0.989, Run Time : 8.63
INFO:root:2019-05-12 12:51:31, Epoch : 1, Step : 8330, Training Loss : 0.05101, Training Acc : 0.972, Run Time : 1.19
INFO:root:2019-05-12 12:51:32, Epoch : 1, Step : 8331, Training Loss : 0.02302, Training Acc : 0.994, Run Time : 0.62
INFO:root:2019-05-12 12:51:41, Epoch : 1, Step : 8332, Training Loss : 0.04966, Training Acc : 0.989, Run Time : 9.04
INFO:root:2019-05-12 12:51:44, Epoch : 1, Step : 8333, Training Loss : 0.07380, Training Acc : 0.983, Run Time : 3.43
INFO:root:2019-05-12 12:51:45, Epoch : 1, Step : 8334, Training Loss : 0.12538, Training Acc : 0.956, Run Time : 0.81
INFO:root:2019-05-12 12:51:56, Epoch : 1, Step : 8335, Training Loss : 0.04981, Training Acc : 0.978, Run Time : 11.22
INFO:root:2019-05-12 12:51:57, Epoch : 1, Step : 8336, Training Loss : 0.04533, Training Acc : 0.983, Run Time : 0.78
INFO:root:2019-05-12 12:51:59, Epoch : 1, Step : 8337, Training Loss : 0.02525, Training Acc : 0.994, Run Time : 1.58
INFO:root:2019-05-12 12:52:12, Epoch : 1, Step : 8338, Training Loss : 0.03597, Training Acc : 0.994, Run Time : 13.28
INFO:root:2019-05-12 12:52:18, Epoch : 1, Step : 8339, Training Loss : 0.03549, Training Acc : 0.994, Run Time : 6.50
INFO:root:2019-05-12 12:52:20, Epoch : 1, Step : 8340, Training Loss : 0.04396, Training Acc : 0.989, Run Time : 1.02
INFO:root:2019-05-12 12:52:36, Epoch : 1, Step : 8341, Training Loss : 0.02602, Training Acc : 0.994, Run Time : 16.95
INFO:root:2019-05-12 12:52:37, Epoch : 1, Step : 8342, Training Loss : 0.06071, Training Acc : 0.978, Run Time : 1.01
INFO:root:2019-05-12 12:52:40, Epoch : 1, Step : 8343, Training Loss : 0.03827, Training Acc : 0.983, Run Time : 2.09
INFO:root:2019-05-12 12:52:51, Epoch : 1, Step : 8344, Training Loss : 0.04074, Training Acc : 0.978, Run Time : 10.96
INFO:root:2019-05-12 12:52:51, Epoch : 1, Step : 8345, Training Loss : 0.02092, Training Acc : 1.000, Run Time : 0.43
INFO:root:2019-05-12 12:52:52, Epoch : 1, Step : 8346, Training Loss : 0.09085, Training Acc : 0.961, Run Time : 0.72
INFO:root:2019-05-12 12:53:00, Epoch : 1, Step : 8347, Training Loss : 0.03557, Training Acc : 0.989, Run Time : 8.51
INFO:root:2019-05-12 12:53:04, Epoch : 1, Step : 8348, Training Loss : 0.05342, Training Acc : 0.972, Run Time : 4.18
INFO:root:2019-05-12 12:53:05, Epoch : 1, Step : 8349, Training Loss : 0.16872, Training Acc : 0.939, Run Time : 0.55
INFO:root:2019-05-12 12:53:24, Epoch : 1, Step : 8350, Training Loss : 0.01993, Training Acc : 1.000, Run Time : 19.26
INFO:root:2019-05-12 12:53:40, Epoch : 1, Step : 8351, Training Loss : 0.01407, Training Acc : 1.000, Run Time : 15.57
INFO:root:2019-05-12 12:53:41, Epoch : 1, Step : 8352, Training Loss : 0.06115, Training Acc : 0.989, Run Time : 0.90
INFO:root:2019-05-12 12:53:41, Epoch : 1, Step : 8353, Training Loss : 0.02292, Training Acc : 0.994, Run Time : 0.71
INFO:root:2019-05-12 12:53:54, Epoch : 1, Step : 8354, Training Loss : 0.03489, Training Acc : 0.994, Run Time : 12.28
INFO:root:2019-05-12 12:53:55, Epoch : 1, Step : 8355, Training Loss : 0.03049, Training Acc : 0.994, Run Time : 1.64
INFO:root:2019-05-12 12:54:04, Epoch : 1, Step : 8356, Training Loss : 0.01855, Training Acc : 1.000, Run Time : 8.97
INFO:root:2019-05-12 12:54:05, Epoch : 1, Step : 8357, Training Loss : 0.02027, Training Acc : 1.000, Run Time : 0.48
INFO:root:2019-05-12 12:54:07, Epoch : 1, Step : 8358, Training Loss : 0.05923, Training Acc : 0.978, Run Time : 1.98
INFO:root:2019-05-12 12:54:16, Epoch : 1, Step : 8359, Training Loss : 0.04446, Training Acc : 0.978, Run Time : 9.06
INFO:root:2019-05-12 12:54:16, Epoch : 1, Step : 8360, Training Loss : 0.02451, Training Acc : 1.000, Run Time : 0.59
INFO:root:2019-05-12 12:54:18, Epoch : 1, Step : 8361, Training Loss : 0.02987, Training Acc : 1.000, Run Time : 2.08
INFO:root:2019-05-12 12:54:29, Epoch : 1, Step : 8362, Training Loss : 0.02702, Training Acc : 0.994, Run Time : 10.40
INFO:root:2019-05-12 12:54:29, Epoch : 1, Step : 8363, Training Loss : 0.06141, Training Acc : 0.983, Run Time : 0.47
INFO:root:2019-05-12 12:54:31, Epoch : 1, Step : 8364, Training Loss : 0.17442, Training Acc : 0.900, Run Time : 1.60
INFO:root:2019-05-12 12:54:42, Epoch : 1, Step : 8365, Training Loss : 0.04907, Training Acc : 0.989, Run Time : 10.97
INFO:root:2019-05-12 12:54:43, Epoch : 1, Step : 8366, Training Loss : 0.17296, Training Acc : 0.928, Run Time : 0.68
INFO:root:2019-05-12 12:54:43, Epoch : 1, Step : 8367, Training Loss : 0.05049, Training Acc : 0.972, Run Time : 0.71
INFO:root:2019-05-12 12:54:59, Epoch : 1, Step : 8368, Training Loss : 0.08233, Training Acc : 0.950, Run Time : 15.73
INFO:root:2019-05-12 12:55:00, Epoch : 1, Step : 8369, Training Loss : 0.05380, Training Acc : 0.989, Run Time : 0.99
INFO:root:2019-05-12 12:55:11, Epoch : 1, Step : 8370, Training Loss : 0.11700, Training Acc : 0.961, Run Time : 10.69
INFO:root:2019-05-12 12:55:11, Epoch : 1, Step : 8371, Training Loss : 0.14435, Training Acc : 0.928, Run Time : 0.40
INFO:root:2019-05-12 12:55:13, Epoch : 1, Step : 8372, Training Loss : 0.04546, Training Acc : 0.989, Run Time : 2.19
INFO:root:2019-05-12 12:55:26, Epoch : 1, Step : 8373, Training Loss : 0.14383, Training Acc : 0.933, Run Time : 12.81
INFO:root:2019-05-12 12:55:27, Epoch : 1, Step : 8374, Training Loss : 0.08158, Training Acc : 0.978, Run Time : 0.44
INFO:root:2019-05-12 12:55:27, Epoch : 1, Step : 8375, Training Loss : 0.02634, Training Acc : 0.994, Run Time : 0.63
INFO:root:2019-05-12 12:55:29, Epoch : 1, Step : 8376, Training Loss : 0.05688, Training Acc : 0.978, Run Time : 1.79
INFO:root:2019-05-12 12:55:40, Epoch : 1, Step : 8377, Training Loss : 0.02074, Training Acc : 1.000, Run Time : 11.18
INFO:root:2019-05-12 12:55:41, Epoch : 1, Step : 8378, Training Loss : 0.03990, Training Acc : 0.994, Run Time : 0.59
INFO:root:2019-05-12 12:55:41, Epoch : 1, Step : 8379, Training Loss : 0.04373, Training Acc : 0.994, Run Time : 0.75
INFO:root:2019-05-12 12:55:54, Epoch : 1, Step : 8380, Training Loss : 0.10032, Training Acc : 0.972, Run Time : 12.32
INFO:root:2019-05-12 12:55:55, Epoch : 1, Step : 8381, Training Loss : 0.24516, Training Acc : 0.906, Run Time : 1.53
INFO:root:2019-05-12 12:55:56, Epoch : 1, Step : 8382, Training Loss : 0.13129, Training Acc : 0.944, Run Time : 0.61
INFO:root:2019-05-12 12:55:57, Epoch : 1, Step : 8383, Training Loss : 0.12628, Training Acc : 0.944, Run Time : 0.65
INFO:root:2019-05-12 12:55:57, Epoch : 1, Step : 8384, Training Loss : 0.10419, Training Acc : 0.956, Run Time : 0.90
INFO:root:2019-05-12 12:56:10, Epoch : 1, Step : 8385, Training Loss : 0.32946, Training Acc : 0.889, Run Time : 12.67
INFO:root:2019-05-12 12:56:11, Epoch : 1, Step : 8386, Training Loss : 0.06442, Training Acc : 0.972, Run Time : 0.48
INFO:root:2019-05-12 12:56:11, Epoch : 1, Step : 8387, Training Loss : 0.13040, Training Acc : 0.950, Run Time : 0.61
INFO:root:2019-05-12 12:56:12, Epoch : 1, Step : 8388, Training Loss : 0.06816, Training Acc : 0.972, Run Time : 0.64
INFO:root:2019-05-12 12:56:13, Epoch : 1, Step : 8389, Training Loss : 0.07697, Training Acc : 0.972, Run Time : 0.86
INFO:root:2019-05-12 12:56:27, Epoch : 1, Step : 8390, Training Loss : 0.03145, Training Acc : 1.000, Run Time : 14.51
INFO:root:2019-05-12 12:56:28, Epoch : 1, Step : 8391, Training Loss : 0.06431, Training Acc : 0.983, Run Time : 0.57
INFO:root:2019-05-12 12:56:29, Epoch : 1, Step : 8392, Training Loss : 0.10240, Training Acc : 0.961, Run Time : 1.51
INFO:root:2019-05-12 12:56:38, Epoch : 1, Step : 8393, Training Loss : 0.06876, Training Acc : 0.972, Run Time : 8.94
INFO:root:2019-05-12 12:56:39, Epoch : 1, Step : 8394, Training Loss : 0.11920, Training Acc : 0.944, Run Time : 0.45
INFO:root:2019-05-12 12:56:40, Epoch : 1, Step : 8395, Training Loss : 0.11725, Training Acc : 0.939, Run Time : 0.84
INFO:root:2019-05-12 12:56:47, Epoch : 1, Step : 8396, Training Loss : 0.13881, Training Acc : 0.950, Run Time : 7.09
INFO:root:2019-05-12 12:56:51, Epoch : 1, Step : 8397, Training Loss : 0.11340, Training Acc : 0.950, Run Time : 4.17
INFO:root:2019-05-12 12:56:51, Epoch : 1, Step : 8398, Training Loss : 0.05401, Training Acc : 0.989, Run Time : 0.39
INFO:root:2019-05-12 12:56:56, Epoch : 1, Step : 8399, Training Loss : 0.10916, Training Acc : 0.967, Run Time : 5.18
INFO:root:2019-05-12 12:57:04, Epoch : 1, Step : 8400, Training Loss : 0.14778, Training Acc : 0.933, Run Time : 7.45
INFO:root:2019-05-12 12:57:19, Epoch : 1, Step : 8401, Training Loss : 0.62246, Training Acc : 0.733, Run Time : 15.30
INFO:root:2019-05-12 12:57:20, Epoch : 1, Step : 8402, Training Loss : 0.60934, Training Acc : 0.772, Run Time : 0.42
INFO:root:2019-05-12 12:57:21, Epoch : 1, Step : 8403, Training Loss : 0.51941, Training Acc : 0.794, Run Time : 1.87
INFO:root:2019-05-12 12:57:37, Epoch : 1, Step : 8404, Training Loss : 0.62143, Training Acc : 0.783, Run Time : 15.29
INFO:root:2019-05-12 12:57:37, Epoch : 1, Step : 8405, Training Loss : 0.47717, Training Acc : 0.839, Run Time : 0.74
INFO:root:2019-05-12 12:57:51, Epoch : 1, Step : 8406, Training Loss : 0.38613, Training Acc : 0.833, Run Time : 13.28
INFO:root:2019-05-12 12:57:52, Epoch : 1, Step : 8407, Training Loss : 0.30738, Training Acc : 0.878, Run Time : 0.84
INFO:root:2019-05-12 12:58:03, Epoch : 1, Step : 8408, Training Loss : 0.34507, Training Acc : 0.872, Run Time : 11.86
INFO:root:2019-05-12 12:58:04, Epoch : 1, Step : 8409, Training Loss : 0.20682, Training Acc : 0.911, Run Time : 0.81
INFO:root:2019-05-12 12:58:05, Epoch : 1, Step : 8410, Training Loss : 0.13499, Training Acc : 0.961, Run Time : 0.39
INFO:root:2019-05-12 12:58:15, Epoch : 1, Step : 8411, Training Loss : 0.15848, Training Acc : 0.933, Run Time : 10.59
INFO:root:2019-05-12 12:58:16, Epoch : 1, Step : 8412, Training Loss : 0.13494, Training Acc : 0.950, Run Time : 0.68
INFO:root:2019-05-12 12:58:17, Epoch : 1, Step : 8413, Training Loss : 0.10764, Training Acc : 0.961, Run Time : 0.61
INFO:root:2019-05-12 12:58:17, Epoch : 1, Step : 8414, Training Loss : 0.11840, Training Acc : 0.961, Run Time : 0.63
INFO:root:2019-05-12 12:58:30, Epoch : 1, Step : 8415, Training Loss : 0.17817, Training Acc : 0.933, Run Time : 13.10
INFO:root:2019-05-12 12:58:31, Epoch : 1, Step : 8416, Training Loss : 0.19355, Training Acc : 0.917, Run Time : 0.64
INFO:root:2019-05-12 12:58:32, Epoch : 1, Step : 8417, Training Loss : 0.11650, Training Acc : 0.944, Run Time : 0.60
INFO:root:2019-05-12 12:58:42, Epoch : 1, Step : 8418, Training Loss : 0.14043, Training Acc : 0.939, Run Time : 10.87
INFO:root:2019-05-12 12:58:43, Epoch : 1, Step : 8419, Training Loss : 0.11919, Training Acc : 0.944, Run Time : 0.44
INFO:root:2019-05-12 12:58:43, Epoch : 1, Step : 8420, Training Loss : 0.09193, Training Acc : 0.956, Run Time : 0.62
INFO:root:2019-05-12 12:58:55, Epoch : 1, Step : 8421, Training Loss : 0.12802, Training Acc : 0.933, Run Time : 11.81
INFO:root:2019-05-12 12:58:56, Epoch : 1, Step : 8422, Training Loss : 0.11372, Training Acc : 0.950, Run Time : 0.65
INFO:root:2019-05-12 12:59:04, Epoch : 1, Step : 8423, Training Loss : 0.12072, Training Acc : 0.950, Run Time : 8.26
INFO:root:2019-05-12 12:59:07, Epoch : 1, Step : 8424, Training Loss : 0.10662, Training Acc : 0.961, Run Time : 3.10
INFO:root:2019-05-12 12:59:08, Epoch : 1, Step : 8425, Training Loss : 0.06403, Training Acc : 0.983, Run Time : 1.20
INFO:root:2019-05-12 12:59:19, Epoch : 1, Step : 8426, Training Loss : 0.25601, Training Acc : 0.889, Run Time : 10.50
INFO:root:2019-05-12 12:59:19, Epoch : 1, Step : 8427, Training Loss : 0.70088, Training Acc : 0.828, Run Time : 0.46
INFO:root:2019-05-12 12:59:20, Epoch : 1, Step : 8428, Training Loss : 0.61697, Training Acc : 0.850, Run Time : 0.59
INFO:root:2019-05-12 12:59:21, Epoch : 1, Step : 8429, Training Loss : 0.68454, Training Acc : 0.856, Run Time : 1.45
INFO:root:2019-05-12 12:59:31, Epoch : 1, Step : 8430, Training Loss : 0.72458, Training Acc : 0.828, Run Time : 10.00
INFO:root:2019-05-12 12:59:32, Epoch : 1, Step : 8431, Training Loss : 0.16362, Training Acc : 0.950, Run Time : 0.56
INFO:root:2019-05-12 12:59:49, Epoch : 1, Step : 8432, Training Loss : 0.29462, Training Acc : 0.911, Run Time : 16.84
INFO:root:2019-05-12 13:00:03, Epoch : 1, Step : 8433, Training Loss : 0.34432, Training Acc : 0.872, Run Time : 13.65
INFO:root:2019-05-12 13:00:19, Epoch : 1, Step : 8434, Training Loss : 0.30371, Training Acc : 0.883, Run Time : 16.83
INFO:root:2019-05-12 13:00:22, Epoch : 1, Step : 8435, Training Loss : 0.41305, Training Acc : 0.828, Run Time : 2.50
INFO:root:2019-05-12 13:00:32, Epoch : 1, Step : 8436, Training Loss : 0.28016, Training Acc : 0.883, Run Time : 10.43
INFO:root:2019-05-12 13:00:33, Epoch : 1, Step : 8437, Training Loss : 0.25294, Training Acc : 0.889, Run Time : 1.19
INFO:root:2019-05-12 13:00:34, Epoch : 1, Step : 8438, Training Loss : 0.17369, Training Acc : 0.939, Run Time : 0.51
INFO:root:2019-05-12 13:00:47, Epoch : 1, Step : 8439, Training Loss : 0.17173, Training Acc : 0.928, Run Time : 12.63
INFO:root:2019-05-12 13:00:47, Epoch : 1, Step : 8440, Training Loss : 0.22708, Training Acc : 0.906, Run Time : 0.72
INFO:root:2019-05-12 13:00:49, Epoch : 1, Step : 8441, Training Loss : 0.20045, Training Acc : 0.917, Run Time : 1.98
INFO:root:2019-05-12 13:01:01, Epoch : 1, Step : 8442, Training Loss : 0.26485, Training Acc : 0.883, Run Time : 11.28
INFO:root:2019-05-12 13:01:01, Epoch : 1, Step : 8443, Training Loss : 0.26130, Training Acc : 0.844, Run Time : 0.80
INFO:root:2019-05-12 13:01:03, Epoch : 1, Step : 8444, Training Loss : 0.46197, Training Acc : 0.783, Run Time : 1.76
INFO:root:2019-05-12 13:01:14, Epoch : 1, Step : 8445, Training Loss : 0.32188, Training Acc : 0.867, Run Time : 10.70
INFO:root:2019-05-12 13:01:15, Epoch : 1, Step : 8446, Training Loss : 0.27720, Training Acc : 0.889, Run Time : 0.70
INFO:root:2019-05-12 13:01:15, Epoch : 1, Step : 8447, Training Loss : 0.13357, Training Acc : 0.967, Run Time : 0.88
INFO:root:2019-05-12 13:01:25, Epoch : 1, Step : 8448, Training Loss : 0.19602, Training Acc : 0.906, Run Time : 9.69
INFO:root:2019-05-12 13:01:26, Epoch : 1, Step : 8449, Training Loss : 0.17759, Training Acc : 0.911, Run Time : 0.80
INFO:root:2019-05-12 13:01:28, Epoch : 1, Step : 8450, Training Loss : 0.24175, Training Acc : 0.889, Run Time : 1.57
INFO:root:2019-05-12 13:01:37, Epoch : 1, Step : 8451, Training Loss : 0.26781, Training Acc : 0.889, Run Time : 9.33
INFO:root:2019-05-12 13:01:38, Epoch : 1, Step : 8452, Training Loss : 0.21911, Training Acc : 0.911, Run Time : 0.69
INFO:root:2019-05-12 13:01:43, Epoch : 1, Step : 8453, Training Loss : 0.24026, Training Acc : 0.900, Run Time : 5.60
INFO:root:2019-05-12 13:01:48, Epoch : 1, Step : 8454, Training Loss : 0.17651, Training Acc : 0.928, Run Time : 4.41
INFO:root:2019-05-12 13:01:48, Epoch : 1, Step : 8455, Training Loss : 0.27650, Training Acc : 0.878, Run Time : 0.64
INFO:root:2019-05-12 13:01:49, Epoch : 1, Step : 8456, Training Loss : 0.23868, Training Acc : 0.894, Run Time : 0.59
INFO:root:2019-05-12 13:01:58, Epoch : 1, Step : 8457, Training Loss : 0.16721, Training Acc : 0.939, Run Time : 8.84
INFO:root:2019-05-12 13:01:58, Epoch : 1, Step : 8458, Training Loss : 0.17044, Training Acc : 0.928, Run Time : 0.77
INFO:root:2019-05-12 13:01:59, Epoch : 1, Step : 8459, Training Loss : 0.31776, Training Acc : 0.867, Run Time : 0.98
INFO:root:2019-05-12 13:02:17, Epoch : 1, Step : 8460, Training Loss : 0.21819, Training Acc : 0.889, Run Time : 17.37
INFO:root:2019-05-12 13:02:32, Epoch : 1, Step : 8461, Training Loss : 0.16589, Training Acc : 0.933, Run Time : 15.46
INFO:root:2019-05-12 13:02:49, Epoch : 1, Step : 8462, Training Loss : 0.17114, Training Acc : 0.906, Run Time : 17.27
INFO:root:2019-05-12 13:02:54, Epoch : 1, Step : 8463, Training Loss : 0.18604, Training Acc : 0.906, Run Time : 4.21
INFO:root:2019-05-12 13:02:54, Epoch : 1, Step : 8464, Training Loss : 0.30860, Training Acc : 0.850, Run Time : 0.51
INFO:root:2019-05-12 13:02:55, Epoch : 1, Step : 8465, Training Loss : 0.21862, Training Acc : 0.900, Run Time : 0.70
INFO:root:2019-05-12 13:02:55, Epoch : 1, Step : 8466, Training Loss : 0.20476, Training Acc : 0.917, Run Time : 0.61
INFO:root:2019-05-12 13:03:07, Epoch : 1, Step : 8467, Training Loss : 0.21414, Training Acc : 0.900, Run Time : 11.06
INFO:root:2019-05-12 13:03:07, Epoch : 1, Step : 8468, Training Loss : 0.26882, Training Acc : 0.900, Run Time : 0.57
INFO:root:2019-05-12 13:03:09, Epoch : 1, Step : 8469, Training Loss : 0.16021, Training Acc : 0.933, Run Time : 1.55
INFO:root:2019-05-12 13:03:26, Epoch : 1, Step : 8470, Training Loss : 0.25824, Training Acc : 0.906, Run Time : 17.63
INFO:root:2019-05-12 13:03:34, Epoch : 1, Step : 8471, Training Loss : 0.13495, Training Acc : 0.944, Run Time : 7.56
INFO:root:2019-05-12 13:03:34, Epoch : 1, Step : 8472, Training Loss : 0.18539, Training Acc : 0.906, Run Time : 0.47
INFO:root:2019-05-12 13:03:35, Epoch : 1, Step : 8473, Training Loss : 0.17753, Training Acc : 0.917, Run Time : 0.70
INFO:root:2019-05-12 13:03:36, Epoch : 1, Step : 8474, Training Loss : 0.20188, Training Acc : 0.900, Run Time : 0.76
INFO:root:2019-05-12 13:03:48, Epoch : 1, Step : 8475, Training Loss : 0.19387, Training Acc : 0.911, Run Time : 12.03
INFO:root:2019-05-12 13:03:49, Epoch : 1, Step : 8476, Training Loss : 0.14950, Training Acc : 0.956, Run Time : 0.76
INFO:root:2019-05-12 13:03:49, Epoch : 1, Step : 8477, Training Loss : 0.22875, Training Acc : 0.894, Run Time : 0.65
INFO:root:2019-05-12 13:03:59, Epoch : 1, Step : 8478, Training Loss : 0.16061, Training Acc : 0.933, Run Time : 10.26
INFO:root:2019-05-12 13:04:00, Epoch : 1, Step : 8479, Training Loss : 0.16665, Training Acc : 0.956, Run Time : 0.82
INFO:root:2019-05-12 13:04:01, Epoch : 1, Step : 8480, Training Loss : 0.41205, Training Acc : 0.822, Run Time : 0.60
INFO:root:2019-05-12 13:04:11, Epoch : 1, Step : 8481, Training Loss : 0.21152, Training Acc : 0.889, Run Time : 10.50
INFO:root:2019-05-12 13:04:12, Epoch : 1, Step : 8482, Training Loss : 0.16544, Training Acc : 0.911, Run Time : 0.42
INFO:root:2019-05-12 13:04:14, Epoch : 1, Step : 8483, Training Loss : 0.21963, Training Acc : 0.889, Run Time : 1.78
INFO:root:2019-05-12 13:04:23, Epoch : 1, Step : 8484, Training Loss : 0.17116, Training Acc : 0.928, Run Time : 9.44
INFO:root:2019-05-12 13:04:23, Epoch : 1, Step : 8485, Training Loss : 0.14463, Training Acc : 0.950, Run Time : 0.40
INFO:root:2019-05-12 13:04:25, Epoch : 1, Step : 8486, Training Loss : 0.15174, Training Acc : 0.944, Run Time : 1.97
INFO:root:2019-05-12 13:04:36, Epoch : 1, Step : 8487, Training Loss : 0.15414, Training Acc : 0.956, Run Time : 10.89
INFO:root:2019-05-12 13:04:37, Epoch : 1, Step : 8488, Training Loss : 0.15397, Training Acc : 0.928, Run Time : 0.69
INFO:root:2019-05-12 13:04:38, Epoch : 1, Step : 8489, Training Loss : 0.10584, Training Acc : 0.961, Run Time : 0.65
INFO:root:2019-05-12 13:04:51, Epoch : 1, Step : 8490, Training Loss : 0.15370, Training Acc : 0.950, Run Time : 13.80
INFO:root:2019-05-12 13:04:52, Epoch : 1, Step : 8491, Training Loss : 0.12476, Training Acc : 0.967, Run Time : 0.77
INFO:root:2019-05-12 13:05:09, Epoch : 1, Step : 8492, Training Loss : 0.16980, Training Acc : 0.933, Run Time : 16.43
INFO:root:2019-05-12 13:05:10, Epoch : 1, Step : 8493, Training Loss : 0.21546, Training Acc : 0.922, Run Time : 1.43
INFO:root:2019-05-12 13:05:23, Epoch : 1, Step : 8494, Training Loss : 0.16715, Training Acc : 0.939, Run Time : 12.76
INFO:root:2019-05-12 13:05:24, Epoch : 1, Step : 8495, Training Loss : 0.17352, Training Acc : 0.917, Run Time : 0.86
INFO:root:2019-05-12 13:05:24, Epoch : 1, Step : 8496, Training Loss : 0.14337, Training Acc : 0.939, Run Time : 0.60
INFO:root:2019-05-12 13:05:37, Epoch : 1, Step : 8497, Training Loss : 0.15693, Training Acc : 0.922, Run Time : 13.06
INFO:root:2019-05-12 13:05:38, Epoch : 1, Step : 8498, Training Loss : 0.12457, Training Acc : 0.956, Run Time : 1.00
INFO:root:2019-05-12 13:05:50, Epoch : 1, Step : 8499, Training Loss : 0.14707, Training Acc : 0.950, Run Time : 11.48
INFO:root:2019-05-12 13:05:51, Epoch : 1, Step : 8500, Training Loss : 0.16858, Training Acc : 0.922, Run Time : 1.35
INFO:root:2019-05-12 13:06:10, Epoch : 1, Step : 8501, Training Loss : 0.15777, Training Acc : 0.922, Run Time : 18.61
INFO:root:2019-05-12 13:06:16, Epoch : 1, Step : 8502, Training Loss : 0.17068, Training Acc : 0.939, Run Time : 6.17
INFO:root:2019-05-12 13:06:17, Epoch : 1, Step : 8503, Training Loss : 0.14005, Training Acc : 0.944, Run Time : 0.97
INFO:root:2019-05-12 13:06:26, Epoch : 1, Step : 8504, Training Loss : 0.12175, Training Acc : 0.956, Run Time : 9.31
INFO:root:2019-05-12 13:06:28, Epoch : 1, Step : 8505, Training Loss : 0.14518, Training Acc : 0.911, Run Time : 1.66
INFO:root:2019-05-12 13:06:40, Epoch : 1, Step : 8506, Training Loss : 0.16950, Training Acc : 0.894, Run Time : 12.15
INFO:root:2019-05-12 13:06:40, Epoch : 1, Step : 8507, Training Loss : 0.11533, Training Acc : 0.950, Run Time : 0.41
INFO:root:2019-05-12 13:06:41, Epoch : 1, Step : 8508, Training Loss : 0.11254, Training Acc : 0.972, Run Time : 0.40
INFO:root:2019-05-12 13:06:42, Epoch : 1, Step : 8509, Training Loss : 0.10242, Training Acc : 0.944, Run Time : 0.68
INFO:root:2019-05-12 13:06:55, Epoch : 1, Step : 8510, Training Loss : 0.15710, Training Acc : 0.911, Run Time : 12.97
INFO:root:2019-05-12 13:06:55, Epoch : 1, Step : 8511, Training Loss : 0.15540, Training Acc : 0.939, Run Time : 0.51
INFO:root:2019-05-12 13:06:56, Epoch : 1, Step : 8512, Training Loss : 0.15519, Training Acc : 0.939, Run Time : 0.88
INFO:root:2019-05-12 13:07:06, Epoch : 1, Step : 8513, Training Loss : 0.12460, Training Acc : 0.939, Run Time : 10.13
INFO:root:2019-05-12 13:07:07, Epoch : 1, Step : 8514, Training Loss : 0.10545, Training Acc : 0.961, Run Time : 0.48
INFO:root:2019-05-12 13:07:07, Epoch : 1, Step : 8515, Training Loss : 0.11311, Training Acc : 0.950, Run Time : 0.60
INFO:root:2019-05-12 13:07:08, Epoch : 1, Step : 8516, Training Loss : 0.09806, Training Acc : 0.961, Run Time : 0.61
INFO:root:2019-05-12 13:07:21, Epoch : 1, Step : 8517, Training Loss : 0.15285, Training Acc : 0.922, Run Time : 12.97
INFO:root:2019-05-12 13:07:21, Epoch : 1, Step : 8518, Training Loss : 0.09829, Training Acc : 0.961, Run Time : 0.64
INFO:root:2019-05-12 13:07:38, Epoch : 1, Step : 8519, Training Loss : 0.10513, Training Acc : 0.950, Run Time : 16.28
INFO:root:2019-05-12 13:07:47, Epoch : 1, Step : 8520, Training Loss : 0.13221, Training Acc : 0.944, Run Time : 9.69
INFO:root:2019-05-12 13:08:05, Epoch : 1, Step : 8521, Training Loss : 0.11058, Training Acc : 0.961, Run Time : 17.50
INFO:root:2019-05-12 13:08:13, Epoch : 1, Step : 8522, Training Loss : 0.12866, Training Acc : 0.956, Run Time : 7.78
INFO:root:2019-05-12 13:08:13, Epoch : 1, Step : 8523, Training Loss : 0.87844, Training Acc : 0.794, Run Time : 0.47
INFO:root:2019-05-12 13:08:14, Epoch : 1, Step : 8524, Training Loss : 0.30817, Training Acc : 0.906, Run Time : 0.56
INFO:root:2019-05-12 13:08:15, Epoch : 1, Step : 8525, Training Loss : 0.42278, Training Acc : 0.894, Run Time : 1.42
INFO:root:2019-05-12 13:08:30, Epoch : 1, Step : 8526, Training Loss : 0.21593, Training Acc : 0.950, Run Time : 15.31
INFO:root:2019-05-12 13:08:32, Epoch : 1, Step : 8527, Training Loss : 0.64457, Training Acc : 0.867, Run Time : 1.67
INFO:root:2019-05-12 13:08:34, Epoch : 1, Step : 8528, Training Loss : 0.13812, Training Acc : 0.961, Run Time : 1.96
INFO:root:2019-05-12 13:08:50, Epoch : 1, Step : 8529, Training Loss : 0.21477, Training Acc : 0.928, Run Time : 15.94
INFO:root:2019-05-12 13:10:11, Epoch : 1, Step : 8530, Training Loss : 0.24787, Training Acc : 0.928, Run Time : 81.36
INFO:root:2019-05-12 13:10:23, Epoch : 1, Step : 8531, Training Loss : 0.23024, Training Acc : 0.917, Run Time : 11.48
INFO:root:2019-05-12 13:10:23, Epoch : 1, Step : 8532, Training Loss : 0.28729, Training Acc : 0.878, Run Time : 0.42
INFO:root:2019-05-12 13:10:24, Epoch : 1, Step : 8533, Training Loss : 0.38813, Training Acc : 0.822, Run Time : 0.42
INFO:root:2019-05-12 13:10:38, Epoch : 1, Step : 8534, Training Loss : 0.39299, Training Acc : 0.856, Run Time : 14.74
INFO:root:2019-05-12 13:10:49, Epoch : 1, Step : 8535, Training Loss : 0.29096, Training Acc : 0.883, Run Time : 10.57
INFO:root:2019-05-12 13:10:50, Epoch : 1, Step : 8536, Training Loss : 0.28301, Training Acc : 0.867, Run Time : 0.84
INFO:root:2019-05-12 13:10:51, Epoch : 1, Step : 8537, Training Loss : 0.12543, Training Acc : 0.956, Run Time : 0.81
INFO:root:2019-05-12 13:10:51, Epoch : 1, Step : 8538, Training Loss : 0.10580, Training Acc : 0.972, Run Time : 0.86
INFO:root:2019-05-12 13:11:06, Epoch : 1, Step : 8539, Training Loss : 0.23650, Training Acc : 0.911, Run Time : 14.28
INFO:root:2019-05-12 13:11:07, Epoch : 1, Step : 8540, Training Loss : 0.20883, Training Acc : 0.922, Run Time : 1.52
INFO:root:2019-05-12 13:11:19, Epoch : 1, Step : 8541, Training Loss : 0.30478, Training Acc : 0.883, Run Time : 12.22
INFO:root:2019-05-12 13:11:30, Epoch : 1, Step : 8542, Training Loss : 0.29681, Training Acc : 0.928, Run Time : 10.50
INFO:root:2019-05-12 13:11:32, Epoch : 1, Step : 8543, Training Loss : 0.18827, Training Acc : 0.944, Run Time : 2.29
INFO:root:2019-05-12 13:11:33, Epoch : 1, Step : 8544, Training Loss : 0.28615, Training Acc : 0.911, Run Time : 0.94
INFO:root:2019-05-12 13:11:46, Epoch : 1, Step : 8545, Training Loss : 0.07498, Training Acc : 0.972, Run Time : 13.18
INFO:root:2019-05-12 13:11:49, Epoch : 1, Step : 8546, Training Loss : 0.15155, Training Acc : 0.956, Run Time : 2.69
INFO:root:2019-05-12 13:11:50, Epoch : 1, Step : 8547, Training Loss : 0.27483, Training Acc : 0.917, Run Time : 0.64
INFO:root:2019-05-12 13:11:50, Epoch : 1, Step : 8548, Training Loss : 0.60490, Training Acc : 0.856, Run Time : 0.76
INFO:root:2019-05-12 13:12:04, Epoch : 1, Step : 8549, Training Loss : 0.45855, Training Acc : 0.878, Run Time : 14.02
INFO:root:2019-05-12 13:12:06, Epoch : 1, Step : 8550, Training Loss : 0.72373, Training Acc : 0.811, Run Time : 1.82
INFO:root:2019-05-12 13:12:07, Epoch : 1, Step : 8551, Training Loss : 0.17119, Training Acc : 0.961, Run Time : 0.88
INFO:root:2019-05-12 13:12:23, Epoch : 1, Step : 8552, Training Loss : 0.36831, Training Acc : 0.911, Run Time : 15.63
INFO:root:2019-05-12 13:12:24, Epoch : 1, Step : 8553, Training Loss : 0.21881, Training Acc : 0.933, Run Time : 1.16
INFO:root:2019-05-12 13:12:26, Epoch : 1, Step : 8554, Training Loss : 0.20023, Training Acc : 0.944, Run Time : 1.63
INFO:root:2019-05-12 13:13:51, Epoch : 1, Step : 8555, Training Loss : 0.23012, Training Acc : 0.911, Run Time : 84.92
INFO:root:2019-05-12 13:13:53, Epoch : 1, Step : 8556, Training Loss : 0.14781, Training Acc : 0.961, Run Time : 2.37
INFO:root:2019-05-12 13:13:54, Epoch : 1, Step : 8557, Training Loss : 0.17605, Training Acc : 0.928, Run Time : 0.65
INFO:root:2019-05-12 13:14:45, Epoch : 1, Step : 8558, Training Loss : 0.29391, Training Acc : 0.922, Run Time : 51.70
INFO:root:2019-05-12 13:14:48, Epoch : 1, Step : 8559, Training Loss : 0.25635, Training Acc : 0.911, Run Time : 2.76
INFO:root:2019-05-12 13:14:50, Epoch : 1, Step : 8560, Training Loss : 0.19459, Training Acc : 0.933, Run Time : 2.02
INFO:root:2019-05-12 13:15:06, Epoch : 1, Step : 8561, Training Loss : 0.25083, Training Acc : 0.906, Run Time : 16.48
INFO:root:2019-05-12 13:15:08, Epoch : 1, Step : 8562, Training Loss : 0.18926, Training Acc : 0.900, Run Time : 1.32
INFO:root:2019-05-12 13:15:26, Epoch : 1, Step : 8563, Training Loss : 0.13195, Training Acc : 0.967, Run Time : 18.68
INFO:root:2019-05-12 13:15:28, Epoch : 1, Step : 8564, Training Loss : 0.14062, Training Acc : 0.961, Run Time : 2.00
INFO:root:2019-05-12 13:15:31, Epoch : 1, Step : 8565, Training Loss : 0.13713, Training Acc : 0.972, Run Time : 2.51
INFO:root:2019-05-12 13:15:45, Epoch : 1, Step : 8566, Training Loss : 0.25025, Training Acc : 0.867, Run Time : 13.63
INFO:root:2019-05-12 13:15:45, Epoch : 1, Step : 8567, Training Loss : 0.40441, Training Acc : 0.811, Run Time : 0.42
INFO:root:2019-05-12 13:16:02, Epoch : 1, Step : 8568, Training Loss : 0.36737, Training Acc : 0.839, Run Time : 17.10
INFO:root:2019-05-12 13:16:03, Epoch : 1, Step : 8569, Training Loss : 0.39518, Training Acc : 0.806, Run Time : 0.67
INFO:root:2019-05-12 13:16:14, Epoch : 1, Step : 8570, Training Loss : 0.20528, Training Acc : 0.922, Run Time : 10.81
INFO:root:2019-05-12 13:16:15, Epoch : 1, Step : 8571, Training Loss : 0.22279, Training Acc : 0.894, Run Time : 1.78
INFO:root:2019-05-12 13:16:26, Epoch : 1, Step : 8572, Training Loss : 0.29331, Training Acc : 0.889, Run Time : 10.85
INFO:root:2019-05-12 13:16:27, Epoch : 1, Step : 8573, Training Loss : 0.20057, Training Acc : 0.911, Run Time : 1.18
INFO:root:2019-05-12 13:16:28, Epoch : 1, Step : 8574, Training Loss : 0.17034, Training Acc : 0.939, Run Time : 0.92
INFO:root:2019-05-12 13:16:40, Epoch : 1, Step : 8575, Training Loss : 0.37236, Training Acc : 0.833, Run Time : 11.87
INFO:root:2019-05-12 13:16:41, Epoch : 1, Step : 8576, Training Loss : 0.37109, Training Acc : 0.817, Run Time : 0.48
INFO:root:2019-05-12 13:16:41, Epoch : 1, Step : 8577, Training Loss : 0.42077, Training Acc : 0.817, Run Time : 0.69
INFO:root:2019-05-12 13:16:44, Epoch : 1, Step : 8578, Training Loss : 0.29764, Training Acc : 0.894, Run Time : 2.70
INFO:root:2019-05-12 13:17:21, Epoch : 1, Step : 8579, Training Loss : 0.35241, Training Acc : 0.844, Run Time : 37.34
INFO:root:2019-05-12 13:17:24, Epoch : 1, Step : 8580, Training Loss : 0.29315, Training Acc : 0.900, Run Time : 2.52
INFO:root:2019-05-12 13:17:25, Epoch : 1, Step : 8581, Training Loss : 0.28806, Training Acc : 0.850, Run Time : 0.60
INFO:root:2019-05-12 13:17:25, Epoch : 1, Step : 8582, Training Loss : 0.26342, Training Acc : 0.889, Run Time : 0.65
INFO:root:2019-05-12 13:17:37, Epoch : 1, Step : 8583, Training Loss : 0.27343, Training Acc : 0.867, Run Time : 11.98
INFO:root:2019-05-12 13:17:49, Epoch : 1, Step : 8584, Training Loss : 0.26209, Training Acc : 0.906, Run Time : 11.48
INFO:root:2019-05-12 13:17:58, Epoch : 1, Step : 8585, Training Loss : 0.22653, Training Acc : 0.911, Run Time : 8.87
INFO:root:2019-05-12 13:18:21, Epoch : 1, Step : 8586, Training Loss : 0.34443, Training Acc : 0.822, Run Time : 23.57
INFO:root:2019-05-12 13:18:41, Epoch : 1, Step : 8587, Training Loss : 0.33272, Training Acc : 0.883, Run Time : 19.74
INFO:root:2019-05-12 13:18:43, Epoch : 1, Step : 8588, Training Loss : 0.37119, Training Acc : 0.811, Run Time : 1.89
INFO:root:2019-05-12 13:18:44, Epoch : 1, Step : 8589, Training Loss : 0.30035, Training Acc : 0.867, Run Time : 1.31
INFO:root:2019-05-12 13:18:56, Epoch : 1, Step : 8590, Training Loss : 0.27042, Training Acc : 0.856, Run Time : 11.86
INFO:root:2019-05-12 13:18:58, Epoch : 1, Step : 8591, Training Loss : 0.16205, Training Acc : 0.928, Run Time : 1.77
INFO:root:2019-05-12 13:19:08, Epoch : 1, Step : 8592, Training Loss : 0.42115, Training Acc : 0.800, Run Time : 10.42
INFO:root:2019-05-12 13:19:09, Epoch : 1, Step : 8593, Training Loss : 0.30274, Training Acc : 0.878, Run Time : 0.88
INFO:root:2019-05-12 13:19:46, Epoch : 1, Step : 8594, Training Loss : 0.16441, Training Acc : 0.950, Run Time : 36.66
INFO:root:2019-05-12 13:20:11, Epoch : 1, Step : 8595, Training Loss : 0.16383, Training Acc : 0.939, Run Time : 25.04
INFO:root:2019-05-12 13:20:23, Epoch : 1, Step : 8596, Training Loss : 0.19945, Training Acc : 0.922, Run Time : 12.55
INFO:root:2019-05-12 13:20:52, Epoch : 1, Step : 8597, Training Loss : 0.18987, Training Acc : 0.917, Run Time : 28.94
INFO:root:2019-05-12 13:20:55, Epoch : 1, Step : 8598, Training Loss : 0.16997, Training Acc : 0.928, Run Time : 2.71
INFO:root:2019-05-12 13:20:56, Epoch : 1, Step : 8599, Training Loss : 0.23084, Training Acc : 0.911, Run Time : 1.03
INFO:root:2019-05-12 13:20:57, Epoch : 1, Step : 8600, Training Loss : 0.27890, Training Acc : 0.856, Run Time : 0.65
INFO:root:2019-05-12 13:21:10, Epoch : 1, Step : 8601, Training Loss : 0.82369, Training Acc : 0.711, Run Time : 13.59
INFO:root:2019-05-12 13:21:11, Epoch : 1, Step : 8602, Training Loss : 0.92917, Training Acc : 0.711, Run Time : 0.42
INFO:root:2019-05-12 13:21:11, Epoch : 1, Step : 8603, Training Loss : 1.52263, Training Acc : 0.522, Run Time : 0.71
INFO:root:2019-05-12 13:21:15, Epoch : 1, Step : 8604, Training Loss : 1.31477, Training Acc : 0.650, Run Time : 3.50
INFO:root:2019-05-12 13:21:25, Epoch : 1, Step : 8605, Training Loss : 1.20234, Training Acc : 0.628, Run Time : 10.11
INFO:root:2019-05-12 13:21:25, Epoch : 1, Step : 8606, Training Loss : 0.93964, Training Acc : 0.611, Run Time : 0.50
INFO:root:2019-05-12 13:21:26, Epoch : 1, Step : 8607, Training Loss : 0.48464, Training Acc : 0.828, Run Time : 1.09
INFO:root:2019-05-12 13:21:40, Epoch : 1, Step : 8608, Training Loss : 0.68469, Training Acc : 0.689, Run Time : 14.02
INFO:root:2019-05-12 13:21:41, Epoch : 1, Step : 8609, Training Loss : 0.82191, Training Acc : 0.667, Run Time : 0.79
INFO:root:2019-05-12 13:21:57, Epoch : 1, Step : 8610, Training Loss : 0.83068, Training Acc : 0.739, Run Time : 15.34
INFO:root:2019-05-12 13:21:58, Epoch : 1, Step : 8611, Training Loss : 0.50120, Training Acc : 0.772, Run Time : 0.89
INFO:root:2019-05-12 13:22:16, Epoch : 1, Step : 8612, Training Loss : 0.77208, Training Acc : 0.706, Run Time : 18.59
INFO:root:2019-05-12 13:22:17, Epoch : 1, Step : 8613, Training Loss : 0.62630, Training Acc : 0.767, Run Time : 1.35
INFO:root:2019-05-12 13:22:21, Epoch : 1, Step : 8614, Training Loss : 0.75145, Training Acc : 0.628, Run Time : 4.03
INFO:root:2019-05-12 13:22:39, Epoch : 1, Step : 8615, Training Loss : 0.79858, Training Acc : 0.656, Run Time : 17.23
INFO:root:2019-05-12 13:22:40, Epoch : 1, Step : 8616, Training Loss : 0.77394, Training Acc : 0.706, Run Time : 1.22
INFO:root:2019-05-12 13:22:41, Epoch : 1, Step : 8617, Training Loss : 0.58412, Training Acc : 0.733, Run Time : 0.87
INFO:root:2019-05-12 13:22:43, Epoch : 1, Step : 8618, Training Loss : 0.80532, Training Acc : 0.639, Run Time : 2.35
INFO:root:2019-05-12 13:23:03, Epoch : 1, Step : 8619, Training Loss : 0.60480, Training Acc : 0.689, Run Time : 19.73
INFO:root:2019-05-12 13:23:29, Epoch : 1, Step : 8620, Training Loss : 0.44779, Training Acc : 0.789, Run Time : 26.05
INFO:root:2019-05-12 13:23:32, Epoch : 1, Step : 8621, Training Loss : 0.96588, Training Acc : 0.572, Run Time : 2.80
INFO:root:2019-05-12 13:23:32, Epoch : 1, Step : 8622, Training Loss : 0.86875, Training Acc : 0.506, Run Time : 0.58
INFO:root:2019-05-12 13:23:33, Epoch : 1, Step : 8623, Training Loss : 0.63799, Training Acc : 0.733, Run Time : 0.62
INFO:root:2019-05-12 13:23:50, Epoch : 1, Step : 8624, Training Loss : 0.89146, Training Acc : 0.633, Run Time : 16.92
INFO:root:2019-05-12 13:23:50, Epoch : 1, Step : 8625, Training Loss : 0.74327, Training Acc : 0.683, Run Time : 0.57
INFO:root:2019-05-12 13:23:51, Epoch : 1, Step : 8626, Training Loss : 0.52536, Training Acc : 0.706, Run Time : 0.59
INFO:root:2019-05-12 13:24:04, Epoch : 1, Step : 8627, Training Loss : 0.54430, Training Acc : 0.739, Run Time : 13.04
INFO:root:2019-05-12 13:24:05, Epoch : 1, Step : 8628, Training Loss : 0.61439, Training Acc : 0.767, Run Time : 0.88
INFO:root:2019-05-12 13:24:14, Epoch : 1, Step : 8629, Training Loss : 0.52816, Training Acc : 0.789, Run Time : 9.45
INFO:root:2019-05-12 13:24:15, Epoch : 1, Step : 8630, Training Loss : 0.56749, Training Acc : 0.811, Run Time : 0.82
INFO:root:2019-05-12 13:24:16, Epoch : 1, Step : 8631, Training Loss : 0.66971, Training Acc : 0.700, Run Time : 1.00
INFO:root:2019-05-12 13:24:28, Epoch : 1, Step : 8632, Training Loss : 0.63673, Training Acc : 0.700, Run Time : 11.50
INFO:root:2019-05-12 13:24:28, Epoch : 1, Step : 8633, Training Loss : 0.49950, Training Acc : 0.783, Run Time : 0.58
INFO:root:2019-05-12 13:24:29, Epoch : 1, Step : 8634, Training Loss : 0.37813, Training Acc : 0.839, Run Time : 0.78
INFO:root:2019-05-12 13:24:49, Epoch : 1, Step : 8635, Training Loss : 0.36226, Training Acc : 0.817, Run Time : 20.04
INFO:root:2019-05-12 13:24:51, Epoch : 1, Step : 8636, Training Loss : 0.43791, Training Acc : 0.783, Run Time : 1.64
INFO:root:2019-05-12 13:25:17, Epoch : 1, Step : 8637, Training Loss : 0.29659, Training Acc : 0.900, Run Time : 26.23
INFO:root:2019-05-12 13:25:35, Epoch : 1, Step : 8638, Training Loss : 0.25707, Training Acc : 0.917, Run Time : 17.72
INFO:root:2019-05-12 13:25:36, Epoch : 1, Step : 8639, Training Loss : 0.53952, Training Acc : 0.744, Run Time : 1.36
INFO:root:2019-05-12 13:25:57, Epoch : 1, Step : 8640, Training Loss : 0.38428, Training Acc : 0.833, Run Time : 20.47
INFO:root:2019-05-12 13:25:58, Epoch : 1, Step : 8641, Training Loss : 0.54212, Training Acc : 0.711, Run Time : 1.57
INFO:root:2019-05-12 13:26:17, Epoch : 1, Step : 8642, Training Loss : 0.50792, Training Acc : 0.756, Run Time : 18.62
INFO:root:2019-05-12 13:26:28, Epoch : 1, Step : 8643, Training Loss : 0.31940, Training Acc : 0.889, Run Time : 11.74
INFO:root:2019-05-12 13:26:46, Epoch : 1, Step : 8644, Training Loss : 0.29271, Training Acc : 0.883, Run Time : 18.02
INFO:root:2019-05-12 13:26:48, Epoch : 1, Step : 8645, Training Loss : 0.32249, Training Acc : 0.872, Run Time : 1.65
INFO:root:2019-05-12 13:26:49, Epoch : 1, Step : 8646, Training Loss : 0.70567, Training Acc : 0.700, Run Time : 0.58
INFO:root:2019-05-12 13:27:04, Epoch : 1, Step : 8647, Training Loss : 0.83394, Training Acc : 0.628, Run Time : 14.95
INFO:root:2019-05-12 13:27:05, Epoch : 1, Step : 8648, Training Loss : 0.93446, Training Acc : 0.661, Run Time : 1.31
INFO:root:2019-05-12 13:27:15, Epoch : 1, Step : 8649, Training Loss : 0.45421, Training Acc : 0.844, Run Time : 9.63
INFO:root:2019-05-12 13:27:19, Epoch : 1, Step : 8650, Training Loss : 0.65793, Training Acc : 0.739, Run Time : 4.33
INFO:root:2019-05-12 13:27:19, Epoch : 1, Step : 8651, Training Loss : 0.73131, Training Acc : 0.628, Run Time : 0.48
INFO:root:2019-05-12 13:27:20, Epoch : 1, Step : 8652, Training Loss : 0.50148, Training Acc : 0.711, Run Time : 0.58
INFO:root:2019-05-12 13:27:23, Epoch : 1, Step : 8653, Training Loss : 0.58852, Training Acc : 0.789, Run Time : 2.73
INFO:root:2019-05-12 13:27:36, Epoch : 1, Step : 8654, Training Loss : 0.49462, Training Acc : 0.800, Run Time : 13.19
INFO:root:2019-05-12 13:27:38, Epoch : 1, Step : 8655, Training Loss : 0.32937, Training Acc : 0.861, Run Time : 2.55
INFO:root:2019-05-12 13:27:52, Epoch : 1, Step : 8656, Training Loss : 0.30935, Training Acc : 0.883, Run Time : 13.24
INFO:root:2019-05-12 13:27:53, Epoch : 1, Step : 8657, Training Loss : 0.42608, Training Acc : 0.811, Run Time : 1.32
INFO:root:2019-05-12 13:28:07, Epoch : 1, Step : 8658, Training Loss : 0.71468, Training Acc : 0.589, Run Time : 13.71
INFO:root:2019-05-12 13:28:19, Epoch : 1, Step : 8659, Training Loss : 0.47791, Training Acc : 0.744, Run Time : 11.98
INFO:root:2019-05-12 13:28:20, Epoch : 1, Step : 8660, Training Loss : 0.73629, Training Acc : 0.611, Run Time : 1.76
INFO:root:2019-05-12 13:28:34, Epoch : 1, Step : 8661, Training Loss : 0.58870, Training Acc : 0.683, Run Time : 13.70
INFO:root:2019-05-12 13:28:35, Epoch : 1, Step : 8662, Training Loss : 0.36418, Training Acc : 0.878, Run Time : 0.98
INFO:root:2019-05-12 13:28:36, Epoch : 1, Step : 8663, Training Loss : 0.35995, Training Acc : 0.856, Run Time : 0.45
INFO:root:2019-05-12 13:28:52, Epoch : 1, Step : 8664, Training Loss : 0.25042, Training Acc : 0.928, Run Time : 16.87
INFO:root:2019-05-12 13:29:08, Epoch : 1, Step : 8665, Training Loss : 0.80899, Training Acc : 0.556, Run Time : 15.31
INFO:root:2019-05-12 13:29:22, Epoch : 1, Step : 8666, Training Loss : 0.48387, Training Acc : 0.756, Run Time : 14.12
INFO:root:2019-05-12 13:29:23, Epoch : 1, Step : 8667, Training Loss : 0.51981, Training Acc : 0.756, Run Time : 0.63
INFO:root:2019-05-12 13:29:23, Epoch : 1, Step : 8668, Training Loss : 0.36283, Training Acc : 0.833, Run Time : 0.88
INFO:root:2019-05-12 13:29:39, Epoch : 1, Step : 8669, Training Loss : 0.40470, Training Acc : 0.811, Run Time : 15.14
INFO:root:2019-05-12 13:29:41, Epoch : 1, Step : 8670, Training Loss : 0.47888, Training Acc : 0.728, Run Time : 2.91
INFO:root:2019-05-12 13:29:52, Epoch : 1, Step : 8671, Training Loss : 0.27073, Training Acc : 0.906, Run Time : 10.57
INFO:root:2019-05-12 13:29:53, Epoch : 1, Step : 8672, Training Loss : 0.28877, Training Acc : 0.906, Run Time : 0.84
INFO:root:2019-05-12 13:30:12, Epoch : 1, Step : 8673, Training Loss : 0.38233, Training Acc : 0.856, Run Time : 18.85
INFO:root:2019-05-12 13:30:13, Epoch : 1, Step : 8674, Training Loss : 0.32890, Training Acc : 0.883, Run Time : 1.45
INFO:root:2019-05-12 13:30:25, Epoch : 1, Step : 8675, Training Loss : 0.38940, Training Acc : 0.861, Run Time : 12.00
INFO:root:2019-05-12 13:30:47, Epoch : 1, Step : 8676, Training Loss : 0.40971, Training Acc : 0.800, Run Time : 21.36
INFO:root:2019-05-12 13:30:54, Epoch : 1, Step : 8677, Training Loss : 0.52591, Training Acc : 0.733, Run Time : 7.16
INFO:root:2019-05-12 13:30:55, Epoch : 1, Step : 8678, Training Loss : 0.53682, Training Acc : 0.733, Run Time : 0.84
INFO:root:2019-05-12 13:30:55, Epoch : 1, Step : 8679, Training Loss : 0.22717, Training Acc : 0.967, Run Time : 0.71
INFO:root:2019-05-12 13:31:06, Epoch : 1, Step : 8680, Training Loss : 0.32977, Training Acc : 0.867, Run Time : 10.72
INFO:root:2019-05-12 13:31:08, Epoch : 1, Step : 8681, Training Loss : 0.38301, Training Acc : 0.872, Run Time : 1.55
INFO:root:2019-05-12 13:31:08, Epoch : 1, Step : 8682, Training Loss : 0.34390, Training Acc : 0.861, Run Time : 0.59
INFO:root:2019-05-12 13:31:09, Epoch : 1, Step : 8683, Training Loss : 0.28893, Training Acc : 0.883, Run Time : 0.73
INFO:root:2019-05-12 13:31:24, Epoch : 1, Step : 8684, Training Loss : 0.37688, Training Acc : 0.822, Run Time : 15.37
INFO:root:2019-05-12 13:31:25, Epoch : 1, Step : 8685, Training Loss : 0.46141, Training Acc : 0.778, Run Time : 0.88
INFO:root:2019-05-12 13:31:48, Epoch : 1, Step : 8686, Training Loss : 0.35903, Training Acc : 0.817, Run Time : 23.21
INFO:root:2019-05-12 13:32:06, Epoch : 1, Step : 8687, Training Loss : 0.30142, Training Acc : 0.872, Run Time : 17.19
INFO:root:2019-05-12 13:32:06, Epoch : 1, Step : 8688, Training Loss : 0.36279, Training Acc : 0.828, Run Time : 0.87
INFO:root:2019-05-12 13:32:07, Epoch : 1, Step : 8689, Training Loss : 0.43109, Training Acc : 0.778, Run Time : 0.64
INFO:root:2019-05-12 13:32:40, Epoch : 1, Step : 8690, Training Loss : 0.42361, Training Acc : 0.778, Run Time : 32.81
INFO:root:2019-05-12 13:32:48, Epoch : 1, Step : 8691, Training Loss : 0.34049, Training Acc : 0.917, Run Time : 8.37
INFO:root:2019-05-12 13:32:50, Epoch : 1, Step : 8692, Training Loss : 0.30286, Training Acc : 0.878, Run Time : 1.71
INFO:root:2019-05-12 13:33:16, Epoch : 1, Step : 8693, Training Loss : 0.41915, Training Acc : 0.822, Run Time : 26.49
INFO:root:2019-05-12 13:33:23, Epoch : 1, Step : 8694, Training Loss : 0.41377, Training Acc : 0.800, Run Time : 6.14
INFO:root:2019-05-12 13:33:26, Epoch : 1, Step : 8695, Training Loss : 0.35759, Training Acc : 0.856, Run Time : 3.39
INFO:root:2019-05-12 13:33:44, Epoch : 1, Step : 8696, Training Loss : 0.33207, Training Acc : 0.856, Run Time : 17.93
INFO:root:2019-05-12 13:33:46, Epoch : 1, Step : 8697, Training Loss : 0.31377, Training Acc : 0.856, Run Time : 1.83
INFO:root:2019-05-12 13:33:46, Epoch : 1, Step : 8698, Training Loss : 0.28119, Training Acc : 0.872, Run Time : 0.60
INFO:root:2019-05-12 13:33:48, Epoch : 1, Step : 8699, Training Loss : 0.28408, Training Acc : 0.883, Run Time : 1.40
INFO:root:2019-05-12 13:34:00, Epoch : 1, Step : 8700, Training Loss : 0.44413, Training Acc : 0.794, Run Time : 12.20
INFO:root:2019-05-12 13:34:35, Epoch : 1, Step : 8701, Training Loss : 0.37252, Training Acc : 0.822, Run Time : 35.30
INFO:root:2019-05-12 13:35:09, Epoch : 1, Step : 8702, Training Loss : 0.42133, Training Acc : 0.806, Run Time : 34.26
INFO:root:2019-05-12 13:36:01, Epoch : 1, Step : 8703, Training Loss : 0.41769, Training Acc : 0.794, Run Time : 51.15
INFO:root:2019-05-12 13:36:05, Epoch : 1, Step : 8704, Training Loss : 0.34836, Training Acc : 0.878, Run Time : 4.82
INFO:root:2019-05-12 13:36:06, Epoch : 1, Step : 8705, Training Loss : 0.27272, Training Acc : 0.889, Run Time : 0.73
INFO:root:2019-05-12 13:36:20, Epoch : 1, Step : 8706, Training Loss : 0.30321, Training Acc : 0.900, Run Time : 13.53
INFO:root:2019-05-12 13:36:20, Epoch : 1, Step : 8707, Training Loss : 0.38948, Training Acc : 0.850, Run Time : 0.44
INFO:root:2019-05-12 13:36:21, Epoch : 1, Step : 8708, Training Loss : 0.41110, Training Acc : 0.844, Run Time : 0.41
INFO:root:2019-05-12 13:36:21, Epoch : 1, Step : 8709, Training Loss : 0.44383, Training Acc : 0.783, Run Time : 0.48
INFO:root:2019-05-12 13:36:22, Epoch : 1, Step : 8710, Training Loss : 0.47092, Training Acc : 0.778, Run Time : 0.75
INFO:root:2019-05-12 13:36:41, Epoch : 1, Step : 8711, Training Loss : 0.40168, Training Acc : 0.839, Run Time : 18.96
INFO:root:2019-05-12 13:36:41, Epoch : 1, Step : 8712, Training Loss : 0.54809, Training Acc : 0.683, Run Time : 0.60
INFO:root:2019-05-12 13:36:42, Epoch : 1, Step : 8713, Training Loss : 0.31316, Training Acc : 0.883, Run Time : 1.18
INFO:root:2019-05-12 13:37:01, Epoch : 1, Step : 8714, Training Loss : 0.39670, Training Acc : 0.833, Run Time : 18.36
INFO:root:2019-05-12 13:37:02, Epoch : 1, Step : 8715, Training Loss : 0.45440, Training Acc : 0.756, Run Time : 1.48
INFO:root:2019-05-12 13:37:25, Epoch : 1, Step : 8716, Training Loss : 0.37987, Training Acc : 0.833, Run Time : 22.26
INFO:root:2019-05-12 13:37:26, Epoch : 1, Step : 8717, Training Loss : 0.31387, Training Acc : 0.850, Run Time : 0.99
INFO:root:2019-05-12 13:37:54, Epoch : 1, Step : 8718, Training Loss : 0.38190, Training Acc : 0.833, Run Time : 28.02
INFO:root:2019-05-12 13:38:21, Epoch : 1, Step : 8719, Training Loss : 0.36080, Training Acc : 0.867, Run Time : 27.09
INFO:root:2019-05-12 13:38:48, Epoch : 1, Step : 8720, Training Loss : 0.32307, Training Acc : 0.883, Run Time : 26.87
INFO:root:2019-05-12 13:38:50, Epoch : 1, Step : 8721, Training Loss : 0.28034, Training Acc : 0.872, Run Time : 2.31
INFO:root:2019-05-12 13:38:51, Epoch : 1, Step : 8722, Training Loss : 0.32296, Training Acc : 0.883, Run Time : 0.74
INFO:root:2019-05-12 13:39:08, Epoch : 1, Step : 8723, Training Loss : 0.44107, Training Acc : 0.772, Run Time : 17.63
INFO:root:2019-05-12 13:39:25, Epoch : 1, Step : 8724, Training Loss : 0.48551, Training Acc : 0.744, Run Time : 16.64
INFO:root:2019-05-12 13:40:29, Epoch : 1, Step : 8725, Training Loss : 0.32109, Training Acc : 0.839, Run Time : 64.56
INFO:root:2019-05-12 13:40:56, Epoch : 1, Step : 8726, Training Loss : 0.52575, Training Acc : 0.750, Run Time : 26.50
INFO:root:2019-05-12 13:41:20, Epoch : 1, Step : 8727, Training Loss : 0.47512, Training Acc : 0.739, Run Time : 24.15
INFO:root:2019-05-12 13:41:41, Epoch : 1, Step : 8728, Training Loss : 0.41395, Training Acc : 0.828, Run Time : 20.53
INFO:root:2019-05-12 13:41:44, Epoch : 1, Step : 8729, Training Loss : 0.46490, Training Acc : 0.778, Run Time : 3.40
INFO:root:2019-05-12 13:41:45, Epoch : 1, Step : 8730, Training Loss : 0.41520, Training Acc : 0.783, Run Time : 1.03
INFO:root:2019-05-12 13:41:54, Epoch : 1, Step : 8731, Training Loss : 0.45323, Training Acc : 0.794, Run Time : 8.77
INFO:root:2019-05-12 13:41:56, Epoch : 1, Step : 8732, Training Loss : 0.48337, Training Acc : 0.767, Run Time : 1.74
INFO:root:2019-05-12 13:41:56, Epoch : 1, Step : 8733, Training Loss : 0.76987, Training Acc : 0.633, Run Time : 0.61
INFO:root:2019-05-12 13:42:14, Epoch : 1, Step : 8734, Training Loss : 0.53515, Training Acc : 0.772, Run Time : 17.63
INFO:root:2019-05-12 13:42:39, Epoch : 1, Step : 8735, Training Loss : 0.40588, Training Acc : 0.794, Run Time : 24.71
INFO:root:2019-05-12 13:42:40, Epoch : 1, Step : 8736, Training Loss : 1.02694, Training Acc : 0.533, Run Time : 1.67
INFO:root:2019-05-12 13:42:41, Epoch : 1, Step : 8737, Training Loss : 0.54820, Training Acc : 0.689, Run Time : 0.63
INFO:root:2019-05-12 13:42:56, Epoch : 1, Step : 8738, Training Loss : 0.45791, Training Acc : 0.806, Run Time : 15.54
INFO:root:2019-05-12 13:42:58, Epoch : 1, Step : 8739, Training Loss : 0.37768, Training Acc : 0.800, Run Time : 1.65
INFO:root:2019-05-12 13:42:59, Epoch : 1, Step : 8740, Training Loss : 0.50667, Training Acc : 0.717, Run Time : 0.65
INFO:root:2019-05-12 13:43:02, Epoch : 1, Step : 8741, Training Loss : 0.63926, Training Acc : 0.717, Run Time : 3.35
INFO:root:2019-05-12 13:43:20, Epoch : 1, Step : 8742, Training Loss : 0.38818, Training Acc : 0.839, Run Time : 18.28
INFO:root:2019-05-12 13:43:21, Epoch : 1, Step : 8743, Training Loss : 0.66043, Training Acc : 0.650, Run Time : 0.99
INFO:root:2019-05-12 13:43:36, Epoch : 1, Step : 8744, Training Loss : 0.71629, Training Acc : 0.650, Run Time : 15.17
INFO:root:2019-05-12 13:43:37, Epoch : 1, Step : 8745, Training Loss : 0.56408, Training Acc : 0.733, Run Time : 1.02
INFO:root:2019-05-12 13:43:55, Epoch : 1, Step : 8746, Training Loss : 0.46684, Training Acc : 0.767, Run Time : 17.16
INFO:root:2019-05-12 13:44:05, Epoch : 1, Step : 8747, Training Loss : 0.53441, Training Acc : 0.722, Run Time : 10.58
INFO:root:2019-05-12 13:44:06, Epoch : 1, Step : 8748, Training Loss : 0.53711, Training Acc : 0.733, Run Time : 0.81
INFO:root:2019-05-12 13:44:07, Epoch : 1, Step : 8749, Training Loss : 0.63872, Training Acc : 0.711, Run Time : 0.75
INFO:root:2019-05-12 13:44:07, Epoch : 1, Step : 8750, Training Loss : 0.47222, Training Acc : 0.811, Run Time : 0.63
INFO:root:2019-05-12 13:44:10, Epoch : 1, Step : 8751, Training Loss : 0.62357, Training Acc : 0.678, Run Time : 2.11
INFO:root:2019-05-12 13:44:34, Epoch : 1, Step : 8752, Training Loss : 0.64178, Training Acc : 0.756, Run Time : 24.16
INFO:root:2019-05-12 13:44:42, Epoch : 1, Step : 8753, Training Loss : 0.40492, Training Acc : 0.794, Run Time : 8.45
INFO:root:2019-05-12 13:44:43, Epoch : 1, Step : 8754, Training Loss : 0.68371, Training Acc : 0.678, Run Time : 1.03
INFO:root:2019-05-12 13:44:58, Epoch : 1, Step : 8755, Training Loss : 0.69164, Training Acc : 0.661, Run Time : 14.83
INFO:root:2019-05-12 13:44:59, Epoch : 1, Step : 8756, Training Loss : 0.33878, Training Acc : 0.839, Run Time : 0.97
INFO:root:2019-05-12 13:45:00, Epoch : 1, Step : 8757, Training Loss : 0.54373, Training Acc : 0.661, Run Time : 0.60
INFO:root:2019-05-12 13:45:16, Epoch : 1, Step : 8758, Training Loss : 0.46518, Training Acc : 0.778, Run Time : 16.11
INFO:root:2019-05-12 13:45:19, Epoch : 1, Step : 8759, Training Loss : 0.45966, Training Acc : 0.783, Run Time : 3.09
INFO:root:2019-05-12 13:45:20, Epoch : 1, Step : 8760, Training Loss : 0.40274, Training Acc : 0.817, Run Time : 1.06
INFO:root:2019-05-12 13:45:41, Epoch : 1, Step : 8761, Training Loss : 0.29765, Training Acc : 0.933, Run Time : 21.51
INFO:root:2019-05-12 13:46:00, Epoch : 1, Step : 8762, Training Loss : 0.33425, Training Acc : 0.917, Run Time : 19.13
INFO:root:2019-05-12 13:46:25, Epoch : 1, Step : 8763, Training Loss : 0.26248, Training Acc : 0.922, Run Time : 24.15
INFO:root:2019-05-12 13:46:27, Epoch : 1, Step : 8764, Training Loss : 0.49581, Training Acc : 0.750, Run Time : 2.28
INFO:root:2019-05-12 13:47:11, Epoch : 1, Step : 8765, Training Loss : 0.35420, Training Acc : 0.889, Run Time : 44.57
INFO:root:2019-05-12 13:47:29, Epoch : 1, Step : 8766, Training Loss : 0.24905, Training Acc : 0.939, Run Time : 17.58
INFO:root:2019-05-12 13:47:30, Epoch : 1, Step : 8767, Training Loss : 0.39274, Training Acc : 0.839, Run Time : 0.93
INFO:root:2019-05-12 13:47:30, Epoch : 1, Step : 8768, Training Loss : 0.24626, Training Acc : 0.911, Run Time : 0.54
INFO:root:2019-05-12 13:47:47, Epoch : 1, Step : 8769, Training Loss : 0.36180, Training Acc : 0.844, Run Time : 16.44
INFO:root:2019-05-12 13:47:48, Epoch : 1, Step : 8770, Training Loss : 0.34141, Training Acc : 0.889, Run Time : 0.90
INFO:root:2019-05-12 13:47:49, Epoch : 1, Step : 8771, Training Loss : 0.31201, Training Acc : 0.861, Run Time : 1.57
INFO:root:2019-05-12 13:48:18, Epoch : 1, Step : 8772, Training Loss : 0.40598, Training Acc : 0.828, Run Time : 28.58
INFO:root:2019-05-12 13:48:25, Epoch : 1, Step : 8773, Training Loss : 0.30179, Training Acc : 0.889, Run Time : 7.46
INFO:root:2019-05-12 13:48:34, Epoch : 1, Step : 8774, Training Loss : 0.20914, Training Acc : 0.944, Run Time : 8.33
INFO:root:2019-05-12 13:48:41, Epoch : 1, Step : 8775, Training Loss : 0.25309, Training Acc : 0.933, Run Time : 6.96
INFO:root:2019-05-12 13:48:41, Epoch : 1, Step : 8776, Training Loss : 0.31074, Training Acc : 0.883, Run Time : 0.63
INFO:root:2019-05-12 13:48:42, Epoch : 1, Step : 8777, Training Loss : 0.45731, Training Acc : 0.783, Run Time : 1.04
INFO:root:2019-05-12 13:49:01, Epoch : 1, Step : 8778, Training Loss : 0.76885, Training Acc : 0.572, Run Time : 18.97
INFO:root:2019-05-12 13:49:03, Epoch : 1, Step : 8779, Training Loss : 0.53926, Training Acc : 0.722, Run Time : 1.39
INFO:root:2019-05-12 13:49:03, Epoch : 1, Step : 8780, Training Loss : 0.90033, Training Acc : 0.450, Run Time : 0.64
INFO:root:2019-05-12 13:49:19, Epoch : 1, Step : 8781, Training Loss : 0.52862, Training Acc : 0.728, Run Time : 15.20
INFO:root:2019-05-12 13:49:20, Epoch : 1, Step : 8782, Training Loss : 0.40824, Training Acc : 0.806, Run Time : 0.92
INFO:root:2019-05-12 13:49:20, Epoch : 1, Step : 8783, Training Loss : 0.36205, Training Acc : 0.856, Run Time : 0.41
INFO:root:2019-05-12 13:49:43, Epoch : 1, Step : 8784, Training Loss : 0.45132, Training Acc : 0.800, Run Time : 22.66
INFO:root:2019-05-12 13:49:43, Epoch : 1, Step : 8785, Training Loss : 0.65291, Training Acc : 0.683, Run Time : 0.70
INFO:root:2019-05-12 13:49:44, Epoch : 1, Step : 8786, Training Loss : 0.31391, Training Acc : 0.878, Run Time : 0.59
INFO:root:2019-05-12 13:50:15, Epoch : 1, Step : 8787, Training Loss : 0.51356, Training Acc : 0.750, Run Time : 31.22
INFO:root:2019-05-12 13:50:17, Epoch : 1, Step : 8788, Training Loss : 0.32398, Training Acc : 0.906, Run Time : 2.29
INFO:root:2019-05-12 13:50:19, Epoch : 1, Step : 8789, Training Loss : 0.45896, Training Acc : 0.794, Run Time : 1.52
INFO:root:2019-05-12 13:50:33, Epoch : 1, Step : 8790, Training Loss : 0.34542, Training Acc : 0.883, Run Time : 13.68
INFO:root:2019-05-12 13:50:34, Epoch : 1, Step : 8791, Training Loss : 0.41504, Training Acc : 0.806, Run Time : 1.44
INFO:root:2019-05-12 13:50:35, Epoch : 1, Step : 8792, Training Loss : 0.32787, Training Acc : 0.878, Run Time : 0.69
INFO:root:2019-05-12 13:50:46, Epoch : 1, Step : 8793, Training Loss : 0.21387, Training Acc : 0.911, Run Time : 10.82
INFO:root:2019-05-12 13:50:46, Epoch : 1, Step : 8794, Training Loss : 0.48388, Training Acc : 0.806, Run Time : 0.46
INFO:root:2019-05-12 13:50:47, Epoch : 1, Step : 8795, Training Loss : 0.21451, Training Acc : 0.928, Run Time : 1.02
INFO:root:2019-05-12 13:51:03, Epoch : 1, Step : 8796, Training Loss : 0.32374, Training Acc : 0.861, Run Time : 16.33
INFO:root:2019-05-12 13:51:06, Epoch : 1, Step : 8797, Training Loss : 0.33777, Training Acc : 0.856, Run Time : 2.18
INFO:root:2019-05-12 13:51:07, Epoch : 1, Step : 8798, Training Loss : 0.26078, Training Acc : 0.883, Run Time : 1.07
INFO:root:2019-05-12 13:51:19, Epoch : 1, Step : 8799, Training Loss : 0.25765, Training Acc : 0.900, Run Time : 12.84
INFO:root:2019-05-12 13:51:21, Epoch : 1, Step : 8800, Training Loss : 0.68879, Training Acc : 0.594, Run Time : 1.38
INFO:root:2019-05-12 13:51:58, Epoch : 1, Step : 8801, Training Loss : 0.83446, Training Acc : 0.672, Run Time : 37.52
INFO:root:2019-05-12 13:53:03, Epoch : 1, Step : 8802, Training Loss : 0.77288, Training Acc : 0.678, Run Time : 64.92
INFO:root:2019-05-12 13:53:05, Epoch : 1, Step : 8803, Training Loss : 0.64834, Training Acc : 0.772, Run Time : 1.43
INFO:root:2019-05-12 13:53:05, Epoch : 1, Step : 8804, Training Loss : 0.53960, Training Acc : 0.767, Run Time : 0.76
INFO:root:2019-05-12 13:53:06, Epoch : 1, Step : 8805, Training Loss : 0.43258, Training Acc : 0.822, Run Time : 1.02
INFO:root:2019-05-12 13:53:23, Epoch : 1, Step : 8806, Training Loss : 0.29575, Training Acc : 0.878, Run Time : 16.28
INFO:root:2019-05-12 13:53:24, Epoch : 1, Step : 8807, Training Loss : 0.33540, Training Acc : 0.839, Run Time : 1.19
INFO:root:2019-05-12 13:53:25, Epoch : 1, Step : 8808, Training Loss : 0.32470, Training Acc : 0.833, Run Time : 0.62
INFO:root:2019-05-12 13:53:25, Epoch : 1, Step : 8809, Training Loss : 0.35686, Training Acc : 0.817, Run Time : 0.61
INFO:root:2019-05-12 13:53:40, Epoch : 1, Step : 8810, Training Loss : 0.33475, Training Acc : 0.839, Run Time : 14.38
INFO:root:2019-05-12 13:53:40, Epoch : 1, Step : 8811, Training Loss : 0.34938, Training Acc : 0.800, Run Time : 0.77
INFO:root:2019-05-12 13:53:41, Epoch : 1, Step : 8812, Training Loss : 0.28022, Training Acc : 0.856, Run Time : 0.61
INFO:root:2019-05-12 13:53:47, Epoch : 1, Step : 8813, Training Loss : 0.33618, Training Acc : 0.811, Run Time : 5.69
INFO:root:2019-05-12 13:53:55, Epoch : 1, Step : 8814, Training Loss : 0.23476, Training Acc : 0.894, Run Time : 8.44
INFO:root:2019-05-12 13:53:56, Epoch : 1, Step : 8815, Training Loss : 0.45931, Training Acc : 0.767, Run Time : 1.29
INFO:root:2019-05-12 13:54:07, Epoch : 1, Step : 8816, Training Loss : 0.39277, Training Acc : 0.767, Run Time : 10.33
INFO:root:2019-05-12 13:54:08, Epoch : 1, Step : 8817, Training Loss : 0.31939, Training Acc : 0.800, Run Time : 1.01
INFO:root:2019-05-12 13:54:29, Epoch : 1, Step : 8818, Training Loss : 0.21592, Training Acc : 0.911, Run Time : 21.70
INFO:root:2019-05-12 13:54:39, Epoch : 1, Step : 8819, Training Loss : 0.17478, Training Acc : 0.933, Run Time : 9.58
INFO:root:2019-05-12 13:55:17, Epoch : 1, Step : 8820, Training Loss : 0.17201, Training Acc : 0.950, Run Time : 38.03
INFO:root:2019-05-12 13:55:19, Epoch : 1, Step : 8821, Training Loss : 0.16635, Training Acc : 0.956, Run Time : 1.73
INFO:root:2019-05-12 13:55:20, Epoch : 1, Step : 8822, Training Loss : 0.21310, Training Acc : 0.911, Run Time : 0.99
INFO:root:2019-05-12 13:55:45, Epoch : 1, Step : 8823, Training Loss : 0.12807, Training Acc : 0.967, Run Time : 25.60
INFO:root:2019-05-12 13:56:00, Epoch : 1, Step : 8824, Training Loss : 0.19355, Training Acc : 0.917, Run Time : 14.94
INFO:root:2019-05-12 13:56:01, Epoch : 1, Step : 8825, Training Loss : 0.20709, Training Acc : 0.928, Run Time : 0.55
INFO:root:2019-05-12 13:56:01, Epoch : 1, Step : 8826, Training Loss : 0.09449, Training Acc : 0.983, Run Time : 0.65
INFO:root:2019-05-12 13:56:17, Epoch : 1, Step : 8827, Training Loss : 0.19590, Training Acc : 0.906, Run Time : 15.78
INFO:root:2019-05-12 13:56:18, Epoch : 1, Step : 8828, Training Loss : 0.17390, Training Acc : 0.950, Run Time : 1.13
INFO:root:2019-05-12 13:56:19, Epoch : 1, Step : 8829, Training Loss : 0.15090, Training Acc : 0.939, Run Time : 0.84
INFO:root:2019-05-12 13:56:41, Epoch : 1, Step : 8830, Training Loss : 0.15371, Training Acc : 0.956, Run Time : 21.66
INFO:root:2019-05-12 13:56:47, Epoch : 1, Step : 8831, Training Loss : 0.10626, Training Acc : 0.983, Run Time : 5.94
INFO:root:2019-05-12 13:56:47, Epoch : 1, Step : 8832, Training Loss : 0.07368, Training Acc : 1.000, Run Time : 0.52
INFO:root:2019-05-12 13:57:07, Epoch : 1, Step : 8833, Training Loss : 0.26971, Training Acc : 0.883, Run Time : 19.25
INFO:root:2019-05-12 13:57:08, Epoch : 1, Step : 8834, Training Loss : 0.16184, Training Acc : 0.956, Run Time : 1.03
INFO:root:2019-05-12 13:57:14, Epoch : 1, Step : 8835, Training Loss : 0.14540, Training Acc : 0.956, Run Time : 6.08
INFO:root:2019-05-12 13:57:25, Epoch : 1, Step : 8836, Training Loss : 0.24085, Training Acc : 0.894, Run Time : 11.38
INFO:root:2019-05-12 13:57:25, Epoch : 1, Step : 8837, Training Loss : 0.30570, Training Acc : 0.867, Run Time : 0.40
INFO:root:2019-05-12 13:58:09, Epoch : 1, Step : 8838, Training Loss : 0.28141, Training Acc : 0.867, Run Time : 43.41
INFO:root:2019-05-12 13:58:29, Epoch : 1, Step : 8839, Training Loss : 0.30444, Training Acc : 0.894, Run Time : 20.15
INFO:root:2019-05-12 13:58:31, Epoch : 1, Step : 8840, Training Loss : 0.10910, Training Acc : 0.978, Run Time : 1.64
INFO:root:2019-05-12 13:58:43, Epoch : 1, Step : 8841, Training Loss : 0.20448, Training Acc : 0.917, Run Time : 12.66
INFO:root:2019-05-12 13:59:04, Epoch : 1, Step : 8842, Training Loss : 0.23989, Training Acc : 0.911, Run Time : 20.76
INFO:root:2019-05-12 13:59:16, Epoch : 1, Step : 8843, Training Loss : 0.12461, Training Acc : 0.956, Run Time : 11.66
INFO:root:2019-05-12 13:59:35, Epoch : 1, Step : 8844, Training Loss : 0.06540, Training Acc : 0.983, Run Time : 18.86
INFO:root:2019-05-12 13:59:36, Epoch : 1, Step : 8845, Training Loss : 0.10281, Training Acc : 0.972, Run Time : 1.75
INFO:root:2019-05-12 13:59:51, Epoch : 1, Step : 8846, Training Loss : 0.07467, Training Acc : 0.994, Run Time : 14.73
INFO:root:2019-05-12 13:59:53, Epoch : 1, Step : 8847, Training Loss : 0.12422, Training Acc : 0.961, Run Time : 1.78
INFO:root:2019-05-12 14:00:09, Epoch : 1, Step : 8848, Training Loss : 0.25461, Training Acc : 0.917, Run Time : 15.93
INFO:root:2019-05-12 14:00:10, Epoch : 1, Step : 8849, Training Loss : 0.17374, Training Acc : 0.950, Run Time : 1.45
INFO:root:2019-05-12 14:00:11, Epoch : 1, Step : 8850, Training Loss : 0.17410, Training Acc : 0.917, Run Time : 0.41
INFO:root:2019-05-12 14:00:12, Epoch : 1, Step : 8851, Training Loss : 0.17838, Training Acc : 0.933, Run Time : 0.92
INFO:root:2019-05-12 14:00:13, Epoch : 1, Step : 8852, Training Loss : 0.16953, Training Acc : 0.944, Run Time : 1.67
INFO:root:2019-05-12 14:00:33, Epoch : 1, Step : 8853, Training Loss : 0.16360, Training Acc : 0.928, Run Time : 19.99
INFO:root:2019-05-12 14:00:34, Epoch : 1, Step : 8854, Training Loss : 0.18017, Training Acc : 0.928, Run Time : 0.84
INFO:root:2019-05-12 14:01:03, Epoch : 1, Step : 8855, Training Loss : 0.37262, Training Acc : 0.878, Run Time : 28.45
INFO:root:2019-05-12 14:01:06, Epoch : 1, Step : 8856, Training Loss : 1.04869, Training Acc : 0.628, Run Time : 3.90
INFO:root:2019-05-12 14:01:07, Epoch : 1, Step : 8857, Training Loss : 0.73999, Training Acc : 0.644, Run Time : 0.92
INFO:root:2019-05-12 14:01:26, Epoch : 1, Step : 8858, Training Loss : 1.03178, Training Acc : 0.628, Run Time : 18.90
INFO:root:2019-05-12 14:01:34, Epoch : 1, Step : 8859, Training Loss : 0.83679, Training Acc : 0.656, Run Time : 8.22
INFO:root:2019-05-12 14:01:36, Epoch : 1, Step : 8860, Training Loss : 0.71067, Training Acc : 0.744, Run Time : 1.23
INFO:root:2019-05-12 14:01:52, Epoch : 1, Step : 8861, Training Loss : 0.16479, Training Acc : 0.939, Run Time : 16.05
INFO:root:2019-05-12 14:01:53, Epoch : 1, Step : 8862, Training Loss : 0.31370, Training Acc : 0.872, Run Time : 1.70
INFO:root:2019-05-12 14:02:09, Epoch : 1, Step : 8863, Training Loss : 0.18078, Training Acc : 0.933, Run Time : 15.47
INFO:root:2019-05-12 14:02:10, Epoch : 1, Step : 8864, Training Loss : 0.30231, Training Acc : 0.872, Run Time : 1.34
INFO:root:2019-05-12 14:02:11, Epoch : 1, Step : 8865, Training Loss : 0.37438, Training Acc : 0.839, Run Time : 0.61
INFO:root:2019-05-12 14:02:24, Epoch : 1, Step : 8866, Training Loss : 0.35868, Training Acc : 0.822, Run Time : 13.03
INFO:root:2019-05-12 14:02:24, Epoch : 1, Step : 8867, Training Loss : 0.25957, Training Acc : 0.878, Run Time : 0.43
INFO:root:2019-05-12 14:02:26, Epoch : 1, Step : 8868, Training Loss : 0.31481, Training Acc : 0.833, Run Time : 2.11
INFO:root:2019-05-12 14:02:40, Epoch : 1, Step : 8869, Training Loss : 0.25908, Training Acc : 0.861, Run Time : 13.08
INFO:root:2019-05-12 14:02:41, Epoch : 1, Step : 8870, Training Loss : 0.15932, Training Acc : 0.933, Run Time : 1.11
INFO:root:2019-05-12 14:02:41, Epoch : 1, Step : 8871, Training Loss : 0.20975, Training Acc : 0.917, Run Time : 0.62
INFO:root:2019-05-12 14:02:42, Epoch : 1, Step : 8872, Training Loss : 0.26210, Training Acc : 0.889, Run Time : 0.70
INFO:root:2019-05-12 14:02:59, Epoch : 1, Step : 8873, Training Loss : 0.18829, Training Acc : 0.950, Run Time : 17.34
INFO:root:2019-05-12 14:03:00, Epoch : 1, Step : 8874, Training Loss : 0.20818, Training Acc : 0.922, Run Time : 0.62
INFO:root:2019-05-12 14:03:01, Epoch : 1, Step : 8875, Training Loss : 0.18734, Training Acc : 0.933, Run Time : 0.91
INFO:root:2019-05-12 14:03:28, Epoch : 1, Step : 8876, Training Loss : 0.21119, Training Acc : 0.911, Run Time : 27.55
INFO:root:2019-05-12 14:03:31, Epoch : 1, Step : 8877, Training Loss : 0.19593, Training Acc : 0.917, Run Time : 2.25
INFO:root:2019-05-12 14:03:32, Epoch : 1, Step : 8878, Training Loss : 0.19393, Training Acc : 0.939, Run Time : 1.59
INFO:root:2019-05-12 14:03:47, Epoch : 1, Step : 8879, Training Loss : 0.13297, Training Acc : 0.961, Run Time : 14.69
INFO:root:2019-05-12 14:03:47, Epoch : 1, Step : 8880, Training Loss : 0.15813, Training Acc : 0.961, Run Time : 0.45
INFO:root:2019-05-12 14:03:51, Epoch : 1, Step : 8881, Training Loss : 0.19194, Training Acc : 0.922, Run Time : 3.87
INFO:root:2019-05-12 14:04:13, Epoch : 1, Step : 8882, Training Loss : 0.18574, Training Acc : 0.933, Run Time : 21.68
INFO:root:2019-05-12 14:04:15, Epoch : 1, Step : 8883, Training Loss : 0.19354, Training Acc : 0.911, Run Time : 2.47
INFO:root:2019-05-12 14:04:16, Epoch : 1, Step : 8884, Training Loss : 0.16811, Training Acc : 0.961, Run Time : 0.61
INFO:root:2019-05-12 14:04:30, Epoch : 1, Step : 8885, Training Loss : 0.15036, Training Acc : 0.928, Run Time : 14.50
INFO:root:2019-05-12 14:04:32, Epoch : 1, Step : 8886, Training Loss : 0.12986, Training Acc : 0.967, Run Time : 1.75
INFO:root:2019-05-12 14:04:53, Epoch : 1, Step : 8887, Training Loss : 0.13466, Training Acc : 0.967, Run Time : 20.74
INFO:root:2019-05-12 14:04:55, Epoch : 1, Step : 8888, Training Loss : 0.11197, Training Acc : 0.978, Run Time : 1.81
INFO:root:2019-05-12 14:04:55, Epoch : 1, Step : 8889, Training Loss : 0.11155, Training Acc : 0.961, Run Time : 0.61
INFO:root:2019-05-12 14:04:56, Epoch : 1, Step : 8890, Training Loss : 0.12277, Training Acc : 0.961, Run Time : 0.76
INFO:root:2019-05-12 14:05:14, Epoch : 1, Step : 8891, Training Loss : 0.17111, Training Acc : 0.950, Run Time : 17.72
INFO:root:2019-05-12 14:05:26, Epoch : 1, Step : 8892, Training Loss : 0.21337, Training Acc : 0.939, Run Time : 11.81
INFO:root:2019-05-12 14:05:37, Epoch : 1, Step : 8893, Training Loss : 0.15444, Training Acc : 0.961, Run Time : 10.82
INFO:root:2019-05-12 14:05:37, Epoch : 1, Step : 8894, Training Loss : 0.51243, Training Acc : 0.811, Run Time : 0.43
INFO:root:2019-05-12 14:05:39, Epoch : 1, Step : 8895, Training Loss : 0.28730, Training Acc : 0.894, Run Time : 2.08
INFO:root:2019-05-12 14:05:51, Epoch : 1, Step : 8896, Training Loss : 0.25998, Training Acc : 0.900, Run Time : 11.96
INFO:root:2019-05-12 14:05:51, Epoch : 1, Step : 8897, Training Loss : 0.23734, Training Acc : 0.922, Run Time : 0.41
INFO:root:2019-05-12 14:05:52, Epoch : 1, Step : 8898, Training Loss : 0.37051, Training Acc : 0.867, Run Time : 0.60
INFO:root:2019-05-12 14:05:53, Epoch : 1, Step : 8899, Training Loss : 0.20684, Training Acc : 0.933, Run Time : 1.04
INFO:root:2019-05-12 14:06:05, Epoch : 1, Step : 8900, Training Loss : 0.12394, Training Acc : 0.972, Run Time : 12.20
INFO:root:2019-05-12 14:06:08, Epoch : 1, Step : 8901, Training Loss : 0.10749, Training Acc : 0.972, Run Time : 2.82
INFO:root:2019-05-12 14:06:19, Epoch : 1, Step : 8902, Training Loss : 0.07771, Training Acc : 0.983, Run Time : 11.27
INFO:root:2019-05-12 14:06:20, Epoch : 1, Step : 8903, Training Loss : 0.09125, Training Acc : 0.983, Run Time : 0.80
INFO:root:2019-05-12 14:06:21, Epoch : 1, Step : 8904, Training Loss : 0.11310, Training Acc : 0.972, Run Time : 0.63
INFO:root:2019-05-12 14:06:22, Epoch : 1, Step : 8905, Training Loss : 0.12641, Training Acc : 0.956, Run Time : 1.27
INFO:root:2019-05-12 14:06:34, Epoch : 1, Step : 8906, Training Loss : 0.12378, Training Acc : 0.972, Run Time : 12.47
INFO:root:2019-05-12 14:06:35, Epoch : 1, Step : 8907, Training Loss : 0.09466, Training Acc : 0.972, Run Time : 0.77
INFO:root:2019-05-12 14:06:36, Epoch : 1, Step : 8908, Training Loss : 0.07291, Training Acc : 0.994, Run Time : 0.53
INFO:root:2019-05-12 14:06:55, Epoch : 1, Step : 8909, Training Loss : 0.05967, Training Acc : 0.994, Run Time : 19.62
INFO:root:2019-05-12 14:07:09, Epoch : 1, Step : 8910, Training Loss : 0.12488, Training Acc : 0.956, Run Time : 13.58
INFO:root:2019-05-12 14:07:10, Epoch : 1, Step : 8911, Training Loss : 0.06900, Training Acc : 0.989, Run Time : 1.29
INFO:root:2019-05-12 14:07:11, Epoch : 1, Step : 8912, Training Loss : 0.15694, Training Acc : 0.933, Run Time : 0.90
INFO:root:2019-05-12 14:07:23, Epoch : 1, Step : 8913, Training Loss : 0.10199, Training Acc : 0.967, Run Time : 12.05
INFO:root:2019-05-12 14:07:25, Epoch : 1, Step : 8914, Training Loss : 0.11619, Training Acc : 0.967, Run Time : 1.94
INFO:root:2019-05-12 14:07:55, Epoch : 1, Step : 8915, Training Loss : 0.42506, Training Acc : 0.456, Run Time : 30.08
wsi/bin/train.py:117: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  data_tumor = Variable(data_tumor.cuda(async=True), volatile=True)
wsi/bin/train.py:121: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  data_normal = Variable(data_normal.cuda(async=True), volatile=True)
Traceback (most recent call last):
  File "wsi/bin/train.py", line 259, in <module>
    main()
  File "wsi/bin/train.py", line 255, in main
    run(args)
  File "wsi/bin/train.py", line 224, in run
    dataloader_normal_valid)
  File "wsi/bin/train.py", line 116, in valid_epoch
    data_tumor, target_tumor = next(dataiter_tumor)
  File "/scratch/sg5591/pyenv/py3.6.3/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 572, in __next__
    raise StopIteration
StopIteration
